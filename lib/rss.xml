<?xml version="1.0" encoding="UTF-8"?><rss xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" version="2.0"><channel><title><![CDATA[University]]></title><description><![CDATA[Obsidian digital garden]]></description><link>http://github.com/dylang/node-rss</link><image><url>lib/media/favicon.png</url><title>University</title><link></link></image><generator>Webpage HTML Export plugin for Obsidian</generator><lastBuildDate>Thu, 24 Apr 2025 14:16:28 GMT</lastBuildDate><atom:link href="lib/rss.xml" rel="self" type="application/rss+xml"/><pubDate>Thu, 24 Apr 2025 14:09:37 GMT</pubDate><ttl>60</ttl><dc:creator></dc:creator><item><title><![CDATA[- C Programming Language -]]></title><description><![CDATA[ 
 ]]></description><link>life,-universe-and-everything/c-and-c++/-c-programming-language-.html</link><guid isPermaLink="false">Life, Universe and Everything/C and C++/- C Programming Language -.md</guid><pubDate>Wed, 15 Jan 2025 11:35:37 GMT</pubDate></item><item><title><![CDATA[- C++ Programming Language -]]></title><description><![CDATA[ 
 ]]></description><link>life,-universe-and-everything/c-and-c++/-c++-programming-language-.html</link><guid isPermaLink="false">Life, Universe and Everything/C and C++/- C++ Programming Language -.md</guid><pubDate>Wed, 15 Jan 2025 11:35:42 GMT</pubDate></item><item><title><![CDATA[Calling a Kernel]]></title><description><![CDATA[ 
 <br>Extension of C programming language, defining and launching <a data-tooltip-position="top" aria-label="CUDA" data-href="CUDA" href="the-guide/computer-science/parallel-computing/cuda.html" class="internal-link" target="_self" rel="noopener nofollow">kernel</a>.<br>
<br>Initialization

<br>No explicit init, first time a runtime function is called

<br>Creates CUDA context




<br>Allocate Memory on Device

<br><img alt="center" src="lib/media/pasted-image-20230206164341.png" style="width: 500px; max-width: 100%;">


<br>Copy between Host and Device

<br><img alt="center" src="lib/media/pasted-image-20230206164414.png"> <img alt="center" src="lib/media/pasted-image-20230206164433.png">


<br>Free Memory on Device

<br><img alt="center" src="lib/media/pasted-image-20230206164509.png">


<br>Error Handling

<br><img alt="center" src="lib/media/pasted-image-20230206164555.png">


<br>Function Declaration Extensions

<br><img alt="center" src="lib/media/pasted-image-20230206170253.png">


<br>Predefined Variables

<br>For thread identification<img alt="center" src="lib/media/pasted-image-20230206170324.png">


<br>CUDA Qualifiers

<br><img alt="center" src="lib/media/pasted-image-20230206171340.png">


<br><br>
<br>Threads are organized in two-level hirarchy of<br>
- Grid = 3D array of blocks<br>
- Block = 3D array of thredas<br>
<img alt="Pasted image 20230206164810.png" src="lib/media/pasted-image-20230206164810.png">
]]></description><link>life,-universe-and-everything/c-and-c++/cuda-programming-interface.html</link><guid isPermaLink="false">Life, Universe and Everything/C and C++/CUDA Programming Interface.md</guid><pubDate>Sun, 26 Jan 2025 21:17:42 GMT</pubDate><enclosure url="lib/media/pasted-image-20230206164341.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src=&quot;lib/media/pasted-image-20230206164341.png&quot;&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[- Git -]]></title><description><![CDATA[ 
 <br><br>$ apt-get install git-core
<br><br><br>To initialize a git repo by creating a .git file, cd into the <a data-tooltip-position="top" aria-label="Linux File and Directory Management" data-href="Linux File and Directory Management" href="life,-universe-and-everything/linux-and-ubuntu/linux-file-and-directory-management.html" class="internal-link" target="_self" rel="noopener nofollow">directory</a> and use <br>git init
<br>Once you have a remote repo setup, you will need to add a remote repo url to your local git config, and set an upstream branch for your local branches. The git remote command offers such utility.<br>git remote add &lt;remote_name&gt; &lt;remote_repo_url&gt;
<br>This command will map remote repository at ＜remote_repo_url＞ to a ref in your local repo under ＜remote_name＞. Once you have mapped the remote repo you can push local branches to it.<br><br><br>Modify some files, then add their updated contents to the index:<br>git add file1 file2 file3   # add specified changes
git commit -a               # automatically add all modifications
<br>You are now ready to commit. You can see what is about to be committed using&nbsp;<a data-tooltip-position="top" aria-label="http://www.kernel.org/pub/software/scm/git/docs/git-diff.html" rel="noopener nofollow" class="external-link" href="http://www.kernel.org/pub/software/scm/git/docs/git-diff.html" target="_blank">git diff</a>&nbsp;with the --cached option (Without --cached,&nbsp;<a data-tooltip-position="top" aria-label="http://www.kernel.org/pub/software/scm/git/docs/git-diff.html" rel="noopener nofollow" class="external-link" href="http://www.kernel.org/pub/software/scm/git/docs/git-diff.html" target="_blank">git diff</a>&nbsp;will show you any changes that you've made but not yet added to the index) or get a brief summary with git status.<br>git commit -m "message"
<br>This will again prompt you for a message describing the change, and then record a new version of the project.<br>Git tracks content not files
Many revision control systems provide an "add" command that tells the system to start tracking changes to a new file. Git's "add" command does something simpler and more powerful:&nbsp;git add&nbsp;is used both for new and newly modified files, and in both cases it takes a snapshot of the given files and stages that content in the index, ready for inclusion in the next commit.
<br><br><br>To communicate with remote repositories, Git allows to user either HTTPS or <a data-tooltip-position="top" aria-label="SSH - Secure Shell" data-href="SSH - Secure Shell" href="the-guide/computer-science/networks/ssh-secure-shell.html" class="internal-link" target="_self" rel="noopener nofollow">SSH</a>. <br>
<br>HTTPS Authentication
https://github.com/your-username/repository.git


<br>Requires username and password or <a data-href="Git Personal Access Tokens" href="life,-universe-and-everything/git/git-personal-access-tokens.html" class="internal-link" target="_self" rel="noopener nofollow">Git Personal Access Tokens</a> regularly
<br>Easier to set up, works behind corporate firewalls
<br>Less secure


<br><a data-tooltip-position="top" aria-label="SSH - Secure Shell" data-href="SSH - Secure Shell" href="the-guide/computer-science/networks/ssh-secure-shell.html" class="internal-link" target="_self" rel="noopener nofollow">SSH</a> Authentication
git@github.com:your-username/repository.git


<br>Uses public-private key instead of password
<br>Requires generating keys and adding the public key to git

<br>Display key via ``cat ~/.ssh/id_ed25519.pub (or other location)
<br>Copy this to your Github settings


<br>May be blocked by corporate networks


<br><br><br><br>
<br><a data-href="Git Branching and Merging" href="life,-universe-and-everything/git/git-branching-and-merging.html" class="internal-link" target="_self" rel="noopener nofollow">Git Branching and Merging</a> 
<br><a data-href="Git Stashing" href="life,-universe-and-everything/git/git-stashing.html" class="internal-link" target="_self" rel="noopener nofollow">Git Stashing</a> - Saving changes made on current branch to e.g. apply them again later
<br>...
<br><br>
<br><a data-href="Git Log" href="life,-universe-and-everything/git/git-log.html" class="internal-link" target="_self" rel="noopener nofollow">Git Log</a> - Reviewing commit / version history
<br><a data-href="Git Diff" href="life,-universe-and-everything/git/git-diff.html" class="internal-link" target="_self" rel="noopener nofollow">Git Diff</a> - Comparing commits / versions
<br>
<br><br><br>
<br><a data-href="Git Directory and Working Directory" href="life,-universe-and-everything/git/git-directory-and-working-directory.html" class="internal-link" target="_self" rel="noopener nofollow">Git Directory and Working Directory</a>
<br><a data-href="Git Object Model" href="life,-universe-and-everything/git/git-object-model.html" class="internal-link" target="_self" rel="noopener nofollow">Git Object Model</a>
<br><a data-href="The Git Index" href="life,-universe-and-everything/git/the-git-index.html" class="internal-link" target="_self" rel="noopener nofollow">The Git Index</a> (staging area)
<br>
]]></description><link>life,-universe-and-everything/git/-git-.html</link><guid isPermaLink="false">Life, Universe and Everything/Git/- Git -.md</guid><pubDate>Sat, 08 Feb 2025 18:37:39 GMT</pubDate></item><item><title><![CDATA[Git Branching and Merging]]></title><description><![CDATA[ 
 <br><br>A single git repository can maintain multiple branches of development. To create a new branch named "experimental", use<br>git branch experimental
<br>If you now run<br>$ git branch
<br>you'll get a list of all existing branches:<br>  experimental
* master
<br>The "experimental" branch is the one you just created, and the "master" branch is a default branch that was created for you automatically. The asterisk marks the branch you are currently on; type<br>git checkout experimental
<br>to switch to the experimental branch. Now edit a file, commit the change, and switch back to the master branch:<br>(edit file)
git commit -a
git checkout master
<br>Check that the change you made is no longer visible, since it was made on the experimental branch and you're back on the master branch.<br>You can make a different change on the master branch:<br>(edit file)
$ git commit -a
<br>at this point the two branches have diverged, with different changes made in each. To merge the changes made in experimental into master, run<br>$ git merge experimental
<br>If the changes don't conflict, you're done. If there are conflicts, markers will be left in the problematic files showing the conflict;<br>$ git diff
<br>will show this. Once you've edited the files to resolve the conflicts,<br>$ git commit -a
<br>will commit the result of the merge. Finally,<br>$ gitk
<br>will show a nice graphical representation of the resulting history.<br><br><br>If you develop on a branch crazy-idea, then regret it, you can always delete the branch with<br>git branch -D crazy-idea
<br>Branches are cheap and easy, so this is a good way to try something out.<br><br><br>You can rejoin two diverging branches of development using&nbsp;<a data-tooltip-position="top" aria-label="http://www.kernel.org/pub/software/scm/git/docs/git-merge.html" rel="noopener nofollow" class="external-link" href="http://www.kernel.org/pub/software/scm/git/docs/git-merge.html" target="_blank">git merge</a>:<br>git merge branchname
<br>merges the changes made in the branch "branchname" into the current branch. If there are conflicts--for example, if the same file is modified in two different ways in the remote branch and the local branch--then you are warned; the output may look something like this:<br>git merge next
# 100% (4/4) done
# Auto-merged file.txt
# CONFLICT (content): Merge conflict in file.txt
# Automatic merge failed; fix conflicts and then commit the result.
<br>Conflict markers are left in the problematic files, and after you resolve the conflicts manually, you can update the index with the contents and run git commit, as you normally would when modifying a file.<br>If you examine the resulting commit using gitk, you will see that it has two parents: one pointing to the top of the current branch, and one to the top of the other branch.<br><br><br>When a merge isn't resolved automatically, git leaves the index and the working tree in a special state that gives you all the information you need to help resolve the merge.<br>Files with conflicts are marked specially in the index, so until you resolve the problem and update the index,&nbsp;<a data-tooltip-position="top" aria-label="http://www.kernel.org/pub/software/scm/git/docs/git-commit.html" rel="noopener nofollow" class="external-link" href="http://www.kernel.org/pub/software/scm/git/docs/git-commit.html" target="_blank">git commit</a>&nbsp;will fail:<br>$ git commit
file.txt: needs merge
<br>Also,&nbsp;<a data-tooltip-position="top" aria-label="http://www.kernel.org/pub/software/scm/git/docs/git-status.html" rel="noopener nofollow" class="external-link" href="http://www.kernel.org/pub/software/scm/git/docs/git-status.html" target="_blank">git status</a>&nbsp;will list those files as "unmerged", and the files with conflicts will have conflict markers added, like this:<br>&lt;&lt;&lt;&lt;&lt;&lt;&lt; HEAD:file.txt
Hello world
=======
Goodbye
&gt;&gt;&gt;&gt;&gt;&gt;&gt; 77976da35a11db4580b80ae27e8d65caf5208086:file.txt
<br>All you need to do is edit the files to resolve the conflicts, and then<br>$ git add file.txt
$ git commit
<br>Note that the commit message will already be filled in for you with some information about the merge. Normally you can just use this default message unchanged, but you may add additional commentary of your own if desired.<br><br><br>The above is all you need to know to resolve a simple merge. But git also provides more information to help resolve conflicts.<br>If you get stuck and decide to just give up and throw the whole mess away, you can always return to the pre-merge state with<br>git reset --hard HEAD
<br>Or, if you've already committed the merge that you want to throw away,<br>git reset --hard ORIG_HEAD
<br>However, this last command can be dangerous in some cases--never throw away a commit if that commit may itself have been merged into another branch, as doing so may confuse further merges.<br><br><br>There is one special case not mentioned above, which is treated differently. Normally, a merge results in a merge commit with two parents, one for each of the two lines of development that were merged.<br>However, if the current branch has not diverged from the other--so every commit present in the current branch is already contained in the other--then git just performs a "fast forward"; the head of the current branch is moved forward to point at the head of the merged-in branch, without any new commits being created.]]></description><link>life,-universe-and-everything/git/git-branching-and-merging.html</link><guid isPermaLink="false">Life, Universe and Everything/Git/Git Branching and Merging.md</guid><pubDate>Thu, 13 Feb 2025 14:26:33 GMT</pubDate></item><item><title><![CDATA[Git Diff]]></title><description><![CDATA[ 
 <br>You can generate diffs between any two versions of your project using&nbsp;<a data-tooltip-position="top" aria-label="http://www.kernel.org/pub/software/scm/git/docs/git-diff.html" rel="noopener nofollow" class="external-link" href="http://www.kernel.org/pub/software/scm/git/docs/git-diff.html" target="_blank">git diff</a>:<br>$ git diff master..test
<br>That will produce the diff between the tips of the two branches. If you'd prefer to find the diff from their common ancestor to test, you can use three dots instead of two:<br>$ git diff master...test
<br><a data-tooltip-position="top" aria-label="http://www.kernel.org/pub/software/scm/git/docs/git-diff.html" rel="noopener nofollow" class="external-link" href="http://www.kernel.org/pub/software/scm/git/docs/git-diff.html" target="_blank">git diff</a>&nbsp;is an incredibly useful tool for figuring out what has changed between any two points in your project's history, or to see what people are trying to introduce in new branches, etc.<br><br>You will commonly use&nbsp;<a data-tooltip-position="top" aria-label="http://www.kernel.org/pub/software/scm/git/docs/git-diff.html" rel="noopener nofollow" class="external-link" href="http://www.kernel.org/pub/software/scm/git/docs/git-diff.html" target="_blank">git diff</a>&nbsp;for figuring out differences between your last commit, your index, and your current working directory. A common use is to simply run<br>$ git diff
<br>which will show you changes in the working directory that are not yet staged for the next commit. If you want to see what&nbsp;is&nbsp;staged for the next commit, you can run<br>$ git diff --cached
<br>which will show you the difference between the index and your last commit; what you would be committing if you run "git commit" without the "-a" option. Lastly, you can run<br>$ git diff HEAD
<br>which shows changes in the working directory since your last commit; what you would be committing if you run "git commit -a".<br><br>If you want to see how your current working directory differs from the state of the project in another branch, you can run something like<br>$ git diff test
<br>This will show you what is different between your current working directory and the snapshot on the 'test' branch. You can also limit the comparison to a specific file or subdirectory by adding a&nbsp;path limiter:<br>$ git diff HEAD -- ./lib 
<br>That command will show the changes between your current working directory and the last commit (or, more accurately, the tip of the current branch), limiting the comparison to files in the 'lib' subdirectory.<br>If you don't want to see the whole patch, you can add the '--stat' option, which will limit the output to the files that have changed along with a little text graph depicting how many lines changed in each file.<br>$&gt;git diff --stat
 layout/book_index_template.html                    |    8 ++-
 text/05_Installing_Git/0_Source.markdown           |   14 ++++++
 text/05_Installing_Git/1_Linux.markdown            |   17 +++++++
 text/05_Installing_Git/2_Mac_104.markdown          |   11 +++++
 text/05_Installing_Git/3_Mac_105.markdown          |    8 ++++
 text/05_Installing_Git/4_Windows.markdown          |    7 +++
 .../1_Getting_a_Git_Repo.markdown                  |    7 +++-
 .../0_ Comparing_Commits_Git_Diff.markdown         |   45 +++++++++++++++++++-
 .../0_ Hosting_Git_gitweb_repoorcz_github.markdown |    4 +-
 9 files changed, 115 insertions(+), 6 deletions(-)
<br>Sometimes that makes it easier to see overall what has changed, to jog your memory.]]></description><link>life,-universe-and-everything/git/git-diff.html</link><guid isPermaLink="false">Life, Universe and Everything/Git/Git Diff.md</guid><pubDate>Thu, 12 Dec 2024 09:37:10 GMT</pubDate></item><item><title><![CDATA[Git Directory and Working Directory]]></title><description><![CDATA[ 
 <br><br>The 'git directory' is the directory that stores all Git's history and meta information for your project - including all of the objects (commits, trees, blobs, tags), all of the pointers to where different branches are and more.<br>There is only one Git Directory per project (as opposed to one per subdirectory like with SVN or CVS), and that directory is (by default, though not necessarily) '.git' in the root of your project. If you look at the contents of that directory, you can see all of your important files:<br>$&gt;tree -L 1
.
|-- HEAD         # pointer to your current branch
|-- config       # your configuration preferences
|-- description  # description of your project 
|-- hooks/       # pre/post action hooks
|-- index        # index file (see next section)
|-- logs/        # a history of where your branches have been
|-- objects/     # your objects (commits, trees, blobs, tags)
`-- refs/        # pointers to your branches
<br>(there may be some other files/directories in there as well, but they are not important for now)<br><br><br>The Git 'working directory' is the directory that holds the current checkout of the files you are working on. Files in this directory are often removed or replaced by Git as you switch branches - this is normal. All your history is stored in the Git Directory; the working directory is simply a temporary checkout place where you can modify the files until your next commit.]]></description><link>life,-universe-and-everything/git/git-directory-and-working-directory.html</link><guid isPermaLink="false">Life, Universe and Everything/Git/Git Directory and Working Directory.md</guid><pubDate>Tue, 20 Aug 2024 12:15:37 GMT</pubDate></item><item><title><![CDATA[Git Log]]></title><description><![CDATA[ 
 ]]></description><link>life,-universe-and-everything/git/git-log.html</link><guid isPermaLink="false">Life, Universe and Everything/Git/Git Log.md</guid><pubDate>Thu, 12 Dec 2024 09:38:18 GMT</pubDate></item><item><title><![CDATA[Git Object Model]]></title><description><![CDATA[ 
 <br><br>All the information needed to represent the history of a project is stored in files referenced by a 40-digit "object name" that looks something like this:<br>6ff87c4664981e4397625791c8ea3bbb5f2279a3
<br>You will see these 40-character strings all over the place in Git. In each case the name is calculated by taking the SHA1 hash of the contents of the object. The SHA1 hash is a cryptographic hash function. What that means to us is that it is virtually impossible to find two different objects with the same name. This has a number of advantages; among others:<br>
<br>Git can quickly determine whether two objects are identical or not, just by comparing names.
<br>Since object names are computed the same way in every repository, the same content stored in two repositories will always be stored under the same name.
<br>Git can detect errors when it reads an object, by checking that the object's name is still the SHA1 hash of its contents.
<br><br>Every object consists of three things - a type, a size and content. The size is simply the size of the contents, the contents depend on what type of object it is, and there are four different types of objects: "blob", "tree", "commit", and "tag".<br>
<br>A "blob" is used to store file data - it is generally a file.
<br>A "tree" is basically like a directory - it references a bunch of other trees and/or blobs (i.e. files and sub-directories)
<br>A "commit" points to a single tree, marking it as what the project looked like at a certain point in time. It contains meta-information about that point in time, such as a timestamp, the author of the changes since the last commit, a pointer to the previous commit(s), etc.
<br>A "tag" is a way to mark a specific commit as special in some way. It is normally used to tag certain commits as specific releases or something along those lines.
<br>Almost all of Git is built around manipulating this simple structure of four different object types. It is sort of its own little filesystem that sits on top of your machine's filesystem.<br><br>It is important to note that this is very different from most SCM systems that you may be familiar with. Subversion, CVS, Perforce, Mercurial and the like all use Delta Storage systems - they store the differences between one commit and the next. Git does not do this - it stores a snapshot of what all the files in your project look like in this tree structure each time you commit. This is a very important concept to understand when using Git.<br><br><br>A blob generally stores the contents of a file.<br><img src="http://shafiul.github.io/gitbook/assets/images/figure/object-blob.png" referrerpolicy="no-referrer"><br>You can use <a data-tooltip-position="top" aria-label="http://www.kernel.org/pub/software/scm/git/docs/git-show.html" rel="noopener nofollow" class="external-link" href="http://www.kernel.org/pub/software/scm/git/docs/git-show.html" target="_blank">git show</a> to examine the contents of any blob. Assuming we have the SHA for a blob, we can examine its contents like this:<br>$ git show 6ff87c4664

 Note that the only valid version of the GPL as far as this project
 is concerned is _this_ particular version of the license (ie v2, not
 v2.2 or v3.x or whatever), unless explicitly otherwise stated.
...
<br>A "blob" object is nothing but a chunk of binary data. It doesn't refer to anything else or have attributes of any kind, not even a file name.<br>Since the blob is entirely defined by its data, if two files in a directory tree (or in multiple different versions of the repository) have the same contents, they will share the same blob object. The object is totally independent of its location in the directory tree, and renaming a file does not change the object that file is associated with.<br><br><br>A tree is a simple object that has a bunch of pointers to blobs and other trees - it generally represents the contents of a directory or subdirectory.<br><img src="http://shafiul.github.io/gitbook/assets/images/figure/object-tree.png" referrerpolicy="no-referrer"><br>The ever-versatile <a data-tooltip-position="top" aria-label="http://www.kernel.org/pub/software/scm/git/docs/git-show.html" rel="noopener nofollow" class="external-link" href="http://www.kernel.org/pub/software/scm/git/docs/git-show.html" target="_blank">git show</a> command can also be used to examine tree objects, but <a data-tooltip-position="top" aria-label="http://www.kernel.org/pub/software/scm/git/docs/git-ls-tree.html" rel="noopener nofollow" class="external-link" href="http://www.kernel.org/pub/software/scm/git/docs/git-ls-tree.html" target="_blank">git ls-tree</a> will give you more details. Assuming we have the SHA for a tree, we can examine it like this:<br>$ git ls-tree fb3a8bdd0ce
100644 blob 63c918c667fa005ff12ad89437f2fdc80926e21c    .gitignore
100644 blob 5529b198e8d14decbe4ad99db3f7fb632de0439d    .mailmap
100644 blob 6ff87c4664981e4397625791c8ea3bbb5f2279a3    COPYING
040000 tree 2fb783e477100ce076f6bf57e4a6f026013dc745    Documentation
100755 blob 3c0032cec592a765692234f1cba47dfdcc3a9200    GIT-VERSION-GEN
100644 blob 289b046a443c0647624607d471289b2c7dcd470b    INSTALL
100644 blob 4eb463797adc693dc168b926b6932ff53f17d0b1    Makefile
100644 blob 548142c327a6790ff8821d67c2ee1eff7a656b52    README
...
<br>As you can see, a tree object contains a list of entries, each with a mode, object type, SHA1 name, and name, sorted by name. It represents the contents of a single directory tree.<br>An object referenced by a tree may be blob, representing the contents of a file, or another tree, representing the contents of a subdirectory. Since trees and blobs, like all other objects, are named by the SHA1 hash of their contents, two trees have the same SHA1 name if and only if their contents (including, recursively, the contents of all subdirectories) are identical. This allows git to quickly determine the differences between two related tree objects, since it can ignore any entries with identical object names.<br>(Note: in the presence of submodules, trees may also have commits as entries. See the Submodules section.)<br>Note that the files all have mode 644 or 755: git actually only pays attention to the executable bit.<br><br><br>The "commit" object links a physical state of a tree with a description of how we got there and why.<br><img src="http://shafiul.github.io/gitbook/assets/images/figure/object-commit.png" referrerpolicy="no-referrer"><br>You can use the --pretty=raw option to <a data-tooltip-position="top" aria-label="http://www.kernel.org/pub/software/scm/git/docs/git-show.html" rel="noopener nofollow" class="external-link" href="http://www.kernel.org/pub/software/scm/git/docs/git-show.html" target="_blank">git show</a> or <a data-tooltip-position="top" aria-label="http://www.kernel.org/pub/software/scm/git/docs/git-log.html" rel="noopener nofollow" class="external-link" href="http://www.kernel.org/pub/software/scm/git/docs/git-log.html" target="_blank">git log</a> to examine your favorite commit:<br>$ git show -s --pretty=raw 2be7fcb476
commit 2be7fcb4764f2dbcee52635b91fedb1b3dcf7ab4
tree fb3a8bdd0ceddd019615af4d57a53f43d8cee2bf
parent 257a84d9d02e90447b149af58b271c19405edb6a
author Dave Watson &lt;dwatson@mimvista.com&gt; 1187576872 -0400
committer Junio C Hamano &lt;gitster@pobox.com&gt; 1187591163 -0700

    Fix misspelling of 'suppress' in docs

    Signed-off-by: Junio C Hamano &lt;gitster@pobox.com&gt;
<br>As you can see, a commit is defined by:<br>
<br>a tree: The SHA1 name of a tree object (as defined below), representing the contents of a directory at a certain point in time.
<br>parent(s): The SHA1 name of some number of commits which represent the immediately previous step(s) in the history of the project. The example above has one parent; merge commits may have more than one. A commit with no parents is called a "root" commit, and represents the initial revision of a project. Each project must have at least one root. A project can also have multiple roots, though that isn't common (or necessarily a good idea).
<br>an author: The name of the person responsible for this change, together with its date.
<br>a committer: The name of the person who actually created the commit, with the date it was done. This may be different from the author; for example, if the author wrote a patch and emailed it to another person who used the patch to create the commit.
<br>a comment describing this commit.
<br>Note that a commit does not itself contain any information about what actually changed; all changes are calculated by comparing the contents of the tree referred to by this commit with the trees associated with its parents. In particular, git does not attempt to record file renames explicitly, though it can identify cases where the existence of the same file data at changing paths suggests a rename. (See, for example, the -M option to <a data-tooltip-position="top" aria-label="http://www.kernel.org/pub/software/scm/git/docs/git-diff.html" rel="noopener nofollow" class="external-link" href="http://www.kernel.org/pub/software/scm/git/docs/git-diff.html" target="_blank">git diff</a>).<br>A commit is usually created by <a data-tooltip-position="top" aria-label="http://www.kernel.org/pub/software/scm/git/docs/git-commit.html" rel="noopener nofollow" class="external-link" href="http://www.kernel.org/pub/software/scm/git/docs/git-commit.html" target="_blank">git commit</a>, which creates a commit whose parent is normally the current HEAD, and whose tree is taken from the content currently stored in the index.<br><br><br>So, now that we've looked at the 3 main object types (blob, tree and commit), let's take a quick look at how they all fit together.<br>If we had a simple project with the following directory structure:<br>$&gt;tree
.
|-- README
`-- lib
    |-- inc
    |   `-- tricks.rb
    `-- mylib.rb

2 directories, 3 files
<br>And we committed this to a Git repository, it would be represented like this:<br><img src="http://shafiul.github.io/gitbook/assets/images/figure/objects-example.png" referrerpolicy="no-referrer"><br>You can see that we have created a tree object for each directory (including the root) and a blob object for each file. Then we have a commit object to point to the root, so we can track what our project looked like when it was committed.<br><br><br><img src="http://shafiul.github.io/gitbook/assets/images/figure/object-tag.png" referrerpolicy="no-referrer"><br>A tag object contains an object name (called simply 'object'), object type, tag name, the name of the person ("tagger") who created the tag, and a message, which may contain a signature, as can be seen using <a data-tooltip-position="top" aria-label="http://www.kernel.org/pub/software/scm/git/docs/git-cat-file.html" rel="noopener nofollow" class="external-link" href="http://www.kernel.org/pub/software/scm/git/docs/git-cat-file.html" target="_blank">git cat-file</a>:<br>$ git cat-file tag v1.5.0
object 437b1b20df4b356c9342dac8d38849f24ef44f27
type commit
tag v1.5.0
tagger Junio C Hamano &lt;junkio@cox.net&gt; 1171411200 +0000

GIT 1.5.0
-----BEGIN PGP SIGNATURE-----
Version: GnuPG v1.4.6 (GNU/Linux)

iD8DBQBF0lGqwMbZpPMRm5oRAuRiAJ9ohBLd7s2kqjkKlq1qqC57SbnmzQCdG4ui
nLE/L9aUXdWeTFPron96DLA=
=2E+0
-----END PGP SIGNATURE-----
<br>See the <a data-tooltip-position="top" aria-label="http://www.kernel.org/pub/software/scm/git/docs/git-tag.html" rel="noopener nofollow" class="external-link" href="http://www.kernel.org/pub/software/scm/git/docs/git-tag.html" target="_blank">git tag</a> command to learn how to create and verify tag objects. (Note that <a data-tooltip-position="top" aria-label="http://www.kernel.org/pub/software/scm/git/docs/git-tag.html" rel="noopener nofollow" class="external-link" href="http://www.kernel.org/pub/software/scm/git/docs/git-tag.html" target="_blank">git tag</a> can also be used to create "lightweight tags", which are not tag objects at all, but just simple references whose names begin with "refs/tags/").]]></description><link>life,-universe-and-everything/git/git-object-model.html</link><guid isPermaLink="false">Life, Universe and Everything/Git/Git Object Model.md</guid><pubDate>Wed, 09 Oct 2024 09:36:31 GMT</pubDate><enclosure url="http://shafiul.github.io/gitbook/assets/images/figure/object-blob.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src=&quot;http://shafiul.github.io/gitbook/assets/images/figure/object-blob.png&quot;&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[Git Personal Access Tokens]]></title><description><![CDATA[ 
 <br>In a Nutshell
A Personal Access Token (PAT) is a secure alternative to passwords for authenticating to <a data-tooltip-position="top" aria-label="- Git -" data-href="- Git -" href="life,-universe-and-everything/git/-git-.html" class="internal-link" target="_self" rel="noopener nofollow">Git</a> services like GitHub, GitLab, and Bitbucket. It provides scoped access to your repositories and can be used with the command line, APIs, and third-party tools instead of using a password.
<br><br>Why ?

<br>Security: Unlike passwords, tokens can have limited permissions (read-only, repo access, etc.), reducing risk.
<br>Required for HTTPS Authentication: GitHub no longer allows password authentication, so PATs are required for command-line Git operations.
<br>Revocable &amp; Expirable: You can revoke a token anytime without affecting your main account password.

<br><br><br><br>When pushing to a remote repository via HTTPS, you’ll use the token instead of your password:<br>git clone https://github.com/your-username/your-repo.git
<br>When prompted for a password, enter the Personal Access Token instead.<br>Alternatively, you can store it securely using a credential manager:<br>git config --global credential.helper cache --timeout=3600  
# Cache for 1 hour
<br><br>If you're making API requests, use the PAT in the Authorization header:<br>curl -H "Authorization: token YOUR_PERSONAL_ACCESS_TOKEN" \
     https://api.github.com/user
<br><br><br>
<br>
Go to GitHub → Click your profile picture → Select "Settings".

<br>
Scroll down and click "Developer Settings".

<br>
Select "Personal Access Tokens" → Click "Tokens (classic)" or "Fine-grained tokens".

<br>
Click "Generate new token".

<br>
Choose between:

<br>Classic Token (Broad access, recommended for general use)
<br>Fine-Grained Token (More granular permissions, recommended for security-conscious use)


<br>
Choose an expiration date to limit token lifetime.

<br>
Select scopes based on your needs:

<br>repo → Full repository access
<br>read:org → Read organization details
<br>workflow → Access GitHub Actions
<br>admin:repo_hook → Manage webhooks
<br>gist → Access gists


<br>
Click "Generate token".

<br>
Copy the token immediately (you won’t see it again).

<br>d_HXVsUTDnUGb-zm8dyb]]></description><link>life,-universe-and-everything/git/git-personal-access-tokens.html</link><guid isPermaLink="false">Life, Universe and Everything/Git/Git Personal Access Tokens.md</guid><pubDate>Thu, 13 Feb 2025 22:18:14 GMT</pubDate></item><item><title><![CDATA[Git Stashing]]></title><description><![CDATA[ 
 <br>While you are in the middle of working on something complicated, you find an unrelated but obvious and trivial bug. You would like to fix it before continuing. You can use&nbsp;<a data-tooltip-position="top" aria-label="http://www.kernel.org/pub/software/scm/git/docs/git-stash.html" rel="noopener nofollow" class="external-link" href="http://www.kernel.org/pub/software/scm/git/docs/git-stash.html" target="_blank">git stash</a>&nbsp;to save the current state of your work, and after fixing the bug (or, optionally after doing so on a different branch and then coming back), unstash the work-in-progress changes.<br>$ git stash save "work in progress for foo feature"
<br>This command will save your changes away to the&nbsp;stash, and reset your working tree and the index to match the tip of your current branch. Then you can make your fix as usual.<br>... edit and test ...
$ git commit -a -m "blorpl: typofix"
<br>After that, you can go back to what you were working on with&nbsp;git stash apply:<br>git stash apply
<br>To apply changes you previously stashed and remove them from the list of stashes, use<br>git stash pop
<br><br><br>You can also use stashing to queue up stashed changes.<br>
If you run 'git stash list' you can see which stashes you have saved:<br>$&gt;git stash list
stash@{0}: WIP on book: 51bea1d... fixed images
stash@{1}: WIP on master: 9705ae6... changed the browse code to the official repo
<br>Then you can apply them individually with <br>git stash apply stash@{1}
<br>You can clear out the list with <br>git stash clear
]]></description><link>life,-universe-and-everything/git/git-stashing.html</link><guid isPermaLink="false">Life, Universe and Everything/Git/Git Stashing.md</guid><pubDate>Tue, 17 Dec 2024 18:33:05 GMT</pubDate></item><item><title><![CDATA[Gitignore File]]></title><description><![CDATA[ 
 <br># Ignore Python bytecode files
*.pyc
__pycache__/

# ignore data
*.pkl
*.csv
*.csv.bak

  
# Ignore directories
data/...
<br>Remove from index (here for all pyc files) <br>git rm --cached $(git ls-files *.pyc)
]]></description><link>life,-universe-and-everything/git/gitignore-file.html</link><guid isPermaLink="false">Life, Universe and Everything/Git/Gitignore File.md</guid><pubDate>Mon, 24 Feb 2025 15:08:46 GMT</pubDate></item><item><title><![CDATA[The Git Index]]></title><description><![CDATA[ 
 <br>The <a data-tooltip-position="top" aria-label="- Git -" data-href="- Git -" href="life,-universe-and-everything/git/-git-.html" class="internal-link" target="_self" rel="noopener nofollow">Git</a> index is used as a staging area between your working directory and your repository. You can use the index to build up a set of changes that you want to commit together. When you create a commit, what is committed is what is currently in the index, not what is in your working directory.<br><br>The easiest way to see what is in the index is with the <a data-tooltip-position="top" aria-label="http://www.kernel.org/pub/software/scm/git/docs/git-status.html" rel="noopener nofollow" class="external-link" href="http://www.kernel.org/pub/software/scm/git/docs/git-status.html" target="_blank">git status</a> command. When you run git status, you can see which files are staged (currently in your index), which are modified but not yet staged, and which are completely untracked.<br>$&gt;git status
# On branch master
# Your branch is behind 'origin/master' by 11 commits, and can be fast-forwarded.
#
# Changes to be committed:
#   (use "git reset HEAD &lt;file&gt;..." to unstage)
#
#   modified:   daemon.c
#
# Changed but not updated:
#   (use "git add &lt;file&gt;..." to update what will be committed)
#
#   modified:   grep.c
#   modified:   grep.h
#
# Untracked files:
#   (use "git add &lt;file&gt;..." to include in what will be committed)
#
#   blametree
#   blametree-init
#   git-gui/git-citool
<br>If you blow the index away entirely, you generally haven't lost any information as long as you have the name of the tree that it described.]]></description><link>life,-universe-and-everything/git/the-git-index.html</link><guid isPermaLink="false">Life, Universe and Everything/Git/The Git Index.md</guid><pubDate>Thu, 13 Feb 2025 22:18:14 GMT</pubDate></item><item><title><![CDATA[- JAX -]]></title><description><![CDATA[ 
 <br><br>import jax
import jax.numpy as jnp

x = jnp.arange(5)
isinstance(x, jax.Array) # True
<br><br>JAX Array objects have a devices method that lets you inspect where the contents of the array are stored. In the simplest cases, this will be a single device:<br>x.devices()
<br>
<br>{CpuDevice(id=0)} = stored on CPU
<br>{cuda(id=0)} = stored on <a data-tooltip-position="top" aria-label="GPU - Graphics Processing Unit" data-href="GPU - Graphics Processing Unit" href="the-guide/computer-science/gpu-graphics-processing-unit.html" class="internal-link" target="_self" rel="noopener nofollow">GPU</a> 
<br>However, the contents of the array can also be sharded, essentially split over multiple devices. This can be inspected via <br>x.sharding
<br>
<br>SingleeviceSharding(device=...) is stored on single device
<br>...<br>
Sharded arrays can be split over multiple devices and/or multiple hosts, see <a data-href="Sharded Computation with JAX" href="life,-universe-and-everything/jax/sharded-computation-with-jax.html" class="internal-link" target="_self" rel="noopener nofollow">Sharded Computation with JAX</a> for more !
<br><br>Accept a function as an argument and perform a useful operation on it. This includes ...<br>
<br><a data-href="Vectorization with JAX" href="life,-universe-and-everything/jax/vectorization-with-jax.html" class="internal-link" target="_self" rel="noopener nofollow">Vectorization with JAX</a> via jax.vmap()
<br><a data-href="Just-in-Time Compilation with JAX" href="life,-universe-and-everything/jax/just-in-time-compilation-with-jax.html" class="internal-link" target="_self" rel="noopener nofollow">Just-in-Time Compilation with JAX</a> via jax.jit()
<br><a data-href="Automatic Differentiation with JAX" href="Automatic Differentiation with JAX" class="internal-link" target="_self" rel="noopener nofollow">Automatic Differentiation with JAX</a>
<br>...
<br><br>The functions above need to be able to optimize sequences of operations, e.g. for efficient parallelization. This is done based on Tracer objects, stand-ins passed to JAX functions to extract the sequence of operations. <br>@jax.jit
def f(x):
  print(x)
  return x + 1

x = jnp.arange(5)
result = f(x)
<br>Traced&lt;ShapedArray(int32[5])&gt;with&lt;DynamicJaxprTrace(level=1/0)&gt;
<br><br>Intermediate representation for sequences of primitive operations <br><a data-href="Jaxprs" href="life,-universe-and-everything/jax/jaxprs.html" class="internal-link" target="_self" rel="noopener nofollow">Jaxprs</a><br><br>Abstraction used in JAX to handle various kinds of structures.<br>
...<br>We can also operate on whole pytrees, e.g through<br>
<br>...
<br><br><br>
<br><a rel="noopener nofollow" class="external-link" href="https://huggingface.co/blog/afmck/jax-tutorial" target="_blank">https://huggingface.co/blog/afmck/jax-tutorial</a>
<br><a rel="noopener nofollow" class="external-link" href="https://kidger.site/thoughts/torch2jax/" target="_blank">https://kidger.site/thoughts/torch2jax/</a>
]]></description><link>life,-universe-and-everything/jax/-jax-.html</link><guid isPermaLink="false">Life, Universe and Everything/JAX/- JAX -.md</guid><pubDate>Fri, 20 Dec 2024 11:46:35 GMT</pubDate></item><item><title><![CDATA[Arrays in JAX]]></title><description><![CDATA[ 
 <br>Collection of useful array manipulation in JAX. All listed here are specific to <a data-tooltip-position="top" aria-label="- JAX -" data-href="- JAX -" href="life,-universe-and-everything/jax/-jax-.html" class="internal-link" target="_self" rel="noopener nofollow">JAX</a>, for operations shared with Numpy go to <a data-href="How to Array Slicing and Indexing" href="life,-universe-and-everything/how-to-array-slicing-and-indexing.html" class="internal-link" target="_self" rel="noopener nofollow">How to Array Slicing and Indexing</a> note.<br><br><br>Because it makes program analysis and transformation optimization more difficult, it is not possible to make in-place updates of array elements. Instead, <a data-tooltip-position="top" aria-label="- JAX -" data-href="- JAX -" href="life,-universe-and-everything/jax/-jax-.html" class="internal-link" target="_self" rel="noopener nofollow">JAX</a> uses a functional update via <br>jax_array = jnp.zeros((3,3), dtype=jnp.float32)

# In place update of JAX's array will yield an error!
jax_array[1, :] = 1.0

# instead use this 
updated_array = jax_array.at[1, :].set(1.0)
<br>updated array:
 [[0. 0. 0.]
 [1. 1. 1.]
 [0. 0. 0.]]
<br><br>All expressions above return a modified copy. However, inside a <a data-tooltip-position="top" aria-label="Just-in-Time Compilation with JAX" data-href="Just-in-Time Compilation with JAX" href="life,-universe-and-everything/jax/just-in-time-compilation-with-jax.html" class="internal-link" target="_self" rel="noopener nofollow">jit</a>-compiled function these expressions are optimized.]]></description><link>life,-universe-and-everything/jax/arrays-in-jax.html</link><guid isPermaLink="false">Life, Universe and Everything/JAX/Arrays in JAX.md</guid><pubDate>Thu, 10 Apr 2025 16:28:50 GMT</pubDate></item><item><title><![CDATA[JAX Pseudorandom Numbers]]></title><description><![CDATA[ 
 <br>Contrary to NumPy, JAX uses an explicit random state, the hidden state of the pseudorandom number generator is passed explicitly to jax.random().  <br>from jax import random

key = random.key(42)
print(key)
<br><br>
<br>Apart from predefined loops or steps, it can be dangerous to feed back a key and split inside the function, better create array of keys in outer loop
]]></description><link>life,-universe-and-everything/jax/jax-pseudorandom-numbers.html</link><guid isPermaLink="false">Life, Universe and Everything/JAX/JAX Pseudorandom Numbers.md</guid><pubDate>Fri, 20 Dec 2024 11:46:35 GMT</pubDate></item><item><title><![CDATA[Jaxprs]]></title><description><![CDATA[ 
 ]]></description><link>life,-universe-and-everything/jax/jaxprs.html</link><guid isPermaLink="false">Life, Universe and Everything/JAX/Jaxprs.md</guid><pubDate>Fri, 20 Dec 2024 11:46:36 GMT</pubDate></item><item><title><![CDATA[Just-in-Time Compilation with JAX]]></title><description><![CDATA[ 
 <br>Tool in <a data-tooltip-position="top" aria-label="- JAX -" data-href="- JAX -" href="life,-universe-and-everything/jax/-jax-.html" class="internal-link" target="_self" rel="noopener nofollow">JAX</a> that compiles python functions into op]]></description><link>life,-universe-and-everything/jax/just-in-time-compilation-with-jax.html</link><guid isPermaLink="false">Life, Universe and Everything/JAX/Just-in-Time Compilation with JAX.md</guid><pubDate>Thu, 13 Feb 2025 22:16:21 GMT</pubDate></item><item><title><![CDATA[Sharded Computation with JAX]]></title><description><![CDATA[ 
 ]]></description><link>life,-universe-and-everything/jax/sharded-computation-with-jax.html</link><guid isPermaLink="false">Life, Universe and Everything/JAX/Sharded Computation with JAX.md</guid><pubDate>Fri, 20 Dec 2024 11:46:36 GMT</pubDate></item><item><title><![CDATA[Vectorization with JAX]]></title><description><![CDATA[ 
 <br>We can vectorize the application of a function over some axes of the inputs using the  jax.vmap function of <a data-tooltip-position="top" aria-label="- JAX -" data-href="- JAX -" href="life,-universe-and-everything/jax/-jax-.html" class="internal-link" target="_self" rel="noopener nofollow">JAX</a> .<br>
Essentially, it lets us apply a function that works on a single example to a whole batch of examples at once, automatically handling broadcasting and looping over the batch dimension.<br><br><br>In most applications, we use arrays of dimensions (batch-size, example.shape()) to store data.  <br>jax.vmap(_fun_, _in_axes=0_, _out_axes=0_, _axis_name=None_, _axis_size=None_, _spmd_axis_name=None_)
<br>
<br>_fun_ is the function over which the batching is applied, should work on a single example. e.g. a vector
<br>_in_axis specifies over which axis to perform the batching, can be a tuple for multiple inputs

<br>0 vectorizes over the first axis, in most contexts this is the batch dimension
<br>None = leave this input constant 


<br>_out_axis specifies along which axes to stack the output

<br>0 will give the batch dimension as the first output dim


<br><br><br>def h(x, y):
    return x * y

X = jnp.array([1, 2, 3])
Y = jnp.array(10)

# Vectorize over X, keep Y constant
vmap_h = jax.vmap(h, in_axes=(0, None))  
result = vmap_h(X, Y)  # Returns [10, 20, 30]

<br>
<br>Vectorize over multiple inputs
<br>def kernel(vec1, vec2):
	return ...

vec_temp = jax.vmap(kernel, in_axes=(0, None)) # outputs vector

vec_kernel = jax.vmap(vec_temp, in_axes=(None, 0)) # outputs matrix
]]></description><link>life,-universe-and-everything/jax/vectorization-with-jax.html</link><guid isPermaLink="false">Life, Universe and Everything/JAX/Vectorization with JAX.md</guid><pubDate>Thu, 13 Feb 2025 22:16:21 GMT</pubDate></item><item><title><![CDATA[Working with PyTrees]]></title><description><![CDATA[ 
 <br><br><br>Funtionality of partial function evaluation via functools.partial can be replicated in a way that is compatible with <a data-tooltip-position="top" aria-label="- JAX -" data-href="- JAX -" href="life,-universe-and-everything/jax/-jax-.html" class="internal-link" target="_self" rel="noopener nofollow">JAX transformations</a>. <br>import jax.numpy as jnp
add_one = Partial(jnp.add, 1)
add_one(2)
<br>Array(3, dtype=int32, weak_type=True)
<br>]]></description><link>life,-universe-and-everything/jax/working-with-pytrees.html</link><guid isPermaLink="false">Life, Universe and Everything/JAX/Working with PyTrees.md</guid><pubDate>Thu, 13 Feb 2025 22:16:21 GMT</pubDate></item><item><title><![CDATA[- LaTeX -]]></title><description><![CDATA[ 
 <br>Software system for typesetting documents.<br>
<br>Basic software is bundled into distributions, for <a data-tooltip-position="top" aria-label="Linux File and Directory Management" data-href="Linux File and Directory Management" href="life,-universe-and-everything/linux-and-ubuntu/linux-file-and-directory-management.html" class="internal-link" target="_self" rel="noopener nofollow">Linux</a> the most commonly used one is <a data-href="TeX Live" href="life,-universe-and-everything/latex/tex-live.html" class="internal-link" target="_self" rel="noopener nofollow">TeX Live</a>.
<br>Packages are distributed through the Comprehensive TeX Archive Network (CTAN)

<br>... a set of Internet sites around the world that offer TEX-related material for download.


<br>
<br><br><br><br><br><br><br>% math packages
\usepackage{amsmath}
\usepackage{amssymb}
]]></description><link>life,-universe-and-everything/latex/-latex-.html</link><guid isPermaLink="false">Life, Universe and Everything/LaTeX/- LaTeX -.md</guid><pubDate>Fri, 20 Dec 2024 15:33:25 GMT</pubDate></item><item><title><![CDATA[Bibliography Management with BibLaTeX]]></title><description><![CDATA[ 
 <br>
<br><a rel="noopener nofollow" class="external-link" href="https://www.overleaf.com/learn/latex/Bibliography_management_in_LaTeX" target="_blank">https://www.overleaf.com/learn/latex/Bibliography_management_in_LaTeX</a>
]]></description><link>life,-universe-and-everything/latex/bibliography-management-with-biblatex.html</link><guid isPermaLink="false">Life, Universe and Everything/LaTeX/Bibliography Management with BibLaTeX.md</guid><pubDate>Sun, 25 Aug 2024 09:46:18 GMT</pubDate></item><item><title><![CDATA[LaTeX Common Issues]]></title><description><![CDATA[ 
 <br><br><br>Prevent <a data-tooltip-position="top" aria-label="LaTeX Figures" data-href="LaTeX Figures" href="life,-universe-and-everything/latex/latex-figures.html" class="internal-link" target="_self" rel="noopener nofollow">figures</a>, tables etc to be placed past a certain point.<br>\usepackage{placeins}
\FloatBarrier
<br><br><br>Use <br>\noindent
<br>to avoid weird indent after Floatbarriers or figures.]]></description><link>life,-universe-and-everything/latex/latex-common-issues.html</link><guid isPermaLink="false">Life, Universe and Everything/LaTeX/LaTeX Common Issues.md</guid><pubDate>Fri, 20 Dec 2024 11:30:12 GMT</pubDate></item><item><title><![CDATA[LaTeX Figures]]></title><description><![CDATA[ 
 <br>Collection of figure-related packages and commands for <a data-tooltip-position="top" aria-label="- LaTeX -" data-href="- LaTeX -" href="life,-universe-and-everything/latex/-latex-.html" class="internal-link" target="_self" rel="noopener nofollow">LaTeX</a>.<br>
<br><a rel="noopener nofollow" class="external-link" href="https://en.wikibooks.org/wiki/LaTeX/Floats,_Figures_and_Captions" target="_blank">https://en.wikibooks.org/wiki/LaTeX/Floats,_Figures_and_Captions</a>
<br><br><br>Requires the subfigure package<br>\usepackage{subfigure}
<br>\begin{figure}
    \centering
    \begin{subfigure}
        \centering
        % Answer: [trim={left bottom right top},clip]
        \includegraphics[trim={2cm 4cm 2cm 4cm}, clip, width=0.8\textwidth]{path1}
        \caption{caption1}
        \label{label1}
    \end{subfigure}
    \begin{subfigure}
        \centering
        % Answer: [trim={left bottom right top},clip]
        \includegraphics[trim={2cm 4cm 2cm 4cm}, clip, width=0.8\textwidth]{path2}
        \caption{caption2}
        \label{label2}
    \end{subfigure}
\end{figure}    
<br>
<br>Trim the image using the graphicx package and by activating cropping
]]></description><link>life,-universe-and-everything/latex/latex-figures.html</link><guid isPermaLink="false">Life, Universe and Everything/LaTeX/LaTeX Figures.md</guid><pubDate>Thu, 20 Feb 2025 09:22:10 GMT</pubDate></item><item><title><![CDATA[LaTeX Weird Symbols]]></title><description><![CDATA[ 
 <br>In a Nutshell
Overview of weird symbols that are not straight-forward to find via Detexify or ChatGPT.
<br><br>]]></description><link>life,-universe-and-everything/latex/latex-weird-symbols.html</link><guid isPermaLink="false">Life, Universe and Everything/LaTeX/LaTeX Weird Symbols.md</guid><pubDate>Mon, 03 Mar 2025 22:10:02 GMT</pubDate></item><item><title><![CDATA[TeX Live]]></title><description><![CDATA[ 
 <br>Most commonly used <a data-tooltip-position="top" aria-label="- LaTeX -" data-href="- LaTeX -" href="life,-universe-and-everything/latex/-latex-.html" class="internal-link" target="_self" rel="noopener nofollow">LaTeX</a> distribution on <a data-tooltip-position="top" aria-label="Linux File and Directory Management" data-href="Linux File and Directory Management" href="life,-universe-and-everything/linux-and-ubuntu/linux-file-and-directory-management.html" class="internal-link" target="_self" rel="noopener nofollow">Linux</a>.]]></description><link>life,-universe-and-everything/latex/tex-live.html</link><guid isPermaLink="false">Life, Universe and Everything/LaTeX/TeX Live.md</guid><pubDate>Thu, 20 Feb 2025 09:22:10 GMT</pubDate></item><item><title><![CDATA[TikZ]]></title><description><![CDATA[ 
 <br>In a Nutshell
<a data-tooltip-position="top" aria-label="- LaTeX -" data-href="- LaTeX -" href="life,-universe-and-everything/latex/-latex-.html" class="internal-link" target="_self" rel="noopener nofollow">LaTeX</a> package to create advanced graphics.
<br>
<br>Great examples available at their website.
<br>Online tool <a rel="noopener nofollow" class="external-link" href="https://tikzmaker.com/editor" target="_blank">https://tikzmaker.com/editor</a>
<br>Matcha <a rel="noopener nofollow" class="external-link" href="https://www.mathcha.io/" target="_blank">https://www.mathcha.io/</a> allows TikZ export
]]></description><link>life,-universe-and-everything/latex/tikz.html</link><guid isPermaLink="false">Life, Universe and Everything/LaTeX/TikZ.md</guid><pubDate>Mon, 03 Mar 2025 22:12:39 GMT</pubDate></item><item><title><![CDATA[- Linux -]]></title><description><![CDATA[ 
 <br>In a Nutshell
Linux is an open-source, Unix-like operating system kernel first created by Linus Torvalds in 1991. The term "Linux" often refers to both the kernel and the operating system that is built around it. Linux is known for being highly customizable, stable, and secure, making it popular for use in servers, desktops, mobile devices, embedded systems, and more.
<br><br><br>
<br>Kernel: The core of the operating system, which handles hardware interactions (e.g., CPU, memory, input/output devices). It manages resources, handles system calls, and ensures that different software applications can run concurrently.
<br><a data-tooltip-position="top" aria-label="Linux Basic Shell Commands" data-href="Linux Basic Shell Commands" href="life,-universe-and-everything/linux-and-ubuntu/linux-basic-shell-commands.html" class="internal-link" target="_self" rel="noopener nofollow">|Shell</a>: A command-line interface (CLI) that allows users to interact with the kernel and other programs by typing commands. Common shells include Bash (Bourne Again Shell), Zsh, and Fish.
<br><a data-tooltip-position="top" aria-label="Linux File and Directory Management" data-href="Linux File and Directory Management" href="life,-universe-and-everything/linux-and-ubuntu/linux-file-and-directory-management.html" class="internal-link" target="_self" rel="noopener nofollow">|File System</a>: Linux uses a hierarchical file system, where all files and directories are part of a single tree, starting at the root directory (/). Popular file systems used in Linux include ext4, XFS, and Btrfs.
<br>Distributions (Distros): While the Linux kernel itself is free and open source, different versions of Linux, called distributions or distros, bundle the kernel with different sets of software, tools, and package management systems. Examples include Ubuntu, Debian, Fedora, CentOS, and Arch Linux.
<br>Package Management: Linux uses package managers (such as apt, yum, dnf, or pacman) to install, update, and remove software. 
<br>Graphical User Interface (GUI): Although Linux can be used entirely from the command line, many Linux distributions come with a GUI to make it more user-friendly. Desktop environments like GNOME, KDE Plasma, and XFCE provide graphical interfaces for interacting with the system.
<br>Benefits of Linux

<br>Open Source: The source code is available for anyone to view, modify, and distribute, leading to a large and active development community.
<br>Security: Linux is considered more secure than many other operating systems due to its robust permissions system and the fact that it is less targeted by malware.
<br>Stability: Linux is known for its reliability and is widely used in servers and critical systems.
<br>Customizability: Because Linux is open-source, users can customize almost every aspect of the operating system to suit their needs.

]]></description><link>life,-universe-and-everything/linux-and-ubuntu/-linux-.html</link><guid isPermaLink="false">Life, Universe and Everything/Linux and Ubuntu/- Linux -.md</guid><pubDate>Fri, 20 Dec 2024 15:33:25 GMT</pubDate></item><item><title><![CDATA[APT - Advanced Package Tool]]></title><description><![CDATA[ 
 <br>In a Nutshell
Package management system used in Debian-based Linux distributions (such as Ubuntu, Linux Mint, and others).
<br><br><br>
<br>apt update: Updates the list of available packages and their versions from the repositories.
<br>apt upgrade: Upgrades all the installed packages to the latest available versions.
<br>apt install &lt;package&gt;: Installs a specified package and its dependencies.
<br>apt remove &lt;package&gt;: Removes an installed package but leaves its configuration files intact.
<br>apt purge &lt;package&gt;: Removes an installed package along with its configuration files.
<br>apt autoremove: Removes unnecessary packages, typically those that were installed as dependencies for packages that are no longer needed.
<br>apt search &lt;package&gt;: Searches for a package in the repositories.
<br>apt show &lt;package&gt;: Displays detailed information about a package.
<br>apt list: Lists installed or available packages, including information about upgradable packages.
<br><br><br>
<br><a data-tooltip-position="top" aria-label="Ubuntu Repositories" data-href="Ubuntu Repositories" href="life,-universe-and-everything/linux-and-ubuntu/ubuntu-repositories.html" class="internal-link" target="_self" rel="noopener nofollow">Repositories</a>: apt interacts with remote repositories (configured in /etc/apt/sources.list or /etc/apt/sources.list.d/) that contain software packages.
<br>Packages: Software in <a data-tooltip-position="top" aria-label="- Linux -" data-href="- Linux -" href="life,-universe-and-everything/linux-and-ubuntu/-linux-.html" class="internal-link" target="_self" rel="noopener nofollow">- Linux -</a> is distributed as packages, which are compressed archives containing all the files necessary for a program to run. These packages also include metadata that defines dependencies and configuration options.
<br>Dependency Management: One of apt's most powerful features is automatic dependency resolution. If you install a package that requires other software, apt will automatically download and install those dependencies.
]]></description><link>life,-universe-and-everything/linux-and-ubuntu/apt-advanced-package-tool.html</link><guid isPermaLink="false">Life, Universe and Everything/Linux and Ubuntu/APT - Advanced Package Tool.md</guid><pubDate>Thu, 13 Feb 2025 22:18:22 GMT</pubDate></item><item><title><![CDATA[Bash and Zsh Shell]]></title><description><![CDATA[ 
 <br>Bash and Zsh are both shells—command-line interpreters that allow users to interact with their operating system through a text-based interface. They are programs that interpret and execute commands entered by the user in a terminal or shell prompt.<br>
<br>Bash is simple, reliable, and POSIX-compliant.
<br>Zsh is more feature-rich and customizable with an advanced user experience
<br>]]></description><link>life,-universe-and-everything/linux-and-ubuntu/bash-and-zsh-shell.html</link><guid isPermaLink="false">Life, Universe and Everything/Linux and Ubuntu/Bash and Zsh Shell.md</guid><pubDate>Wed, 23 Apr 2025 22:01:49 GMT</pubDate></item><item><title><![CDATA[GNU GRUB - Grand Unified Bootloader]]></title><description><![CDATA[ 
 <br>In a Nutshell
Bootloader package that is commonly used on Unix-like operating systems, including <a data-tooltip-position="top" aria-label="- Linux -" data-href="- Linux -" href="life,-universe-and-everything/linux-and-ubuntu/-linux-.html" class="internal-link" target="_self" rel="noopener nofollow">- Linux -</a> distributions. Its primary function is to load the operating system into memory during the boot process and hand control over to it.
<br><br><img alt="center" src="lib/media/pasted-image-20241220091608.png"><br>You can use<br>grup&gt; ls &lt;path&gt;
<br>to inspect available options. Filenames are usually in the format (name).<br>grub&gt; set root=&lt;...&gt;       
grub&gt; linux /boot/vmlinuz-&lt;...&gt;-generic root=/dev/&lt;...&gt;   
grub&gt; initrd /boot/initrd.img-&lt;...&gt;-generic  # Load initramfs
grub&gt; boot                      # Boot the system
]]></description><link>life,-universe-and-everything/linux-and-ubuntu/gnu-grub-grand-unified-bootloader.html</link><guid isPermaLink="false">Life, Universe and Everything/Linux and Ubuntu/GNU GRUB - Grand Unified Bootloader.md</guid><pubDate>Thu, 13 Feb 2025 22:18:22 GMT</pubDate><enclosure url="lib/media/pasted-image-20241220091608.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src=&quot;lib/media/pasted-image-20241220091608.png&quot;&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[Linux Basic Shell Commands]]></title><description><![CDATA[ 
 <br>In a Nutshell
Collection of commands to use in <a data-tooltip-position="top" aria-label="- Linux -" data-href="- Linux -" href="life,-universe-and-everything/linux-and-ubuntu/-linux-.html" class="internal-link" target="_self" rel="noopener nofollow">Unix-like system shells</a>, such as Bash (Bourne Again SHell) or Zsh (Oh-My-Zsh or MacOS).
<br><br><br>
<br><a data-href="Linux File and Directory Management" href="life,-universe-and-everything/linux-and-ubuntu/linux-file-and-directory-management.html" class="internal-link" target="_self" rel="noopener nofollow">Linux File and Directory Management</a>
<br><br>
<br>nano: Simple text editor
<br>vi or vim: Powerful text editor
<br>grep: pattern matching
<br>awk: advanced text processing
<br><br>
<br><a data-href="Linux System Information and Management" href="life,-universe-and-everything/linux-and-ubuntu/linux-system-information-and-management.html" class="internal-link" target="_self" rel="noopener nofollow">Linux System Information and Management</a>
<br><br>
<br><a data-href="Linux Network Management" href="life,-universe-and-everything/linux-and-ubuntu/linux-network-management.html" class="internal-link" target="_self" rel="noopener nofollow">Linux Network Management</a>
<br><br>
<br><a data-href="APT - Advanced Package Tool" href="life,-universe-and-everything/linux-and-ubuntu/apt-advanced-package-tool.html" class="internal-link" target="_self" rel="noopener nofollow">APT - Advanced Package Tool</a>
<br><br>
<br>source execute a script within the current shell
<br><br>
<br>echo is used to print a string
<br><br><br>auto-choose best driver for hardware<br>sudo ubuntu-drivers install
<br>check available drivers<br>sudo ubuntu-drivers list
<br>check current driver<br>cat /proc/driver/nvidia/version
]]></description><link>life,-universe-and-everything/linux-and-ubuntu/linux-basic-shell-commands.html</link><guid isPermaLink="false">Life, Universe and Everything/Linux and Ubuntu/Linux Basic Shell Commands.md</guid><pubDate>Thu, 13 Feb 2025 22:18:22 GMT</pubDate></item><item><title><![CDATA[Linux Environment Variables]]></title><description><![CDATA[ 
 <br>In a Nutshell
Environment variables are key-value pairs used by the operating system to store system-wide or session-specific settings. The following is for <a data-tooltip-position="top" aria-label="- Linux -" data-href="- Linux -" href="life,-universe-and-everything/linux-and-ubuntu/-linux-.html" class="internal-link" target="_self" rel="noopener nofollow">- Linux -</a>, but they also exist on MacOS and Windows.
<br>For example:<br>
<br>HOME: Points to your home directory (e.g., /Users/yourname or /home/yourname).
<br>PATH: A list of directories the shell searches for executables.
<br>USER: The current username.
<br>PYTHONPATH: Tells <a data-tooltip-position="top" aria-label="- Python Programming Language -" data-href="- Python Programming Language -" href="life,-universe-and-everything/python/-python-programming-language-.html" class="internal-link" target="_self" rel="noopener nofollow">Python where to look for modules and packages</a>.
<br><br><br>To access the value stored in a variable, use the $, e.g. <br>echo $HOME # reads out environment variable and prints it
<br>To define a new variable from the terminal, use<br>MY_VAR="Hello, World!"
<br>or define it in the <a data-tooltip-position="top" aria-label="The bashrc and bash_profile Scripts" data-href="The bashrc and bash_profile Scripts" href="life,-universe-and-everything/linux-and-ubuntu/the-bashrc-and-bash_profile-scripts.html" class="internal-link" target="_self" rel="noopener nofollow">.bash_profile or .bashrc</a> script.]]></description><link>life,-universe-and-everything/linux-and-ubuntu/linux-environment-variables.html</link><guid isPermaLink="false">Life, Universe and Everything/Linux and Ubuntu/Linux Environment Variables.md</guid><pubDate>Thu, 10 Apr 2025 16:27:49 GMT</pubDate></item><item><title><![CDATA[Linux File and Directory Management]]></title><description><![CDATA[ 
 <br><br><br>The <a data-href="- Linux -" href="life,-universe-and-everything/linux-and-ubuntu/-linux-.html" class="internal-link" target="_self" rel="noopener nofollow">- Linux -</a> file system is organized as a single rooted tree, starting from the root directory (/). This root directory contains various sub-directories, each of which may contain further sub-directories and files. Key directories include:<br>/
├── bin          # Essential binaries for all users (e.g., `ls`, `cp`).
├── boot         # Files for booting, e.g. bootloader config
├── dev          # Device files, e.g. for hard drives
├── etc          # System-wide configs and runtime scripts
├── home
│   └── username
├── lib          # shared libraries and kernel modules
├── lib64 
├── media
├── mnt          # Mount point for temp. mounted file systems
├── opt
├── proc         # Process and system information
├── root         # Home directory for the root user.
├── run
├── sbin         # System binaries for admin (e.g., `fsck`, `ifconfig`).
├── srv
├── sys
├── tmp          # temporary files, usually deleted at reboot
├── usr          # Secondary hierarchy for user applications and files. 
│   ├── bin
│   ├── lib
│   ├── lib64
│   ├── local
│   ├── sbin
│   ├── share
│   └── src
└── var           # Variable data, e.g. logs
    ├── cache
    ├── lib
    ├── log
    ├── mail
    ├── opt
    ├── run
    ├── spool
    └── tmp
<br><br><br><br>The ls command is used to list files and directories in a given directory.<br><br><br><br>The cd command is used to change the current working directory.<br><br><br><br>The pwd command prints the full path to the current working directory.<br><br><br><br>The mkdir command is used to create new directories.<br><br><br><br>The rm command is used to remove files or directories.<br><br><br><br>The cp command is used to copy files and directories.<br><br><br><br>The mv command is used to move or rename files and directories.<br><br><br><br>The touch command is used to create an empty file or update the modification time of an existing file.<br><br><br><br>The chmod command is used to change the permissions of a file or directory.<br><br><br><br>The which command is used to locate a command's executable file in the system's PATH.<br>]]></description><link>life,-universe-and-everything/linux-and-ubuntu/linux-file-and-directory-management.html</link><guid isPermaLink="false">Life, Universe and Everything/Linux and Ubuntu/Linux File and Directory Management.md</guid><pubDate>Thu, 13 Feb 2025 22:18:22 GMT</pubDate></item><item><title><![CDATA[Linux Network Management]]></title><description><![CDATA[ 
 <br><br>The ping command is used to check the network connectivity between your machine and another host.<br><br><br>The ip command is a more modern and powerful utility for network management, replacing many older tools such as ifconfig and route.<br><br><br>The ssh command allows secure remote login to a server using SSH (Secure Shell).<br><br><br>The scp command allows you to securely copy files and directories between your local machine and a remote server or between remote servers.<br><br><br>The wget command is used to download files from the web, supporting HTTP, HTTPS, and FTP protocols.<br><br><br>The curl command is used to interact with web servers using various protocols, including HTTP, HTTPS, FTP, and more.<br><br><br>Display network connections, routing tables, interface statistics, and more.<br><br><br>The traceroute command is used to trace the path that packets take to reach a specific network host, which helps in diagnosing network issues.<br>]]></description><link>life,-universe-and-everything/linux-and-ubuntu/linux-network-management.html</link><guid isPermaLink="false">Life, Universe and Everything/Linux and Ubuntu/Linux Network Management.md</guid><pubDate>Thu, 13 Feb 2025 22:04:06 GMT</pubDate></item><item><title><![CDATA[Linux System Information and Management]]></title><description><![CDATA[ 
 <br><br><br><br>The htop command (improves top) provides a real-time view of the system's resource usage, including processes, CPU, memory, and more.<br><br><br>The nvtop command is a real-time monitoring tool designed specifically for NVIDIA GPUs, showing resource utilization, temperature, and other GPU-related information in a visual format.<br><br><br>The ps command reports a snapshot of the current processes running on the system.<br><br><br><br><br>The who command shows who is logged into the system, including their login time and terminal.<br><br><br>The whoami command shows the current logged-in user's username.<br><br><br><br><br>The df command reports the amount of free and used disk space on all mounted filesystems.<br><br><br>The du command estimates the disk space used by files and directories.<br><br><br>The free command shows the system's memory usage, including RAM and swap space.<br><br><br><br><br>The uname command prints information about the system's kernel and hardware.<br>]]></description><link>life,-universe-and-everything/linux-and-ubuntu/linux-system-information-and-management.html</link><guid isPermaLink="false">Life, Universe and Everything/Linux and Ubuntu/Linux System Information and Management.md</guid><pubDate>Thu, 16 Jan 2025 15:56:09 GMT</pubDate></item><item><title><![CDATA[OpenSSH]]></title><description><![CDATA[ 
 <br>In a Nutshell
OpenSSH is an open-source implementation of the <a data-tooltip-position="top" aria-label="SSH - Secure Shell" data-href="SSH - Secure Shell" href="the-guide/computer-science/networks/ssh-secure-shell.html" class="internal-link" target="_self" rel="noopener nofollow">SSH protocol</a>, widely used in <a data-tooltip-position="top" aria-label="Linux File and Directory Management" data-href="Linux File and Directory Management" href="life,-universe-and-everything/linux-and-ubuntu/linux-file-and-directory-management.html" class="internal-link" target="_self" rel="noopener nofollow">Linux</a>, MacOS and Windows. 
<br><br><br>We can use ssh-keygen, a tool for generating public and private SSH keys. For modern Ed25519 keys, we can use  <br>ssh-keygen -t ed25519 -C "your-email@example.com"
<br>Should we need a key generated via the older RSA cryptographic algorithm, we can use <br>ssh-keygen -t rsa -b 4096 -C "your-email@example.com"
<br>instead (-b 4096 sets a strong 4096-bit key length).<br>After this, you will be prompted to choose a saving location, the default one is usually under ~/.ssh/id_ed25519.<br>The next prompt will ask for a passphrase you can enter as a password (can be left out, but recommended). This will add an additional layer of security. For convenience, we can use the  ssh-agent utility to later cache that passphrase. <br>To actually see our passphrase, we can simply use <br>ls -l ~/.ssh/id_ed25519*
<br>which should show<br>-rw-------  1 user user  464 Feb 8 12:34 id_ed25519     # private
-rw-r--r--  1 user user  100 Feb 8 12:34 id_ed25519.pub # public
<br>Warning
Keep your private key secret !
<br>Cachind and Auto-Loading in Profile
Now that the key is generated, we can cache it via the ssh-agent by using
eval "$(ssh-agent -s)"
ssh-add ~/.ssh/id_ed25519 # pay attention to filename

in our <a data-tooltip-position="top" aria-label="The bashrc and bash_profile Scripts" data-href="The bashrc and bash_profile Scripts" href="life,-universe-and-everything/linux-and-ubuntu/the-bashrc-and-bash_profile-scripts.html" class="internal-link" target="_self" rel="noopener nofollow">bashrc</a> or zshrc file.
<br><br>ssh - Secure Shell
The ssh command is used to establish a secure SSH connection to a remote host.
<br><br>scp - Secure Copy
The scp command is used to securely copy files between hosts over SSH.
<br><br>sftp - Secure File Transfer Protocol
The sftp command provides a secure alternative to FTP for transferring files over SSH.
<br><br>sshd - SSH Daemon
The sshd command runs the OpenSSH server and listens for incoming SSH connections.
<br>]]></description><link>life,-universe-and-everything/linux-and-ubuntu/openssh.html</link><guid isPermaLink="false">Life, Universe and Everything/Linux and Ubuntu/OpenSSH.md</guid><pubDate>Thu, 17 Apr 2025 15:12:53 GMT</pubDate></item><item><title><![CDATA[Syncing OneDrive via RClone]]></title><description><![CDATA[ 
 <br>In a Nutshell
My current workaround to use Onerive with <a data-href="- Linux -" href="life,-universe-and-everything/linux-and-ubuntu/-linux-.html" class="internal-link" target="_self" rel="noopener nofollow">- Linux -</a>.
<br><br>Make source and dest identical, modifying destination only.<br>Sync all files from cloud to local<br>rclone sync -i -P onedrive: ~/OneDrive
<br>Note that this will only work the specified way !! Best practice is to do this for single directories only, as I e.g. never save files under /Uni from anywhere but my laptop.<br>For university files only <br># cloud to local
rclone sync -i -P onedrive:Dokumente/Uni ~/OneDrive/Dokumente/Uni
# local to cloud
rclone sync -i -P ~/OneDrive/Dokumente/Uni onedrive:Dokumente/Uni
<br>For personal stuff only  (cloud to local)<br># cloud to local
rclone sync -i -P onedrive:Dokumente/Persönlich ~/OneDrive/Dokumente/Persönlich
# local to cloud
rclone sync -i -P ~/OneDrive/Dokumente/Persönlich onedrive:Dokumente/Persönlich
]]></description><link>life,-universe-and-everything/linux-and-ubuntu/syncing-onedrive-via-rclone.html</link><guid isPermaLink="false">Life, Universe and Everything/Linux and Ubuntu/Syncing OneDrive via RClone.md</guid><pubDate>Wed, 23 Apr 2025 21:43:22 GMT</pubDate></item><item><title><![CDATA[The bashrc and bash_profile Scripts]]></title><description><![CDATA[ 
 <br>In a Nutshell
In <a data-tooltip-position="top" aria-label="- Linux -" data-href="- Linux -" href="life,-universe-and-everything/linux-and-ubuntu/-linux-.html" class="internal-link" target="_self" rel="noopener nofollow">- Linux -</a> and Unix-like operating systems, the ~/.bashrc and ~/.bash_profile scripts are configuration files for the Bash shell. These files allow users to customize their shell environment, including aliases, functions, environment variables, and more.
<br><br><br><a data-href="Linux Basic Shell Commands" href="life,-universe-and-everything/linux-and-ubuntu/linux-basic-shell-commands.html" class="internal-link" target="_self" rel="noopener nofollow">Linux Basic Shell Commands</a>Colon-separated list of directories that tells the shell where to look for executable files when you type a <a data-tooltip-position="top" aria-label="Linux Basic Shell Commands" data-href="Linux Basic Shell Commands" href="life,-universe-and-everything/linux-and-ubuntu/linux-basic-shell-commands.html" class="internal-link" target="_self" rel="noopener nofollow">shell command</a>.<br>
For example, when you type ls in your terminal, the shell searches through each directory in the PATH to find the ls executable. If it finds it, it runs the command. If not, you get a "command not found" error.<br>To view your current PATH, you can use<br>echo $PATH$
<br>Common PATH directories include<br>
<br>System Binaries:

<br>/bin: Basic system utilities (e.g., ls, cat, cp, mv).
<br>/usr/bin: Standard binaries for user programs (e.g., awk, sed, python).
<br>/usr/sbin: System binaries for administrative tasks (e.g., ifconfig, service).


<br>Local Binaries:

<br>/usr/local/bin: Executables installed manually or by tools like Homebrew.
<br>/usr/local/sbin: Administrative binaries installed manually.


<br>Optional Directories:

<br>/sbin: System administrative commands (e.g., mount, shutdown).
<br>/snap/bin: Used for Snap packages (on Linux systems).


<br>Custom or Developer Tools:

<br>$HOME/bin: A personal directory for user-installed binaries.
<br>Directories added by tools like Node.js (~/.npm/bin), Python (~/.local/bin), Java (~/jdk/bin), etc.


<br>The default PATH for Unix-like systems usually is <br>/usr/local/bin:/usr/bin:/bin:/usr/local/sbin:/usr/sbin:/sbin
<br>where the colons serve as seperators.<br>First Match Execution
The first match is executed. For example, if ls exists in both /bin and /usr/local/bin, and /usr/local/bin is first in the PATH, it will use /usr/local/bin/ls.
<br><br><br>Both are configuration files, used in slightly different scenarios.<br><br>
<br>Source ~/.bashrc from ~/.bash_profile: It’s common to include ~/.bashrc inside ~/.bash_profile to ensure that non-login shell configurations are available in login shells as well. This avoids duplicating configurations.
<br>Modern Linux systems often use ~/.bash_profile only to source ~/.bashrc because graphical terminals (non-login shells) are the most commonly used.
<br>Alternative files like ~/.profile or /etc/bashrc may also be used depending on the distribution or global settings.
<br><br># ~/.bash_profile
export PATH=$HOME/bin:$PATH
export EDITOR=vim

# Source ~/.bashrc to include its settings
if [ -f ~/.bashrc ]; then
    source ~/.bashrc
fi
<br><br># ~/.bashrc
alias ll='ls -la'
alias gs='git status'

# Set a fancy command prompt
PS1='\u@\h:\w\$ '

# Shell functions 
function mkcd() { 
	mkdir -p "$1" &amp;&amp; cd "$1" 
}
]]></description><link>life,-universe-and-everything/linux-and-ubuntu/the-bashrc-and-bash_profile-scripts.html</link><guid isPermaLink="false">Life, Universe and Everything/Linux and Ubuntu/The bashrc and bash_profile Scripts.md</guid><pubDate>Thu, 13 Feb 2025 22:18:22 GMT</pubDate></item><item><title><![CDATA[Ubuntu Repositories]]></title><description><![CDATA[ 
 <br>In a Nutshell
Software archives to store Ubuntu programs available for download. Make it easy to install new software, while also providing a high level of security, since the software is thoroughly tested and built specifically for each version of Ubuntu.
<br><br>The four main repositories are:<br>
<br>Main&nbsp;- Canonical-supported free and open-source software.<br>

<br>Universe&nbsp;- Community-maintained free and open-source software.<br>

<br>Restricted&nbsp;- Proprietary drivers for devices.<br>

<br>Multiverse&nbsp;- Software restricted by copyright or legal issues.
<br>These repositories are stored on debian-based (which use e.g. <a data-tooltip-position="top" aria-label="APT - Advanced Package Tool" data-href="APT - Advanced Package Tool" href="life,-universe-and-everything/linux-and-ubuntu/apt-advanced-package-tool.html" class="internal-link" target="_self" rel="noopener nofollow">apt</a> package tool) systems under<br>/etc/apt/sources.list
<br><br><br>Repository information apart from the main ones above can be stored in additional .list files in the directory<br>/etc/apt/sources.list.d
]]></description><link>life,-universe-and-everything/linux-and-ubuntu/ubuntu-repositories.html</link><guid isPermaLink="false">Life, Universe and Everything/Linux and Ubuntu/Ubuntu Repositories.md</guid><pubDate>Sun, 12 Jan 2025 11:26:25 GMT</pubDate></item><item><title><![CDATA[- Matplotlib -]]></title><description><![CDATA[ 
 <br><br><br><br><br>
<br>OO vs Pyplot <a rel="noopener nofollow" class="external-link" href="https://matplotlib.org/matplotblog/posts/pyplot-vs-object-oriented-interface/" target="_blank">https://matplotlib.org/matplotblog/posts/pyplot-vs-object-oriented-interface/</a>
]]></description><link>life,-universe-and-everything/matplotlib/-matplotlib-.html</link><guid isPermaLink="false">Life, Universe and Everything/Matplotlib/- Matplotlib -.md</guid><pubDate>Thu, 13 Feb 2025 22:16:58 GMT</pubDate></item><item><title><![CDATA[3D Plots with Matplotlib]]></title><description><![CDATA[ 
 <br>In a Nutshell
Collection of useful information for creating good 3D plots with the <a data-tooltip-position="top" aria-label="- Matplotlib -" data-href="- Matplotlib -" href="life,-universe-and-everything/matplotlib/-matplotlib-.html" class="internal-link" target="_self" rel="noopener nofollow">Matplotlib</a> <a data-tooltip-position="top" aria-label="- Python Programming Language -" data-href="- Python Programming Language -" href="life,-universe-and-everything/python/-python-programming-language-.html" class="internal-link" target="_self" rel="noopener nofollow">Python</a> package.
<br><br>
<br>Viewing Angles<br>
- <a rel="noopener nofollow" class="external-link" href="https://matplotlib.org/stable/api/toolkits/mplot3d/view_angles.html" target="_blank">https://matplotlib.org/stable/api/toolkits/mplot3d/view_angles.html</a><br>
<img alt="center" src="lib/media/pasted-image-20240918163811.png" style="width: 300px; max-width: 100%;">
<br><br><br>
<br><a rel="noopener nofollow" class="external-link" href="https://stackoverflow.com/questions/31506361/grid-zorder-seems-not-to-take-effect-matplotlib" target="_blank">https://stackoverflow.com/questions/31506361/grid-zorder-seems-not-to-take-effect-matplotlib</a>
<br><br>
<br>quiver(X,Y,U,V) can take arrays of coordinates, enabling to draw vector field with single line of code

<br>Default for arguments  uses uv, which is in screen coordinates (percentages, U=V means )
<br>angles='xy' to draw in data coordinates 


]]></description><link>life,-universe-and-everything/matplotlib/3d-plots-with-matplotlib.html</link><guid isPermaLink="false">Life, Universe and Everything/Matplotlib/3D Plots with Matplotlib.md</guid><pubDate>Thu, 10 Apr 2025 16:27:50 GMT</pubDate><enclosure url="lib/media/pasted-image-20240918163811.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src=&quot;lib/media/pasted-image-20240918163811.png&quot;&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[Matplotlib FuncAnimation]]></title><description><![CDATA[ 
 <br>TODO - Visualizer in Admittance<br><br><br>Only redraw changing elements of a plot.<br><a rel="noopener nofollow" class="external-link" href="https://matplotlib.org/stable/users/explain/animations/blitting.html" target="_blank">https://matplotlib.org/stable/users/explain/animations/blitting.html</a><br>def update_plot(frame):
	...

def init():
	...

ani = FuncAnimation(fig, update_plot, 
				         init_func=init, 
				         blit=True, 
				         interval=100, 
				         cache_frame_data=False)
]]></description><link>life,-universe-and-everything/matplotlib/matplotlib-funcanimation.html</link><guid isPermaLink="false">Life, Universe and Everything/Matplotlib/Matplotlib FuncAnimation.md</guid><pubDate>Tue, 11 Mar 2025 17:13:47 GMT</pubDate></item><item><title><![CDATA[- PyBullet -]]></title><description><![CDATA[<a class="tag" href="?query=tag:Robotics" style="background-color: rgb(4, 108, 116); color: white; font-weight: 700; border: none; border-radius: 1em; padding: 0.2em 0.5em;">#Robotics</a> <a class="tag" href="?query=tag:Machine-Learning" style="background-color: rgb(4, 108, 116); color: white; font-weight: 700; border: none; border-radius: 1em; padding: 0.2em 0.5em;">#Machine-Learning</a> 
 <br>In a Nutshell
<a data-tooltip-position="top" aria-label="- Python Programming Language -" data-href="- Python Programming Language -" href="life,-universe-and-everything/python/-python-programming-language-.html" class="internal-link" target="_self" rel="noopener nofollow">Python</a> module for physics simulation, particularly focused on <a href=".?query=tag:Robotics" class="tag" target="_blank" rel="noopener nofollow">#Robotics</a> , physics-based gaming, and <a href=".?query=tag:Machine-Learning" class="tag" target="_blank" rel="noopener nofollow">#Machine-Learning</a>  applications.
<br><br><br>Available via the <a data-tooltip-position="top" aria-label="- Python Programming Language -" data-href="- Python Programming Language -" href="life,-universe-and-everything/python/-python-programming-language-.html" class="internal-link" target="_self" rel="noopener nofollow">Python</a> <a data-tooltip-position="top" aria-label="Conda and Pip for Python" data-href="Conda and Pip for Python" href="life,-universe-and-everything/python/conda-and-pip-for-python.html" class="internal-link" target="_self" rel="noopener nofollow">Pip package manager</a><br>pip install pybullet
sudo apt-get install libgl1-mesa-glx # may be req for rendering
<br>Installation can be verified via<br><br><br><br><br><br>import pybullet as p
import time

# Connect to the physics server
p.connect(p.GUI)

# Load a plane and a robot model (e.g., URDF file)
plane_id = p.loadURDF("plane.urdf")
robot_id = p.loadURDF("r2d2.urdf", basePosition=[0, 0, 1])

# Run the simulation
for _ in range(1000):
    p.stepSimulation()
    time.sleep(1./240.)

# Disconnect from the server
p.disconnect()
]]></description><link>life,-universe-and-everything/pybullet/-pybullet-.html</link><guid isPermaLink="false">Life, Universe and Everything/PyBullet/- PyBullet -.md</guid><pubDate>Thu, 10 Apr 2025 16:27:50 GMT</pubDate></item><item><title><![CDATA[PyBullet Camera Simulation]]></title><description><![CDATA[ 
 <br>how to set up cameras in <a data-tooltip-position="top" aria-label="- PyBullet -" data-href="- PyBullet -" href="life,-universe-and-everything/pybullet/-pybullet-.html" class="internal-link" target="_self" rel="noopener nofollow">Pybullet</a> simulations.<br><br>Used t transform world coordinates into the cameras point of view.<br><br>Used to compute image from fixed camera. Transforms camera frame coordinates into clip space, describing how D points from the cameras point of view are mapped to a D image<br><br><br>from pyquaternion import Quaternion
import numpy as np


def cvK2BulletP(K, w, h, near, far):
    """
    cvKtoPulletP converst the K interinsic matrix as calibrated using Opencv
    and ROS to the projection matrix used in openGL and Pybullet.

    :param K:  OpenCV 3x3 camera intrinsic matrix
    :param w:  Image width
    :param h:  Image height
    :near:     The nearest objects to be included in the render
    :far:      The furthest objects to be included in the render
    :return:   4x4 projection matrix as used in openGL and pybullet
    """ 
    f_x = K[0,0]
    f_y = K[1,1]
    c_x = K[0,2]
    c_y = K[1,2]
    A = (near + far)/(near - far)
    B = 2 * near * far / (near - far)

    projection_matrix = [
                        [2/w * f_x,  0,          (w - 2*c_x)/w,  0],
                        [0,          2/h * f_y,  (2*c_y - h)/h,  0],
                        [0,          0,          A,              B],
                        [0,          0,          -1,             0]]
    #The transpose is needed for respecting the array structure of the OpenGL
    return np.array(projection_matrix).T.reshape(16).tolist()


def cvPose2BulletView(q, t):
    """
    cvPose2BulletView gets orientation and position as used 
    in ROS-TF and opencv and coverts it to the view matrix used 
    in openGL and pyBullet.
    
    :param q: ROS orientation expressed as quaternion [qx, qy, qz, qw] 
    :param t: ROS postion expressed as [tx, ty, tz]
    :return:  4x4 view matrix as used in pybullet and openGL
    
    """
    q = Quaternion([q[3], q[0], q[1], q[2]])
    R = q.rotation_matrix

    T = np.vstack([np.hstack([R, np.array(t).reshape(3,1)]),
                              np.array([0, 0, 0, 1])])
    # Convert opencv convention to python convention
    # By a 180 degrees rotation along X
    Tc = np.array([[1,   0,    0,  0],
                   [0,  -1,    0,  0],
                   [0,   0,   -1,  0],
                   [0,   0,    0,  1]]).reshape(4,4)
    
    # pybullet pse is the inverse of the pose from the ROS-TF
    T=Tc@np.linalg.inv(T)
    # The transpose is needed for respecting the array structure of the OpenGL
    viewMatrix = T.T.reshape(16)
    return viewMatrix
<br>projectionMatrix = cvK2BulletP(K, w, h, near, far)
viewMatrix = cvPose2BulletView(q, t)

_, _, rgb, depth, segmentation = b.getCameraImage(W, H, viewMatrix, projectionMatrix, shadow = True)
]]></description><link>life,-universe-and-everything/pybullet/pybullet-camera-simulation.html</link><guid isPermaLink="false">Life, Universe and Everything/PyBullet/PyBullet Camera Simulation.md</guid><pubDate>Wed, 19 Mar 2025 11:32:54 GMT</pubDate></item><item><title><![CDATA[PyBullet Robot Simulation]]></title><description><![CDATA[ 
 <br>Warning
In Pybullet 
]]></description><link>life,-universe-and-everything/pybullet/pybullet-robot-simulation.html</link><guid isPermaLink="false">Life, Universe and Everything/PyBullet/PyBullet Robot Simulation.md</guid><pubDate>Fri, 17 Jan 2025 11:56:31 GMT</pubDate></item><item><title><![CDATA[- Python Programming Language -]]></title><description><![CDATA[ 
 <br>In a Nutshell

<br>Considered an interpreted language because the bytecode is interpreted at runtime by the Python Virtual Machine. However, it has a compilation step to bytecode (not machine code) and can benefit from JIT compilation in some implementations like PyPy.
<br>Memory Management: Automatic garbage collection and reference counting simplify memory handling but incur performance overhead.
<br>Performance: Slower than <a data-tooltip-position="top" aria-label="- C Programming Language -" data-href="- C Programming Language -" href="life,-universe-and-everything/c-and-c++/-c-programming-language-.html" class="internal-link" target="_self" rel="noopener nofollow">C</a> and <a data-tooltip-position="top" aria-label="- C++ Programming Language -" data-href="- C++ Programming Language -" href="life,-universe-and-everything/c-and-c++/-c++-programming-language-.html" class="internal-link" target="_self" rel="noopener nofollow">C++</a> due to dynamic typing, higher-level abstractions, and garbage collection.
<br>Paralellism: <a data-tooltip-position="top" aria-label="Multithreading" data-href="Multithreading" href="the-guide/computer-science/parallel-computing/multithreading.html" class="internal-link" target="_self" rel="noopener nofollow">Multithreading</a> is limited by the <a data-tooltip-position="top" aria-label="GIL - Global Interpreter Lock" data-href="GIL - Global Interpreter Lock" href="the-guide/computer-science/gil-global-interpreter-lock.html" class="internal-link" target="_self" rel="noopener nofollow">Global Interpreter Lock (GIL)</a> (<a data-tooltip-position="top" aria-label="Threading Module (Python Standard Library)" data-href="Threading Module (Python Standard Library)" href="life,-universe-and-everything/python/threading-module-(python-standard-library).html" class="internal-link" target="_self" rel="noopener nofollow">threading module</a>), but <a data-tooltip-position="top" aria-label="Multiprocessing" data-href="Multiprocessing" href="the-guide/computer-science/parallel-computing/multiprocessing.html" class="internal-link" target="_self" rel="noopener nofollow">multiprocessing</a> can be used for parallelism (<a data-tooltip-position="top" aria-label="Multiprocessing Module (Python Standard Library)" data-href="Multiprocessing Module (Python Standard Library)" href="life,-universe-and-everything/python/multiprocessing-module-(python-standard-library).html" class="internal-link" target="_self" rel="noopener nofollow">multiprocessing module</a>).
<br>Dynamic Typing: Variables don't require explicit types, quick development but with a performance cost.

<br><br><br><br><br><br><br>A method or attribute with a single leading underscore (_) is considered "protected" by convention. This means it is intended for internal use within a class or module and should not be accessed directly by outside code. It is not enforced by Python—it's simply a signal to developers that these methods or attributes are part of the internal implementation.<br>
Example:<br>class MyClass:     
	def __init__(self):         
		self._internal_var = 42  # Protected variable
<br><br>A method or attribute with a double leading underscore triggers name mangling. This mechanism changes the method's or variable's name internally to prevent accidental name conflicts in subclasses. It's primarily used to make methods or attributes "private" to the class, though it doesn't provide strict encapsulation.<br>Example:<br>class MyClass:     
	def __init__(self):         
		self.__private_var = 42  
		# Name mangling applied  
    def __private_method(self):         
		return "This is private"
<br>Internally, __private_var is renamed to _MyClass__private_var to avoid conflicts in subclasses.<br><br>Methods or attributes with double underscores on both sides are known as magic methods or special methods. These methods are predefined by Python to allow you to customize the behavior of objects for built-in operations such as string representation, object comparison, and arithmetic operations. Examples include __init__ (constructor), __str__ (string representation), and __add__ (addition operator).<br>Example:<br>class MyClass:     
	def __init__(self, value):         
		self.value = value          
	def __add__(self, other):         
		return MyClass(self.value + other.value)
<br>Magic methods allow objects to interact seamlessly with Python's syntax and built-in functions.]]></description><link>life,-universe-and-everything/python/-python-programming-language-.html</link><guid isPermaLink="false">Life, Universe and Everything/Python/- Python Programming Language -.md</guid><pubDate>Wed, 23 Apr 2025 22:23:20 GMT</pubDate></item><item><title><![CDATA[Asynchio Library (Python Standard Library)]]></title><description><![CDATA[ 
 ]]></description><link>life,-universe-and-everything/python/asynchio-library-(python-standard-library).html</link><guid isPermaLink="false">Life, Universe and Everything/Python/Asynchio Library (Python Standard Library).md</guid><pubDate>Fri, 03 Jan 2025 10:04:28 GMT</pubDate></item><item><title><![CDATA[Conda and Pip for Python]]></title><description><![CDATA[ 
 <br><br>What is Pip
<a data-tooltip-position="top" aria-label="https://pip.pypa.io/en/stable/" rel="noopener nofollow" class="external-link" href="https://pip.pypa.io/en/stable/" target="_blank">Pip</a> is the <a data-tooltip-position="top" aria-label="- Python Programming Language -" data-href="- Python Programming Language -" href="life,-universe-and-everything/python/-python-programming-language-.html" class="internal-link" target="_self" rel="noopener nofollow">Python</a> Packaging Authority’s recommended tool for installing packages from the <a data-tooltip-position="top" aria-label="https://pypi.org/" rel="noopener nofollow" class="external-link" href="https://pypi.org/" target="_blank">Python Package Index</a>, PyPI. Pip installs Python software packaged as <a data-tooltip-position="top" aria-label="Python Wheels" data-href="Python Wheels" href="life,-universe-and-everything/python/python-wheels.html" class="internal-link" target="_self" rel="noopener nofollow">wheels</a> or source distributions. The latter may require that the system have compatible compilers, and possibly libraries, installed before invoking pip to succeed.
<br># install package in this directory (pyproject.toml file)
pip install . 
# if local module is under development, immediately accounts for changes
pip install -e . 
# list all installed packages
pip list
# give detailed information for package
pip show &lt;name&gt;
<br>Cleaning Up<br>
Pip does not offer similar cleanup commands as conda, but <br>pip install pipdeptree
pipdeptree
<br>shows a tree that can be used to identify orphaned packages. Also, pip stores a cache of downloaded packages to speed up future installation, which can be deleted using<br>pip cache purge
<br><br><br>What is Conda 
<a data-tooltip-position="top" aria-label="https://conda.io/docs/" rel="noopener nofollow" class="external-link" href="https://conda.io/docs/" target="_blank">Conda</a> is a cross platform package and environment manager that installs and manages conda packages from the <a data-tooltip-position="top" aria-label="https://repo.anaconda.com/" rel="noopener nofollow" class="external-link" href="https://repo.anaconda.com/" target="_blank">Anaconda repository</a> as well as from the <a data-tooltip-position="top" aria-label="https://anaconda.org/" rel="noopener nofollow" class="external-link" href="https://anaconda.org/" target="_blank">Anaconda Cloud</a>. Conda packages are binaries. There is never a need to have compilers available to install them. Additionally conda packages are not limited to Python software. They may also contain C or C++ libraries, R packages or any other software.
<br>If not selected when installing, initialize<br>conda init            # set up conda, creates entry in .bashrc
conda activate        # activate base env 
<br>to update <a data-tooltip-position="top" aria-label="The bashrc and bash_profile Scripts" data-href="The bashrc and bash_profile Scripts" href="life,-universe-and-everything/linux-and-ubuntu/the-bashrc-and-bash_profile-scripts.html" class="internal-link" target="_self" rel="noopener nofollow">bashrc file</a>.<br>Displaying detailed Information about Version and Directories<br>conda info
<br>Creating, Removing and activating Envs<br># create env from yml file
conda env create -f environment.yml
# create yml file from env
conda env export &gt; environment_droplet.yml
# list all the conda environment available  
conda info --envs  
# Create new environment named as `envname`  
conda create --name envname
# Remove environment and its dependencies  
conda remove --name envname --all
# Clone an existing environment  
conda create --name clone_envname --clone envname
<br>Cleaning Up<br>conda clean --all        # delete unused packages caches, caches, tarballs
<br>Enironment-Specifis Information<br>conda list -n myenv
<br>Misc<br>
<br>For Windows

<br>Edit PATH under environment variables, add miniconda3, miniconda3\Libary and miniconda3...
<br>...


<br><br><br>
<br>Using conda and pip together - <a rel="noopener nofollow" class="external-link" href="https://docs.conda.io/projects/conda/en/latest/user-guide/tasks/manage-environments.html#pip-in-env" target="_blank">https://docs.conda.io/projects/conda/en/latest/user-guide/tasks/manage-environments.html#pip-in-env</a>
]]></description><link>life,-universe-and-everything/python/conda-and-pip-for-python.html</link><guid isPermaLink="false">Life, Universe and Everything/Python/Conda and Pip for Python.md</guid><pubDate>Thu, 10 Apr 2025 16:27:50 GMT</pubDate></item><item><title><![CDATA[Data Structures in Python]]></title><description><![CDATA[ 
 <br>
<br>Lists are implemented as dynamic arrays, where elements are stored contiguously, but the size of the array can grow or shrink automatically as items are added or removed.
<br>Dictionaries and sets are implemented using hash tables, offering average  time complexity for operations like insert, delete, and access.
<br>]]></description><link>life,-universe-and-everything/python/data-structures-in-python.html</link><guid isPermaLink="false">Life, Universe and Everything/Python/Data Structures in Python.md</guid><pubDate>Wed, 15 Jan 2025 10:29:52 GMT</pubDate></item><item><title><![CDATA[Modules, Packages and Imports in Python]]></title><description><![CDATA[ 
 <br><br><br>Packages collect related modules together within a single tree-like hierarchy, for large systems these can be further divided into sub-packages.<br>mypackage/
├── __init__.py
├── module1.py
├── module2.py
└── subpackage/
    ├── __init__.py
    └── submodule.py
<br>A directory needs a special __init__.py file to indicate that it is a package (can be empty). A common use for this file is to handle imports<br># mypackage/__init__.py
from .module1 import function1
from .module2 import function2
<br>This enables simpler imports<br>from mypackage import function1, function2
<br><br><br>Inside a <a data-tooltip-position="top" aria-label="Modules, Packages and Imports in Python" data-href="Modules, Packages and Imports in Python" href="life,-universe-and-everything/python/modules,-packages-and-imports-in-python.html" class="internal-link" target="_self" rel="noopener nofollow">package hierarchy</a>, we can use the regular dots to import any module.<br>from ..subpackage import module
from subpackage.submodule import submodule
<br>Alternatively, e.g. when the above is not possible due to using <a data-tooltip-position="top" aria-label="Jupyter Notebook Anomalies" data-href="Jupyter Notebook Anomalies" href="life,-universe-and-everything/jupyter-notebook-anomalies.html" class="internal-link" target="_self" rel="noopener nofollow">Jupyter notebooks</a>, the sys module allows us to manually add the any directory to the <a data-href="PYTHONPATH" href="life,-universe-and-everything/python/pythonpath.html" class="internal-link" target="_self" rel="noopener nofollow">PYTHONPATH</a> via<br>import sys
sys.path.append("..") # adds parent directory
]]></description><link>life,-universe-and-everything/python/modules,-packages-and-imports-in-python.html</link><guid isPermaLink="false">Life, Universe and Everything/Python/Modules, Packages and Imports in Python.md</guid><pubDate>Thu, 10 Apr 2025 17:30:40 GMT</pubDate></item><item><title><![CDATA[Multiprocessing Module (Python Standard Library)]]></title><description><![CDATA[ 
 <br>In a Nutshell
Provides functionalities to run multiple <a data-tooltip-position="top" aria-label="Threads vs. Processes" data-href="Threads vs. Processes" href="the-guide/computer-science/parallel-computing/threads-vs.-processes.html" class="internal-link" target="_self" rel="noopener nofollow">processes</a> concurrently. 
<br>]]></description><link>life,-universe-and-everything/python/multiprocessing-module-(python-standard-library).html</link><guid isPermaLink="false">Life, Universe and Everything/Python/Multiprocessing Module (Python Standard Library).md</guid><pubDate>Mon, 23 Dec 2024 09:43:54 GMT</pubDate></item><item><title><![CDATA[Pathlib Module (Python Standard Library)]]></title><description><![CDATA[ 
 <br>In a Nutshell
Introduced with <a data-tooltip-position="top" aria-label="- Python Programming Language -" data-href="- Python Programming Language -" href="life,-universe-and-everything/python/-python-programming-language-.html" class="internal-link" target="_self" rel="noopener nofollow">Python</a> 3.4, more flexible and object-oriented module to handle filesystem paths compared to the older os.path.

<br>Object-Oriented Design - based on a Path class that overloads operators like /
<br>Cross-Platform Compatibility - automatically handles differences between different operating systems
<br>File and Directory Operations - built-in functions for filesystem operations such as checking for existence, i/o and iterating through directories

<br><img alt="center" src="lib/media/pasted-image-20250120154016.png" style="width: 400px; max-width: 100%;"><br>....<br><br><br><br>]]></description><link>life,-universe-and-everything/python/pathlib-module-(python-standard-library).html</link><guid isPermaLink="false">Life, Universe and Everything/Python/Pathlib Module (Python Standard Library).md</guid><pubDate>Thu, 10 Apr 2025 16:27:50 GMT</pubDate><enclosure url="lib/media/pasted-image-20250120154016.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src=&quot;lib/media/pasted-image-20250120154016.png&quot;&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[Python Wheels]]></title><description><![CDATA[ 
 <br>In a Nutshell 
Package format that contains the files necessary for installing a <a data-tooltip-position="top" aria-label="- Python Programming Language -" data-href="- Python Programming Language -" href="life,-universe-and-everything/python/-python-programming-language-.html" class="internal-link" target="_self" rel="noopener nofollow">Python package</a>. Successor of the old .egg format.
<br><br>A Python wheel is a .whl file, essentially a zip archive with specific structure for Python tools like <a data-tooltip-position="top" aria-label="Conda and Pip for Python" data-href="Conda and Pip for Python" href="life,-universe-and-everything/python/conda-and-pip-for-python.html" class="internal-link" target="_self" rel="noopener nofollow">Pip</a> to understand. It contains:<br>
<br>Python code (modules, packages, etc.)
<br>Compiled extensions (e.g., .so, .pyd, .dll files)
<br>Metadata such as version info, dependencies, and required Python versions.
<br>Example of a wheel file structure:<br>mymodule-1.0.0-cp39-cp39-win_amd64.whl
    ├── mymodule/
    ├── mymodule/__init__.py
    ├── mymodule/extension.cpython-39-x86_64-linux-gnu.so
    ├── METADATA
    ├── WHEEL
    ├── INSTALLER
    ├── RECORD
    └── INFO/
<br>When you run:<br>pip install &lt;some-package&gt;
<br>
<br>If a wheel is available for your platform and Python version, pip will download the .whl file and install the package directly.
<br>If no wheel is available, pip will fall back to downloading the source code and compiling it (which can be slower and require additional build tools).
<br><br><br>The name of a wheel file follows a standard pattern:<br>&lt;package-name&gt;-&lt;version&gt;-&lt;python-tag&gt;-&lt;abi-tag&gt;-&lt;platform-tag&gt;.whl
<br>
<br>Python tag: Indicates the version(s) of Python the wheel is compatible with (e.g., cp39 for CPython 3.9).
<br>ABI tag: Refers to the Application Binary Interface (e.g., cp39 for CPython 3.9, or none for no ABI requirements).
<br>Platform tag: Refers to the platform (e.g., win_amd64 for 64-bit Windows).
<br>For example:<br>
<br>numpy-1.21.2-cp39-cp39-win_amd64.whl: This is the numpy package for Python 3.9 on 64-bit Windows.
<br><br><br>To create a wheel for a package you are developing, you would typically use a tool like setuptools in combination with wheel. For example:<br>
<br>Ensure you have wheel installed:
<br>pip install wheel
<br>
<br>Create a wheel from your package source:
<br>python setup.py bdist_wheel    
<br>This generates a .whl file that can be uploaded to PyPI (the Python Package Index) or shared directly with others for easy installation.]]></description><link>life,-universe-and-everything/python/python-wheels.html</link><guid isPermaLink="false">Life, Universe and Everything/Python/Python Wheels.md</guid><pubDate>Thu, 10 Apr 2025 16:27:50 GMT</pubDate></item><item><title><![CDATA[PYTHONPATH]]></title><description><![CDATA[ 
 <br>PYTHONPATH Environment Variable
PYTHONPATH is an <a data-tooltip-position="top" aria-label="Linux Environment Variables" data-href="Linux Environment Variables" href="life,-universe-and-everything/linux-and-ubuntu/linux-environment-variables.html" class="internal-link" target="_self" rel="noopener nofollow">environment variable</a> that tells the <a data-tooltip-position="top" aria-label="- Python Programming Language -" data-href="- Python Programming Language -" href="life,-universe-and-everything/python/-python-programming-language-.html" class="internal-link" target="_self" rel="noopener nofollow">Python Interpreter</a> where to look for modules and packages.<br>
For example, when you run the import my_module command, Python will search through the directories listed in the PYTHONPATH (in addition to the default directories) to find my_module.
]]></description><link>life,-universe-and-everything/python/pythonpath.html</link><guid isPermaLink="false">Life, Universe and Everything/Python/PYTHONPATH.md</guid><pubDate>Thu, 10 Apr 2025 17:26:58 GMT</pubDate></item><item><title><![CDATA[Socket Module (Python Standard Library)]]></title><description><![CDATA[ 
 ]]></description><link>life,-universe-and-everything/python/socket-module-(python-standard-library).html</link><guid isPermaLink="false">Life, Universe and Everything/Python/Socket Module (Python Standard Library).md</guid><pubDate>Fri, 03 Jan 2025 10:25:36 GMT</pubDate></item><item><title><![CDATA[Threading Module (Python Standard Library)]]></title><description><![CDATA[ 
 <br>In a Nutshell
High-level interface for working with <a data-tooltip-position="top" aria-label="Threads vs. Processes" data-href="Threads vs. Processes" href="the-guide/computer-science/parallel-computing/threads-vs.-processes.html" class="internal-link" target="_self" rel="noopener nofollow">threads</a> in <a data-tooltip-position="top" aria-label="- Python Programming Language -" data-href="- Python Programming Language -" href="life,-universe-and-everything/python/-python-programming-language-.html" class="internal-link" target="_self" rel="noopener nofollow">Python</a>.
<br><br>GIL and Python
Because of the issues with <a data-tooltip-position="top" aria-label="- Python Programming Language -" data-href="- Python Programming Language -" href="life,-universe-and-everything/python/-python-programming-language-.html" class="internal-link" target="_self" rel="noopener nofollow">Python</a> and <a data-tooltip-position="top" aria-label="GIL - Global Interpreter Lock" data-href="GIL - Global Interpreter Lock" href="the-guide/computer-science/gil-global-interpreter-lock.html" class="internal-link" target="_self" rel="noopener nofollow">GIL</a>, Python threads are mainly used for I/O-bounds tasks or network / database operations. For CPU-bound tasks consider <a data-tooltip-position="top" aria-label="Multiprocessing Module (Python Standard Library)" data-href="Multiprocessing Module (Python Standard Library)" href="life,-universe-and-everything/python/multiprocessing-module-(python-standard-library).html" class="internal-link" target="_self" rel="noopener nofollow">the multiprocessing module</a>.
<br>
<br>Main Thread

<br>The primary thread in which the Python program starts execution. Other threads can be created from the main thread.


<br>Daemon Threads

<br>Threads that run in the background and do not prevent the program from exiting when the main thread finishes. These are set using thread.setDaemon(True).


<br><br><br>The module is based on a Thread class that spawns a new thread. <br>import threading

def print_numbers():
    for i in range(5):
        print(i)

# Create a thread to run the function
thread = threading.Thread(target=print_numbers,
						  args=..., # target args as Tuple
						  kwargs=..., # target args as dict
						  daemon=True, # enable dameon 
						  )

# Start the thread
thread.start()

# Check if thread is still execute
thread.is_alive()

# Wait for the thread to complete
thread.join()
<br><br><br>A lock is a synchronization primitive that threads have to lock and unlock in order to continue execution. Thereby, only one thread can execute code while the lock is active. <br># basic lock
lock = threading.Lock()

# limits the number of threads that can access a resource
semaphore = threading.Semaphore(3)

def critical_section():
    with lock: # automatically handles lock.aquire() and lock.release()
        # Code that should be accessed by one thread at a time
        pass
        
	with semaphore:
		# Code that only 3 threads can access simultaneously
		pass


# Reentrant lock allows recursice aquisition

# Shared resource
counter = 0

# Reentrant lock
rlock = threading.RLock()

def recursive_increment(n):
    global counter
    with rlock:  # First lock
        counter += n
        print(f"Incremented by {n}, counter: {counter}")
        if n &gt; 0:
            recursive_increment(n-1)  # Recursive call

# Start thread that will call recursive_increment
t = threading.Thread(target=recursive_increment, args=(3,))
t.start()
t.join()
<br><br><br>For more involved coordination between multiple threads, we can use events and conditions. <br><br><br><br>The threading.local() function provides a way to store data that is local to a thread, i.e., each thread can store its own copy of the data without interfering with other threads.<br>thread_local = threading.local()  

def process():     
	thread_local.value = 42  # Each thread gets its own value`
]]></description><link>life,-universe-and-everything/python/threading-module-(python-standard-library).html</link><guid isPermaLink="false">Life, Universe and Everything/Python/Threading Module (Python Standard Library).md</guid><pubDate>Wed, 23 Apr 2025 22:23:02 GMT</pubDate></item><item><title><![CDATA[- PyTorch -]]></title><description><![CDATA[ 
 <br>In a Nutshell
Open-source machine learning library developed primarily by Facebook's AI Research (FAIR) lab. It is widely used for applications such as deep learning, computer vision, and natural language processing.
<br><br>Findings<br>
<br>Summary for models <a rel="noopener nofollow" class="external-link" href="https://pypi.org/project/torch-summary/" target="_blank">https://pypi.org/project/torch-summary/</a>
]]></description><link>life,-universe-and-everything/pytorch/-pytorch-.html</link><guid isPermaLink="false">Life, Universe and Everything/PyTorch/- PyTorch -.md</guid><pubDate>Thu, 13 Feb 2025 22:28:05 GMT</pubDate></item><item><title><![CDATA[Basic GPU Management in PyTorch]]></title><description><![CDATA[ 
 <br>Info
PyTorch leverages <a data-tooltip-position="top" aria-label="GPU - Graphics Processing Unit" data-href="GPU - Graphics Processing Unit" href="the-guide/computer-science/gpu-graphics-processing-unit.html" class="internal-link" target="_self" rel="noopener nofollow">GPU</a> acceleration via <a data-href="CUDA" href="the-guide/computer-science/parallel-computing/cuda.html" class="internal-link" target="_self" rel="noopener nofollow">CUDA</a>.
<br><br><br>import torch
print(f"GPU Available: {torch.cuda.is_available()}")
print(f"Number of GPUs: {torch.cuda.device_count()}")
print(f"Current GPU: {torch.cuda.current_device()}")
print(f"GPU Name: {torch.cuda.get_device_name(0)}")

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
<br><br><br>Compare with <a data-href="Executing a CUDA Kernel" href="the-guide/computer-science/parallel-computing/executing-a-cuda-kernel.html" class="internal-link" target="_self" rel="noopener nofollow">Executing a CUDA Kernel</a> note.<br>tensor = tensor.to(device)  # Preferred method

model = model.to(device)    # Moves all parameters/buffers to GPU
<br><br><br>TODO - make own note for this, important !]]></description><link>life,-universe-and-everything/pytorch/basic-gpu-management-in-pytorch.html</link><guid isPermaLink="false">Life, Universe and Everything/PyTorch/Basic GPU Management in PyTorch.md</guid><pubDate>Thu, 27 Feb 2025 16:13:57 GMT</pubDate></item><item><title><![CDATA[Dataloader in PyTorch]]></title><description><![CDATA[ 
 <br><br>
<br>Number of Workers 'num_workers'

<br>Controls how many subprocesses are created to lead data in parallel,  means data is only loaded in main process.
<br>Heuristic - set num of workers to approx number of CPU cores
<br>Large datasets and/or datasets with many complicated transformations can benefit from more workers


]]></description><link>life,-universe-and-everything/pytorch/dataloader-in-pytorch.html</link><guid isPermaLink="false">Life, Universe and Everything/PyTorch/Dataloader in PyTorch.md</guid><pubDate>Thu, 13 Feb 2025 22:28:08 GMT</pubDate></item><item><title><![CDATA[PyTorch Troubles]]></title><description><![CDATA[ 
 <br>Info
Some troubles I ran into when using <a data-tooltip-position="top" aria-label="- PyTorch -" data-href="- PyTorch -" href="life,-universe-and-everything/pytorch/-pytorch-.html" class="internal-link" target="_self" rel="noopener nofollow">PyTorch</a> ...
<br><br><br>If input values approach , torch.acos can give NaN-values, since it can only handle values between those values.<br>
Usually solved by limiting input via a small number via <br>val = torch.clamp(val, -1.0 + 1e-8, 1.0 - 1e-8)
<br>]]></description><link>life,-universe-and-everything/pytorch/pytorch-troubles.html</link><guid isPermaLink="false">Life, Universe and Everything/PyTorch/PyTorch Troubles.md</guid><pubDate>Thu, 13 Feb 2025 22:16:04 GMT</pubDate></item><item><title><![CDATA[ROS - Creating a Service]]></title><description><![CDATA[ 
 <br><br>Info
Introduced for tasks that should not proceed until a result is returned (which is not given my regular messages).

<br>Service Server: The node that provides the service, processes requests, and sends back responses.
<br>Service Client: The node that calls the service, sending requests and waiting for responses.

<br>We cannot visualize services with rqt_graph, but we can list all active ones via <br>ros2 service list
<br>and get additional information about the type via<br>ros2 service type &lt;name&gt;
<br><br><br>A service is defined via a .srv file that specifies its structure. <br>
<br>The fields above the line define the request
<br>The fields below the line define the response
<br># service takes two integers and returns one
int64 a
int64 b
---
int64 sum
<br><br>Sending a Request via Terminal<br>ros2 service call &lt;service-name&gt; &lt;serive-type&gt; "{'type': input, ...}"
<br><br><br>After defining a custom .srv file, place it in the srv directory of the <a data-tooltip-position="top" aria-label="ROS - Creating Nodes and Packages" data-href="ROS - Creating Nodes and Packages" href="life,-universe-and-everything/ros/ros-creating-nodes-and-packages.html" class="internal-link" target="_self" rel="noopener nofollow">ROS package</a> created in your <a data-tooltip-position="top" aria-label="ROS - Workspaces" data-href="ROS - Workspaces" href="life,-universe-and-everything/ros/ros-workspaces.html" class="internal-link" target="_self" rel="noopener nofollow">ROS workspace</a>.<br>&lt;package-name&gt;/
├── srv/
│   └── AddTwoInts.srv
├── ...

<br>Creating a Service Node<br># add_two_ints_server.py
import rclpy
from rclpy.node import Node
from my_package.srv import AddTwoInts

class AddTwoIntsServer(Node):
    def __init__(self):
        super().__init__('add_two_ints_server')
        self.srv = self.create_service(AddTwoInts, 'add_two_ints', self.add_two_ints_callback)

    def add_two_ints_callback(self, request, response):
        response.sum = request.a + request.b
        self.get_logger().info(f'Request: {request.a} + {request.b} = {response.sum}')
        return response

def main(args=None):
    rclpy.init(args=args)
    node = AddTwoIntsServer()
    rclpy.spin(node)
    rclpy.shutdown()

if __name__ == '__main__':
    main()

<br>Creating a Client Node<br># add_two_ints_client.py
import rclpy
from rclpy.node import Node
from my_package.srv import AddTwoInts

class AddTwoIntsClient(Node):
    def __init__(self):
        super().__init__('add_two_ints_client')
        self.client = self.create_client(AddTwoInts, 'add_two_ints')
        while not self.client.wait_for_service(timeout_sec=1.0):
            self.get_logger().info('Waiting for service...')
        self.request = AddTwoInts.Request()
        self.request.a = 3 # default
        self.request.b = 4 # default

    def send_request(self, a, b):
        self.request.a = a 
        self.request.b = b 
        self.future = self.client.call_async(self.request)
        rclpy.spin_until_future_complete(self, self.future)
        return self.future.result()

def main(args=None):
    rclpy.init(args=args)
    node = AddTwoIntsClient()
    response = node.send_request()
    node.get_logger().info(f'Result: {node.request.a} + {node.request.b} = {response.sum}')
    rclpy.shutdown()

if __name__ == '__main__':
    main()

]]></description><link>life,-universe-and-everything/ros/ros-creating-a-service.html</link><guid isPermaLink="false">Life, Universe and Everything/ROS/ROS - Creating a Service.md</guid><pubDate>Wed, 23 Apr 2025 21:44:32 GMT</pubDate></item><item><title><![CDATA[ROS - Creating Nodes and Packages]]></title><description><![CDATA[ 
 <br><br>Run the following after creating a .../src directory in addition to the ones created by colcon build in a <a data-tooltip-position="top" aria-label="ROS - Workspaces" data-href="ROS - Workspaces" href="life,-universe-and-everything/ros/ros-workspaces.html" class="internal-link" target="_self" rel="noopener nofollow">workspace</a> :<br>ros2 pkg create &lt;name&gt; --build-type &lt;type&gt; --dependencies &lt;dependency&gt;
<br>
<br>build type either ament_cmake (CPP) or ament_python
<br>dependencies used for the package, e.g. rclpy
<br><br><br>This will create a directory of the form<br>src/
   ├── &lt;package-name&gt;
   │   └── __init__.py
   │   └── &lt;my-node-name&gt;.py  # file name, add new node code here
   ├── resource
   ├── test
   ├── package.xml            # information for e.g. publishing
   ├── setup.cfg
   ├── setup.py
<br>Making a custom node executable<br>
After creating a custom class under &lt;name&gt; in the  &lt;my-node-name&gt; file, go into src/setup.py and add the following under entry_points<br>  entry_points={
    'console_scripts': [
      "&lt;exec-name&gt; = &lt;package-name&gt;.&lt;file-name&gt;:main" # pick an executable name here used to run the node later
    ]
  }
<br>Save, rebuild the package via colcon build (ws path) and source everything again to be able to <a data-tooltip-position="top" aria-label="ROS - Fundamentals" data-href="ROS - Fundamentals" href="life,-universe-and-everything/ros/ros-fundamentals.html" class="internal-link" target="_self" rel="noopener nofollow">run the node</a> via ros run. Alternatively, we can directly use the modified node by using the symlink flag<br>colcon build --symlink-install
<br>Declaring Dependencies
When using any dependency (python package) in the code, we need to add an entry to the package.xml file.
<br>It has the form<br>&lt;depend&gt;&lt;package-name&gt;&lt;/depend&gt;
<br><br>Creating custom <a data-tooltip-position="top" aria-label="ROS - Fundamentals" data-href="ROS - Fundamentals" href="life,-universe-and-everything/ros/ros-fundamentals.html" class="internal-link" target="_self" rel="noopener nofollow">Publisher</a><br>
In the constructor of your custom node class include<br>self.&lt;publisher-name&gt; = self.create_publisher(msg_type, 
											  &lt;topic-string&gt;,
											  &lt;Queue-size&gt;)
<br>with<br>
<br>The message type from std_msgs package
<br>The topic name (string)
<br>The number of messages that can be stored for late subscribers (int)
<br><br>Creating custom <a data-tooltip-position="top" aria-label="ROS - Fundamentals" data-href="ROS - Fundamentals" href="life,-universe-and-everything/ros/ros-fundamentals.html" class="internal-link" target="_self" rel="noopener nofollow">Subscriber</a><br>
In the constructor of your custom node class include<br>self.&lt;subscriber-name&gt; = self.create_subscribtion(msg_type,
											      &lt;topic-string&gt;,
											      callback_fun,
											      &lt;Queue-size&gt;)
<br>The callback function has to take the message as an argument. Use ros2 topic info ... to inspect the topic string.<br><br><br>Publisher to send Velocity Commands in Turtlesim<br>#!/usr/bin/env python 3
# turtlesim_custom/circle_publisher.py

import rclpy
from rclpy.node import Node
from geometry_msgs.msg import Twist  # For sending velocity commands

class CirclePublisher(Node):
    def __init__(self):
        super().__init__('circle_publisher')
        # Create a publisher for the turtle's velocity
        self.publisher_ = self.create_publisher(Twist, '/turtle1/cmd_vel', 10)
        
        # Create a timer to publish at 0.1-second intervals (10 Hz)
        self.timer = self.create_timer(0.1, self.publish_velocity)

    def publish_velocity(self):
        # Create a Twist message to move the turtle in a circle
        msg = Twist()
        msg.linear.x = 1.0   # Forward speed
        msg.angular.z = 1.0  # Rotation speed
        self.publisher_.publish(msg)
        
        self.get_logger().info('Publishing circular velocity: linear=1.0, angular=1.0')

def main(args=None):
    rclpy.init(args=args)
    node = CirclePublisher()
    rclpy.spin(node)  # Keep the node running
    node.destroy_node()
    rclpy.shutdown()

if __name__ == '__main__':
    main()

<br>Subscriber to log position Data<br>
The subscriber node will listen to the turtle’s pose topic (/turtle1/pose) and log the position data.<br>#!/usr/bin/env python 3
# turtlesim_custom/pose_subscriber.py

import rclpy
from rclpy.node import Node
from turtlesim.msg import Pose  # For receiving turtle position data

class PoseSubscriber(Node):
    def __init__(self):
        super().__init__('pose_subscriber')
        # Create a subscription to the turtle's pose
        self.subscription = self.create_subscription(
            Pose,
            '/turtle1/pose',
            self.pose_callback,
            10)
        self.subscription  # Prevent unused variable warning

    def pose_callback(self, msg):
        # Callback to log the turtle's pose
        self.get_logger().info(f'Turtle Pose - x: {msg.x:.2f}, y: {msg.y:.2f}, theta: {msg.theta:.2f}')

def main(args=None):
    rclpy.init(args=args)
    node = PoseSubscriber()
    rclpy.spin(node)  # Keep the node running
    node.destroy_node()
    rclpy.shutdown()

if __name__ == '__main__':
    main()

<br>Required Updates to Setup File<br>from setuptools import setup

package_name = 'turtlesim_custom'

setup(
    name=package_name,
    version='0.0.0',
    packages=[package_name],
    install_requires=['setuptools'],
    zip_safe=True,
    maintainer='your_name',
    maintainer_email='your_email@example.com',
    description='Custom publisher and subscriber for turtlesim',
    license='Apache License 2.0',
    entry_points={
        'console_scripts': [
            'circle_publisher = turtlesim_custom.circle_publisher:main',
            'pose_subscriber = turtlesim_custom.pose_subscriber:main',
        ],
    },
)

]]></description><link>life,-universe-and-everything/ros/ros-creating-nodes-and-packages.html</link><guid isPermaLink="false">Life, Universe and Everything/ROS/ROS - Creating Nodes and Packages.md</guid><pubDate>Wed, 23 Apr 2025 21:44:32 GMT</pubDate></item><item><title><![CDATA[ROS - Fundamentals]]></title><description><![CDATA[<a class="tag" href="?query=tag:Robotics" style="background-color: rgb(4, 108, 116); color: white; font-weight: 700; border: none; border-radius: 1em; padding: 0.2em 0.5em;">#Robotics</a> 
 <br>In a Nutshell
Open-source middleware (interface layer) framework for developing software in <a href=".?query=tag:Robotics" class="tag" target="_blank" rel="noopener nofollow">#Robotics</a>.
<br><img alt="center" src="lib/media/pasted-image-20241101104829.png" style="width: 400px; max-width: 100%;"><br><br>Installation<br>
<a rel="noopener nofollow" class="external-link" href="http://wiki.ros.org/Installation/Ubuntu" target="_blank">http://wiki.ros.org/Installation/Ubuntu</a><br>Add to *.bashrc, see <a data-href="ROS - Workspaces" href="life,-universe-and-everything/ros/ros-workspaces.html" class="internal-link" target="_self" rel="noopener nofollow">ROS - Workspaces</a>, <a data-href="ROS - Creating Nodes and Packages" href="life,-universe-and-everything/ros/ros-creating-nodes-and-packages.html" class="internal-link" target="_self" rel="noopener nofollow">ROS - Creating Nodes and Packages</a><br>source /opt/ros/humble/setup.bash
source /usr/share/colcon_argcomplete/hook/colcon-argcomplete.bash
source ~/&lt;ws-name&gt;/install/setup.bash
<br><br><br><br>Source setup files in every new shell you open to initialize the environment. This will load the default <a data-tooltip-position="top" aria-label="ROS - Workspaces" data-href="ROS - Workspaces" href="life,-universe-and-everything/ros/ros-workspaces.html" class="internal-link" target="_self" rel="noopener nofollow">workspace</a> based on the installed distribution.<br># this assumes default installation directory
source /opt/ros/humble/setup.bash

source /opt/ros/humble/setup.zsh
<br>To check if this was successful, the command <br>echo $ROS_DISTRO
<br>should print the name of the installed ROS distro.<br>Check environment variables <br>printenv | grep -i ROS
<br><br><br><br>Info
ROS nodes are basically any program that also have access to the ROS functionalities. They are independent single-purpose processes that perform computations and communicate with each other.
<br><img alt="center" src="lib/media/pasted-image-20241101104933.png" style="width: 300px; max-width: 100%;"><br>Every node in ROS is part of a package, to list them all see<br>ros2 pkg list
<br>To start a specific node in a specific package run <br>ros run &lt;package-name&gt; &lt;node-name&gt; 
<br>To verify a node is running, we can list all running nodes via <br>ros2 node list
<br>and access additional information by then using <br>ros2 node info /&lt;name&gt;
<br>We can also <a data-tooltip-position="top" aria-label="ROS - Workspaces" data-href="ROS - Workspaces" href="life,-universe-and-everything/ros/ros-workspaces.html" class="internal-link" target="_self" rel="noopener nofollow">create custom packages with custom nodes</a>.<br>File- vs. Node- vs. Package Name
The name of the node does not have to be the same as the file in which the code is written. The name we use to start a node via the command line is determined by the argument we pass to the constructor of the node base class.<br>
The name of the package is the path at which we build it using colcon.
<br>Print Node <a data-tooltip-position="top" aria-label="Graph" data-href="Graph" href="the-guide/mathematics/graph-theory/graph.html" class="internal-link" target="_self" rel="noopener nofollow">Graph</a><br>rqt_graph
<br><br><br>Info
Messages define the data structure sent between nodes, e.g. sensor readings, actuator commands, or position data. There are pre-defined message types, but custom types can be created in .msg files.
Topics are named buses over which nodes exchange these messages. Each topic has a specific purpose, like conveying sensor data or commands to the robot's actuators.
Nodes can publish messages to a topic, and other nodes can subscribe to that topic to receive the messages.
<br><img alt="center" src="lib/media/pasted-image-20241101105505.png" style="width: 350px; max-width: 100%;"><br>We can list all topics using <br>ros2 topics list
<br>and access additional information such as number of publishers and subscribers as well as the message type via <br>ros2 topic info &lt;topic-name&gt;
<br>To inspect a certain topic, use<br>ros2 topic echo &lt;topic-name&gt;
<br>This auto-creates an additional subscriber.<br><br><br>Info
A <a data-tooltip-position="top" aria-label="ROS - Creating a Service" data-href="ROS - Creating a Service" href="life,-universe-and-everything/ros/ros-creating-a-service.html" class="internal-link" target="_self" rel="noopener nofollow">service</a> provides a way for nodes to perform synchronous, request-response type operations. This is different to topics, as these are used for continuous data streaming.
<br>We cannot visualize services with rqt_graph, but we can list all active ones via <br>ros2 service list
<br>and get additional information about the type via<br>ros2 service type &lt;name&gt;
]]></description><link>life,-universe-and-everything/ros/ros-fundamentals.html</link><guid isPermaLink="false">Life, Universe and Everything/ROS/ROS - Fundamentals.md</guid><pubDate>Wed, 23 Apr 2025 21:44:32 GMT</pubDate><enclosure url="lib/media/pasted-image-20241101104829.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src=&quot;lib/media/pasted-image-20241101104829.png&quot;&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[ROS - Workspaces]]></title><description><![CDATA[ 
 <br><br>Based on the command-line tool colcon. In order to use auto-completion, we need to source this as well using<br>source /usr/share/colcon_argcomplete/hook/colcon-argcomplete.bash
<br>A workspace is basically just a folder. We can ...<br>
<br>Create a new workspace
<br>mkdir -p ~/ros2_ws/src # create dirs, src is for workspace
cd ~/ros2_ws
<br>
<br>Build it using Colcon
<br>colcon build # creates additional files build, install and log
<br>
<br>Source the new workspace to overlay the environment
<br>source ~/ros2_ws/install/setup.bash
<br><br><br>Method used to extend or modify an existing <a data-tooltip-position="top" aria-label="ROS - Fundamentals" data-href="ROS - Fundamentals" href="life,-universe-and-everything/ros/ros-fundamentals.html" class="internal-link" target="_self" rel="noopener nofollow">ROS</a> workspace without changing the original workspace's structure. Overlays allow developers to layer additional packages, libraries, or changes on top of a base workspace, making it easier to manage different configurations, experiment with new features, or work with customized code while keeping the original environment intact.<br>The basic use-case can be summarized exactly as above via<br>
<br>Create a new workspace
<br>mkdir -p ~/ros2_overlay/src
cd ~/ros2_overlay
<br>
<br>Build it using Colcon
<br>colcon build
<br>
<br>Source the new workspace to overlay the environment
<br>source ~/ros2_overlay/install/setup.bash
<br>We can now add or modify files in the src directory of the new workspace without affecting the underlying worspace(s) while still having access to their functionalities.<br>
To switch back, we can simply source the base workspace setup file.]]></description><link>life,-universe-and-everything/ros/ros-workspaces.html</link><guid isPermaLink="false">Life, Universe and Everything/ROS/ROS - Workspaces.md</guid><pubDate>Wed, 23 Apr 2025 21:44:32 GMT</pubDate></item><item><title><![CDATA[Corrupted bash_profile]]></title><description><![CDATA[ 
 <br> Use <br>export PATH=/usr/bin:/bin:/usr/sbin:/sbin:/usr/local/bin
<br>to gain access to basic commands again, then use nano to change.<br>
<br><a data-href="The bashrc and bash_profile Scripts" href="life,-universe-and-everything/linux-and-ubuntu/the-bashrc-and-bash_profile-scripts.html" class="internal-link" target="_self" rel="noopener nofollow">The bashrc and bash_profile Scripts</a>
]]></description><link>life,-universe-and-everything/corrupted-bash_profile.html</link><guid isPermaLink="false">Life, Universe and Everything/Corrupted bash_profile.md</guid><pubDate>Wed, 23 Apr 2025 13:49:12 GMT</pubDate></item><item><title><![CDATA[Firmware and Boot Processes - BIOS vs UEFI, MBR vs GPT]]></title><description><![CDATA[ 
 <br><br>Both used to initialize hardware and start boot process<br>
<br>UEFI 

<br>Modern and faster, replaces BIOS
<br>More secure (secure boot), has GUI
<br>Used GPT  partition scheme


<br>BIOS

<br>Legacy, text based
<br>Uses MBR partition scheme
<br>Lacks large disk support


<br><br><br>]]></description><link>life,-universe-and-everything/firmware-and-boot-processes-bios-vs-uefi,-mbr-vs-gpt.html</link><guid isPermaLink="false">Life, Universe and Everything/Firmware and Boot Processes - BIOS vs UEFI, MBR vs GPT.md</guid><pubDate>Wed, 23 Apr 2025 13:21:42 GMT</pubDate></item><item><title><![CDATA[How to Array Slicing and Indexing]]></title><description><![CDATA[ 
 <br>Examples are for <a data-tooltip-position="top" aria-label="https://numpy.org/devdocs/index.html" rel="noopener nofollow" class="external-link" href="https://numpy.org/devdocs/index.html" target="_blank">Numpy</a>, also works for <a data-tooltip-position="top" aria-label="- JAX -" data-href="- JAX -" href="life,-universe-and-everything/jax/-jax-.html" class="internal-link" target="_self" rel="noopener nofollow">JAX</a> and <a data-tooltip-position="top" aria-label="- PyTorch -" data-href="- PyTorch -" href="life,-universe-and-everything/pytorch/-pytorch-.html" class="internal-link" target="_self" rel="noopener nofollow">PyTorch</a>.<br>arr = np.ones((10,10,3)) # [batch x samples x dim]
<br><br><br><br>Extract all dimensions of samples  in batches .<br>test = arr[1:5,1:5,:] # (4,4,3)
<br><br>Extract  component of all samples in all batches.<br>test = arr[...,1] # (10,10)
test = arr[...,[1]] # (10,10,1)
<br><br><br><br>Divide each row of a matrix by an element given by the respective element in a vector.<br>A = jnp.array([[2.0, 4.0, 6.0],
			   [8.0, 10.0, 12.0], 
			   [14.0, 16.0, 18.0]]) 
v = jnp.array([2.0, 4.0, 6.0]) 
# Divide each row of A by the corresponding element in v 
result = A / v[:, None] 
# v[:, None] reshapes v to (m, 1) for broadcasting
<br>
<br>Same can be done for multiplication by *
]]></description><link>life,-universe-and-everything/how-to-array-slicing-and-indexing.html</link><guid isPermaLink="false">Life, Universe and Everything/How to Array Slicing and Indexing.md</guid><pubDate>Thu, 10 Apr 2025 16:39:49 GMT</pubDate></item><item><title><![CDATA[Hydra]]></title><description><![CDATA[ 
 <br>In a Nutshell
Configuration management framework, allows advanced configuration file management via composition, overrides etc.
<br><img alt="center" src="lib/media/pasted-image-20240805194420.png" style="width: 200px; max-width: 100%;"><br><br>pip install hydra-core --upgrade
<br>import hydra
import omegaconf
<br>To configure the main entry point of the <a data-tooltip-position="top" aria-label="- Python Programming Language -" data-href="- Python Programming Language -" href="life,-universe-and-everything/python/-python-programming-language-.html" class="internal-link" target="_self" rel="noopener nofollow">python</a> script to use hydra we assume the directory structure<br>my_project/
├── conf/
│   └── config.yaml
└── script.py

<br>and use the line<br>@hydra.main(version_base=None, config_path="conf", config_name="config")
<br>
<br>version_base=None

<br>Specify hydra version to use, in most cases not relevant


<br>config_path="conf"

<br>Directory to look for config files, will only find it if in same level as script


<br>config_name="config"

<br>Tells Hydra to look for a file named config.yaml (other formats supported) in the directory specified above


<br>After that, the function has to accept the config as an input parameter <br>def main(cfg: omegaconf.DictConfig):
	# ...
<br><br><br>Combine configuration files from multiple sources with command-line arguments. You can override any parameter given in a file via the command line.<br># conf/config.yaml
app:
  name: my_app
  version: 1.0
<br># conf/db.yaml
database:
  driver: mysql
  user: root
  password: secret

<br>Running the initial config file via @hydra... and adding another one via the command line will yield a combined configuration, with priority for the command line. This also enables us to override specific parameters<br>python example.py db=conf/db.yaml database.user=admin
<br><br>Hydra facilitates running multiple configurations in parallel or in sequence, useful for hyperparameter tuning and experimentation.<br># conf/config.yaml
batch_size: 32
learning_rate: 0.001
<br>python example.py -m batch_size=16,32 learning_rate=0.001,0.01
]]></description><link>life,-universe-and-everything/hydra.html</link><guid isPermaLink="false">Life, Universe and Everything/Hydra.md</guid><pubDate>Thu, 10 Apr 2025 16:27:50 GMT</pubDate><enclosure url="lib/media/pasted-image-20240805194420.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src=&quot;lib/media/pasted-image-20240805194420.png&quot;&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[Jupyter Notebook Anomalies]]></title><description><![CDATA[ 
 <br>Imports
Jupyter Notebooks are not considered part of a package, the normal <a data-tooltip-position="top" aria-label="Modules, Packages and Imports in Python" data-href="Modules, Packages and Imports in Python" href="life,-universe-and-everything/python/modules,-packages-and-imports-in-python.html" class="internal-link" target="_self" rel="noopener nofollow">import</a> structure does not work. Instead, use sys or absolute imports
]]></description><link>life,-universe-and-everything/jupyter-notebook-anomalies.html</link><guid isPermaLink="false">Life, Universe and Everything/Jupyter Notebook Anomalies.md</guid><pubDate>Wed, 23 Apr 2025 14:02:59 GMT</pubDate></item><item><title><![CDATA[PlantUML]]></title><description><![CDATA[ 
 <br>In a Nutshell
Tool to create UML diagrams. 
<br><br><br><img src="https://www.plantuml.com/plantuml/png/JKv12i8m4Bm7yeS53ohY1tfjKoYqAhKVI9CL0saRsgGeujzDYAtkOPcTsPbTIeJcqmHfw2VU8Rme0Ly7CAxLIbHexuFSxIDx13EgcLlC5MgPGFCq69MDLmQBmg9SB0dKQDtOnwKt9PHieUWjZZSk4DY3TqPxHQrK5i9R_VU4z0o6g7bGSWrXlUj-VWnJxb27GulrmfhwN4QS5UpAlfYLB6tg8lLP7m00" usemap="#JKv12i8m4Bm7yeS53ohY1tfjKoYqAhKVI9CL0saRsgGeujzDYAtkOPcTsPbTIeJcqmHfw2VU8Rme0Ly7CAxLIbHexuFSxIDx13EgcLlC5MgPGFCq69MDLmQBmg9SB0dKQDtOnwKt9PHieUWjZZSk4DY3TqPxHQrK5i9R_VU4z0o6g7bGSWrXlUj-VWnJxb27GulrmfhwN4QS5UpAlfYLB6tg8lLP7m00"><br><br><br><img src="https://www.plantuml.com/plantuml/png/ZLHDZzem4Br7odyOSLaePDLBBwXTxMx8IaXKhHRK7f5nPXDBmOviXrsqxN-lFnAS05NB0H9cF6_clJUEetkbYTJDlegZ6tWaYb58Aw8KBF25SQQPu0fUcIxXYM6LAo0yXw-eIv6hE8eZwf9J8k4zZi1y4dXfi5hX0QivCAxX2tpww4DJS60PBnZ7yMGE1y7o41BrUUHtGF2X05ABLvGEe2l02jFkrR9d1MzWoKkKJ1DE4R8tigyhZ2EoKreIgi43_i2IqUg4E8MzEFnFzRRT2gcMZFQ8cG8jYIpmaMYDyZWure5z_fKaWxDsGjUh8_iAtLF89JlWnOWFHI6n85Q_RaGOstCbwSYSTQHImW7VcD86BCSQpHVNrN4IH_Q0i2tvn3cCx772HmRSegHYNml5Bb6eu9emOvsV9MAbPkslXU92jsZMLgBHTfn9DvhB2yD9eOaUJcVBVp0z-KpJKeSryfDdlDa7LT6-FHb7RexrISplsURvsye_BvRPEckVirMMRfwNwOb5lmhE_gbP12qRgeKCrVygTW8xBtDOX6waxTRkrNOxliYO38roeUcYrOOU0zbzRA-T0YD6Dx4fsS-ulRqBP1ZgmecMlBNf28KsOTKJDB2WJl6JOgbjbl5YhRtRtWVBtaN7b9XrxVLoPPF-dJkGvHahXwXALgivF2WbA7F_pFhsdxahg-TKRw4akUlPsbqdcCFkE1eoDdH_ez3Qx7naLskmjICleKs-DmoR-_KF" usemap="#ZLHDZzem4Br7odyOSLaePDLBBwXTxMx8IaXKhHRK7f5nPXDBmOviXrsqxN-lFnAS05NB0H9cF6_clJUEetkbYTJDlegZ6tWaYb58Aw8KBF25SQQPu0fUcIxXYM6LAo0yXw-eIv6hE8eZwf9J8k4zZi1y4dXfi5hX0QivCAxX2tpww4DJS60PBnZ7yMGE1y7o41BrUUHtGF2X05ABLvGEe2l02jFkrR9d1MzWoKkKJ1DE4R8tigyhZ2EoKreIgi43_i2IqUg4E8MzEFnFzRRT2gcMZFQ8cG8jYIpmaMYDyZWure5z_fKaWxDsGjUh8_iAtLF89JlWnOWFHI6n85Q_RaGOstCbwSYSTQHImW7VcD86BCSQpHVNrN4IH_Q0i2tvn3cCx772HmRSegHYNml5Bb6eu9emOvsV9MAbPkslXU92jsZMLgBHTfn9DvhB2yD9eOaUJcVBVp0z-KpJKeSryfDdlDa7LT6-FHb7RexrISplsURvsye_BvRPEckVirMMRfwNwOb5lmhE_gbP12qRgeKCrVygTW8xBtDOX6waxTRkrNOxliYO38roeUcYrOOU0zbzRA-T0YD6Dx4fsS-ulRqBP1ZgmecMlBNf28KsOTKJDB2WJl6JOgbjbl5YhRtRtWVBtaN7b9XrxVLoPPF-dJkGvHahXwXALgivF2WbA7F_pFhsdxahg-TKRw4akUlPsbqdcCFkE1eoDdH_ez3Qx7naLskmjICleKs-DmoR-_KF"><br><br><img src="https://www.plantuml.com/plantuml/png/NOxF2i8m38VlXRv3s4Lz3XdUPA1uZstM1Rjsj9q8uhiRguWkf_pvmk_JQWKTcWiwWlQ6u-G9rhiKvuc3WqGU2ram7dEc31m0ONhZ6wq-7JGd6jXuoWtSjGAf-W2j9rCIcosct_Lo730vIJiKB1o3LW-jj6ZjOv78hrINufKj0LhBBmwzwFGnZVLBR8BmaDWD1MA_p3uEBFfPz2k6vpYNVuQ2hRSd" usemap="#NOxF2i8m38VlXRv3s4Lz3XdUPA1uZstM1Rjsj9q8uhiRguWkf_pvmk_JQWKTcWiwWlQ6u-G9rhiKvuc3WqGU2ram7dEc31m0ONhZ6wq-7JGd6jXuoWtSjGAf-W2j9rCIcosct_Lo730vIJiKB1o3LW-jj6ZjOv78hrINufKj0LhBBmwzwFGnZVLBR8BmaDWD1MA_p3uEBFfPz2k6vpYNVuQ2hRSd"><br><br><br>Connections are used for instances rather than general classes.<br><img src="https://www.plantuml.com/plantuml/png/u-LIL_1FoafDBb5moKnCBqhCLN2jICmjo4bLqF3CAov9p4jEBLR8JrD8paaiBaujrkJboeSeJquiCbIevkLI08AqqjGSdzIor1m5MmLpKrwkMcHLHjXK6Hc0LV42hNVEpyi3AitCIrU0Q2hcvG8RhgiBqMwbe9GCKPEQeb2IhmG0" usemap="#u-LIL_1FoafDBb5moKnCBqhCLN2jICmjo4bLqF3CAov9p4jEBLR8JrD8paaiBaujrkJboeSeJquiCbIevkLI08AqqjGSdzIor1m5MmLpKrwkMcHLHjXK6Hc0LV42hNVEpyi3AitCIrU0Q2hcvG8RhgiBqMwbe9GCKPEQeb2IhmG0">]]></description><link>life,-universe-and-everything/plantuml.html</link><guid isPermaLink="false">Life, Universe and Everything/PlantUML.md</guid><pubDate>Sun, 13 Apr 2025 19:26:11 GMT</pubDate><enclosure url="https://www.plantuml.com/plantuml/png/JKv12i8m4Bm7yeS53ohY1tfjKoYqAhKVI9CL0saRsgGeujzDYAtkOPcTsPbTIeJcqmHfw2VU8Rme0Ly7CAxLIbHexuFSxIDx13EgcLlC5MgPGFCq69MDLmQBmg9SB0dKQDtOnwKt9PHieUWjZZSk4DY3TqPxHQrK5i9R_VU4z0o6g7bGSWrXlUj-VWnJxb27GulrmfhwN4QS5UpAlfYLB6tg8lLP7m00" length="0" type="false"/><content:encoded>&lt;figure&gt;&lt;img src=&quot;https://www.plantuml.com/plantuml/png/JKv12i8m4Bm7yeS53ohY1tfjKoYqAhKVI9CL0saRsgGeujzDYAtkOPcTsPbTIeJcqmHfw2VU8Rme0Ly7CAxLIbHexuFSxIDx13EgcLlC5MgPGFCq69MDLmQBmg9SB0dKQDtOnwKt9PHieUWjZZSk4DY3TqPxHQrK5i9R_VU4z0o6g7bGSWrXlUj-VWnJxb27GulrmfhwN4QS5UpAlfYLB6tg8lLP7m00&quot;&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[ROS uses different Python Interpreter]]></title><description><![CDATA[ 
 <br>Local ros2 environment uses different python version<br>import sys
print(sys.executable)
<br>gives<br>/usr/bin/python3
<br>but <br>which python
<br>gives<br>/home/m50046089/miniconda3/bin/python
<br>
<br>Could add to python path, but that would also be necessary for all modules that noahr imports
export PYTHONPATH=$PYTHONPATH:/path/to/noahr/package


]]></description><link>life,-universe-and-everything/ros-uses-different-python-interpreter.html</link><guid isPermaLink="false">Life, Universe and Everything/ROS uses different Python Interpreter.md</guid><pubDate>Wed, 23 Apr 2025 13:56:17 GMT</pubDate></item><item><title><![CDATA[tmux]]></title><description><![CDATA[ 
 <br>In a Nutshell
Terminal multiplexer, allows to control a collection of terminals from a single screen. Each collection of pseudo terminals (session) can continue to run in the background, even when e.g. disconnecting from a <a data-tooltip-position="top" aria-label="SSH - Secure Shell" data-href="SSH - Secure Shell" href="the-guide/computer-science/networks/ssh-secure-shell.html" class="internal-link" target="_self" rel="noopener nofollow">ssh</a> session.
<br><br><br>tmux sessions are managed by a single server, which exits after all sessions are killed. Each session is displayed on screen via a client connecting to the server. Both are separate <a data-tooltip-position="top" aria-label="Threads vs. Processes" data-href="Threads vs. Processes" href="the-guide/computer-science/parallel-computing/threads-vs.-processes.html" class="internal-link" target="_self" rel="noopener nofollow">processes</a> communicating via a <a data-tooltip-position="top" aria-label="Network Socket" data-href="Network Socket" href="the-guide/computer-science/networks/network-socket.html" class="internal-link" target="_self" rel="noopener nofollow">socket</a> in the <a data-tooltip-position="top" aria-label="Linux File and Directory Management > The Linux File System" data-href="Linux File and Directory Management#The Linux File System" href="life,-universe-and-everything/linux-and-ubuntu/linux-file-and-directory-management.html#The_Linux_File_System" class="internal-link" target="_self" rel="noopener nofollow">directory</a> /tmp.<br>Starting a session<br>tmux new-session -s session_name
# use flag -d to create in detached mode
<br>Attaching/Detaching to/from a session<br>tmux attach-session -t session_name    # -t to identify by name
<br>To detach, use prefix key Ctrl-b and then press d.<br>List all sessions<br>tmux ls
<br>Kill a session<br>tmux kill-session -t session_name       # -t to identify by name
<br><br><br>Windows are like separate tabs within a tmux session, comparable to tabs in a browser. Panes divide a single further into subareas.<br>Create/Kill a window<br>
When outside a session, use e.g.<br>tmux new-window 'htop'
<br>to create a new window running the htop process viewer. When inside a session, use prefix keys:<br><br><br><br>]]></description><link>life,-universe-and-everything/tmux.html</link><guid isPermaLink="false">Life, Universe and Everything/tmux.md</guid><pubDate>Wed, 23 Apr 2025 14:02:55 GMT</pubDate></item><item><title><![CDATA[Drawing-A2C]]></title><description><![CDATA[ 
 Criticfitcompute AdvantageActoroptimize using PGT loss with advantage functionsample trajectorychunks]]></description><link>miscellaneous/attachments/drawings/drawing-a2c.excalidraw.html</link><guid isPermaLink="false">Miscellaneous/Attachments/Drawings/Drawing-A2C.excalidraw.md</guid><pubDate>Thu, 24 Apr 2025 13:28:50 GMT</pubDate></item><item><title><![CDATA[Drawing-Binormale]]></title><description><![CDATA[ 
 Frame]]></description><link>miscellaneous/attachments/drawings/drawing-binormale.excalidraw.html</link><guid isPermaLink="false">Miscellaneous/Attachments/Drawings/Drawing-Binormale.excalidraw.md</guid><pubDate>Thu, 24 Apr 2025 13:28:50 GMT</pubDate></item><item><title><![CDATA[Drawing-Flächenkurve]]></title><description><![CDATA[ 
 Frame]]></description><link>miscellaneous/attachments/drawings/drawing-flächenkurve.excalidraw.html</link><guid isPermaLink="false">Miscellaneous/Attachments/Drawings/Drawing-Flächenkurve.excalidraw.md</guid><pubDate>Thu, 24 Apr 2025 13:28:51 GMT</pubDate></item><item><title><![CDATA[Drawing-Kurven]]></title><description><![CDATA[ 
 UP?:=]]></description><link>miscellaneous/attachments/drawings/drawing-kurven.excalidraw.html</link><guid isPermaLink="false">Miscellaneous/Attachments/Drawings/Drawing-Kurven.excalidraw.md</guid><pubDate>Thu, 24 Apr 2025 13:28:51 GMT</pubDate></item><item><title><![CDATA[Drawing-Raumkurven]]></title><description><![CDATA[ 
 UP?:=]]></description><link>miscellaneous/attachments/drawings/drawing-raumkurven.excalidraw.html</link><guid isPermaLink="false">Miscellaneous/Attachments/Drawings/Drawing-Raumkurven.excalidraw.md</guid><pubDate>Thu, 24 Apr 2025 13:28:52 GMT</pubDate></item><item><title><![CDATA[Drawing-SAC]]></title><description><![CDATA[ 
 ]]></description><link>miscellaneous/attachments/drawings/drawing-sac.excalidraw.html</link><guid isPermaLink="false">Miscellaneous/Attachments/Drawings/Drawing-SAC.excalidraw.md</guid><pubDate>Thu, 24 Apr 2025 13:28:52 GMT</pubDate></item><item><title><![CDATA[Drawing-Schmiegekreis]]></title><description><![CDATA[ 
 ]]></description><link>miscellaneous/attachments/drawings/drawing-schmiegekreis.excalidraw.html</link><guid isPermaLink="false">Miscellaneous/Attachments/Drawings/Drawing-Schmiegekreis.excalidraw.md</guid><pubDate>Thu, 24 Apr 2025 13:28:53 GMT</pubDate></item><item><title><![CDATA[Drawing-Schmiegeparabel]]></title><description><![CDATA[ 
 ]]></description><link>miscellaneous/attachments/drawings/drawing-schmiegeparabel.excalidraw.html</link><guid isPermaLink="false">Miscellaneous/Attachments/Drawings/Drawing-Schmiegeparabel.excalidraw.md</guid><pubDate>Thu, 24 Apr 2025 13:28:54 GMT</pubDate></item><item><title><![CDATA[Drawing-TRPO]]></title><description><![CDATA[ 
 Criticfitcompute Advantagewith GAEActor- compute Vanilla Gradient- compute natural Gradient- Line Searchcompute surrogate losswith importance Samplingsample trajectorychunks]]></description><link>miscellaneous/attachments/drawings/drawing-trpo.excalidraw.html</link><guid isPermaLink="false">Miscellaneous/Attachments/Drawings/Drawing-TRPO.excalidraw.md</guid><pubDate>Thu, 24 Apr 2025 13:28:55 GMT</pubDate></item><item><title><![CDATA[Boundary Element Method]]></title><description><![CDATA[ 
 <br>Advantages<br>
<br>Only boundary needs to be discretized
<br>Exterior problems with unbounded domains and bounded boundaries can be handled as easily as interior problems
<br>Disadvantages<br>
<br>Require explicit knowledge of fundamental solution
<br>More involved maths
<br>Naive implentation yields fully populated matrices
<br>Limited to linear PDE's in homogeneous domains
<br>Steps<br>
<br><a data-tooltip-position="top" aria-label="Spaces and Traces" data-href="Spaces and Traces" href="miscellaneous/bem/spaces-and-traces.html" class="internal-link" target="_self" rel="noopener nofollow">Mathematical Model</a>
<br><a data-href="Representation Formula" href="miscellaneous/bem/representation-formula.html" class="internal-link" target="_self" rel="noopener nofollow">Representation Formula</a>
<br><a data-href="Boundary Integral Operators" href="miscellaneous/bem/boundary-integral-operators.html" class="internal-link" target="_self" rel="noopener nofollow">Boundary Integral Operators</a>
<br><a data-href="Boundary Integral Equations" href="miscellaneous/bem/boundary-integral-equations.html" class="internal-link" target="_self" rel="noopener nofollow">Boundary Integral Equations</a>
<br><a data-tooltip-position="top" aria-label="Boundary Elements" data-href="Boundary Elements" href="miscellaneous/bem/boundary-elements.html" class="internal-link" target="_self" rel="noopener nofollow">Boundary Elements and Discrete Equations</a>
<br><a data-tooltip-position="top" aria-label="Solving the linear System in BEM" data-href="Solving the linear System in BEM" href="Solving the linear System in BEM" class="internal-link" target="_self" rel="noopener nofollow">Solution of the Linear System</a>
<br><a data-href="Interpretation of BEM" href="Interpretation of BEM" class="internal-link" target="_self" rel="noopener nofollow">Interpretation of BEM</a>
]]></description><link>miscellaneous/bem/boundary-element-method.html</link><guid isPermaLink="false">Miscellaneous/BEM/Boundary Element Method.md</guid><pubDate>Tue, 13 Jun 2023 11:08:27 GMT</pubDate></item><item><title><![CDATA[Boundary Elements]]></title><description><![CDATA[ 
 <br>FEM functions on the boundary. <br>
<br>Decomposition of boundary  into boundary elements  with barycenter  <img alt="center" src="lib/media/pasted-image-20230613115503.png" style="width: 400px; max-width: 100%;">
<br>Mesh Characterization - dimension 

<br>Mesh Size

<br>Local 
<br>Global 
<br>Minimal 


<br>Quasi-Uniform 
<br>Shape Regular


<br>Assumptions

<br>Polyhedral Boundary - no errors by geometry approx


<br>Characteristic Functions 

<br>Piecewise constant functions on 
<br>Trial space for decomposition above is 


<br><br><br>Obtain linear system of equations, as approximate solution only depends on parameters for functions above. Seek solution  for equation<br>
<br>Collocation Method

<br>Requires equation to be satisfied in points
<br>One integral (Operator V)
<br>Leads to unsymmetric system
<br>No convergence proof


<br>Galerkin Method

<br>Multiply equation with test functions and integrate
<br>Double Integral (Operator V, testing)
<br>Symmetric system
<br>Convergence proofs available


]]></description><link>miscellaneous/bem/boundary-elements.html</link><guid isPermaLink="false">Miscellaneous/BEM/Boundary Elements.md</guid><pubDate>Tue, 13 Jun 2023 10:43:07 GMT</pubDate><enclosure url="lib/media/pasted-image-20230613115503.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src=&quot;lib/media/pasted-image-20230613115503.png&quot;&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[Boundary Integral Equations]]></title><description><![CDATA[ 
 <br><a data-tooltip-position="top" aria-label="Representation Formula" data-href="Representation Formula" href="miscellaneous/bem/representation-formula.html" class="internal-link" target="_self" rel="noopener nofollow">Representation formulas</a> give solution  in interior of domain , but needs both Neumann and Dirichlet data. Obtain missing Cauchy data by using boundary integral equations.<br><br>Methods where the boundary integral equation results from the representation formula<br><br>
<br>Dirichlet Problem

<br>Interior Dirichlet boundary value problem The solution is given by the <a data-tooltip-position="top" aria-label="Representation Formula" data-href="Representation Formula" href="miscellaneous/bem/representation-formula.html" class="internal-link" target="_self" rel="noopener nofollow">representation formula</a> and reads First, we have to find  (see <a data-tooltip-position="top" aria-label="Spaces and Traces" data-href="Spaces and Traces" href="miscellaneous/bem/spaces-and-traces.html" class="internal-link" target="_self" rel="noopener nofollow">trace spaces</a>). This can be achieved using the equation 


<br>Neumann Problem

<br>Interior Neumann boundary value problem

<br>Solvability condition


<br>The solution is given by the <a data-tooltip-position="top" aria-label="Representation Formula" data-href="Representation Formula" href="miscellaneous/bem/representation-formula.html" class="internal-link" target="_self" rel="noopener nofollow">representation formula</a> and reads First, we have to find the Dirichlet datum  (see <a data-tooltip-position="top" aria-label="Spaces and Traces" data-href="Spaces and Traces" href="miscellaneous/bem/spaces-and-traces.html" class="internal-link" target="_self" rel="noopener nofollow">trace spaces</a>). This can be achieved using the equation
<br>Hypersingular operator has non-trivial kernel, no unique solution. Consider:

<br>Quotient space  
<br>Gauged form




<br>Poincare-Steklov Operator

<br>Dirichlet to Neumann map with 


<br>Exterior Problems

<br>....


<br><br><br>
<br>Add <a data-tooltip-position="top" aria-label="Representation Formula" data-href="Representation Formula" href="miscellaneous/bem/representation-formula.html" class="internal-link" target="_self" rel="noopener nofollow">representation formulas</a> of interior and exterior domains, leads to formula in terms of the jump of the Cauchy data
<br>Dirichlet 

<br>Solve interior and exterior Dirichlet problem simultaneously using  and letting  and solving 

<br>Solution does not coincide with interior or exterior solution, interior or exterior <a data-tooltip-position="top" aria-label="Spaces and Traces" data-href="Spaces and Traces" href="miscellaneous/bem/spaces-and-traces.html" class="internal-link" target="_self" rel="noopener nofollow">Dirichlet trace</a> operator yields a boundary integral equation.




<br>Neumann 

<br>Solve interior and exterior Neumann problem simultaneously using  and letting  and solving 

<br>Solution does not coincide with interior or exterior solution, interior or exterior <a data-tooltip-position="top" aria-label="Spaces and Traces" data-href="Spaces and Traces" href="miscellaneous/bem/spaces-and-traces.html" class="internal-link" target="_self" rel="noopener nofollow">Neumann trace</a> operator yields a boundary integral equation.




]]></description><link>miscellaneous/bem/boundary-integral-equations.html</link><guid isPermaLink="false">Miscellaneous/BEM/Boundary Integral Equations.md</guid><pubDate>Tue, 13 Jun 2023 13:53:21 GMT</pubDate></item><item><title><![CDATA[Boundary Integral Operators]]></title><description><![CDATA[ 
 <br>
<br>Single Layer Operator - apply <a data-tooltip-position="top" aria-label="Spaces and Traces" data-href="Spaces and Traces" href="miscellaneous/bem/spaces-and-traces.html" class="internal-link" target="_self" rel="noopener nofollow">interior trace operator</a> to <a data-tooltip-position="top" aria-label="Representation Formula" data-href="Representation Formula" href="miscellaneous/bem/representation-formula.html" class="internal-link" target="_self" rel="noopener nofollow">single layer potential</a>

<br>Integral Representation 
<br>Properties

<br>Bounded
<br>Self-Adjoint
<br>Elliptic




<br><br>
<br>Double Layer Operator - apply <a data-tooltip-position="top" aria-label="Spaces and Traces" data-href="Spaces and Traces" href="miscellaneous/bem/spaces-and-traces.html" class="internal-link" target="_self" rel="noopener nofollow">interior trace operator</a> to <a data-tooltip-position="top" aria-label="Representation Formula" data-href="Representation Formula" href="miscellaneous/bem/representation-formula.html" class="internal-link" target="_self" rel="noopener nofollow">double layer potential</a>

<br>Integral Representation
<br>Properties

<br>Bounded
<br>-adjoint to following operator




<br><br>
<br>Adjoint Double Layer Operator - apply <a data-tooltip-position="top" aria-label="Spaces and Traces" data-href="Spaces and Traces" href="miscellaneous/bem/spaces-and-traces.html" class="internal-link" target="_self" rel="noopener nofollow">interior normal derivative operator</a> to <a data-tooltip-position="top" aria-label="Representation Formula" data-href="Representation Formula" href="miscellaneous/bem/representation-formula.html" class="internal-link" target="_self" rel="noopener nofollow">single layer potential</a> 

<br>Integral Representation
<br>Properties

<br>Bounded
<br>- adjoint to ordinary double layer operator




<br><br>
<br>Hypersingular Operator - apply <a data-tooltip-position="top" aria-label="Spaces and Traces" data-href="Spaces and Traces" href="miscellaneous/bem/spaces-and-traces.html" class="internal-link" target="_self" rel="noopener nofollow">interior normal derivative operator</a> to <a data-tooltip-position="top" aria-label="Representation Formula" data-href="Representation Formula" href="miscellaneous/bem/representation-formula.html" class="internal-link" target="_self" rel="noopener nofollow">double layer potential</a> 

<br>Integral Representation
<br>Properties

<br>Self-Adjoint
<br>Elliptic




<br><br><br>
<br>Projection  of the form  obtained from system of <a data-tooltip-position="top" aria-label="Boundary Integral Equations" data-href="Boundary Integral Equations" href="miscellaneous/bem/boundary-integral-equations.html" class="internal-link" target="_self" rel="noopener nofollow">boundary integral equations</a>  leads to relations

<br>
<br>
<br>
<br>


]]></description><link>miscellaneous/bem/boundary-integral-operators.html</link><guid isPermaLink="false">Miscellaneous/BEM/Boundary Integral Operators.md</guid><pubDate>Thu, 25 May 2023 08:51:11 GMT</pubDate></item><item><title><![CDATA[Representation Formula]]></title><description><![CDATA[ 
 <br>Represent solution of PDE in the domain by the means of boundary potentials.<br><br>
<br><a data-href="Green's Identities" href="the-guide/mathematics/analysis-and-calculus/green's-identities.html" class="internal-link" target="_self" rel="noopener nofollow">Green's Identities</a>
<br>Laplace
<br><br>
<br>Single Layer Potential  

<br>Continuous map from density function  on boundary  to a harmonic function  in the domain .


<br>Double Layer Potential  

<br>Cotinuous map from density function  on boundary  to a harmonic function  in the domain .


<br><br><br>
<br>Interior<br>
- Laplace 
<br>Exterior<br>
- Laplace 
]]></description><link>miscellaneous/bem/representation-formula.html</link><guid isPermaLink="false">Miscellaneous/BEM/Representation Formula.md</guid><pubDate>Tue, 13 Jun 2023 12:26:14 GMT</pubDate></item><item><title><![CDATA[Spaces and Traces]]></title><description><![CDATA[ 
 <br><br><img alt="center" src="lib/media/pasted-image-20230525105605.png" style="width: 500px; max-width: 100%;"><br>
<br><a data-tooltip-position="top" aria-label="Inner Product Space and Hilbert Space" data-href="Inner Product Space and Hilbert Space" href="the-guide/mathematics/functional-analysis-and-calculus-of-variations/inner-product-space-and-hilbert-space.html" class="internal-link" target="_self" rel="noopener nofollow">Inner Product Space and Hilbert Space</a>  

<br>Norm 


<br>Energy Space  

<br>Norm 
<br>We write  for functions in  with vanishing trace.


<br>Space of Vector fields in  whose weak divergence is in  

<br>Norm 


<br>Space of functions in  whose weak Laplacian is in  

<br>


<br><br><br>
<br>Linear mapping that extracts suitable values from a field.
<br>Interior Dirichlet Trace 

<br>Mapping  continuous, surjective, not injectiv
<br>Exterior Dirichlet Trace 


<br>Interior Neumann Trace 

<br>With duality 


<br>Interior Normal Trace 
<br><br><br>
<br> 

<br>Norm 
<br>Image of energy space


<br> 

<br>Norm 


<br>Duality of  and  
<br>]]></description><link>miscellaneous/bem/spaces-and-traces.html</link><guid isPermaLink="false">Miscellaneous/BEM/Spaces and Traces.md</guid><pubDate>Mon, 24 Feb 2025 23:20:55 GMT</pubDate><enclosure url="lib/media/pasted-image-20230525105605.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src=&quot;lib/media/pasted-image-20230525105605.png&quot;&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[Porter's Five Forces]]></title><description><![CDATA[ 
 <br>In a Nutshell
Strategic tool to analyze the competitive environment of an industry.
<br><br>Identifies five key forces that determine attractiveness and profitability of an industry for a company:<br>Threat of New Entrants
How easy it is for new companies to enter the market and compete.

<br>Key Factors:

<br>Economies of scale
<br>Capital requirements
<br>Access to distribution channels
<br>Government regulations
<br>Brand loyalty



<br>Examples

<br>Tech Industry: High entry barriers due to capital intensity and intellectual property (e.g., semiconductor manufacturing requires billions in investment).
<br>Retail Industry: Lower barriers due to minimal capital requirements (e.g., online stores can be set up relatively cheaply).
<br>Pharmaceuticals: Regulatory requirements like FDA approvals make it hard for new entrants.

<br><br>Bargaining Power of Suppliers
The ability of suppliers to influence the price or terms of supply.

<br>Key Factors:

<br>Number of suppliers vs. buyers
<br>Uniqueness of the supplier's product or service
<br>Switching costs for buyers



<br>Examples

<br>Automotive Industry: Suppliers of unique car components, like specialized microchips, hold significant power during shortages.
<br>Fashion Industry: A surplus of fabric suppliers reduces their bargaining power.
<br>Energy Sector: Oil-producing nations in OPEC have high bargaining power due to limited global alternatives.

<br><br>Bargaining Power of Buyers
The ability of customers to negotiate lower prices or demand better quality and service.

<br>Key Factors:

<br>Number of buyers vs. suppliers
<br>Importance of the product to the buyer
<br>Switching costs for buyers



<br>Examples

<br>Retail Giants: Companies like Walmart or Amazon leverage their scale to demand lower prices from suppliers.
<br>Luxury Goods: Individual buyers of luxury brands (e.g., Rolex) have little bargaining power as the brand holds significant value.
<br>B2B Markets: A small number of buyers in the aerospace industry (e.g., Boeing and Airbus) gives them significant power over suppliers.

<br><br>Threat of Substitutes
The likelihood that customers will switch to a different product or service that meets the same need.

<br>Key Factors:

<br>Availability of substitutes
<br>Price-performance tradeoff
<br>Buyer switching costs



<br>Examples

<br>Transportation: Ride-sharing services (e.g., Uber and Lyft) threaten traditional taxis and public transit.
<br>Energy: Renewable energy (e.g., solar and wind) substitutes for fossil fuels.
<br>Entertainment: Streaming platforms like Netflix and Disney+ compete with traditional cable TV.

<br><br>Industry Rivals
The intensity of competition among existing companies in the market.

<br>Key Factors:

<br>Number of competitors
<br>Industry growth rate
<br>Product differentiation
<br>Exit barriers



<br>Examples

<br>Airline Industry: Highly competitive with price wars and minimal differentiation between carriers.
<br>Soft Drinks: Rivalry between Coke and Pepsi focuses on branding, advertising, and product innovation.
<br>Smartphones: Apple and Samsung constantly compete through technological advancements and marketing strategies

]]></description><link>miscellaneous/economics/porter&apos;s-five-forces.html</link><guid isPermaLink="false">Miscellaneous/Economics/Porter&apos;s Five Forces.md</guid><pubDate>Sun, 12 Jan 2025 12:35:54 GMT</pubDate></item><item><title><![CDATA[Das urheberrechtliche Werk]]></title><description><![CDATA[ 
 <br>
<br>Werk

<br>In Deutschland 3-gliedriger Wertbegriff (Literatur, Kunst, Wissenschaft)
<br>In anderen Ländern oft anders
<br>Nicht abschließender Katalog ("insbesondere")


<br>Urheber

<br>Schöpfer des Werkes , nur natürliche Person
<br>Miturheber

<br>Wenn mehrere ein Werk gemeinsam schaffen, ohne das sich Anteile besonders verwerten lassen (ansonsten jeder Urheber eines Teils, auhc wenn gemeinsam verwendet)
<br>Anteil an Gesamtwerk unerheblich




<br><a data-tooltip-position="top" aria-label="Inhalt des Urheberrechts" data-href="Inhalt des Urheberrechts" href="miscellaneous/law/patent-und-urheberrecht/inhalt-des-urheberrechts.html" class="internal-link" target="_self" rel="noopener nofollow">Urheberrecht</a>

<br>Nur für persönlich geistige Schöpfungen (Deutschland)

<br>Leitfrage - Wo ist die Persönlichkeit ?
<br>Nicht übertragbar (Deutschland)
<br>Bis 70 Jahre nach dem Tod, Ablauf Kalenderjahr (Persönlichkeitsschutz)


<br><a data-tooltip-position="top" aria-label="Softwarespezifisches Urheberrecht" data-href="Softwarespezifisches Urheberrecht" href="miscellaneous/law/softwarerecht/softwarespezifisches-urheberrecht.html" class="internal-link" target="_self" rel="noopener nofollow">Computerprogramme</a> gesondert, eigene geistige Schöpfung anderer Maßstab
<br>Urheberschaftsvermutung

<br>Wer auf Werk genannt ist Urheber bis Gegenteil bewiesen


<br>Amtliche Werke ausgenommen
<br>Bearbeitungen und Übersetzungen sind eigene Werke, sofern Sie mit Einwilligung des Urhebers angefertigt wurden


]]></description><link>miscellaneous/law/patent-und-urheberrecht/das-urheberrechtliche-werk.html</link><guid isPermaLink="false">Miscellaneous/Law/Patent- und Urheberrecht/Das urheberrechtliche Werk.md</guid><pubDate>Sat, 22 Jul 2023 08:36:35 GMT</pubDate></item><item><title><![CDATA[Grundlagen Urheberrecht]]></title><description><![CDATA[ 
 <br><br>
<br>Immaterialgüterrecht

<br>Teile der Rechtsordnung, die Schutz nicht materieller Güter (geistiges Eigentum) betreffen. Insbesondere

<br>Gewerblicher Rechtsschutz

<br><a data-href="Patentrecht" href="miscellaneous/law/patent-und-urheberrecht/patentrecht.html" class="internal-link" target="_self" rel="noopener nofollow">Patentrecht</a> 

<br>Schutz technischer Erfindungen, insbesondere Patentgesetz, Patentübereinkommen (EPÜ) und Verordnungen der EU


<br>Gebrauchsmustergesezt

<br>Schützt Erfinder mit Ausnahme von Verfahrenserfindungen, ergänzt Patentrecht. Erwerb durch Eintrag des Registers des Patent- und Markenamtes.


<br>Designgesetz

<br>Schutz von Design, welches neu ist und Eigenart hat.


<br>Markenrecht

<br>Teile der Rechtsordnung, die Schutz von Kennzeichen dienen. Insbesondere Markengestz und europäische Gemeinschaftsmarkenverordnung.




<br><a data-tooltip-position="top" aria-label="Inhalt des Urheberrechts" data-href="Inhalt des Urheberrechts" href="miscellaneous/law/patent-und-urheberrecht/inhalt-des-urheberrechts.html" class="internal-link" target="_self" rel="noopener nofollow">Urheberrecht</a>
<br>Persönlichkeitsrecht

<br>Teil des Grundgesetzes, offenes Rahmenrecht (Inhalt nicht abschließend umschrieben). Schutz der Privatssphäre, Ehre,

<br>Recht am eigenen Bild

<br>Nur Teilregelungen




<br>Postmortaler Persönlihckeitsschutz

<br>Eigentlich verblasst Schutz mit Zeit, kann nur von nahen Angehörigen oder Berechitgten, die der Betroffene zu Lebzeiten dazu berufen hat angefochten werden




<br>Namensschutz

<br>Absolutes Recht, unbefugtes Verwenden des Namens, die zu Zuordnungsverwirrung (Dritter, der Namen verwendet erzeugt Verwirrung über Identität der Person, auch ohne wirkich Verwechslung) Verletzung schutzwürdiger Interessen zur Folge hat führt zu Namensanmaßung.






<br>Halbleiterschutzgesetzt

<br>Schutz von dreidimensionalen Strukturen mikroelektrischer Halbleitererzeugnisse


<br>Sortenschutzrecht

<br>Schutz von Pflanzensorte, wenn unterscheidbar, homogen, beständig, neu und mit Sortenbezeichnung.


<br>Gesetz gegen unlauteren Wettbewerb

<br>Schutz des Interesses der Allgemeinheit an unverfälschtem Wettbewerb.


<br>Kunsturhebergesetz

<br>Mit UrhG außer Kraft, nur Bildnisschutz übernommen.


<br><br><br>
<br>Person der Zeitgeschichte

<br>Personen im Blick der Öffentlichkeit. Bildnisse dürfen ohne Einwilligung verbreitet und zur Schau gestellt werden.


<br>Informationsinteresse der Öffentlichkeit an Straftaten

<br>Straftaten gehören zum Zeitgeschehen, Täter muss Befrieidgung des Informationsinteresses der Öffentlichkeit dulden, auhc wenn diese in seinen Persönlihckeitsbereich eindringen.


<br>Gerechtigkeitsprinzip der Priorität

<br>Wer zuerst Name einer Domain registriert hat Anrecht auf diese.


<br>Verwertungsgesellschaft

<br>Meist Verein oder Gmbh, nimmt Nutzungs, Einwilligungs und Vergütungsansprüche welche sich aus UrhG ergeben für mehrere Urheber wahr, bspw. GEMA, VG Wort
<br>Bestimmte Vergütungsansprüche könen nur durch Verwertungsgesellschaft geltend gemacht werden


]]></description><link>miscellaneous/law/patent-und-urheberrecht/grundlagen-urheberrecht.html</link><guid isPermaLink="false">Miscellaneous/Law/Patent- und Urheberrecht/Grundlagen Urheberrecht.md</guid><pubDate>Thu, 20 Jul 2023 07:31:15 GMT</pubDate></item><item><title><![CDATA[Inhalt des Urheberrechts]]></title><description><![CDATA[ 
 <br>
<br>Urheberpersönlichkeitsrecht

<br>§11 S. 1 UrhG schützt Urheber in pers. + geistiger beziehung zum werk und dessen Nutzung
<br>§11 S. 2 UrhG Sicherung Vergütung


<br>Vermietrecht

<br>Entgeldliche Gebrauchsüberlassung, ausschließliches Recht des Urhebers
<br>Verleihen im Gegensatz ohen Entgeld


<br>§12 UrhG - Veröffentlichungsrecht

<br>Urheber entscheidet über Erstveröffentlichung
<br>Benutzung ist Berechtigungsfrei
<br>Urheberrecht greift auch im privaten ( kein gewerblicher Rechtsschutz)


<br>§13 UrhG - Annerkennung der Urheberschaft

<br>Urheber hat Recht auf Anerkennung (Namensnennungsrecht)


<br>§14 UrhG - Entstellungsschutz

<br>Oft Konflikt mit §903 BGB Eigentum


<br>§ 15 UrhG Verwertung

<br>Absatz 1 - Körperliche Werkverwertung

<br>Urheber hat ausschließliches Recht sein Werk körperlich zu verwerten, insbesondere
<br>§16 Abs. 1 UrhG - Vervielfältigung 
<br>§17 Abs. 1 UrhG - Verbreitungsrecht 

<br>Film auf Leinwand nicht, DVD wird zwischengespeichert, daher körperlich
<br>Beschränkt auf Erstverbreitung -  Erschöpfungsgrundsatz

<br>Mit Erstverbreitung hat Rechtsinhaber seinen Verwertungsansprich erschöpft, danach keine Einwilligung mehr (Ausnahme Vermieten)




<br>§18 Ausstellungsrecht 


<br>Absatz 2 - Unkörperliche Werkverwertung

<br>Urheber hat ausschließliches Recht sein Werk in unkörperlicher Form öffentlich widerzugeben (öffentlich = keine persönliche Beziehung), insbesondere
<br>Vortragen, Aufführen, Vorführen
<br>§19 a drahtlose Zugänglichmachung für Öffentlichkeit an Ort ihrer Wahl

<br>Z.B. Streaming, braucht Zustimmung
<br>Kein Erschöpfungsgrundsatz = interessant für Anbieter


<br>§20 UrhG - Senderecht

<br>Sendemast, nicht auf Anfrage  keine Zustimmung






<br>§28 Abs. 1 UrhG - Urheberrecht is vererbbar
<br>§29 Abs. 1 UrhG - Urheberrecht unübertragbar und unverzichtbar

<br>Ausnahme Testament, Einräumung Nutzungsrecht


<br>§31 Abs. 1,2 UhrG - Einfaches Nutzungsrecht

<br>Berechtigt den Inhaber, das Werk auf die erlaubte Art zu nutzen, ohne dass eine Nutzung durch andere ausgeschlossen ist.


<br>§31 Abs. 1,3 UhrG - Ausschließliches Nutzungsrecht

<br>Berechtigt den Inhaber, das Werk unter Ausschluss aller anderen Personen auf die erlaubte Art zu nutzen und Nutzungsrechte einzuräumen.


<br>§31 Abs. 5 Zweckübertragungslehre

<br>Im Zweifel über Art des utzungsrechts bestimmt Vertrag
<br>Im-Zweifel-Regel Tendenz des UrhG so weit wie möglich beim Urheber zu bleiben


<br>§32 a UrhG Fairness / Bestsellerregelung

<br>Wenn nach Einräumung der Verwertung das Werk unverhältnismäßig mehr profit etc generiert, ist aus Verlanegen des Urhebers der Vertrag neu auszulegen


<br>§44 a - 63 a UrhG Schranken des Urheberrechts

<br>§ 44 a Grundsätzlich Dauer der Vervielfältigung egal, problematisch bei digitaler Technik, hier extra geregelt für Zwischenspeicherung bei techn. Verfahren


<br>§ 51 UrhG - Zitatrecht

<br>Werk darf zitiert werden, Quellenangabe muss beachtet werden
<br>Zitat muss sachgerechten Umfang haben


<br>§53 UrhG - Privatkopie

<br>Vervielfältigungen zum privaten gebrauch zustimmungsfrei


<br>§53 Abs. 1 + 3 UrhG 

<br>Vergütungspflicht

<br>Herstellerabgabe




<br>§ 63 UrhG - Quellenangabe

<br>Pflicht zur Quellenangabe bei Zitation


<br>§ 87 a ff. UrhG - Datenbankschutz

<br>Investitionsschutz, pers. geist. Schöpfung muss aus Auswahl der Elemente und deren verknüpfung und Abfragemöglichkeit hervorgehen


<br>§95 a Schutz technischer Maßnahmen
<br>§ 98 Abs. 1 UrhG - Vernichtung gewährt dem Rechtsinhaber ein Vernichtungsrecht gegenüber allen rechtswidrig hergestellten, verbreiteten oder zur rechtswidrigen Verbreitung bestimmten Vervielfältigungsstücken einräumt. Der Vernichtungsanspruch setzt kein Verschulden des Anspruchsgegners voraus.
]]></description><link>miscellaneous/law/patent-und-urheberrecht/inhalt-des-urheberrechts.html</link><guid isPermaLink="false">Miscellaneous/Law/Patent- und Urheberrecht/Inhalt des Urheberrechts.md</guid><pubDate>Tue, 25 Jul 2023 07:26:48 GMT</pubDate></item><item><title><![CDATA[Internationales Urheberrecht]]></title><description><![CDATA[ 
 <br>
<br>Territorialitätsprinzip

<br>Nationale Immaterialgüterrechte bleiben auf Staatsgebiet begrenzt
<br>Gegensatz zu Universalitätsprinzip


<br>Schutzlandprinzip

<br>Beurteilung von Urheberrechtsverletzungen nach Recth des Landes, in dem Schutz in Anspruch genommen wird (nicht wo Schutz gesucht wird).


<br>Verträge / Abkommen

<br>revidierte Berner Übereinkunft

<br>Bildung von Staatenverband
<br>Inländerbehandlung, Recht für fremde Staatsbürger gültig als wäre er Bürger
<br>Länder verschiedenen Versionen beigetreten, e.g. Russland, USA
<br>Copyright Logo 


<br>TRIPS  WPÜ

<br>rBÜ, erweitert um moderne Probleme
<br>Inländerbehandlung, jedes Land räumt Bürgern von Mitgliedern chtz von Eigentum ein, der dem von eigenen Bürgern gleich kommt


<br>Rom 1-VO

<br>Regelt welches Recht für internationale Verträge anwendbar


<br>Rom 2-VO

<br><a data-tooltip-position="top" aria-label="Inhalt des Urheberrechts" data-href="Inhalt des Urheberrechts" href="miscellaneous/law/patent-und-urheberrecht/inhalt-des-urheberrechts.html" class="internal-link" target="_self" rel="noopener nofollow">UrhG</a>-Verletzungen


<br>EugV-VO


]]></description><link>miscellaneous/law/patent-und-urheberrecht/internationales-urheberrecht.html</link><guid isPermaLink="false">Miscellaneous/Law/Patent- und Urheberrecht/Internationales Urheberrecht.md</guid><pubDate>Sat, 22 Jul 2023 08:47:48 GMT</pubDate></item><item><title><![CDATA[Patent- und Urheberrecht Notizen]]></title><description><![CDATA[ 
 <br>Vorlesung 1 - <a data-tooltip-position="top" aria-label="Grundlagen Urheberrecht" data-href="Grundlagen Urheberrecht" href="miscellaneous/law/patent-und-urheberrecht/grundlagen-urheberrecht.html" class="internal-link" target="_self" rel="noopener nofollow">Einführung</a><br>
<br>
<br><br>Vorlesung 2 - <a data-tooltip-position="top" aria-label="Grundlagen Urheberrecht" data-href="Grundlagen Urheberrecht" href="miscellaneous/law/patent-und-urheberrecht/grundlagen-urheberrecht.html" class="internal-link" target="_self" rel="noopener nofollow">Allgemeines</a><br>
<br>...
<br><br>Vorlesung 3 - <a data-href="Das urheberrechtliche Werk" href="miscellaneous/law/patent-und-urheberrecht/das-urheberrechtliche-werk.html" class="internal-link" target="_self" rel="noopener nofollow">Das urheberrechtliche Werk</a><br>
<br>In Deutschland 3-gliedriger Wertbegriff (Literatur, Kunst, Wissenschaft)

<br>In anderen Ländern oft anders
<br>Nicht abschließender Katalog ("insbesondere")


<br>Warum Urheberrecht so lange Laufzeit (70 Jahre nach Tod)?

<br>Persöhnlichkeitsschutz


<br>Leitfrage - Wo ist die Persönlichkeit ?

<br>Nicht Inhalt, Funktion, sondern Individualität geschützt
<br>Alles was durch Funktionalität vorgegeben fällt aus Urheberrecht


<br>In Deutschland Urheberrecht nicht übertragbar

<br>In anderen Ländern (z.B. USA/GB) schon
<br>Verwaltungsrechte übertragbar


<br>Anreger ist meist nicht Mit-Urheber
<br><br>Vorlesung 4 - <a data-href="Inhalt des Urheberrechts" href="miscellaneous/law/patent-und-urheberrecht/inhalt-des-urheberrechts.html" class="internal-link" target="_self" rel="noopener nofollow">Inhalt des Urheberrechts</a> 1<br>
<br>
Urheber entscheidet über Erstveröffentlichung

<br>
Urheberrechtlich ist Benutzung Berechtigungsfrei

<br>
Urheberrecht kein gewerlicher Rechtsschutz, greift auch im Privaten

<br>
§13 UrhG 

<br>Urheber hat Recht auf Anerkennung (Namensnennungsrecht)


<br>
§14 UrhG - Entstellungsschutz

<br>Oft Konflikt mit §903 BGB Eigentum


<br>
§15 UrhG - Abs. 1- Befugnisse der Werknutzung

<br>Urheber hat Recht sein Werk körperlich zu verwerten
<br>Körperliche Verwertung insbesondere / nicht abschließend

<br>Vervielfältigung §16
<br>Verbreitungsrecht §17
<br>Ausstellungsrecht §18


<br>Urheber hat ausschließlich Recht Werk in unkörperlicher Form öffentlich wiederzugeben
<br>Unkörperlicher Verwertung
<br>Widergabe öffentlich, wenn für Mehrzahl von mitgliedern der Öffentlichkeit bestimmt
<br>Was ist Öffentlich ?

<br>Keine persönliche Beziehung




<br>
§16 UrhG - Vervielfältigung

<br>...


<br>
§17 UrhG - Abs. 1 - Verbreitung

<br>Zwingend körperlich

<br>Film auf Leinwand nicht körperlich
<br>Film (Streaming/DVD) wird zwingend zwischengespeichert, daher ebenfalls körperlich


<br>Beschränkt auf Erstverbreitung - Erschöpfungsgrundsatz Abs. 2 (körperlich)


<br>
§20 Senderecht

<br>Sendemast, nicht auf Anfrage  keine Zustimmung


<br>
§19a Veröffentlichung

<br>Z.B. Streaming, braucht Zustimmung


<br><br>Vorlesung 5 - <a data-href="Inhalt des Urheberrechts" href="miscellaneous/law/patent-und-urheberrecht/inhalt-des-urheberrechts.html" class="internal-link" target="_self" rel="noopener nofollow">Inhalt des Urheberrechts</a> 2<br>
<br>§15 Abs. 1 vs. §15 Abs.2 grundverschiedene Vorschriften

<br>nicht-körperlich verlockend für Anbieter, da kein Erschöpfungsgrundsatz


<br>§15 - Abs. 2 - nicht-körperliche Werksverwertung

<br>§19a drahtlose Zugänglichmachung für Öffentlichkeit an Ort ihrer Wahl


<br><br>Vorlesung 6 - <a data-href="Inhalt des Urheberrechts" href="miscellaneous/law/patent-und-urheberrecht/inhalt-des-urheberrechts.html" class="internal-link" target="_self" rel="noopener nofollow">Inhalt des Urheberrechts</a> 3<br><br>Vorlesung 7<br>
<br>§28 Urheberrecht is vererblich, ansonsten nicht übertragbar und nicht verzichtbar (§29)

<br>Fundamentaler Unterschied zu angels. geprägtes Recht
<br>Einzelne Ausnahmen in Europa, Deutschland keine


<br>§31Urheber kann Recht auf Nutzung (evtll. mit Einschränkungen) einräumen

<br>Abs. 5 Vetragszweck wichtig, insbesondere Aufzählung sinnvoll !
<br>Im Zweifelsfall bleiben alle Rechte beim Urheber, vertraglich absichern !


<br>Internationales Urheberrecht

<br>Territorialprinzip (Gegensatz zu Universalitätsprinzip, e.g. bei Materialgüterrecht), deutsches Urgeberrecht endet mit Hoheitsgebiet

<br>Schutzlandprinzip


<br>revidierte Berner Übereinkunft

<br>Bildung von Staatenverband
<br>Inländerbehandlung, Recht für fremde Staatsbürger gültig als wäre er Bürger
<br>Länder verschiedenen Versionen beigetreten, e.g. Russland, USA
<br>Copyright Logo 


<br>TRIPS  WPÜ

<br>rBÜ, erweitert um moderne Probleme




<br><br>Vorlesung 8 <a data-href="Theorie des gewerblichen Rechtsschutzes" href="miscellaneous/law/patent-und-urheberrecht/theorie-des-gewerblichen-rechtsschutzes.html" class="internal-link" target="_self" rel="noopener nofollow">Theorie des gewerblichen Rechtsschutzes</a> <br>
<br>Allg: Immaterialgüterrecht stärken führt zwangsläufig dazu, dass Allgemeinheit etwas genommen wird
<br>Im Gegensatz zum Urheberrecht entsteht Patenrecht nicht, wird erteilt
<br>Rechtfertigung Patentschutz

<br>Anspornungstheorie, Belohnungstheorie, Vertragstheorie, Naturrechtstheorie


<br>§37 PatG Monopolrecht 20 Jahre
<br>§1a PatG isolierter Bestandteil des menschlichen Körpers (insb. gensequenz) kann eine patentierbare Erfindung sein
<br>§1 PatG (3) Keine Erfindung: Entdeckungen, Software

<br>Software als solche (?) ggfs doch Patentierbar 


<br><br>Vorlesung 9 <a data-href="Patentrecht" href="miscellaneous/law/patent-und-urheberrecht/patentrecht.html" class="internal-link" target="_self" rel="noopener nofollow">Patentrecht</a><br>
<br>Miterfinder

<br>Keine vertragliche Regelung - Bruchteilsgesellschaft

<br>§741 BGB
<br>§747 BGB


<br>Vertragliche Vereinbarung - BGB-Gesellschaft

<br>§705 BGB
<br>§719 BGB




<br>Arbeitnehmer-Erfindungs-Gesetz (öffentlicher Dienst, privat, ...)

<br>Gebundene Erfindungen / Diensterfindungen

<br>Während der Dauer des Arbeitsverhältnisses, entweder Auftrag oder privat aber maßgeblich auf Arbeiten des Betriebes basierend
<br>Kann vom Arbeitgeber in Anspruch genommen werden, muss gemeldet werden


<br>Freie Erfindung

<br>Muss ebenfalls gemeldet werden, Arbeitgeber bekommt nicht-ausschließliches Nutzungsrecht


<br>§42 Besonderheiten Hochschulen

<br>Bei Verwertung der Erfindung geht 70 Prozent der Einnahmen an diese




<br>§34 PatG 
<br>Merkblatt für Patentanmelder verfügbar
<br><br>Vorlesung 10 - Wirkung bei Patenten<br>
<br>§9 PatG pendant zu §15 UrhG

<br>Keine Unterscheidung körperlich - unkörperlich (Benutzung)
<br>Patentschutz weiterreichend als Urheberrecht
<br>§9 S.2 Nr.1 Erzeugnispatente

<br>Verbietet 6 Benutzungshandlungen

<br>Herstellen
<br>Anbieten
<br>In-Verkehr-Bringen
<br>Gebrauch
<br>Einfuhr
<br>Besitz


<br>Nicht erfaßt

<br>Beschreibung, Vorführung, Verfilmung




<br>§9 S.2 Nr.2 Verfahrenspatente 

<br>Verbietet 2 Benutzungshandlungen

<br>Anbieten des Verfahrens zur Anwendung




<br>§9 S.2 Nr.3 Verfahrenspatente die der Herstellung von Erzeugnissen dienen

<br>Verbietet Benutzungshandlungen gemäß Nr.2
<br>Zusätzliche Handlungen auf das Erzeugnis bezogen




<br>§11 PatG Erlaubte Handlungen

<br>Handlungen im privaten  PatG ist gewerblicher Rechtsschutz
<br>Handlungen zu Versuchszwecken
<br>...


<br>§13 Wirkung tritt nicht ein, wenn Regierung anordnet, dass Erfindung im Interesse der öffentlichen Wohlfahrt benutzt werden soll.
]]></description><link>miscellaneous/law/patent-und-urheberrecht/patent-und-urheberrecht-notizen.html</link><guid isPermaLink="false">Miscellaneous/Law/Patent- und Urheberrecht/Patent- und Urheberrecht Notizen.md</guid><pubDate>Fri, 21 Jul 2023 11:18:08 GMT</pubDate></item><item><title><![CDATA[Patentrecht]]></title><description><![CDATA[ 
 <br><br>
<br>Patenterteilungsanspruch

<br>Identisch zu Patenterteilungsanspruch
<br>Wenn Vorrausetzungen erfüllt ist Staat zur erteilung eines Patentes verpflichtet
<br>Im Gegensatz zum <a data-tooltip-position="top" aria-label="Inhalt des Urheberrechts" data-href="Inhalt des Urheberrechts" href="miscellaneous/law/patent-und-urheberrecht/inhalt-des-urheberrechts.html" class="internal-link" target="_self" rel="noopener nofollow">UrhG</a> wird Patentrecht erteilt


<br>Erfinder

<br>Jeder, dessen Handlungen Auswirkungen auf Enderfolg hatten


<br>Miterfinder

<br>Keine vertragliche Regelung - Bruchteilsgesellschaft

<br>§741 BGB
<br>§747 BGB


<br>Vertragliche Vereinbarung - BGB-Gesellschaft

<br>§705 BGB
<br>§719 BGB




<br>Patentanmeldung

<br>Patent muss angemeldet werden,
<br>Stempel mit Eingangsdatum entscheidend bei Doppelerfindung
<br>Prüfungsverfahren beginnt mit Anfrage


<br>Offenlegung

<br>Erfolgt 18 Monate nach Antrag, egal ob Patent erteilt wurde oder nicht


<br>Einspruch

<br>Bis 9 Monate nach Veröffentlichung möglich


<br>Arbeitnehmer-Erfindungs-Gesetz (öffentlicher Dienst, privat, ...)

<br>Gebundene Erfindungen / Diensterfindungen

<br>Während der Dauer des Arbeitsverhältnisses, entweder Auftrag oder privat aber maßgeblich auf Arbeiten des Betriebes basierend
<br>Kann vom Arbeitgeber in Anspruch genommen werden, muss gemeldet werden


<br>Freie Erfindung

<br>Muss ebenfalls gemeldet werden, Arbeitgeber bekommt nicht-ausschließliches Nutzungsrecht


<br>§42 Besonderheiten Hochschulen

<br>Bei Verwertung der Erfindung geht 70 Prozent der Einnahmen an diese




<br>Europäisches Patent

<br>Bündel nationaler Patente (25 Mitgliedsstaaten)
<br>Kosten höher als einzelne nationale Patente
<br>Patentgericht mit Sitz in Paris, Außenstellen in London und München 


<br><br><br>
<br>§1 PatG

<br>Abs. 1 Patente für <a data-tooltip-position="top" aria-label="Theorie des gewerblichen Rechtsschutzes" data-href="Theorie des gewerblichen Rechtsschutzes" href="miscellaneous/law/patent-und-urheberrecht/theorie-des-gewerblichen-rechtsschutzes.html" class="internal-link" target="_self" rel="noopener nofollow">Erfindungen</a> auf allen Gebieten der Technik erteilt, sofern sie:

<br>Neu sind

<br>Nicht dem <a data-tooltip-position="top" aria-label="Theorie des gewerblichen Rechtsschutzes" data-href="Theorie des gewerblichen Rechtsschutzes" href="miscellaneous/law/patent-und-urheberrecht/theorie-des-gewerblichen-rechtsschutzes.html" class="internal-link" target="_self" rel="noopener nofollow">Stand der Technik</a> angehören


<br>Auf einer <a data-tooltip-position="top" aria-label="Theorie des gewerblichen Rechtsschutzes" data-href="Theorie des gewerblichen Rechtsschutzes" href="miscellaneous/law/patent-und-urheberrecht/theorie-des-gewerblichen-rechtsschutzes.html" class="internal-link" target="_self" rel="noopener nofollow">erfinderischen Tätigkeit</a> beruhen
<br>Gewerblich anwendbar sind


<br>Abs. 3 Entdeckungen nicht patentierbare
<br>Isolierter Bestandteil des menschl. Körpers (insb. Gensequenz) kann patentiert werden


<br>§ 9 PatG - Alleine Patentinhaber darf Erfindung nutzen

<br>Weitreichender als <a data-tooltip-position="top" aria-label="Inhalt des Urheberrechts" data-href="Inhalt des Urheberrechts" href="miscellaneous/law/patent-und-urheberrecht/inhalt-des-urheberrechts.html" class="internal-link" target="_self" rel="noopener nofollow">UrhG</a>, keine Unterscheidung körperlich - unkörperlich
<br>S. 2 Nr. 1 - Erzeugnispatente

<br>Verbietet 6 Benutzungshandlungen

<br>Herstellen
<br>Anbieten
<br>In-Verkehr-Bringen
<br>Gebrauch
<br>Einfuhr
<br>Besitz


<br>Nicht erfaßt

<br>Beschreibung, Vorführung, Verfilmung




<br>S.2 Nr. 3 - Verfahrenspatente

<br>Verbietet Benutzungshandlungen gemäß Nr.2
<br>Zusätzliche Handlungen auf das Erzeugnis bezogen




<br>§11 PatG Erlaubte Handlungen

<br>Handlungen im privaten  PatG ist gewerblicher Rechtsschutz
<br>Handlungen zu Versuchszwecken


<br>§15 Abs. 1 PatG - Rechtsübertragung

<br>Patentrecht geht auf Erben über, kann übertragen werden
<br>Gegensatz zum <a data-tooltip-position="top" aria-label="Inhalt des Urheberrechts" data-href="Inhalt des Urheberrechts" href="miscellaneous/law/patent-und-urheberrecht/inhalt-des-urheberrechts.html" class="internal-link" target="_self" rel="noopener nofollow">UrhG</a>


<br>§13 PatG - Wirkung tritt nicht ein, wenn Regierung anordnet, dass Erfindung im Interesse der öffentlichen Wohlfahrt benutzt werden soll.
<br>Beendigung des Patentschutzes

<br>§16 Ablauf nach 20 Jahren nach Tag der Anmeldung
<br>§21 Widerruf
<br>§22 Nichtigerklärung


<br>§ 139 PatG 

<br>Abs. 1 - Unterlassungsanspruch

<br>Wer entgegen §9-13 eine patentierbare Erfindung benutzt kann vom Geschädigten zu Unterlassung gezwungen werden, auch wenn zuwiderhandlung erstmalig droht.


<br>Abs. 2 - Schadenersatzanspruch (verschuldensunabhängig) Dem verletzen zur Wahl stehen

<br>

<br>Ersatz des ihm entstandenen unmittelbaren Schadens einschließlich des entgangenen Gewinns. 


<br>

<br>Herausgabe des vom Verletzer selbst erzielten Gewinnss


<br>

<br>Lizenzanalogie Zahlung einer angemessenen Lizenzgebühr. 






]]></description><link>miscellaneous/law/patent-und-urheberrecht/patentrecht.html</link><guid isPermaLink="false">Miscellaneous/Law/Patent- und Urheberrecht/Patentrecht.md</guid><pubDate>Tue, 25 Jul 2023 08:04:25 GMT</pubDate></item><item><title><![CDATA[Theorie des gewerblichen Rechtsschutzes]]></title><description><![CDATA[ 
 <br>
<br>Rechtfertigung des <a data-tooltip-position="top" aria-label="Patentrecht" data-href="Patentrecht" href="miscellaneous/law/patent-und-urheberrecht/patentrecht.html" class="internal-link" target="_self" rel="noopener nofollow">Patentschutzes</a>

<br>Naturrechtstheorie

<br>Erfindung gebührt demjenigen, dessen Geist sie entsprungen ist


<br>Belohnungstheorie

<br>Patentrecht soll Erfinder ermöglichen gegen Offenbarung seiner Erfindung einen Gewinn zu erzielen


<br>Anspornungstheorie

<br>Förderung des technischen Fortschrittes


<br>Vertragstheorie

<br>VErtrag Erfinder und Staat/Gesellschaft, Erläuterung Belohnungs und Anspornungstheorie




<br>Erfindung

<br>Lehre zum planmäßigen Handeln unter Einsatz beherrschbarer Naturkräfte zur Erreichung eines kausal übersehbaren Erfolgs


<br>Stand der Technik

<br>Alle Kenntnisse, die vor dem für den Zeitrang der Anmeldung maßgeblichen Tag durch schriftliche oder mündliche Beschreibung, durch Benutzung oder in sonstiger Weise der Öffentlichkeit zugänglich gemacht worden sind.


<br>Erfinderische Tätigkeit

<br>Wenn für den Fachmann nicht in naheliegender Weise aus dem Stand der Technik ergebend.


]]></description><link>miscellaneous/law/patent-und-urheberrecht/theorie-des-gewerblichen-rechtsschutzes.html</link><guid isPermaLink="false">Miscellaneous/Law/Patent- und Urheberrecht/Theorie des gewerblichen Rechtsschutzes.md</guid><pubDate>Thu, 20 Jul 2023 07:31:15 GMT</pubDate></item><item><title><![CDATA[Inhalt Softwarerecht]]></title><description><![CDATA[ 
 <br>
<br>Bestimmungen für Sprachwereke des <a data-tooltip-position="top" aria-label="Inhalt des Urheberrechts" data-href="Inhalt des Urheberrechts" href="miscellaneous/law/patent-und-urheberrecht/inhalt-des-urheberrechts.html" class="internal-link" target="_self" rel="noopener nofollow">UrhG</a> auch für Software gültig, alles hier zusätzlich, sofern nicht anders bestimmt
<br>Zusatzklauseln §69 UrhG <a data-href="Softwarespezifisches Urheberrecht" href="miscellaneous/law/softwarerecht/softwarespezifisches-urheberrecht.html" class="internal-link" target="_self" rel="noopener nofollow">Softwarespezifisches Urheberrecht</a>
<br><a data-href="Softwarevertragsrecht" href="miscellaneous/law/softwarerecht/softwarevertragsrecht.html" class="internal-link" target="_self" rel="noopener nofollow">Softwarevertragsrecht</a>
]]></description><link>miscellaneous/law/softwarerecht/inhalt-softwarerecht.html</link><guid isPermaLink="false">Miscellaneous/Law/Softwarerecht/Inhalt Softwarerecht.md</guid><pubDate>Fri, 21 Jul 2023 11:09:45 GMT</pubDate></item><item><title><![CDATA[Softwarerecht Notizen]]></title><description><![CDATA[ 
 <br>


<br><br>Vorlesung 2<br>
<br>EuGh will techn. Fortschritt bewahren, Ideen icht zu monopolisieren
<br>Zweithersteller dürfen ähnliche / vergleichbare Programme schaffen
<br>Urheberrecht für 1:1 Kopie, sonst schwammig
<br><br>Vorlesung 3<br>
<br>Alles was können und/oder Mühe erfordert ist Schützenswert aus sicht USA/GB

<br>In Kontinentaleuropa egal, Persönlichkeit steht im Mittelpunkt


<br>Wenn nicht europäisch Vorgegeben (Programmcode / Datenbank) geht es um Ausdruck der Persönlichkeit, im  Zweifel gegen vermeindl. Rechtsinhaber
<br>Als Freelancer nicht in Vertrag erwähnt  kein Eingriff ohne Zustimmung, Auftraggeber ist Auftragneher ausgeliefert
<br><br>Vorlesung 4<br>
<br>§8 UrhG (1,2,3)  in Entwicklervertrag regeln !

<br>Egal zu welchem Zeitpunkt in welchem Rahmen (außer geregelt), alle sind Miturheber
<br>Alle Urheber müssen bei Eingriffen etc. zustimmen
<br>Wider Treu und Glauben schwer zu widerlegen
<br>Miturheber können alleine Ansprüche geltend machen, Leistungen können jedoch nur für alle Miturheber verlangt werden (kosten tragen auch alle).
<br>Erträgnisse nach Umfang der Mitwirkung der Urheber (wenn nicht anders vereinbart)

<br>Wenn nicht anders nachzuweisen alle das Gleiche




<br>§69c UrhG (eng mit §69d verknüpft)

<br>...


<br>§15 UrhG 

<br>Vergänglichkeit des Mediums für Software kritisch


<br><br>Vorlesung 5 - PSP Cheat Software<br>
<br>§69a Was umfasst Software

<br>Quell und Maschinencode sowie alle Entwurfsmaterialien


<br>... 
<br><br>Vorlesung 6<br><br>Vorlesung 7<br>
<br><a data-tooltip-position="top" aria-label="Softwarevertragsrecht" data-href="Softwarevertragsrecht" href="miscellaneous/law/softwarerecht/softwarevertragsrecht.html" class="internal-link" target="_self" rel="noopener nofollow">Softwarevertragsrecht</a>

<br>Software as a Service

<br>Mietvertrag


<br>§327 BGB Verbraucherverträge digitale Produkte

<br>Absatz 2 - Vorschriften des Paragraph sind auf Produkte anzuwenden (statt Kaufrecht), die digitale Produkte enthalten 

<br>Abs. 3 - Abs. 2 gilt nicht, wenn Ware Funktion ohne digitale Produkte nicht erfüllen kann


<br>Abs. 1 - Vorschriften des Paragraph gelten für Bereitstellung digitaler Dienstleistungen


<br>§327f Aktualisierungspflicht des Überlassenden neu für Software
<br>§327i Rechte des Verbrauchers bei Mängeln


<br>Schaden durch Softwarefehler

<br>Hühnerpestentscheidung des BGH 1968

<br>Hersteller Impfstoff  Tierarzt  Halter
<br>Kein Vertrag zwischen Halter und Hersteller, Schadenersatzanspruch ?

<br>Beweislastumkehr, Hersteller muss beweisen, dass er nicht schuldhaft gehandelt hat 




<br>Boeing  Fluggesellschaft  Hinterbliebene 


<br><br>Vorlesung 8<br>
<br>GPL am weitesten verbreitete Open-Source Lizenzbedingung 
<br>§158 (2) BGB auflösende Bedingung, Bedingung nach der etwas aufgelöst wird
<br>Open-Source Vertrag ist Schenkung

<br>Schenker haftet


<br>Wenn in Klausel zu viel ausgeschlossen / eingeschlossen etc. wird, ist ganze Klasuel unwirksam
<br>Prüfungsreihenfolge
<br>AGB ? -&gt; für eine Vielzahl von Verträgen vorformulierte Klauseln

<br>§305 (1) BGB Definition, (2) Bedingungen
<br>§309 BGB Klauselverbote ohne Wertungsmöglichkeit

<br>§308 BGB Klauselverbote mit Wertungsmöglichkeit 

<br>§307 (2) BGB konkrete Beispiele der Unwirksamkeit

<br>§307 (1) BGB Generalklausel, unangemessene Klausel unwirksam








<br><br>Vorlesung 9 - Softwarespezifische Klauseln<br>
<br>Erschöpfungsgrundsatz gilt nicht für Mietverträge 
<br>Sicherungskopie darf verboten werden, wenn diese nicht erforderlich ist.
<br><br>Vorlesung 10 - Nicht softwarespezifische Klauseln<br>
<br>§ 276 BGB 3 Haftung wegen Vorsatz kann dem Schuldner nicht im Voraus erlassen werden.
<br>§14 ProdHG nicht ausschließbar 
<br>Fahrlässigkeit über BGB, nicht ausschließbar sind: 

<br>§309 Nr. 7 Verletzung von Leben, Köper, Gesundheit + grobes Verschulden 
<br>§307 Abs. 2 Nr. 1 + 2 


<br>Kardinalpflichten BGB = Grundsätzliche / wesentliche Pflichten, nicht weiter erläutert (aber ständige Rechtssprechung)

<br>Für Software nicht definiert


<br>Grundsatz der Transparenz (nur AGB, nicht individualvertraglich) = muss für Durchschnittskunde ersichtlich sein

<br>Kurze Sätze in Absätze unterteilt, damit möglichst wenig unwirksam wird !
<br>Haftungsklauseln allgemein problematisch zu verfassen


<br>Sprachwahl, nicht übersetzte Klauseln von vornerein intransparent
<br>Freie Rechtswahl im Unternehmerfall (Rom 1), Verbraucher durch Aufenthalt gesichert
]]></description><link>miscellaneous/law/softwarerecht/softwarerecht-notizen.html</link><guid isPermaLink="false">Miscellaneous/Law/Softwarerecht/Softwarerecht Notizen.md</guid><pubDate>Fri, 21 Jul 2023 11:18:59 GMT</pubDate></item><item><title><![CDATA[Softwarespezifisches Urheberrecht]]></title><description><![CDATA[ 
 <br>
<br>
Softwarerecht ist speziellen Vorschriften des <a data-tooltip-position="top" aria-label="Inhalt des Urheberrechts" data-href="Inhalt des Urheberrechts" href="miscellaneous/law/patent-und-urheberrecht/inhalt-des-urheberrechts.html" class="internal-link" target="_self" rel="noopener nofollow">UrhG</a> unterworfen

<br>
Computerprogramm ist als Sprachwerk urheberrechtlich geschützt

<br>
Bildschirmoberfläche zählt nicht zum Programm, kann einzeln geschützt werden

<br>
Dateiformat und Programmiersprache nicht Teil des Programmes, kann aber einzeln geschützt werden

<br>
§ 69a UrhG

<br>Abs. 1 - Gestalt Computerprogramme Quell und Maschinencode sowie alle Entwurfsmaterialien

<br>Nicht eindeutige Definition, meist wird WIPO Mustervorschrift gefolgt: Computerprogramme sind das in jeder Form, Sprache und Notation oder in jedem Code gewählte Ausdrucksmittel für eine Folge von Befehlen, die dazu dient, einen Computer zur Ausführung einer bestimmten Aufgabe oder Funktion zu veranlassen.
<br>Unklare Formulierung über Gestalt von Computerprogrammen 
<br>Bei Dienstverhältnis erhält Arbeitgeber des Programmierers alle Befugnisse, sofern nicht anders geregelt


<br>Abs.2 - Ausdruck des Computerprogramms: Einigkeit darüber, dass Programmiersprache irrelevant
<br>Abs. 3 - Computerprogramme geschützt, wenn eigene geistige Schöpfung, anderer Maßstab als andere Werke (keine Qualität oder Äestethik)

<br>Darf kein Banalprogramm sein




<br>
§ 69c UrhG Nr.1 Verfielfältigung

<br>Nicht eindeutig definiert, unveränderte Kopie ist Verfielfältigung, nach EuGh auch Laden in den Arbeitsspeicher und Programmlauf.


<br>
§ 69c UrhG Nr.2 Umarbeitung

<br>Übersetzung, die Bearbeitung, das Arrangement und andere Umarbeitungen eines Computerprogramms sowie die Vervielfältigung der erzielten Ergebnisse


<br>
§ 69c UrhG Nr.3 Verbreitung

<br>
§ 69c UrhG Nr.4 Öffentliche Widergabe

<br>Öffentliche Zugänglichmachung gemäß §19 UrhG besondere Bedeutung


<br>
§ 69d UrhG Bestimmungsgemäße Benutzung

<br>Abs. 1 - Grenzen der durch vertragliche Vereinbarungen Nutzungsbeschränkung Rechtssprechung überlassen

<br>§69 c Nr 1,2 bedürfen keiner Zustimmung, wenn sie der ordnungsgem. Benutzung dienen, z.B. Fehlerberichtigung


<br>Abs. 2 - Erstellung einer Sicherheitskopie darf nicht untersagt werden, Anzahl zweifelhaft.
<br>Abs. 3 - Experimentierklausel, Funcktion de programms darf ohne Zustimmung nachvollzogen werden


<br>
§69e UrhG Dekompilierung

<br>Zulässig, wenn gemäß §69c Nr. 1,2 vorgenommen, um kompatibilität mit unabhängigen Programm zu erzielen


<br>
§69f UrhG Vernichtung

<br>Zusätzlich zu §98 <a data-tooltip-position="top" aria-label="Inhalt des Urheberrechts" data-href="Inhalt des Urheberrechts" href="miscellaneous/law/patent-und-urheberrecht/inhalt-des-urheberrechts.html" class="internal-link" target="_self" rel="noopener nofollow">UrhG</a> auch gegenüber Personen, die das Programm besitzen bzw. dessen Eigentümer, ohne das diese selbst Urheberrechtsverletzungen begehen


]]></description><link>miscellaneous/law/softwarerecht/softwarespezifisches-urheberrecht.html</link><guid isPermaLink="false">Miscellaneous/Law/Softwarerecht/Softwarespezifisches Urheberrecht.md</guid><pubDate>Sat, 22 Jul 2023 08:31:47 GMT</pubDate></item><item><title><![CDATA[Softwarevertragsrecht]]></title><description><![CDATA[ 
 <br>
<br>Kaufvertrag

<br>§433 Abs. 1 BGB


<br>Mietvertrag

<br>§535 Abs. 1 BGB - Gebrauch während Mietzeit
<br>Kein Erschöpfungsgrundsatz


<br>Werkvertrag

<br>§631 Abs. 1 BGB - Herstellung / Veränderung eines verpsrochenen Werkes 


<br>Lizenzvertrag

<br>Mischvertrag, Kennzeichnend für alle Lizenzen ist, dass sich der Lizenzgeber im Lizenzvertrag verpflichtet, dem Lizenznehmer den Gebrauch eines nichtkörperlichen, geistigen Gutes im vereinbarten Umfang zu gewähren, und soweit dies erforderlich ist, entsprechende Rechte einzuräumen, ohne jedoch das Immaterialgut selbst aufzugeben.


<br>Werklieferungsvertrag
<br>Software als Sache

<br>BGH hat widerholt entschieden, dass eine auf einem Datenträger verkörperte Standardsoftware als bewegliche Sache anzusehen ist, auf die je nach der vereinbarten Überlassungsform Miet- oder Kaufrecht anwendbar ist. Dies soll auch für die Online-Nutzung von Software gelten
<br>§453 Abs.1 BGB Kauf von Sachen
<br>Wenn Sachqualität verneint als sonsitger Gegenstand


<br>Arten

<br>Standardsoftware - geringe Anpassungen an Hardware des Nutzers
<br>Individualsoftware - für den konkreten Anwendungsfall entwickelt


<br>Freeware

<br>Darf unentgeltlich genutzt werden 


<br>Public Domain Software

<br>Zusätzlich zu Freeware macht Rechtsinhaber keine Urheberrechte mehr gültig


<br>Shareware

<br>Entgeltanspruch entsteht nach vorher festgelegter Testphase


<br>Open Source Software

<br>Offener Quellcode zum Weiterentwickeln. bedingungen unterschiedlich, Lizenzvertrag
<br>General Public License (GPL) - ca 2/3 Programme, Copyleft-Prinzip, Weiterentwicklung muss wieder Open Source sein
<br>Open Source ist Schenkung, Schenker haftet


<br><br>
<br>Besitzverschaffung

<br>Grundsätzlich erfolgen sowohl die Übergabe der Kaufsache (§ 433 Abs. 1 S. 1 BGB) als auch die Gebrauchsgewährung an der Mietsache (§ 535 Abs. 1 S. 1 BGB) durch Besitzverschaffung
<br>Bei Software as a Service Mietvertrag lediglich mit Gebrauchsüberlassung
<br>Eigentumsübertragung wie bei Kauf fpr software Downloads umstritten


<br>§ 312i Abs. 1 BGB Vertrag im elektronischen Geschäftsverkehr

<br>Sobald Vertrag über Telemedien abgeschlossen wurde


<br>§ 312c Abs. 1 S. 1 BGB Fernabsatzverträge

<br>Verträge mittels Fernkommunikationsmitteln, ausgenommen Verträge, die nicht im Rahmen eines für den Fernabsatz organisierten Vertriebs- oder Dienstleistungssystem


<br>Willenserklärung

<br>Liegt bei Internetgeschäften oft unter Abwesenden vor §130 BGB
<br>Integrität = inh. Vollständigkeit, Unverändertheit
<br>Authentizität = betroffene Personen können identifiziert werden


<br>§ 312d Abs. 1 BGB Informationspflicht bei Fernabsatzverträgen

<br>Verküfer muss zu BGB informieren


<br>§ 312g Abs. 2 Nr. 6 BGB  Sonderregelung Widerruf 

<br>Soweit nichts anderes vereinbart haben, nicht bei Verträgen zur Lieferung von Ton- oder Videoaufnahmen oder Computersoftware in einer versiegelten Packung, wenn die Versiegelung nach der Lieferung entfernt wurde.


<br>§§ 312i und j BGB weitere (zusätzliche) Informationspflichten im elektronischen Geschäftsverkehr

<br>Bestellbestätigung


<br>§327 BGB Verbraucherverträge digitale Produkte

<br>Abs. 1 - Vorschriften des Paragraph gelten für Bereitstellung digitaler Dienstleistungen
<br>Abs. 2 - Vorschriften des Paragraph sind auf Produkte anzuwenden (statt Kaufrecht), die digitale Produkte enthalten 
<br>Abs. 3 - Abs. 2 gilt nicht, wenn Ware Funktion ohne digitale Produkte nicht erfüllen kann
<br>§327f BGB Aktualisierungspflicht des Überlassenden neu für Software
<br>§327i BGB Rechte des Verbrauchers bei Mängeln

<br>Nicht softwarespezifische




<br>§355 BGB Widerrufsrecht

<br>Unbegründet innerhalb von 14 Tagen, dann nicht mehr an Vertrag gebunden


<br>§ 357 Abs. 6 BGB Rücksendung

<br>Verbraucher trägt Kosten, sofern er darüber gemäß §312d (1) informiert wurde


<br>Schaden durch Softwarefehler

<br>Hühnerpestentscheidung des BGH 1968

<br>Hersteller Impfstoff  Tierarzt  Halter
<br>Kein Vertrag zwischen Halter und Hersteller, Schadenersatzanspruch ?

<br>Beweislastumkehr, Hersteller muss beweisen, dass er nicht schuldhaft gehandelt hat 




<br>Boeing  Fluggesellschaft  Hinterbliebene 


<br><br>
<br>Produkthaftung

<br>§1 Abs. 1 ProdHG	

<br>Verschuldensunabhängig
<br>Schadensersatzanspruch, wenn durch den Fehler eines Produkts ein Mensch getötet, sein Körper oder seine Gesundheit verletzt oder eine Sache beschädigt wird.
<br>Sachbeschädigung muss eine andere Sache als das fehlerhafte Produkt beschädigt worden sein und diese Sache darüber hinaus ihrer Art nach gewöhnlich für den privaten Ge- oder Verbrauch bestimmt und vom Geschädigten hierfür auch hauptsächlich verwendet worden sein. 
<br>Haftungshöchstgrenze


<br>§1 Abs.2 Nr. 5 ProdGH - Ausreißer

<br>Einstufung als Gefährdungshaftung (ohne Verschulden) relativiert, wenn Fehler nach Stand der Techik nicht zu erkennen war (nachzuweisen nach §1 Abs.4 S. 5)


<br>§§4,5 ProdHG Hersteller des End- und Teilproduktes sowie EG.Importeur zu Schadenersatz verpflichtet
<br>§11 ProdHG - Bagatellgrenze Selbstbehalt von 500€
<br>§14 ProdHG - ProdHG kann im Vorfeld nicht ausgeschlossen werden 


<br>§ 823 Abs. 1 BGB Produzentenhaftung

<br>Verschuldensabhängig
<br>Beweistlastumkehr zu Lasten des Herstellers
<br>Oft im Anschluss an ProdHG zu prüfen, wenn Schaden nicht vollst. durch diesen gedeckt


<br><br><br>
<br>§305 Abs. 1 S.1 BGB - AGB's sind für eine Vielzahl von Verträgen vorformulierte Klauseln

<br>Bei Individualverträgen nicht anwendbar
<br>Bei kollidierenden AGBs (beide vertragspartner haben AGB) werden übereinstimmende Klauseln verwendt


<br>§ 305 Abs. 2 Nr. 1 BGB Hinweispflicht

<br>Kunde muss ausdrücklich auf AGB's hingewiesen werden


<br>§ 305 Abs. 2 Nr. 1 BGB Zumutbarkeit

<br>Kunde muss ohne jur. Ausbildung in der Lage sein, AGB's zu verstehen


<br>Verbot der geltungserhaltenden Reduktion

<br>Wenn in Klausel zu viel ausgeschlossen / eingeschlossen etc. wird, ist ganze Klasuel unwirksam
<br>Insbesondere bleiben in sich zulässige Teile nicht wirksam


<br>Prüfungsreihenfolge

<br>§305 (1) BGB Definition (oben)
<br>§305 (2) BGB Bedingungen

<br>Nr.1 - Hinweispflicht
<br>Nr.2 - Kenntnisnahmemöglcichkeit / Zumutbarkeit


<br>§309 BGB Klauselverbote ohne Wertungsmöglichkeit

<br>§308 BGB Klauselverbote mit Wertungsmöglichkeit 

<br>§307 (2) BGB konkrete Beispiele der Unwirksamkeit

<br>§307 (1) BGB Generalklausel, 

<br>Transparenzgebot, unangemessene / unklare Klausel unwirksam

<br>Salvatorische Klauseln meist unwirksam












<br><br><br>
<br>Völliger Ausschluss regelm. unwirksam. Begrenzung möglich, muss aber mit Grenzen:

<br>Fahrlässigkeit über BGB, nicht ausschließbar sind: 

<br>§309 Nr. 7 Verletzung von Leben, Köper, Gesundheit + grobes Verschulden 
<br>§307 Abs. 2 Nr. 1 + 2 


<br>Kardinalpflichten BGB = Grundsätzliche / wesentliche Pflichten, nicht weiter erläutert (aber ständige Rechtssprechung)

<br>Für Software nicht definiert


<br>Teilbarkeitsrechtssprechung - Wenn Klausel nach streicehn unwirksamen teils noch aus sich heraus sinvvol, verständlich dann zulässig


]]></description><link>miscellaneous/law/softwarerecht/softwarevertragsrecht.html</link><guid isPermaLink="false">Miscellaneous/Law/Softwarerecht/Softwarevertragsrecht.md</guid><pubDate>Sat, 22 Jul 2023 09:22:56 GMT</pubDate></item><item><title><![CDATA[Daily Note Template]]></title><description><![CDATA[ 
 ]]></description><link>miscellaneous/templates/daily-note-template.html</link><guid isPermaLink="false">Miscellaneous/Templates/Daily Note Template.md</guid><pubDate>Wed, 04 Dec 2024 12:41:33 GMT</pubDate></item><item><title><![CDATA[Research Paper Template]]></title><description><![CDATA[<a class="tag" href="?query=tag:Paper" style="background-color: rgb(4, 108, 116); color: white; font-weight: 700; border: none; border-radius: 1em; padding: 0.2em 0.5em;">#Paper</a> 
 <br>In a Nutshell
2-3 sentences, potentially from abstract
<br>Motivation / Abstract<br>
<br>Why was this paper written ?
<br><br><br>
<br>Structure not fixed, adapt to paper:

<br>Mathematical formulation
<br>Most important concepts
<br>...


<br><br><br>
<br>....
<br><br>The Good

<br>Advantages / Problems that were solved / good formulations / ...

<br>The Bad and the Ugly 

<br>Remaining challenges / required improvements
<br>Overstated arguments / significant shortcomings / overfitting to problems at hand / ...

<br><br>Directions and Potential Improvements 

<br>Connections and ideas to make improvements

<br><br><br>Only add <a href=".?query=tag:Paper" class="tag" target="_blank" rel="noopener nofollow">#Paper</a>  if I have a Note about it in my vault<br>
<br>Link to Note

<br>Short summary how and why they are related


]]></description><link>miscellaneous/templates/research-paper-template.html</link><guid isPermaLink="false">Miscellaneous/Templates/Research Paper Template.md</guid><pubDate>Fri, 04 Apr 2025 11:48:22 GMT</pubDate></item><item><title><![CDATA[Bachelor Thesis]]></title><description><![CDATA[ 
 <br>]]></description><link>miscellaneous/bachelor-thesis.html</link><guid isPermaLink="false">Miscellaneous/Bachelor Thesis.md</guid><pubDate>Wed, 02 Apr 2025 10:55:02 GMT</pubDate></item><item><title><![CDATA[Blog IRobMan]]></title><description><![CDATA[ 
 <br>]]></description><link>miscellaneous/blog-irobman.html</link><guid isPermaLink="false">Miscellaneous/Blog IRobMan.md</guid><pubDate>Thu, 13 Feb 2025 20:43:27 GMT</pubDate></item><item><title><![CDATA[Convex Optimization Mini Project]]></title><description><![CDATA[ 
 <br>]]></description><link>miscellaneous/convex-optimization-mini-project.html</link><guid isPermaLink="false">Miscellaneous/Convex Optimization Mini Project.md</guid><pubDate>Thu, 13 Feb 2025 20:47:46 GMT</pubDate></item><item><title><![CDATA[Student Project Report]]></title><description><![CDATA[ 
 <br>]]></description><link>miscellaneous/student-project-report.html</link><guid isPermaLink="false">Miscellaneous/Student Project Report.md</guid><pubDate>Thu, 13 Feb 2025 20:51:14 GMT</pubDate></item><item><title><![CDATA[- Notation - Data Assimilation - A Mathematical Perspective -]]></title><description><![CDATA[<a class="tag" href="?query=tag:Notation" style="background-color: rgb(4, 108, 116); color: white; font-weight: 700; border: none; border-radius: 1em; padding: 0.2em 0.5em;">#Notation</a> 
 <br>Notation
Stick to unified notation as much as possible. Group together related subjects and make a <a href=".?query=tag:Notation" class="tag" target="_blank" rel="noopener nofollow">#Notation</a> note. If possible, extend information from blogs etc. to fit. Many symbols are used for more than one concept, try to not do that in single branch of mathematics.
For clarity and better comparison, I split up the notation overview into rough categories:

<br><a data-href="- Notation - General -" href="notation-world/-notation-general-.html" class="internal-link" target="_self" rel="noopener nofollow">- Notation - General -</a>
<br><a data-href="- Notation - Geometry, Group and Lie Theory -" href="notation-world/-notation-geometry,-group-and-lie-theory-.html" class="internal-link" target="_self" rel="noopener nofollow">- Notation - Geometry, Group and Lie Theory -</a>
<br><a data-href="- Notation - Machine Learning -" href="notation-world/-notation-machine-learning-.html" class="internal-link" target="_self" rel="noopener nofollow">- Notation - Machine Learning -</a>
<br><a data-href="- Notation - ODEs, Robotics, Dynamics and Control -" href="notation-world/-notation-odes,-robotics,-dynamics-and-control-.html" class="internal-link" target="_self" rel="noopener nofollow">- Notation - ODEs, Robotics, Dynamics and Control -</a>
<br><a data-href="- Notation - Probability, Statistics and related Fields -" href="notation-world/-notation-probability,-statistics-and-related-fields-.html" class="internal-link" target="_self" rel="noopener nofollow">- Notation - Probability, Statistics and related Fields -</a>
<br><a data-href="- Notation - Integral Transforms, Signals and Communication -" href="notation-world/-notation-integral-transforms,-signals-and-communication-.html" class="internal-link" target="_self" rel="noopener nofollow">- Notation - Integral Transforms, Signals and Communication -</a>
<br><a data-href="- Notation - Data Assimilation - A Mathematical Perspective -" href="notation-world/-notation-data-assimilation-a-mathematical-perspective-.html" class="internal-link" target="_self" rel="noopener nofollow">- Notation - Data Assimilation - A Mathematical Perspective -</a>

<br><br>Caution
Throughout the course,  denotes both the <a data-tooltip-position="top" aria-label="Measure" data-href="Measure" href="the-guide/mathematics/measure-theory/measure.html" class="internal-link" target="_self" rel="noopener nofollow">probability measure</a> whenever its argument is a Borel set and a <a data-tooltip-position="top" aria-label="Probability Density Function" data-href="Probability Density Function" href="the-guide/mathematics/probability-theory/probability-density-function.html" class="internal-link" target="_self" rel="noopener nofollow">density</a> whenever its argument is a point in .
<br><br><br>]]></description><link>notation-world/-notation-data-assimilation-a-mathematical-perspective-.html</link><guid isPermaLink="false">Notation World/- Notation - Data Assimilation - A Mathematical Perspective -.md</guid><pubDate>Wed, 23 Apr 2025 13:29:53 GMT</pubDate></item><item><title><![CDATA[- Notation - General -]]></title><description><![CDATA[<a class="tag" href="?query=tag:Notation" style="background-color: rgb(4, 108, 116); color: white; font-weight: 700; border: none; border-radius: 1em; padding: 0.2em 0.5em;">#Notation</a> 
 <br>In a Nutshell
Overview of notation used in my notes, particularly for courses I took during my masters. If possible, extend information from blogs etc. to fit. Many symbols are used for more than one concept, try to not do that in single branch of mathematics.
<br>Notation
Stick to unified notation as much as possible. Group together related subjects and make a <a href=".?query=tag:Notation" class="tag" target="_blank" rel="noopener nofollow">#Notation</a> note. If possible, extend information from blogs etc. to fit. Many symbols are used for more than one concept, try to not do that in single branch of mathematics.
For clarity and better comparison, I split up the notation overview into rough categories:

<br><a data-href="- Notation - General -" href="notation-world/-notation-general-.html" class="internal-link" target="_self" rel="noopener nofollow">- Notation - General -</a>
<br><a data-href="- Notation - Geometry, Group and Lie Theory -" href="notation-world/-notation-geometry,-group-and-lie-theory-.html" class="internal-link" target="_self" rel="noopener nofollow">- Notation - Geometry, Group and Lie Theory -</a>
<br><a data-href="- Notation - Machine Learning -" href="notation-world/-notation-machine-learning-.html" class="internal-link" target="_self" rel="noopener nofollow">- Notation - Machine Learning -</a>
<br><a data-href="- Notation - ODEs, Robotics, Dynamics and Control -" href="notation-world/-notation-odes,-robotics,-dynamics-and-control-.html" class="internal-link" target="_self" rel="noopener nofollow">- Notation - ODEs, Robotics, Dynamics and Control -</a>
<br><a data-href="- Notation - Probability, Statistics and related Fields -" href="notation-world/-notation-probability,-statistics-and-related-fields-.html" class="internal-link" target="_self" rel="noopener nofollow">- Notation - Probability, Statistics and related Fields -</a>
<br><a data-href="- Notation - Integral Transforms, Signals and Communication -" href="notation-world/-notation-integral-transforms,-signals-and-communication-.html" class="internal-link" target="_self" rel="noopener nofollow">- Notation - Integral Transforms, Signals and Communication -</a>
<br><a data-href="- Notation - Data Assimilation - A Mathematical Perspective -" href="notation-world/-notation-data-assimilation-a-mathematical-perspective-.html" class="internal-link" target="_self" rel="noopener nofollow">- Notation - Data Assimilation - A Mathematical Perspective -</a>

<br>The following are more general areas or areas that don't deserve their own note (yet).<br><br><br><br><br><br><br><br><br><br><br><br><br>]]></description><link>notation-world/-notation-general-.html</link><guid isPermaLink="false">Notation World/- Notation - General -.md</guid><pubDate>Wed, 23 Apr 2025 13:29:53 GMT</pubDate></item><item><title><![CDATA[- Notation - Geometry, Group and Lie Theory -]]></title><description><![CDATA[<a class="tag" href="?query=tag:Notation" style="background-color: rgb(4, 108, 116); color: white; font-weight: 700; border: none; border-radius: 1em; padding: 0.2em 0.5em;">#Notation</a> 
 <br>Notation
Stick to unified notation as much as possible. Group together related subjects and make a <a href=".?query=tag:Notation" class="tag" target="_blank" rel="noopener nofollow">#Notation</a> note. If possible, extend information from blogs etc. to fit. Many symbols are used for more than one concept, try to not do that in single branch of mathematics.
For clarity and better comparison, I split up the notation overview into rough categories:

<br><a data-href="- Notation - General -" href="notation-world/-notation-general-.html" class="internal-link" target="_self" rel="noopener nofollow">- Notation - General -</a>
<br><a data-href="- Notation - Geometry, Group and Lie Theory -" href="notation-world/-notation-geometry,-group-and-lie-theory-.html" class="internal-link" target="_self" rel="noopener nofollow">- Notation - Geometry, Group and Lie Theory -</a>
<br><a data-href="- Notation - Machine Learning -" href="notation-world/-notation-machine-learning-.html" class="internal-link" target="_self" rel="noopener nofollow">- Notation - Machine Learning -</a>
<br><a data-href="- Notation - ODEs, Robotics, Dynamics and Control -" href="notation-world/-notation-odes,-robotics,-dynamics-and-control-.html" class="internal-link" target="_self" rel="noopener nofollow">- Notation - ODEs, Robotics, Dynamics and Control -</a>
<br><a data-href="- Notation - Probability, Statistics and related Fields -" href="notation-world/-notation-probability,-statistics-and-related-fields-.html" class="internal-link" target="_self" rel="noopener nofollow">- Notation - Probability, Statistics and related Fields -</a>
<br><a data-href="- Notation - Integral Transforms, Signals and Communication -" href="notation-world/-notation-integral-transforms,-signals-and-communication-.html" class="internal-link" target="_self" rel="noopener nofollow">- Notation - Integral Transforms, Signals and Communication -</a>
<br><a data-href="- Notation - Data Assimilation - A Mathematical Perspective -" href="notation-world/-notation-data-assimilation-a-mathematical-perspective-.html" class="internal-link" target="_self" rel="noopener nofollow">- Notation - Data Assimilation - A Mathematical Perspective -</a>

<br><br><br><img alt="center" src="lib/media/pasted-image-20240727142149.png" style="width: 500px; max-width: 100%;"><br>Elementary Differential Geometry<br>
Introductory course that focuses on curves and surfaces, mostly in two and three-dimensional space that are given by parametrizations in the embedding space.<br><br>Riemannian Differential Geometry<br>
Extends ideas from the elementary differential geometry to arbitrary dimensions and removes the need of an embedding entirely.<br><br><br><br>...<br><br><br><br>Studies manifolds whose elements also satisfy group axioms, yielding an exceptionally rich structure that finds applications especially for matrix groups.<br><br><br>]]></description><link>notation-world/-notation-geometry,-group-and-lie-theory-.html</link><guid isPermaLink="false">Notation World/- Notation - Geometry, Group and Lie Theory -.md</guid><pubDate>Wed, 23 Apr 2025 13:29:53 GMT</pubDate><enclosure url="lib/media/pasted-image-20240727142149.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src=&quot;lib/media/pasted-image-20240727142149.png&quot;&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[- Notation - Integral Transforms, Signals and Communication -]]></title><description><![CDATA[<a class="tag" href="?query=tag:Notation" style="background-color: rgb(4, 108, 116); color: white; font-weight: 700; border: none; border-radius: 1em; padding: 0.2em 0.5em;">#Notation</a> 
 <br>Notation
Stick to unified notation as much as possible. Group together related subjects and make a <a href=".?query=tag:Notation" class="tag" target="_blank" rel="noopener nofollow">#Notation</a> note. If possible, extend information from blogs etc. to fit. Many symbols are used for more than one concept, try to not do that in single branch of mathematics.
For clarity and better comparison, I split up the notation overview into rough categories:

<br><a data-href="- Notation - General -" href="notation-world/-notation-general-.html" class="internal-link" target="_self" rel="noopener nofollow">- Notation - General -</a>
<br><a data-href="- Notation - Geometry, Group and Lie Theory -" href="notation-world/-notation-geometry,-group-and-lie-theory-.html" class="internal-link" target="_self" rel="noopener nofollow">- Notation - Geometry, Group and Lie Theory -</a>
<br><a data-href="- Notation - Machine Learning -" href="notation-world/-notation-machine-learning-.html" class="internal-link" target="_self" rel="noopener nofollow">- Notation - Machine Learning -</a>
<br><a data-href="- Notation - ODEs, Robotics, Dynamics and Control -" href="notation-world/-notation-odes,-robotics,-dynamics-and-control-.html" class="internal-link" target="_self" rel="noopener nofollow">- Notation - ODEs, Robotics, Dynamics and Control -</a>
<br><a data-href="- Notation - Probability, Statistics and related Fields -" href="notation-world/-notation-probability,-statistics-and-related-fields-.html" class="internal-link" target="_self" rel="noopener nofollow">- Notation - Probability, Statistics and related Fields -</a>
<br><a data-href="- Notation - Integral Transforms, Signals and Communication -" href="notation-world/-notation-integral-transforms,-signals-and-communication-.html" class="internal-link" target="_self" rel="noopener nofollow">- Notation - Integral Transforms, Signals and Communication -</a>
<br><a data-href="- Notation - Data Assimilation - A Mathematical Perspective -" href="notation-world/-notation-data-assimilation-a-mathematical-perspective-.html" class="internal-link" target="_self" rel="noopener nofollow">- Notation - Data Assimilation - A Mathematical Perspective -</a>

<br><br><br><br><br>]]></description><link>notation-world/-notation-integral-transforms,-signals-and-communication-.html</link><guid isPermaLink="false">Notation World/- Notation - Integral Transforms, Signals and Communication -.md</guid><pubDate>Wed, 23 Apr 2025 13:29:53 GMT</pubDate></item><item><title><![CDATA[- Notation - Machine Learning -]]></title><description><![CDATA[<a class="tag" href="?query=tag:Notation" style="background-color: rgb(4, 108, 116); color: white; font-weight: 700; border: none; border-radius: 1em; padding: 0.2em 0.5em;">#Notation</a> 
 <br>Notation
Stick to unified notation as much as possible. Group together related subjects and make a <a href=".?query=tag:Notation" class="tag" target="_blank" rel="noopener nofollow">#Notation</a> note. If possible, extend information from blogs etc. to fit. Many symbols are used for more than one concept, try to not do that in single branch of mathematics.
For clarity and better comparison, I split up the notation overview into rough categories:

<br><a data-href="- Notation - General -" href="notation-world/-notation-general-.html" class="internal-link" target="_self" rel="noopener nofollow">- Notation - General -</a>
<br><a data-href="- Notation - Geometry, Group and Lie Theory -" href="notation-world/-notation-geometry,-group-and-lie-theory-.html" class="internal-link" target="_self" rel="noopener nofollow">- Notation - Geometry, Group and Lie Theory -</a>
<br><a data-href="- Notation - Machine Learning -" href="notation-world/-notation-machine-learning-.html" class="internal-link" target="_self" rel="noopener nofollow">- Notation - Machine Learning -</a>
<br><a data-href="- Notation - ODEs, Robotics, Dynamics and Control -" href="notation-world/-notation-odes,-robotics,-dynamics-and-control-.html" class="internal-link" target="_self" rel="noopener nofollow">- Notation - ODEs, Robotics, Dynamics and Control -</a>
<br><a data-href="- Notation - Probability, Statistics and related Fields -" href="notation-world/-notation-probability,-statistics-and-related-fields-.html" class="internal-link" target="_self" rel="noopener nofollow">- Notation - Probability, Statistics and related Fields -</a>
<br><a data-href="- Notation - Integral Transforms, Signals and Communication -" href="notation-world/-notation-integral-transforms,-signals-and-communication-.html" class="internal-link" target="_self" rel="noopener nofollow">- Notation - Integral Transforms, Signals and Communication -</a>
<br><a data-href="- Notation - Data Assimilation - A Mathematical Perspective -" href="notation-world/-notation-data-assimilation-a-mathematical-perspective-.html" class="internal-link" target="_self" rel="noopener nofollow">- Notation - Data Assimilation - A Mathematical Perspective -</a>

<br><br><br><br><br>]]></description><link>notation-world/-notation-machine-learning-.html</link><guid isPermaLink="false">Notation World/- Notation - Machine Learning -.md</guid><pubDate>Wed, 23 Apr 2025 13:29:53 GMT</pubDate></item><item><title><![CDATA[- Notation - ODEs, Robotics, Dynamics and Control -]]></title><description><![CDATA[<a class="tag" href="?query=tag:Notation" style="background-color: rgb(4, 108, 116); color: white; font-weight: 700; border: none; border-radius: 1em; padding: 0.2em 0.5em;">#Notation</a> 
 <br>Notation
Stick to unified notation as much as possible. Group together related subjects and make a <a href=".?query=tag:Notation" class="tag" target="_blank" rel="noopener nofollow">#Notation</a> note. If possible, extend information from blogs etc. to fit. Many symbols are used for more than one concept, try to not do that in single branch of mathematics.
For clarity and better comparison, I split up the notation overview into rough categories:

<br><a data-href="- Notation - General -" href="notation-world/-notation-general-.html" class="internal-link" target="_self" rel="noopener nofollow">- Notation - General -</a>
<br><a data-href="- Notation - Geometry, Group and Lie Theory -" href="notation-world/-notation-geometry,-group-and-lie-theory-.html" class="internal-link" target="_self" rel="noopener nofollow">- Notation - Geometry, Group and Lie Theory -</a>
<br><a data-href="- Notation - Machine Learning -" href="notation-world/-notation-machine-learning-.html" class="internal-link" target="_self" rel="noopener nofollow">- Notation - Machine Learning -</a>
<br><a data-href="- Notation - ODEs, Robotics, Dynamics and Control -" href="notation-world/-notation-odes,-robotics,-dynamics-and-control-.html" class="internal-link" target="_self" rel="noopener nofollow">- Notation - ODEs, Robotics, Dynamics and Control -</a>
<br><a data-href="- Notation - Probability, Statistics and related Fields -" href="notation-world/-notation-probability,-statistics-and-related-fields-.html" class="internal-link" target="_self" rel="noopener nofollow">- Notation - Probability, Statistics and related Fields -</a>
<br><a data-href="- Notation - Integral Transforms, Signals and Communication -" href="notation-world/-notation-integral-transforms,-signals-and-communication-.html" class="internal-link" target="_self" rel="noopener nofollow">- Notation - Integral Transforms, Signals and Communication -</a>
<br><a data-href="- Notation - Data Assimilation - A Mathematical Perspective -" href="notation-world/-notation-data-assimilation-a-mathematical-perspective-.html" class="internal-link" target="_self" rel="noopener nofollow">- Notation - Data Assimilation - A Mathematical Perspective -</a>

<br><br><br>This notation is used in pure mathematics, where no special application is considered.<br><br>Note
In contrast, the study of <a data-tooltip-position="top" aria-label="Static and Dynamic Systems" data-href="Static and Dynamic Systems" href="the-guide/robotics,-dynamics-and-control/dynamics/static-and-dynamic-systems.html" class="internal-link" target="_self" rel="noopener nofollow">dynamical systems</a> in physics and engineering usually replaces the independent variable  by the time . 
<br><br><br>In robotics, we have to additionally incorporate the two common settings of control etc. in joint <a data-tooltip-position="top" aria-label="Space" data-href="Space" href="the-guide/mathematics/general-stuff/space.html" class="internal-link" target="_self" rel="noopener nofollow">space</a> / <a data-tooltip-position="top" aria-label="Generalized Coordinates, Configuration or Joint Space" data-href="Generalized Coordinates, Configuration or Joint Space" href="the-guide/robotics,-dynamics-and-control/generalized-coordinates,-configuration-or-joint-space.html" class="internal-link" target="_self" rel="noopener nofollow">configuration space</a> (where a point is a joint configuration ) or task space (where a point is a position in euclidean space  or with orientation ) and their fundamental relationship via <a data-tooltip-position="top" aria-label="Kinematics" data-href="Kinematics" href="the-guide/robotics,-dynamics-and-control/kinematics/kinematics.html" class="internal-link" target="_self" rel="noopener nofollow">forward kinematics</a> In general, we consider a manipulator with  degrees of freedom. In the following, all terms are potentially time-dependent.<br><br>More general control settings usually base their models on a more general state variable , usually in combination with the assumptions of a LTI.<br>]]></description><link>notation-world/-notation-odes,-robotics,-dynamics-and-control-.html</link><guid isPermaLink="false">Notation World/- Notation - ODEs, Robotics, Dynamics and Control -.md</guid><pubDate>Wed, 23 Apr 2025 13:29:53 GMT</pubDate></item><item><title><![CDATA[- Notation - Probability, Statistics and related Fields -]]></title><description><![CDATA[<a class="tag" href="?query=tag:Notation" style="background-color: rgb(4, 108, 116); color: white; font-weight: 700; border: none; border-radius: 1em; padding: 0.2em 0.5em;">#Notation</a> 
 <br>Notation
Stick to unified notation as much as possible. Group together related subjects and make a <a href=".?query=tag:Notation" class="tag" target="_blank" rel="noopener nofollow">#Notation</a> note. If possible, extend information from blogs etc. to fit. Many symbols are used for more than one concept, try to not do that in single branch of mathematics.
For clarity and better comparison, I split up the notation overview into rough categories:

<br><a data-href="- Notation - General -" href="notation-world/-notation-general-.html" class="internal-link" target="_self" rel="noopener nofollow">- Notation - General -</a>
<br><a data-href="- Notation - Geometry, Group and Lie Theory -" href="notation-world/-notation-geometry,-group-and-lie-theory-.html" class="internal-link" target="_self" rel="noopener nofollow">- Notation - Geometry, Group and Lie Theory -</a>
<br><a data-href="- Notation - Machine Learning -" href="notation-world/-notation-machine-learning-.html" class="internal-link" target="_self" rel="noopener nofollow">- Notation - Machine Learning -</a>
<br><a data-href="- Notation - ODEs, Robotics, Dynamics and Control -" href="notation-world/-notation-odes,-robotics,-dynamics-and-control-.html" class="internal-link" target="_self" rel="noopener nofollow">- Notation - ODEs, Robotics, Dynamics and Control -</a>
<br><a data-href="- Notation - Probability, Statistics and related Fields -" href="notation-world/-notation-probability,-statistics-and-related-fields-.html" class="internal-link" target="_self" rel="noopener nofollow">- Notation - Probability, Statistics and related Fields -</a>
<br><a data-href="- Notation - Integral Transforms, Signals and Communication -" href="notation-world/-notation-integral-transforms,-signals-and-communication-.html" class="internal-link" target="_self" rel="noopener nofollow">- Notation - Integral Transforms, Signals and Communication -</a>
<br><a data-href="- Notation - Data Assimilation - A Mathematical Perspective -" href="notation-world/-notation-data-assimilation-a-mathematical-perspective-.html" class="internal-link" target="_self" rel="noopener nofollow">- Notation - Data Assimilation - A Mathematical Perspective -</a>

<br><br><br><br><br><br><br><br><br>]]></description><link>notation-world/-notation-probability,-statistics-and-related-fields-.html</link><guid isPermaLink="false">Notation World/- Notation - Probability, Statistics and related Fields -.md</guid><pubDate>Wed, 23 Apr 2025 13:29:53 GMT</pubDate></item><item><title><![CDATA[Approximate Gaussian Filters]]></title><description><![CDATA[ 
 <br>In a Nutshell
Class of algorithms that generalizes a minimization property of the Kalman filter update to general distributions at the cost of not propagating information about the <a data-tooltip-position="top" aria-label="Covariance and Variance" data-href="Covariance and Variance" href="the-guide/mathematics/statistics/covariance-and-variance.html" class="internal-link" target="_self" rel="noopener nofollow">Covariance</a>.
<br><br><br>Assumption
For simplicity and because it covers a lot of applications, we consider the linear observation . The nonlinear generalizationcan also be solved in many cases, but attention has to be payed to the resulting <a data-tooltip-position="top" aria-label="Optimization Problem" data-href="Optimization Problem" href="the-guide/mathematics/optimization/convex-optimization-lecture/optimization-problem.html" class="internal-link" target="_self" rel="noopener nofollow">optimization problem</a> and its convergence properties, because it is no longer always <a data-tooltip-position="top" aria-label="Quadratic Program (QP)" data-href="Quadratic Program (QP)" href="the-guide/mathematics/optimization/convex-optimization-lecture/quadratic-program-(qp).html" class="internal-link" target="_self" rel="noopener nofollow">quadratic</a>.
<br>The motivation for this family of algorithms is that the update of the <a data-tooltip-position="top" aria-label="Kalman Filter" data-href="Kalman Filter" href="the-guide/computational-statistics/data-assimilation/filtering-algorithms/kalman-filter.html" class="internal-link" target="_self" rel="noopener nofollow">Kalman filter</a> mean can be expressed as a <a data-tooltip-position="top" aria-label="Optimization Problem" data-href="Optimization Problem" href="the-guide/mathematics/optimization/convex-optimization-lecture/optimization-problem.html" class="internal-link" target="_self" rel="noopener nofollow">minimization problem</a> of the form where the  and  are computed as in the prediction step of the <a data-tooltip-position="top" aria-label="Kalman Filter" data-href="Kalman Filter" href="the-guide/computational-statistics/data-assimilation/filtering-algorithms/kalman-filter.html" class="internal-link" target="_self" rel="noopener nofollow">Kalman filter</a>. <br>Proof
Computing the <a data-tooltip-position="top" aria-label="Derivative, Gradient, Jacobian and Hessian > The Gradient" data-href="Derivative, Gradient, Jacobian and Hessian#The Gradient" href="the-guide/mathematics/analysis-and-calculus/derivative,-gradient,-jacobian-and-hessian.html#The_Gradient" class="internal-link" target="_self" rel="noopener nofollow">gradient</a> of  (component-wise inner product rule), we obtain Inserting the identitieswe obtain .
<br>Intuition
The first term forces you to be close to data, the second to be close to the computed  prediction, giving us a compromise weighted by the precision matrices or how much we trust each component. For high variances (low precisions), the term has low influence on  and thereby the optimization result.
<br><br><br>The idea of the Approximate Gaussian Filter family is to use this fact for the entire update, while either ignoring or only approximately computing the update of the covariance. Mathematically, the methods proceed as follows: where  and  have to be chosen. <br>General Update
With the same arguments as the <a data-tooltip-position="top" aria-label="Kalman Filter" data-href="Kalman Filter" href="the-guide/computational-statistics/data-assimilation/filtering-algorithms/kalman-filter.html" class="internal-link" target="_self" rel="noopener nofollow">proof of the Kalman filter update</a>, we get the general update equations Note that we are now required to explicitly choose a covariance at every step, which yields different algorithms.
<br>Disadvantages
All algorithms that follow are termed Approximate Gaussian because they implicitly invoke a Gaussian approximation. Mathematically, the above formulas are only correct if which is not true in general. Therefore, the statistical accuracy of these methods is often invalidated. 
<br>Advantages

<br>The methods are usually rather efficient because the quadratic effort of computing the covariance is avoided.
<br>Despite the statistical problems, the algorithms are still used in practice, because the <a data-tooltip-position="top" aria-label="Assessing the Quality of Data-Assimilation Algorithms" data-href="Assessing the Quality of Data-Assimilation Algorithms" href="the-guide/computational-statistics/data-assimilation/assessing-the-quality-of-data-assimilation-algorithms.html" class="internal-link" target="_self" rel="noopener nofollow">Signal Estimation properties</a> can be powerful in applications. For a detailed discussion see <a data-href="Long-Time Behavior of Filters" href="the-guide/computational-statistics/data-assimilation/filtering-algorithms/long-time-behavior-of-filters.html" class="internal-link" target="_self" rel="noopener nofollow">Long-Time Behavior of Filters</a>.

<br><br><br>All these methods implicitly assume  to be <a data-tooltip-position="top" aria-label="Gaussian Distribution" data-href="Gaussian Distribution" href="the-guide/mathematics/probability-theory/gaussian-distribution.html" class="internal-link" target="_self" rel="noopener nofollow">Gaussian</a>, hence the name. In many cases, this can be a good approximation. Even in cases when it is not, the method empirically still shows good <a data-tooltip-position="top" aria-label="Estimator" data-href="Estimator" href="the-guide/mathematics/statistics/estimator.html" class="internal-link" target="_self" rel="noopener nofollow">estimation</a> properties. All the following algorithms are derived in that way, essentially differing only in how they assume the <a data-tooltip-position="top" aria-label="Covariance and Variance" data-href="Covariance and Variance" href="the-guide/mathematics/statistics/covariance-and-variance.html" class="internal-link" target="_self" rel="noopener nofollow">covariance</a> and the noise on the dynamics.<br>3DVAR
Simplest approach, choose  and a fixed covariance , in many cases .  This yields the set of updatesMost importantly, this enables computation of one fixed inverse / <a data-tooltip-position="top" aria-label="LU-Decomposition" data-href="LU-Decomposition" href="the-guide/mathematics/linear-algebra/lu-decomposition.html" class="internal-link" target="_self" rel="noopener nofollow">LU-decomposition</a> in the beginning, making the algorithm much more efficient.
<br>
<br>Noise-free dynamics assumption is used in many cases, but it is not necessary in general.
<br>Name stems from the fact that the method is variational (minimization) and in many practical applications over .

<br>Contrast to <a data-tooltip-position="top" aria-label="Variational Smoothing Methods" data-href="Variational Smoothing Methods" href="the-guide/computational-statistics/data-assimilation/smoothing-algorithms/variational-smoothing-methods.html" class="internal-link" target="_self" rel="noopener nofollow">4DVar</a>, which includes time as fourth dimension


<br><br>Extended Kalman Filter (ExKF)
This algorithm specifically addresses the noise-free () non-linear dynamics (linear observation) setting. The idea is to linearize the dynamics for the prediction  covariance around the last iterate, yielding the predictionwhere  denotes the <a data-tooltip-position="top" aria-label="Derivative, Gradient, Jacobian and Hessian" data-href="Derivative, Gradient, Jacobian and Hessian" href="the-guide/mathematics/analysis-and-calculus/derivative,-gradient,-jacobian-and-hessian.html" class="internal-link" target="_self" rel="noopener nofollow">Jacobian</a>. For the analysis step, we use this linear approximation and get
<br>
<br>In some settings, the algorithm is introduces as a linearization of both the observation and the dynamics, but in our setting the observation already is.
<br>Noise-free dynamics assumption is used in many cases, but it is not necessary in general.
<br>For linear dynamics, it is just the regular <a data-href="Kalman Filter" href="the-guide/computational-statistics/data-assimilation/filtering-algorithms/kalman-filter.html" class="internal-link" target="_self" rel="noopener nofollow">Kalman Filter</a>
<br><br>Another algorithm that is used a lot in practice for its efficiency and performance rather than its theoretical properties is the Ensemble Kalman Filter. It can be motivated using the family of cost functionswhich are coupled through .<br>
<br>The prediction step uses the non-linear dynamics to evolve
<br>The analysis step computes updated particles by minimizing the cost function, where the covariance depends on the entire set of previous particles . The result is We then use the set of new particles to make <a data-tooltip-position="top" aria-label="Monte Carlo Integration" data-href="Monte Carlo Integration" href="the-guide/mathematics/probability-theory/monte-carlo-integration.html" class="internal-link" target="_self" rel="noopener nofollow">Monte Carlo</a> approximations for the <a data-tooltip-position="top" aria-label="The Filtering Problem" data-href="The Filtering Problem" href="the-guide/computational-statistics/data-assimilation/filtering-algorithms/the-filtering-problem.html" class="internal-link" target="_self" rel="noopener nofollow">filtering</a> measures
<br>(Perturbed Observation) Ensemble Kalman Filter (EnKF)
Uses the minimization approach for an ensemble of particles and estimates the new mean and covariance using all members, thereby adding further coupling. We specifically describe the perturbed observation variation of the algorithm, which combines the estimates simply using a mean. The prediction step isfollowed by the analysis step viaAs the name already suggests, each particle sees an observation perturbed by an independent draw from . The ensemble is not prescribed to be Gaussian, since we allow the particles to evolve under non-linear dynamics and do not evolve the covariance.
<br>
<br>In the linear case, reproduces the true posterior in the large particle limit (sample covariance becomes true covariance)
<br>Disadvantage
Except for linear problems, the approximations do not converge to the true  and  for .
<br>Additional Information
Inherently assumes uniform weights over the particles, can be generalized via <a data-href="The Particle Filter" href="the-guide/computational-statistics/data-assimilation/filtering-algorithms/the-particle-filter.html" class="internal-link" target="_self" rel="noopener nofollow">The Particle Filter</a> 
<br><br>Ensemble Square-Root Kalman Filter
A more sophisticated version of the above EnKF can be obtained by defining the analysis step in a way that the particles produce an estimate of the covariance that exactly satisfies the Kalman identityMore specifically, we use samples  that have exactly this sample covariance to use the same predictionfollowed by an adapted analysis step
<br>There are several ways to design  appropriately, we will show one referred to as the ensemble transform Kalman Filter. For this, we define which means that now . For that, we want do design  transformation, such that withFor this, we need to pick a transformation  that has an eigenvector  in order to preserve the mean of the ensemble while also having the desired propertyAssuming that the standard matrix square-root is used and that the transformation is symmetric and positive definit (for unique result), we can choose]]></description><link>the-guide/computational-statistics/data-assimilation/filtering-algorithms/approximate-gaussian-filters.html</link><guid isPermaLink="false">The Guide/Computational Statistics/Data Assimilation/Filtering Algorithms/Approximate Gaussian Filters.md</guid><pubDate>Sat, 25 Jan 2025 17:52:28 GMT</pubDate></item><item><title><![CDATA[Kalman Filter]]></title><description><![CDATA[ 
 <br>In a Nutshell
<a data-tooltip-position="top" aria-label="The Filtering Problem" data-href="The Filtering Problem" href="the-guide/computational-statistics/data-assimilation/filtering-algorithms/the-filtering-problem.html" class="internal-link" target="_self" rel="noopener nofollow">Filtering</a> method to track linear <a data-tooltip-position="top" aria-label="Static and Dynamic Systems" data-href="Static and Dynamic Systems" href="the-guide/robotics,-dynamics-and-control/dynamics/static-and-dynamic-systems.html" class="internal-link" target="_self" rel="noopener nofollow">dynamics</a> model with <a data-tooltip-position="top" aria-label="Gaussian Distribution" data-href="Gaussian Distribution" href="the-guide/mathematics/probability-theory/gaussian-distribution.html" class="internal-link" target="_self" rel="noopener nofollow">Gaussian</a> noise. All <a data-tooltip-position="top" aria-label="Probability Distribution" data-href="Probability Distribution" href="the-guide/mathematics/probability-theory/probability-distribution.html" class="internal-link" target="_self" rel="noopener nofollow">distributions</a> in a <a data-href="Bayesian Filtering for Tracking" href="the-guide/robotics,-dynamics-and-control/perception/bayesian-filtering-for-tracking.html" class="internal-link" target="_self" rel="noopener nofollow">Bayesian Filtering for Tracking</a> context are assumed to be <a data-tooltip-position="top" aria-label="Gaussian Distribution" data-href="Gaussian Distribution" href="the-guide/mathematics/probability-theory/gaussian-distribution.html" class="internal-link" target="_self" rel="noopener nofollow">Gaussian</a>, which is why you only have to store the <a data-tooltip-position="top" aria-label="Expectations" data-href="Expectations" href="the-guide/mathematics/statistics/expectations.html" class="internal-link" target="_self" rel="noopener nofollow">mean</a> and the <a data-tooltip-position="top" aria-label="Covariance and Variance" data-href="Covariance and Variance" href="the-guide/mathematics/statistics/covariance-and-variance.html" class="internal-link" target="_self" rel="noopener nofollow">variance</a> while dealing with simpler integrals. 
<br><br>Assumptions
In the context of the <a data-href="Data Assimilation" href="the-guide/computational-statistics/data-assimilation/data-assimilation.html" class="internal-link" target="_self" rel="noopener nofollow">Data Assimilation</a> setting of <a data-tooltip-position="top" aria-label="Static and Dynamic Systems" data-href="Static and Dynamic Systems" href="the-guide/robotics,-dynamics-and-control/dynamics/static-and-dynamic-systems.html" class="internal-link" target="_self" rel="noopener nofollow">stochastic linear  dynamics</a>with Additionally, all probabilities are <a data-href="Gaussian Distribution" href="the-guide/mathematics/probability-theory/gaussian-distribution.html" class="internal-link" target="_self" rel="noopener nofollow">Gaussian Distribution</a>, characterized by <a data-tooltip-position="top" aria-label="Expectations" data-href="Expectations" href="the-guide/mathematics/statistics/expectations.html" class="internal-link" target="_self" rel="noopener nofollow">mean</a> and <a data-tooltip-position="top" aria-label="Covariance and Variance" data-href="Covariance and Variance" href="the-guide/mathematics/statistics/covariance-and-variance.html" class="internal-link" target="_self" rel="noopener nofollow">covariance</a>.
<br>Our aim is to update the <a data-tooltip-position="top" aria-label="The Filtering Problem" data-href="The Filtering Problem" href="the-guide/computational-statistics/data-assimilation/filtering-algorithms/the-filtering-problem.html" class="internal-link" target="_self" rel="noopener nofollow">filtering</a> <a data-tooltip-position="top" aria-label="Probability Distribution" data-href="Probability Distribution" href="the-guide/mathematics/probability-theory/probability-distribution.html" class="internal-link" target="_self" rel="noopener nofollow">distribution</a>  from time  to . Based on the assumptions above, all distributions computed from them are also Gaussian, yielding the updates of mean and covariance <br>Theorem 4.1 - Non-zero Covariance
If , then  for all  and  
<br>The following is based on the (very reasonable) assumption that . From the prediction step, we getsince the noise  is <a data-tooltip-position="top" aria-label="Statistical Independence" data-href="Statistical Independence" href="the-guide/mathematics/statistics/statistical-independence.html" class="internal-link" target="_self" rel="noopener nofollow">independent</a> from the data. For the <a data-tooltip-position="top" aria-label="Covariance and Variance" data-href="Covariance and Variance" href="the-guide/mathematics/statistics/covariance-and-variance.html" class="internal-link" target="_self" rel="noopener nofollow">covariance</a>, we can write out the <a data-tooltip-position="top" aria-label="Expectations" data-href="Expectations" href="the-guide/mathematics/statistics/expectations.html" class="internal-link" target="_self" rel="noopener nofollow">expectation</a>for which it holds that , because  (first term positive semi-definite) by induction and . <br>For analysis, we can consider the exponent of the posterior and equate quadratic and linear terms on both sides to obtain meanand covarianceof the next step. All other terms are absorbed into the normalization constant. The positive definiteness follows again from .<br><br>Reformulated Updates
The Kalman filter update can be rewritten to only require invertion of an  <a data-tooltip-position="top" aria-label="Matrices" data-href="Matrices" href="the-guide/mathematics/linear-algebra/matrices.html" class="internal-link" target="_self" rel="noopener nofollow">matrix</a> (observable state by observable state) instead of  (state by state) as above (usually ). This yields the intermediate updateswith  as above.
<br>Proof
We notice that the update step  can be reformulated using the <a data-tooltip-position="top" aria-label="Woodbury Identity" data-href="Woodbury Identity" href="the-guide/mathematics/linear-algebra/woodbury-identity.html" class="internal-link" target="_self" rel="noopener nofollow">Woodbury formula</a>, yielding Consequently, we can see that where the last step  follows from the identity (see above)
<br><img alt="center" src="lib/media/pasted-image-20240301190300.png" style="width: 500px; max-width: 100%;"><br>Advantages

<br>Simple in theory and computation
<br>Well understood 

<br>Disadvantages

<br>Only single hypothesis because of unimodal distribution (Gaussian only has one peak)
<br>Restricted class of motions because of linear model

<br><br><br>
<br><a data-tooltip-position="top" aria-label="Approximate Gaussian Filters" data-href="Approximate Gaussian Filters" href="the-guide/computational-statistics/data-assimilation/filtering-algorithms/approximate-gaussian-filters.html" class="internal-link" target="_self" rel="noopener nofollow">Extended Kalman Filter (EKF)</a>

<br>Use non-linear, differentiable state transition and observation model
<br>Linearize around current estimate Taylor expansion
<br>Can diverge, but found many applications


<br>Unscented Kalman Filter (UKF)

<br>...


<br><a data-tooltip-position="top" aria-label="Approximate Gaussian Filters" data-href="Approximate Gaussian Filters" href="the-guide/computational-statistics/data-assimilation/filtering-algorithms/approximate-gaussian-filters.html" class="internal-link" target="_self" rel="noopener nofollow">Ensemble Kalman Filter (EnKF)</a>

<br>Uses an ensemble of particles each separated by observation noise


<br><a data-tooltip-position="top" aria-label="Approximate Gaussian Filters" data-href="Approximate Gaussian Filters" href="the-guide/computational-statistics/data-assimilation/filtering-algorithms/approximate-gaussian-filters.html" class="internal-link" target="_self" rel="noopener nofollow">Ensemble Square-Root Kalman Filter</a>

<br>Adapts the covariance to fulfill the Kalman identity


<br><br><br>
<br>Prediction 

<br>In one dimension, the state simply evolves by scaling, yielding 
<br>The <a data-tooltip-position="top" aria-label="Expectations" data-href="Expectations" href="the-guide/mathematics/statistics/expectations.html" class="internal-link" target="_self" rel="noopener nofollow">mean</a> can be updated by simply shifting 
<br>The variance is updated using <a data-tooltip-position="top" aria-label="Gaussian Distribution" data-href="Gaussian Distribution" href="the-guide/mathematics/probability-theory/gaussian-distribution.html" class="internal-link" target="_self" rel="noopener nofollow">laws for Gaussians</a> via 


<br>Correction 

<br>As above, the measurements / observations are mapped via scaling, yielding
<br>The mean is updated using <a data-tooltip-position="top" aria-label="Gaussian Distribution" data-href="Gaussian Distribution" href="the-guide/mathematics/probability-theory/gaussian-distribution.html" class="internal-link" target="_self" rel="noopener nofollow">laws for Gaussians</a> via 
<br>The variance is updated using <a data-tooltip-position="top" aria-label="Gaussian Distribution" data-href="Gaussian Distribution" href="the-guide/mathematics/probability-theory/gaussian-distribution.html" class="internal-link" target="_self" rel="noopener nofollow">laws for Gaussians</a> via 


]]></description><link>the-guide/computational-statistics/data-assimilation/filtering-algorithms/kalman-filter.html</link><guid isPermaLink="false">The Guide/Computational Statistics/Data Assimilation/Filtering Algorithms/Kalman Filter.md</guid><pubDate>Fri, 14 Mar 2025 09:44:21 GMT</pubDate><enclosure url="lib/media/pasted-image-20240301190300.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src=&quot;lib/media/pasted-image-20240301190300.png&quot;&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[Long-Time Behavior of Filters]]></title><description><![CDATA[<a class="tag" href="?query=tag:Data-Assimilation" style="background-color: rgb(4, 108, 116); color: white; font-weight: 700; border: none; border-radius: 1em; padding: 0.2em 0.5em;">#Data-Assimilation</a> 
 <br>With the exception of <a data-tooltip-position="top" aria-label="Kalman Filter" data-href="Kalman Filter" href="the-guide/computational-statistics/data-assimilation/filtering-algorithms/kalman-filter.html" class="internal-link" target="_self" rel="noopener nofollow">the Kalman filter</a> for linear problems and the <a data-tooltip-position="top" aria-label="The Particle Filter" data-href="The Particle Filter" href="the-guide/computational-statistics/data-assimilation/filtering-algorithms/the-particle-filter.html" class="internal-link" target="_self" rel="noopener nofollow">particle filter</a> in the general case, none of the <a data-tooltip-position="top" aria-label="The Filtering Problem" data-href="The Filtering Problem" href="the-guide/computational-statistics/data-assimilation/filtering-algorithms/the-filtering-problem.html" class="internal-link" target="_self" rel="noopener nofollow">filtering</a> algorithms discussed in the <a href=".?query=tag:Data-Assimilation" class="tag" target="_blank" rel="noopener nofollow">#Data-Assimilation</a> course give accurate approximations in terms of the <a data-tooltip-position="top" aria-label="Assessing the Quality of Data-Assimilation Algorithms" data-href="Assessing the Quality of Data-Assimilation Algorithms" href="the-guide/computational-statistics/data-assimilation/assessing-the-quality-of-data-assimilation-algorithms.html" class="internal-link" target="_self" rel="noopener nofollow">Bayesian quality assessment test</a>. They can, however perform well in the signal estimation sense, which allows them to work well in many applications<br>The short-term effect of any filter is dominated by the prior because of the lack of data, which is why mathematically, it is only useful to discuss the results in the limit of infinite iterations to determine convergence properties.<br>In this context, it is of interest to study <br>
<br>whether an algorithm can recover the signal underlying the data when iterated long enough 
<br>whether this holds for poorly initialized priors
<br>For this, we need the data to be rich enough to stabilize the inherent instabilities of the <a data-tooltip-position="top" aria-label="Static and Dynamic Systems" data-href="Static and Dynamic Systems" href="the-guide/robotics,-dynamics-and-control/dynamics/static-and-dynamic-systems.html" class="internal-link" target="_self" rel="noopener nofollow">dynamical</a> <a data-tooltip-position="top" aria-label="Static and Dynamic Systems" data-href="Static and Dynamic Systems" href="the-guide/robotics,-dynamics-and-control/dynamics/static-and-dynamic-systems.html" class="internal-link" target="_self" rel="noopener nofollow">system</a>. Note that it theoretically suffices to only observe the unstable directions, since the model dynamics enable recovery in the space of stable directions<br><br><br>We study the <a data-tooltip-position="top" aria-label="Data Assimilation" data-href="Data Assimilation" href="the-guide/computational-statistics/data-assimilation/data-assimilation.html" class="internal-link" target="_self" rel="noopener nofollow">linear guiding example</a> using the one-dimensional <a data-tooltip-position="top" aria-label="Kalman Filter" data-href="Kalman Filter" href="the-guide/computational-statistics/data-assimilation/filtering-algorithms/kalman-filter.html" class="internal-link" target="_self" rel="noopener nofollow">Kalman filter</a>. The dynamics simplifies toBased on the formulas we know, we can derive closed-form expressions for the covariance and the meanwhere we define the update <a data-tooltip-position="top" aria-label="Mathematical Relations" data-href="Mathematical Relations" href="the-guide/mathematics/general-stuff/mathematical-relations.html" class="internal-link" target="_self" rel="noopener nofollow">function</a>  with This shows that in this special case, the <a data-tooltip-position="top" aria-label="Covariance and Variance" data-href="Covariance and Variance" href="the-guide/mathematics/statistics/covariance-and-variance.html" class="internal-link" target="_self" rel="noopener nofollow">covariances</a> evolve independently of the data and the means, satisfying an autonomous nonlinear dynamical system. For , the system becomes linear.<br>We consider the limit of . First, we derive the fixed points   (insert left and right) of the covariance update by solving the quadratic equationThe roots are then given bywhich provided  yields one positive and one negative root. Additionally, provided that , the update only has quadratic terms, ensuring  for all . Lastly, the update equation also demonstrates that  for all times (bounded by data variance).<br>With these observations, we may focus on the positive  that we know to be bounded by  to derive stability results. <br>
<br>Deterministic Case  - In this case, we obtainwhich in turn yields three cases depending on :

<br> =  <a data-tooltip-position="top" aria-label="Ljapunov Stability of Dynamical Systems" data-href="Ljapunov Stability of Dynamical Systems" href="the-guide/robotics,-dynamics-and-control/dynamics/ljapunov-stability-of-dynamical-systems.html" class="internal-link" target="_self" rel="noopener nofollow">globally asymptotically stable</a>
<br> =  globally asymptotically stable
<br> = both are neutrally stable


<br>Stochastic Case  - In this case we may derive the two identitiesFrom the first, using , we can obtain the bounds while for the second we getby using the bound . The fixed point of  is always stable.
<br><br>Advantages
For this (very limiting) case, the Kalman filter accurately recovers the signal provided the observational noise is small. This allows for poor initializations far from the true signal.
<br>
<br>Look at formula for mean: if , we simply get the true dynamics. For the other cases, we recover the observations, so the result depends on the noise level.
<br><br><br>We base the following on a true signal , yielding Additionally, we assume that At this point we write out the update of the 3DVar algorithm for the true signal  and the estimate With the result and the triangle inequality, we can derive an iterative error update of the formThe observation covariance is bound by assumption. For the other part, we can control the term through the covariance that we choose. Based on a Lipschitz constant , the above is bounded bywhich using the <a data-href="Gronwall Lemma" href="the-guide/mathematics/general-stuff/gronwall-lemma.html" class="internal-link" target="_self" rel="noopener nofollow">Gronwall Lemma</a> gives the following result.<br>Theorem 4.10 - 3DVar Signal Estimation Error Bound
Based on the assumptions above and if we choose a <a data-tooltip-position="top" aria-label="Covariance and Variance" data-href="Covariance and Variance" href="the-guide/mathematics/statistics/covariance-and-variance.html" class="internal-link" target="_self" rel="noopener nofollow">covariance</a> , such that is globally <a data-tooltip-position="top" aria-label="Continuity" data-href="Continuity" href="the-guide/mathematics/analysis-and-calculus/continuity.html" class="internal-link" target="_self" rel="noopener nofollow">Lipschitz</a> with constant  (<a data-tooltip-position="top" aria-label="Contraction Mapping" data-href="Contraction Mapping" href="the-guide/mathematics/general-stuff/contraction-mapping.html" class="internal-link" target="_self" rel="noopener nofollow">contraction mapping</a>) in any norm , there is a constant , such that 
<br>Additional Information
The theorem shows that it is possible to design filters that recover from poor initialization when iterated long enough. The key is that by choosing  appropriately, we can make  a <a data-tooltip-position="top" aria-label="Contraction Mapping" data-href="Contraction Mapping" href="the-guide/mathematics/general-stuff/contraction-mapping.html" class="internal-link" target="_self" rel="noopener nofollow">contraction mapping</a>. This involves a subtle interplay between the dynamics and the observation, indicating that the success of this approach is determined by whether the unstable parts of the dynamics are observed or not.
<br>
<br>Fully Observable System  - we further assume  and choose . We can then derive with . If  is globally Lipschitz with constant , we can choose ,such that i.e. make  sufficiently small for the filter to contract. This corresponds to trusting the data sufficiently (increasing model variance ) in comparison to the model (). This process is called variance inflation.
<br>Partially Observable System  - Based on the same definitions as above, we obtainThis shows that for partial observability, we additionally require a relationship between the observable subspace and the expanding subspace of the dynamics. 
<br>For more information, see <a data-href="Observability and Detectability of LTI Systems" href="the-guide/robotics,-dynamics-and-control/dynamics/observability-and-detectability-of-lti-systems.html" class="internal-link" target="_self" rel="noopener nofollow">Observability and Detectability of LTI Systems</a> and <a data-href="Controllability and Stabilizability of LTI Systems" href="the-guide/robotics,-dynamics-and-control/dynamics/controllability-and-stabilizability-of-lti-systems.html" class="internal-link" target="_self" rel="noopener nofollow">Controllability and Stabilizability of LTI Systems</a>.]]></description><link>the-guide/computational-statistics/data-assimilation/filtering-algorithms/long-time-behavior-of-filters.html</link><guid isPermaLink="false">The Guide/Computational Statistics/Data Assimilation/Filtering Algorithms/Long-Time Behavior of Filters.md</guid><pubDate>Sat, 12 Apr 2025 12:26:41 GMT</pubDate></item><item><title><![CDATA[Sequential Importance Resampling (SIR)]]></title><description><![CDATA[ 
 <br>In a Nutshell
Simplest implementation of the <a data-tooltip-position="top" aria-label="The Particle Filter" data-href="The Particle Filter" href="the-guide/computational-statistics/data-assimilation/filtering-algorithms/the-particle-filter.html" class="internal-link" target="_self" rel="noopener nofollow">particle filter</a>. Instead of analytical <a data-tooltip-position="top" aria-label="Probability Distribution" data-href="Probability Distribution" href="the-guide/mathematics/probability-theory/probability-distribution.html" class="internal-link" target="_self" rel="noopener nofollow">distributions</a>, we use an ensemble of  samples. Using <a data-tooltip-position="top" aria-label="Monte Carlo Integration" data-href="Monte Carlo Integration" href="the-guide/mathematics/probability-theory/monte-carlo-integration.html" class="internal-link" target="_self" rel="noopener nofollow">MC integration</a> and <a data-tooltip-position="top" aria-label="Importance Sampling" data-href="Importance Sampling" href="the-guide/mathematics/probability-theory/importance-sampling.html" class="internal-link" target="_self" rel="noopener nofollow">importance sampling</a> with importance density derived from the <a data-tooltip-position="top" aria-label="Markov Kernel" data-href="Markov Kernel" href="the-guide/computational-statistics/data-assimilation/markov-kernel.html" class="internal-link" target="_self" rel="noopener nofollow">Markov kernel</a>, we can approximately evolve them just as we evolved the Gaussians of the <a data-href="Kalman Filter" href="the-guide/computational-statistics/data-assimilation/filtering-algorithms/kalman-filter.html" class="internal-link" target="_self" rel="noopener nofollow">Kalman Filter</a> analytically.<br>
Because the algorithm resamples unlikely samples, it is sometimes called bootstrap filter.
<br><br>Whereas the <a data-href="Kalman Smoother" href="the-guide/computational-statistics/data-assimilation/smoothing-algorithms/kalman-smoother.html" class="internal-link" target="_self" rel="noopener nofollow">Kalman Smoother</a> and the <a data-tooltip-position="top" aria-label="Approximate Gaussian Filters" data-href="Approximate Gaussian Filters" href="the-guide/computational-statistics/data-assimilation/filtering-algorithms/approximate-gaussian-filters.html" class="internal-link" target="_self" rel="noopener nofollow">EnKF</a> use equally weighted particles, the particle filter uses the data to adjust the weights instead of incorporating them into the particle update. <br>Starting from an Dirac <a data-tooltip-position="top" aria-label="Measure" data-href="Measure" href="the-guide/mathematics/measure-theory/measure.html" class="internal-link" target="_self" rel="noopener nofollow">measure</a> , we want to find a scheme to evolve the weights according to <br>
<br>Prediction - approximate the prediction phase by sampling new particles  from the <a data-tooltip-position="top" aria-label="Markov Chain" data-href="Markov Chain" href="the-guide/computational-statistics/data-assimilation/markov-chain.html" class="internal-link" target="_self" rel="noopener nofollow">Markov chain</a> while keeping the weights
<br>Analysis - incorporate information about the data by re-weighting via <a data-href="Bayes Theorem" href="the-guide/mathematics/statistics/bayes-theorem.html" class="internal-link" target="_self" rel="noopener nofollow">Bayes Theorem</a> while keeping the positions
<br>Resampling - as noted in <a data-tooltip-position="top" aria-label="The Particle Filter" data-href="The Particle Filter" href="the-guide/computational-statistics/data-assimilation/filtering-algorithms/the-particle-filter.html" class="internal-link" target="_self" rel="noopener nofollow">degeneracy phenomenon</a>, the above approach can perform poorly if most particles weights approach . To overcome this we can simply delete weights below a certain weight threshold and then duplicate others by samplingThe probabilistic nature of the Markov transition and the noise will spread the highly likely particles again.
<br>Algorithm

<br>Init , 
<br>repeat

<br>Draw  new particles from  and assign uniform weights
<br>Redefine  based on new samples from step above
<br>Evolve particles via <a data-tooltip-position="top" aria-label="Markov Kernel" data-href="Markov Kernel" href="the-guide/computational-statistics/data-assimilation/markov-kernel.html" class="internal-link" target="_self" rel="noopener nofollow">Marko kernel</a>
<br>Update weights of particles via <a data-href="Bayes Theorem" href="the-guide/mathematics/statistics/bayes-theorem.html" class="internal-link" target="_self" rel="noopener nofollow">Bayes Theorem</a>
<br>Redefine
<br>



<br><br><br>Resampling Formulation - We want to prove the convergence for increasing number of particles. The true filtering distribution satisfies the iteration where  evolves a particle via the (probabilistic) <a data-href="Markov Kernel" href="the-guide/computational-statistics/data-assimilation/markov-kernel.html" class="internal-link" target="_self" rel="noopener nofollow">Markov Kernel</a> while  applies <a data-href="Bayes Theorem" href="the-guide/mathematics/statistics/bayes-theorem.html" class="internal-link" target="_self" rel="noopener nofollow">Bayes Theorem</a>. The particle filtering algorithm evolveswhere we used a trick to switch evolving  and resampling . <br>Intuition
We switched from resampling and then evolving to evolving and then resampling, which does not matter since we re-weight according to the likelihood and prior afterwards. The alternative algorithm simply switches two steps, but over multiple iterations produces the same outcome (statistically).
<br>Analysing the Resampling Error and Error Propagation - To understand the filter, we need to understand the implication of the resampling on the error between the true and the particle filter distribution. This includes the induced error as well as its propagation.<br>All the operators  and  map the <a data-tooltip-position="top" aria-label="Space" data-href="Space" href="the-guide/mathematics/general-stuff/space.html" class="internal-link" target="_self" rel="noopener nofollow">space</a> of <a data-tooltip-position="top" aria-label="Measure" data-href="Measure" href="the-guide/mathematics/measure-theory/measure.html" class="internal-link" target="_self" rel="noopener nofollow">probability measures</a> onto itself. We write  in the sense in order to avoid the use of densities. The mappings are defined as  and and The first two are deterministic (we can compute the integral because we know distribution , eliminating the stochasticity of sampling noise), while  involves artificially sampling single particles and is therefore not deterministic. Since the randomness thereby only stems from , we can use <a data-tooltip-position="top" aria-label="Metrics on Spaces of Probability Measures" data-href="Metrics on Spaces of Probability Measures" href="the-guide/computational-statistics/data-assimilation/metrics-on-spaces-of-probability-measures.html" class="internal-link" target="_self" rel="noopener nofollow">the root mean square distance</a>.<br>Let  denote an element of  for each . We use this as a random variable describing the randomness required for the sampling operator. Between two random probability measures , we can define a <a data-tooltip-position="top" aria-label="Metrics on Spaces of Probability Measures" data-href="Metrics on Spaces of Probability Measures" href="the-guide/computational-statistics/data-assimilation/metrics-on-spaces-of-probability-measures.html" class="internal-link" target="_self" rel="noopener nofollow">metric</a>based on . We use this distance in the following proof to insert the triangle inequality. <br>The following theorem verifies that the particle filter does converge to the true filtering distribution in the limit of infinite particles:<br>Theorem 4.5 
Assuming there exists a , such that for all  and  it holds that for the likelihood  we have that with the <a data-tooltip-position="top" aria-label="Metrics on Spaces of Probability Measures" data-href="Metrics on Spaces of Probability Measures" href="the-guide/computational-statistics/data-assimilation/metrics-on-spaces-of-probability-measures.html" class="internal-link" target="_self" rel="noopener nofollow">root mean square distance</a>
<br>
<br>The distance between the true and resampled distribution is bounded, larger number of particles or larger  helps to lower
<br>Additional Information

<br>The theorem shows that for any timestep , the <a data-tooltip-position="top" aria-label="The Filtering Problem" data-href="The Filtering Problem" href="the-guide/computational-statistics/data-assimilation/filtering-algorithms/the-filtering-problem.html" class="internal-link" target="_self" rel="noopener nofollow">filtering distribution</a> is approximated reasonably well by the bootstrap algorithm in the sense that the measure apporaches the true one for infinite particles. However, since , the number of required particles grows with  (exponent .
<br>If  is very small convergence can be very slow and the number of required particles can become infeasible.
<br>In applications, the likelihood may even be unbounded, requiring more refined analysis. However, for <a data-tooltip-position="top" aria-label="Ergodicity" data-href="Ergodicity" href="the-guide/computational-statistics/data-assimilation/ergodicity.html" class="internal-link" target="_self" rel="noopener nofollow">ergodic</a> <a data-tooltip-position="top" aria-label="Markov Kernel" data-href="Markov Kernel" href="the-guide/computational-statistics/data-assimilation/markov-kernel.html" class="internal-link" target="_self" rel="noopener nofollow">Markov kernels</a> this is possible.

<br>Proof
Proof is based on helping lemmas below. By the triangle inequality and using , we havewhere  uses the metric we introduced above. Iterating this based on  gives the result.
<br><br>For the proof above we needed a couple of Lemmas. Those are proven below.<br>Lemma 4.7
The sampling operator  satisfiesThe effect of the resampling step can be bounded above.
<br>Proof
We consider any measure  and an <a data-tooltip-position="top" aria-label="Statistical Independence" data-href="Statistical Independence" href="the-guide/mathematics/statistics/statistical-independence.html" class="internal-link" target="_self" rel="noopener nofollow">i.i.d.</a> sequence  with . The randomness arises from applying the sampling operatorWe define , yielding Because the samples are independent, we have thatIf additionally , we deducesince . With this, we can followwhich is independent of . We can thereby use the supremum to obtain the result.
<br>Lemma 4.8
Since  is a <a data-tooltip-position="top" aria-label="Markov Kernel" data-href="Markov Kernel" href="the-guide/computational-statistics/data-assimilation/markov-kernel.html" class="internal-link" target="_self" rel="noopener nofollow">Markov kernel</a>, we get the bound The Markov kernel implies a <a data-tooltip-position="top" aria-label="Contraction Mapping" data-href="Contraction Mapping" href="the-guide/mathematics/general-stuff/contraction-mapping.html" class="internal-link" target="_self" rel="noopener nofollow">contraction mapping</a>.
<br>Proof
We start by defining a clever auxiliary functionsthat is the expected value if  after one step of the Markov chain started from . For any , we have and thereby also Noting thatif follows that Thus
<br>
<br>Idea: absorb operator into  that is still bounded the same as  in order to just redefine function for distance
<br>Second sup argument is that the set of  is larger than that constructed from  !
<br>Could also divide by sup of f and take arbitrary f, assumption to enable conclusions basically
<br>Lemma 4.9
Using the same assumptions as for Theorem 4.5 above, we have The re-weighting scales the distance.
<br>For , we can rewrite Additionally, it holds by assumption that  and , since ... . ThusAgain by assumption , which yields  because we assume again that  to make connections to the square distance. All together, we getfrom which the result follows. ]]></description><link>the-guide/computational-statistics/data-assimilation/filtering-algorithms/sequential-importance-resampling-(sir).html</link><guid isPermaLink="false">The Guide/Computational Statistics/Data Assimilation/Filtering Algorithms/Sequential Importance Resampling (SIR).md</guid><pubDate>Sat, 28 Sep 2024 05:39:04 GMT</pubDate></item><item><title><![CDATA[SIR with Optimal Proposal]]></title><description><![CDATA[ 
 <br>In a Nutshell
<a data-tooltip-position="top" aria-label="The Particle Filter" data-href="The Particle Filter" href="the-guide/computational-statistics/data-assimilation/filtering-algorithms/the-particle-filter.html" class="internal-link" target="_self" rel="noopener nofollow">Particle filter</a> that improves upon <a data-href="Sequential Importance Resampling (SIR)" href="the-guide/computational-statistics/data-assimilation/filtering-algorithms/sequential-importance-resampling-(sir).html" class="internal-link" target="_self" rel="noopener nofollow">Sequential Importance Resampling (SIR)</a> by using <a data-tooltip-position="top" aria-label="Importance Sampling" data-href="Importance Sampling" href="the-guide/mathematics/probability-theory/importance-sampling.html" class="internal-link" target="_self" rel="noopener nofollow">importance sampling</a> via a <a data-tooltip-position="top" aria-label="The Filtering Problem" data-href="The Filtering Problem" href="the-guide/computational-statistics/data-assimilation/filtering-algorithms/the-filtering-problem.html" class="internal-link" target="_self" rel="noopener nofollow">proposal distribution</a> that is conditioned on the <a data-tooltip-position="top" aria-label="Data Assimilation" data-href="Data Assimilation" href="the-guide/computational-statistics/data-assimilation/data-assimilation.html" class="internal-link" target="_self" rel="noopener nofollow">data</a>.
<br><br>Instead of moving particles via the <a data-tooltip-position="top" aria-label="Markov Kernel" data-href="Markov Kernel" href="the-guide/computational-statistics/data-assimilation/markov-kernel.html" class="internal-link" target="_self" rel="noopener nofollow">Markov kernel</a> , we use a conditioned kernel  with <a data-tooltip-position="top" aria-label="Probability Density Function" data-href="Probability Density Function" href="the-guide/mathematics/probability-theory/probability-density-function.html" class="internal-link" target="_self" rel="noopener nofollow">density</a> . The weights are found as before via <a data-href="Bayes Theorem" href="the-guide/mathematics/statistics/bayes-theorem.html" class="internal-link" target="_self" rel="noopener nofollow">Bayes Theorem</a>Simply inserting the Markov kernel above brings us back to the update of the <a data-tooltip-position="top" aria-label="Sequential Importance Resampling (SIR)" data-href="Sequential Importance Resampling (SIR)" href="the-guide/computational-statistics/data-assimilation/filtering-algorithms/sequential-importance-resampling-(sir).html" class="internal-link" target="_self" rel="noopener nofollow">SIR</a> <a data-tooltip-position="top" aria-label="The Filtering Problem" data-href="The Filtering Problem" href="the-guide/computational-statistics/data-assimilation/filtering-algorithms/the-filtering-problem.html" class="internal-link" target="_self" rel="noopener nofollow">filter</a>. Notice that because of the normalization step, the<br>
]]></description><link>the-guide/computational-statistics/data-assimilation/filtering-algorithms/sir-with-optimal-proposal.html</link><guid isPermaLink="false">The Guide/Computational Statistics/Data Assimilation/Filtering Algorithms/SIR with Optimal Proposal.md</guid><pubDate>Thu, 19 Sep 2024 06:41:40 GMT</pubDate></item><item><title><![CDATA[The Filtering Problem]]></title><description><![CDATA[<a class="tag" href="?query=tag:Data-Assimilation" style="background-color: rgb(4, 108, 116); color: white; font-weight: 700; border: none; border-radius: 1em; padding: 0.2em 0.5em;">#Data-Assimilation</a> 
 <br>In a Nutshell
Online setting for <a data-href="Data Assimilation" href="the-guide/computational-statistics/data-assimilation/data-assimilation.html" class="internal-link" target="_self" rel="noopener nofollow">Data Assimilation</a>, determine information on the state of the signal at any point, therefore only considering the accumulated data up until time-step , which we denote . The process of filtering is concerned with determining the <a data-tooltip-position="top" aria-label="Probability Density Function" data-href="Probability Density Function" href="the-guide/mathematics/probability-theory/probability-density-function.html" class="internal-link" target="_self" rel="noopener nofollow">pdf</a>  associated with the probability <a data-href="Measure" href="the-guide/mathematics/measure-theory/measure.html" class="internal-link" target="_self" rel="noopener nofollow">Measure</a> of the <a data-tooltip-position="top" aria-label="Random Variable" data-href="Random Variable" href="the-guide/mathematics/probability-theory/random-variable.html" class="internal-link" target="_self" rel="noopener nofollow">random variable</a>  and the sequential update of this as the index increments.
<br><br><br>
<br><a data-href="Kalman Filter" href="the-guide/computational-statistics/data-assimilation/filtering-algorithms/kalman-filter.html" class="internal-link" target="_self" rel="noopener nofollow">Kalman Filter</a> - linear <a data-tooltip-position="top" aria-label="Static and Dynamic Systems" data-href="Static and Dynamic Systems" href="the-guide/robotics,-dynamics-and-control/dynamics/static-and-dynamic-systems.html" class="internal-link" target="_self" rel="noopener nofollow">dynamics</a> and observation, all randomness via <a data-tooltip-position="top" aria-label="Gaussian Distribution" data-href="Gaussian Distribution" href="the-guide/mathematics/probability-theory/gaussian-distribution.html" class="internal-link" target="_self" rel="noopener nofollow">Gaussians</a>

<br>Yields closed-form posterior


<br><a data-href="Approximate Gaussian Filters" href="the-guide/computational-statistics/data-assimilation/filtering-algorithms/approximate-gaussian-filters.html" class="internal-link" target="_self" rel="noopener nofollow">Approximate Gaussian Filters</a> - Generalization of <a data-href="Kalman Filter" href="the-guide/computational-statistics/data-assimilation/filtering-algorithms/kalman-filter.html" class="internal-link" target="_self" rel="noopener nofollow">Kalman Filter</a> to nonlinear case

<br>Robust, but no convergence theorems for measures


<br><a data-href="The Particle Filter" href="the-guide/computational-statistics/data-assimilation/filtering-algorithms/the-particle-filter.html" class="internal-link" target="_self" rel="noopener nofollow">The Particle Filter</a>

<br>Convergence to true distribution
<br>Not robust in high dimensions


<br><br>In contrast to <a data-tooltip-position="top" aria-label="The Smoothing Problem" data-href="The Smoothing Problem" href="the-guide/computational-statistics/data-assimilation/smoothing-algorithms/the-smoothing-problem.html" class="internal-link" target="_self" rel="noopener nofollow">smoothing</a>, the filtering formulation does not yield closed-form solutions even for the <a data-tooltip-position="top" aria-label="Gaussian Distribution" data-href="Gaussian Distribution" href="the-guide/mathematics/probability-theory/gaussian-distribution.html" class="internal-link" target="_self" rel="noopener nofollow">Gaussian</a> setting we use in the <a href=".?query=tag:Data-Assimilation" class="tag" target="_blank" rel="noopener nofollow">#Data-Assimilation</a> course. The following formulas serve as a starting point for various approaches that try to approximate distributions.<br>
<br>
Prediction  - We can derive this map directly based on the <a data-tooltip-position="top" aria-label="Markov Chain" data-href="Markov Chain" href="the-guide/computational-statistics/data-assimilation/markov-chain.html" class="internal-link" target="_self" rel="noopener nofollow">Markovian</a> assumption via where we also used the fact that  by the <a data-href="Data Processing Inequality" href="the-guide/information-theory/information-theory-1/channel-coding/data-processing-inequality.html" class="internal-link" target="_self" rel="noopener nofollow">Data Processing Inequality</a>. In terms of measures, this step maps  to .

<br>
Analysis  via <a data-href="Bayes Theorem" href="the-guide/mathematics/statistics/bayes-theorem.html" class="internal-link" target="_self" rel="noopener nofollow">Bayes Theorem</a> - Again arguing by the <a data-href="Data Processing Inequality" href="the-guide/information-theory/information-theory-1/channel-coding/data-processing-inequality.html" class="internal-link" target="_self" rel="noopener nofollow">Data Processing Inequality</a>, we can see that  and continue via <a data-href="Bayes Theorem" href="the-guide/mathematics/statistics/bayes-theorem.html" class="internal-link" target="_self" rel="noopener nofollow">Bayes Theorem</a> to obtain In terms of measures, this step maps  to .

<br>
Filtering Update - Conceptually, the above operations can be written via the mappings where  is linear and independent of  because the underlying <a data-tooltip-position="top" aria-label="Markov Decision Process" data-href="Markov Decision Process" href="the-guide/machine-learning/reinforcement-learning/markov-decision-process.html" class="internal-link" target="_self" rel="noopener nofollow">Markov process</a> does not change.  on the other hand is non-linear and depends on  because the data at each step changes.

<br><br><br>As a simple example that can be computed analytically, we can assume that the <a data-tooltip-position="top" aria-label="Static and Dynamic Systems" data-href="Static and Dynamic Systems" href="the-guide/robotics,-dynamics-and-control/dynamics/static-and-dynamic-systems.html" class="internal-link" target="_self" rel="noopener nofollow">stochastic dynamics model</a> is  and that all our distributions are <a data-tooltip-position="top" aria-label="Gaussian Distribution" data-href="Gaussian Distribution" href="the-guide/mathematics/probability-theory/gaussian-distribution.html" class="internal-link" target="_self" rel="noopener nofollow">Gaussians</a>. In this case, we have <br>
<br>Initial distribution 
<br>Noise on the state 
<br>Observation operator 
<br>Observation noise <br>
and want to find an explicit formula for
<br>Updates  for the intermediate state distribution 
<br>We can characterize the prediction distribution  as <a data-tooltip-position="top" aria-label="Gaussian Distribution" data-href="Gaussian Distribution" href="the-guide/mathematics/probability-theory/gaussian-distribution.html" class="internal-link" target="_self" rel="noopener nofollow">Gaussian</a>  by computing To characterize  as a Gaussian, we first employ the enumerator of <a data-href="Bayes Theorem" href="the-guide/mathematics/statistics/bayes-theorem.html" class="internal-link" target="_self" rel="noopener nofollow">Bayes Theorem</a> to writeWe now have to find an expression for  in order to use the <a data-tooltip-position="top" aria-label="Gaussian Distribution" data-href="Gaussian Distribution" href="the-guide/mathematics/probability-theory/gaussian-distribution.html" class="internal-link" target="_self" rel="noopener nofollow">Lemma on normalization for Gaussians</a>. The exponentials allow us to to a coefficient comparison by writing out all the binomials above and absorb all linear terms into the unknown normalization constant. We obtain where the <a data-tooltip-position="top" aria-label="Covariance and Variance" data-href="Covariance and Variance" href="the-guide/mathematics/statistics/covariance-and-variance.html" class="internal-link" target="_self" rel="noopener nofollow">variance</a> is indeed non-negative. ]]></description><link>the-guide/computational-statistics/data-assimilation/filtering-algorithms/the-filtering-problem.html</link><guid isPermaLink="false">The Guide/Computational Statistics/Data Assimilation/Filtering Algorithms/The Filtering Problem.md</guid><pubDate>Wed, 23 Apr 2025 21:55:33 GMT</pubDate></item><item><title><![CDATA[The Particle Filter]]></title><description><![CDATA[<a class="tag" href="?query=tag:Data-Assimilation" style="background-color: rgb(4, 108, 116); color: white; font-weight: 700; border: none; border-radius: 1em; padding: 0.2em 0.5em;">#Data-Assimilation</a> <a class="tag" href="?query=tag:Intelligent-Robotic-Manipulation-Course" style="background-color: rgb(4, 108, 116); color: white; font-weight: 700; border: none; border-radius: 1em; padding: 0.2em 0.5em;">#Intelligent-Robotic-Manipulation-Course</a> 
 <br>In a Nutshell
Particle filtering uses a set of weighted particles (also called samples) to represent the <a data-tooltip-position="top" aria-label="Bayes Theorem" data-href="Bayes Theorem" href="the-guide/mathematics/statistics/bayes-theorem.html" class="internal-link" target="_self" rel="noopener nofollow">posterior</a> of a <a data-tooltip-position="top" aria-label="Stochastic Process" data-href="Stochastic Process" href="the-guide/mathematics/probability-theory/stochastic-process.html" class="internal-link" target="_self" rel="noopener nofollow">stochastic process</a> given the noisy and/or partial observations. The state-space model can be nonlinear and the initial state and noise distributions can take any form required. In contrast to <a data-href="Approximate Gaussian Filters" href="the-guide/computational-statistics/data-assimilation/filtering-algorithms/approximate-gaussian-filters.html" class="internal-link" target="_self" rel="noopener nofollow">Approximate Gaussian Filters</a>, it can be proven that particle filters approach the true <a data-tooltip-position="top" aria-label="The Filtering Problem" data-href="The Filtering Problem" href="the-guide/computational-statistics/data-assimilation/filtering-algorithms/the-filtering-problem.html" class="internal-link" target="_self" rel="noopener nofollow">posterior filtering ditribution</a> in the infinite particle limit. Another name that is often uses is Sequential Monte Carlo Methods.
<br>Compared to e.g. the basic <a data-tooltip-position="top" aria-label="Kalman Filter" data-href="Kalman Filter" href="the-guide/computational-statistics/data-assimilation/filtering-algorithms/kalman-filter.html" class="internal-link" target="_self" rel="noopener nofollow">Kalman filter</a>, this enables arbitrary distributions, multimodal support and more hypotheses. In general, these methods approximate a complex model rather than exactly representing a simplified model.<br><img alt="center" src="lib/media/pasted-image-20240314111601.png" style="width: 400px; max-width: 100%;"><br>Intuition
Intuitively, particle filters are close to the <a data-tooltip-position="top" aria-label="Approximate Gaussian Filters" data-href="Approximate Gaussian Filters" href="the-guide/computational-statistics/data-assimilation/filtering-algorithms/approximate-gaussian-filters.html" class="internal-link" target="_self" rel="noopener nofollow">ensemble Kalman filter</a>, but they do not use uniform weights over the particles and can extend the method e.g. by resampling.
<br><br> In our <a href=".?query=tag:Data-Assimilation" class="tag" target="_blank" rel="noopener nofollow">#Data-Assimilation</a> setting, we consider the measures  and , corresponding to the <a data-tooltip-position="top" aria-label="Probability Density Function" data-href="Probability Density Function" href="the-guide/mathematics/probability-theory/probability-density-function.html" class="internal-link" target="_self" rel="noopener nofollow">densities</a>  (Analysis) and  (Prediction), respectively. The basic form of a particle filter extends the idea of the Ensemble Kalman Filter to allow varying weights inadding the condition that the weights sum up to . This leaves us to find update rules of the formfor prediction and analysis, whereas the EnKF only considered updating the positions.<br>
<br>Prediction can be rewritten as 
<br>Analysis can be rewritten as Writing the updates like that is important to make sense of them in the absence of <a data-tooltip-position="top" aria-label="Lebesgue Measure" data-href="Lebesgue Measure" href="the-guide/mathematics/measure-theory/lebesgue-measure.html" class="internal-link" target="_self" rel="noopener nofollow">Lebesgue</a> <a data-tooltip-position="top" aria-label="Probability Density Function" data-href="Probability Density Function" href="the-guide/mathematics/probability-theory/probability-density-function.html" class="internal-link" target="_self" rel="noopener nofollow">densities</a>, as we rely on Dirac masses. The analysis step in particular allows us to formulate
<br>Lemma - Re-weighting for Analysis Step
Based on the reformulation above for the analysis step, we can formulate a re-weighting procedure to get <a data-tooltip-position="top" aria-label="Expectations" data-href="Expectations" href="the-guide/mathematics/statistics/expectations.html" class="internal-link" target="_self" rel="noopener nofollow">expectations</a> under  from expectations under  via 
<br><br><br>The following was a more applied introduction to the topic given in the <a href=".?query=tag:Intelligent-Robotic-Manipulation-Course" class="tag" target="_blank" rel="noopener nofollow">#Intelligent-Robotic-Manipulation-Course</a> during my Masters. This formulation essentially already incorporates the <a data-href="SIR with Optimal Proposal" href="the-guide/computational-statistics/data-assimilation/filtering-algorithms/sir-with-optimal-proposal.html" class="internal-link" target="_self" rel="noopener nofollow">SIR with Optimal Proposal</a> by allowing a proposal conditioned on the data. <br>Instead of analytical <a data-tooltip-position="top" aria-label="Probability Distribution" data-href="Probability Distribution" href="the-guide/mathematics/probability-theory/probability-distribution.html" class="internal-link" target="_self" rel="noopener nofollow">distributions</a> we use an ensemble of  samples. Using <a data-href="Monte Carlo Integration" href="the-guide/mathematics/probability-theory/monte-carlo-integration.html" class="internal-link" target="_self" rel="noopener nofollow">Monte Carlo Integration</a> and <a data-href="Importance Sampling" href="the-guide/mathematics/probability-theory/importance-sampling.html" class="internal-link" target="_self" rel="noopener nofollow">Importance Sampling</a>, we can approximately evolve them just as we evolved the Gaussians of the <a data-href="Kalman Filter" href="the-guide/computational-statistics/data-assimilation/filtering-algorithms/kalman-filter.html" class="internal-link" target="_self" rel="noopener nofollow">Kalman Filter</a> analytically.<br>
<br>Applying <a data-href="Importance Sampling" href="the-guide/mathematics/probability-theory/importance-sampling.html" class="internal-link" target="_self" rel="noopener nofollow">Importance Sampling</a>

<br>Characterize <a data-tooltip-position="top" aria-label="Bayes Theorem" data-href="Bayes Theorem" href="the-guide/mathematics/statistics/bayes-theorem.html" class="internal-link" target="_self" rel="noopener nofollow">posterior</a> <a data-tooltip-position="top" aria-label="Probability Mass Function" data-href="Probability Mass Function" href="the-guide/mathematics/probability-theory/probability-mass-function.html" class="internal-link" target="_self" rel="noopener nofollow">pdf</a> using samples with importance weight
<br>We can now approximate the <a data-tooltip-position="top" aria-label="Bayes Theorem" data-href="Bayes Theorem" href="the-guide/mathematics/statistics/bayes-theorem.html" class="internal-link" target="_self" rel="noopener nofollow">posterior</a> by 
<br>For the next step, we perform sequential update

<br>Particle update
<br>Weight update - compare color to <a data-href="Bayesian Filtering for Tracking" href="the-guide/robotics,-dynamics-and-control/perception/bayesian-filtering-for-tracking.html" class="internal-link" target="_self" rel="noopener nofollow">Bayesian Filtering for Tracking</a>




<br>Finding an importance density is not trivial, in many cases the choice of the <a data-href="Markov Kernel" href="the-guide/computational-statistics/data-assimilation/markov-kernel.html" class="internal-link" target="_self" rel="noopener nofollow">Markov Kernel</a> , called transitional prior yields a very simple weight update of . This leads to the well-understood <a data-href="Sequential Importance Resampling (SIR)" href="the-guide/computational-statistics/data-assimilation/filtering-algorithms/sequential-importance-resampling-(sir).html" class="internal-link" target="_self" rel="noopener nofollow">Sequential Importance Resampling (SIR)</a> algorithm.<br><br>Degeneracy Phenomenon
For many methods, after some iterations, most particles will have negligible weights, leading to a large computational effort for minimal contribution to the approximated posterior. To measure this effect, we can use the effective sample size where lower values indicate degeneracy. 
<br>
<br>We can work against this by eliminating samples with low weights and sample new particles from the other samples (bootstrapping). Since the particles will evolve noisy, this will yield the desired result. The sampling can be realized by e.g. employing <a data-href="Inverse Transform Sampling" href="the-guide/computational-statistics/inverse-transform-sampling.html" class="internal-link" target="_self" rel="noopener nofollow">Inverse Transform Sampling</a>
]]></description><link>the-guide/computational-statistics/data-assimilation/filtering-algorithms/the-particle-filter.html</link><guid isPermaLink="false">The Guide/Computational Statistics/Data Assimilation/Filtering Algorithms/The Particle Filter.md</guid><pubDate>Wed, 18 Dec 2024 13:19:15 GMT</pubDate><enclosure url="lib/media/pasted-image-20240314111601.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src=&quot;lib/media/pasted-image-20240314111601.png&quot;&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[Kalman Smoother]]></title><description><![CDATA[ 
 <br>In a Nutshell
Simple <a data-tooltip-position="top" aria-label="Data Assimilation" data-href="Data Assimilation" href="the-guide/computational-statistics/data-assimilation/data-assimilation.html" class="internal-link" target="_self" rel="noopener nofollow">smoothing</a> algorithm for discrete-time linear <a data-tooltip-position="top" aria-label="Static and Dynamic Systems" data-href="Static and Dynamic Systems" href="the-guide/robotics,-dynamics-and-control/dynamics/static-and-dynamic-systems.html" class="internal-link" target="_self" rel="noopener nofollow">dynamics</a> and <a data-tooltip-position="top" aria-label="Gaussian Distribution" data-href="Gaussian Distribution" href="the-guide/mathematics/probability-theory/gaussian-distribution.html" class="internal-link" target="_self" rel="noopener nofollow">Gaussian</a> <a data-tooltip-position="top" aria-label="Probability Distribution" data-href="Probability Distribution" href="the-guide/mathematics/probability-theory/probability-distribution.html" class="internal-link" target="_self" rel="noopener nofollow">distributions</a> for state and noise. Results in explicit expression for <a data-tooltip-position="top" aria-label="Bayes Theorem" data-href="Bayes Theorem" href="the-guide/mathematics/statistics/bayes-theorem.html" class="internal-link" target="_self" rel="noopener nofollow">posterior</a>, but relies on very restrictive assumptions. 
<br><br>Assumptions

<br>All distributions are <a data-tooltip-position="top" aria-label="Gaussian Distribution" data-href="Gaussian Distribution" href="the-guide/mathematics/probability-theory/gaussian-distribution.html" class="internal-link" target="_self" rel="noopener nofollow">Gaussian</a>
<br>Dynamics and observation operator are linear

<br>Based on these assumptions, the implications of <a data-tooltip-position="top" aria-label="The Smoothing Problem" data-href="The Smoothing Problem" href="the-guide/computational-statistics/data-assimilation/smoothing-algorithms/the-smoothing-problem.html" class="internal-link" target="_self" rel="noopener nofollow">Theorem 2.8</a> ,we can explicitly compute the Gaussian <a data-tooltip-position="top" aria-label="Measure" data-href="Measure" href="the-guide/mathematics/measure-theory/measure.html" class="internal-link" target="_self" rel="noopener nofollow">probability measure</a> of the posterior. Specifically, we assume a linear <a data-tooltip-position="top" aria-label="Static and Dynamic Systems" data-href="Static and Dynamic Systems" href="the-guide/robotics,-dynamics-and-control/dynamics/static-and-dynamic-systems.html" class="internal-link" target="_self" rel="noopener nofollow">stochastic dynamics model</a> governed by with  and .<br><br><br>The mean trajectory  is computed by solving with the componentswhich is the unique minimizer of the <a data-tooltip-position="top" aria-label="Optimization Problem" data-href="Optimization Problem" href="the-guide/mathematics/optimization/convex-optimization-lecture/optimization-problem.html" class="internal-link" target="_self" rel="noopener nofollow">optimization problem</a> given byand thereby a maximizer for the posterior  (compare to ). The result can easily be verified by component-wise differentiation.<br>As in the <a data-tooltip-position="top" aria-label="Data Assimilation" data-href="Data Assimilation" href="the-guide/computational-statistics/data-assimilation/data-assimilation.html" class="internal-link" target="_self" rel="noopener nofollow">data assimilation setting</a>, we require  (<a data-tooltip-position="top" aria-label="Covariance and Variance" data-href="Covariance and Variance" href="the-guide/mathematics/statistics/covariance-and-variance.html" class="internal-link" target="_self" rel="noopener nofollow">covariance</a> of initial condition), , (<a data-tooltip-position="top" aria-label="Covariance and Variance" data-href="Covariance and Variance" href="the-guide/mathematics/statistics/covariance-and-variance.html" class="internal-link" target="_self" rel="noopener nofollow">covariance</a> of dynamics noise) and  (covariance of observation noise) to be positiv-definit (spd) defined on  (-dimensional,  time-steps). Furthermore, based on <a data-tooltip-position="top" aria-label="Gaussian Distribution" data-href="Gaussian Distribution" href="the-guide/mathematics/probability-theory/gaussian-distribution.html" class="internal-link" target="_self" rel="noopener nofollow">Lemma 1.6</a> (see results closed-form solutions) and the problem above already being a quadratic form, we can deduce that the <a data-tooltip-position="top" aria-label="Covariance and Variance" data-href="Covariance and Variance" href="the-guide/mathematics/statistics/covariance-and-variance.html" class="internal-link" target="_self" rel="noopener nofollow">covariance</a> <a data-tooltip-position="top" aria-label="Matrices" data-href="Matrices" href="the-guide/mathematics/linear-algebra/matrices.html" class="internal-link" target="_self" rel="noopener nofollow">matrix</a>  as the inverse of the precision matrix is simply given by the <a data-tooltip-position="top" aria-label="Derivative, Gradient, Jacobian and Hessian > The Hessian" data-href="Derivative, Gradient, Jacobian and Hessian#The Hessian" href="the-guide/mathematics/analysis-and-calculus/derivative,-gradient,-jacobian-and-hessian.html#The_Hessian" class="internal-link" target="_self" rel="noopener nofollow">Hessian</a> of the above formulation. The result isWe compute the local contribution  via<br>
<br>Initial state 
<br>Diagonal 
<br>Off-diagonal 
<br>Final state 
<br><br>Deterministic Dynamics
For the <a data-tooltip-position="top" aria-label="The Smoothing Problem" data-href="The Smoothing Problem" href="the-guide/computational-statistics/data-assimilation/smoothing-algorithms/the-smoothing-problem.html" class="internal-link" target="_self" rel="noopener nofollow">special case</a> of deterministic dynamics, we can simplify the formulas by using the exact same steps as above to obtain the probability measure with precision matrix The mean subsequently solves the linear system
]]></description><link>the-guide/computational-statistics/data-assimilation/smoothing-algorithms/kalman-smoother.html</link><guid isPermaLink="false">The Guide/Computational Statistics/Data Assimilation/Smoothing Algorithms/Kalman Smoother.md</guid><pubDate>Mon, 03 Mar 2025 15:30:25 GMT</pubDate></item><item><title><![CDATA[Markov-Chain Monte Carlo Methods]]></title><description><![CDATA[ 
 <br>In a Nutshell
Category of algorithms in statistics and Bayesian machine learning, particularly <a data-href="Data Assimilation" href="the-guide/computational-statistics/data-assimilation/data-assimilation.html" class="internal-link" target="_self" rel="noopener nofollow">Data Assimilation</a> that tries to solve the problem of dealing with intractable <a data-tooltip-position="top" aria-label="Probability Distribution" data-href="Probability Distribution" href="the-guide/mathematics/probability-theory/probability-distribution.html" class="internal-link" target="_self" rel="noopener nofollow">distributions</a>, i.e. posteriors. Uses a <a data-href="Markov Chain" href="the-guide/computational-statistics/data-assimilation/markov-chain.html" class="internal-link" target="_self" rel="noopener nofollow">Markov Chain</a> to stochastically explore the <a data-tooltip-position="top" aria-label="Typical Sets" data-href="Typical Sets" href="the-guide/information-theory/information-theory-1/data-compression/typical-sets.html" class="internal-link" target="_self" rel="noopener nofollow">typical set</a> and generate a grid of samples across the region with high probability. 
<br><img alt="center" src="lib/media/pasted-image-20231019114754.png" style="width: 400px; max-width: 100%;"><br><br>Based on the <a data-tooltip-position="top" aria-label="Probabilistic View of Dynamical Systems" data-href="Probabilistic View of Dynamical Systems" href="the-guide/computational-statistics/data-assimilation/probabilistic-view-of-dynamical-systems.html" class="internal-link" target="_self" rel="noopener nofollow">probabilistic perspective on dynamical systems</a>, MCMC tries to generate  samples using a Markov chain to approximate an underlying <a data-tooltip-position="top" aria-label="Probability Distribution" data-href="Probability Distribution" href="the-guide/mathematics/probability-theory/probability-distribution.html" class="internal-link" target="_self" rel="noopener nofollow">distribution</a>. The resulting method is much more flexible than the <a data-href="Kalman Smoother" href="the-guide/computational-statistics/data-assimilation/smoothing-algorithms/kalman-smoother.html" class="internal-link" target="_self" rel="noopener nofollow">Kalman Smoother</a>, but does not provide closed form solutions.<br>With the <a data-tooltip-position="top" aria-label="Ergodicity" data-href="Ergodicity" href="the-guide/computational-statistics/data-assimilation/ergodicity.html" class="internal-link" target="_self" rel="noopener nofollow">ergodicity</a> theorems, we know that a <a data-tooltip-position="top" aria-label="Markov Chain" data-href="Markov Chain" href="the-guide/computational-statistics/data-assimilation/markov-chain.html" class="internal-link" target="_self" rel="noopener nofollow">Markov chain</a> with an invariant <a data-tooltip-position="top" aria-label="Measure" data-href="Measure" href="the-guide/mathematics/measure-theory/measure.html" class="internal-link" target="_self" rel="noopener nofollow">measure</a> that fulfills the ergodicity condition allows us to compute <a data-tooltip-position="top" aria-label="Expectations" data-href="Expectations" href="the-guide/mathematics/statistics/expectations.html" class="internal-link" target="_self" rel="noopener nofollow">expectations</a> by considering the average over the time-steps (time average = space average). <br>Theorem 3.3 - The Backbone of MCMC
We assume a Markov chain  with <a data-tooltip-position="top" aria-label="Markov Chain" data-href="Markov Chain" href="the-guide/computational-statistics/data-assimilation/markov-chain.html" class="internal-link" target="_self" rel="noopener nofollow">invariant measure</a>  and <a data-tooltip-position="top" aria-label="Lebesgue Measure" data-href="Lebesgue Measure" href="the-guide/mathematics/measure-theory/lebesgue-measure.html" class="internal-link" target="_self" rel="noopener nofollow">Lebesque density</a>  for all time-steps . If the chain is <a data-tooltip-position="top" aria-label="Ergodicity" data-href="Ergodicity" href="the-guide/computational-statistics/data-assimilation/ergodicity.html" class="internal-link" target="_self" rel="noopener nofollow">ergodic</a>, then for every bounded continuous  we have with initial condition . Particularly, if there exists a <a data-tooltip-position="top" aria-label="Measure" data-href="Measure" href="the-guide/mathematics/measure-theory/measure.html" class="internal-link" target="_self" rel="noopener nofollow">probability measure</a>  on  and , such that for all  and all Borel <a data-tooltip-position="top" aria-label="Set" data-href="Set" href="the-guide/mathematics/general-stuff/set.html" class="internal-link" target="_self" rel="noopener nofollow">sets</a> , we have , then we can also bound the <a data-tooltip-position="top" aria-label="Metrics on Spaces of Probability Measures" data-href="Metrics on Spaces of Probability Measures" href="the-guide/computational-statistics/data-assimilation/metrics-on-spaces-of-probability-measures.html" class="internal-link" target="_self" rel="noopener nofollow">distance</a> between the measure at time-step  and the invariant one viaThen there is also , such that where  converges weakly to  as .
<br>
<br>The condition just guarantees that we visit every state with non-zero probability (prob of going from  to any point in  is bounded from below)
<br> measures <a data-tooltip-position="top" aria-label="Covariance and Variance" data-href="Covariance and Variance" href="the-guide/mathematics/statistics/covariance-and-variance.html" class="internal-link" target="_self" rel="noopener nofollow">variance</a> of the <a data-tooltip-position="top" aria-label="Estimator" data-href="Estimator" href="the-guide/mathematics/statistics/estimator.html" class="internal-link" target="_self" rel="noopener nofollow">estimator</a> for  multiplied by . An alternative interpretation is that  measure the correlation in the chain, where lower values indicate lower correlations between the Marko Chain samples. Choosing parameters that lower  without increasing the computational cost too much are of central importance in application. 
<br>The most-widely used approach is the <a data-href="Metropolis-Hastings MCMC" href="the-guide/computational-statistics/data-assimilation/smoothing-algorithms/metropolis-hastings-mcmc.html" class="internal-link" target="_self" rel="noopener nofollow">Metropolis-Hastings MCMC</a>, which enforces an invariant measure via the even stricter <a data-tooltip-position="top" aria-label="Detailed Balance Condition" data-href="Detailed Balance Condition" href="the-guide/computational-statistics/data-assimilation/detailed-balance-condition.html" class="internal-link" target="_self" rel="noopener nofollow">detailed balance condition</a>. <br><br><br>
<br>Why focus on the <a data-tooltip-position="top" aria-label="Typical Sets" data-href="Typical Sets" href="the-guide/information-theory/information-theory-1/data-compression/typical-sets.html" class="internal-link" target="_self" rel="noopener nofollow">typical set</a> ?

<br><a data-href="Expectations" href="the-guide/mathematics/statistics/expectations.html" class="internal-link" target="_self" rel="noopener nofollow">Expectations</a> is computed via integral  with distribution . Therefore, a point's relevance is determined by the product of <a data-tooltip-position="top" aria-label="Probability Density Function" data-href="Probability Density Function" href="the-guide/mathematics/probability-theory/probability-density-function.html" class="internal-link" target="_self" rel="noopener nofollow">density</a> and volume element. With higher dimensions, there is more and more volume outside any given region<img alt="center" src="lib/media/pasted-image-20231019111944.png" style="width: 500px; max-width: 100%;">
<br>The sweet-spot is the typical set, where the product contributes a lot to the integral. With higher dimensions, this leads to a concentration of this set because the relative volume becomes smaller and smaller.<img alt="center" src="lib/media/pasted-image-20231019114347.png" style="width: 250px; max-width: 100%;">
<br><a data-tooltip-position="top" aria-label="Estimator" data-href="Estimator" href="the-guide/mathematics/statistics/estimator.html" class="internal-link" target="_self" rel="noopener nofollow">Estimating</a> <a data-tooltip-position="top" aria-label="Expectations" data-href="Expectations" href="the-guide/mathematics/statistics/expectations.html" class="internal-link" target="_self" rel="noopener nofollow">expectation</a> only using the typical set becomes very efficient for high dimensions !


<br><br><br>
<br>Ideal 

<br>Converge to the <a data-tooltip-position="top" aria-label="Typical Sets" data-href="Typical Sets" href="the-guide/information-theory/information-theory-1/data-compression/typical-sets.html" class="internal-link" target="_self" rel="noopener nofollow">typical set</a>, initial exploration, explore details<img alt="center" src="lib/media/pasted-image-20231023101713.png" style="width: 400px; max-width: 100%;">
<br>Drastically improve precision by removing the initial iterates from <a data-tooltip-position="top" aria-label="Monte Carlo Integration" data-href="Monte Carlo Integration" href="the-guide/mathematics/probability-theory/monte-carlo-integration.html" class="internal-link" target="_self" rel="noopener nofollow">MC</a> <a data-tooltip-position="top" aria-label="Estimator" data-href="Estimator" href="the-guide/mathematics/statistics/estimator.html" class="internal-link" target="_self" rel="noopener nofollow">estimator</a> !


<br>Pathological

<br>Can get stuck in regions with large curvature<img alt="center" src="lib/media/pasted-image-20231023101932.png" style="width: 500px; max-width: 100%;">


]]></description><link>the-guide/computational-statistics/data-assimilation/smoothing-algorithms/markov-chain-monte-carlo-methods.html</link><guid isPermaLink="false">The Guide/Computational Statistics/Data Assimilation/Smoothing Algorithms/Markov-Chain Monte Carlo Methods.md</guid><pubDate>Tue, 17 Sep 2024 12:34:25 GMT</pubDate><enclosure url="lib/media/pasted-image-20231019114754.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src=&quot;lib/media/pasted-image-20231019114754.png&quot;&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[Metropolis-Hastings MCMC]]></title><description><![CDATA[ 
 <br>In a Nutshell
Class of <a data-tooltip-position="top" aria-label="Markov-Chain Monte Carlo Methods" data-href="Markov-Chain Monte Carlo Methods" href="the-guide/computational-statistics/data-assimilation/smoothing-algorithms/markov-chain-monte-carlo-methods.html" class="internal-link" target="_self" rel="noopener nofollow">MCMC methods</a> that enforces the <a data-tooltip-position="top" aria-label="Detailed Balance Condition" data-href="Detailed Balance Condition" href="the-guide/computational-statistics/data-assimilation/detailed-balance-condition.html" class="internal-link" target="_self" rel="noopener nofollow">detailed balance</a> condition by adding a rejection / acceptance test after every step. It thereby ensures that the Markov chain has an <a data-tooltip-position="top" aria-label="Markov Chain" data-href="Markov Chain" href="the-guide/computational-statistics/data-assimilation/markov-chain.html" class="internal-link" target="_self" rel="noopener nofollow">invariant measure</a> and enables us to generate samples solely based on prior and likelihood without the need to compute normalization constants.
<br><br><br>In context of the <a data-tooltip-position="top" aria-label="Data Assimilation" data-href="Data Assimilation" href="the-guide/computational-statistics/data-assimilation/data-assimilation.html" class="internal-link" target="_self" rel="noopener nofollow">data assimilation</a> lecture, our aim is to approximate the <a data-tooltip-position="top" aria-label="Bayes Theorem" data-href="Bayes Theorem" href="the-guide/mathematics/statistics/bayes-theorem.html" class="internal-link" target="_self" rel="noopener nofollow">posterior</a> distribution given by the measure  on  with density .<br>Given a <a data-tooltip-position="top" aria-label="Probability Density Function" data-href="Probability Density Function" href="the-guide/mathematics/probability-theory/probability-density-function.html" class="internal-link" target="_self" rel="noopener nofollow">pdf</a>/<a data-tooltip-position="top" aria-label="Measure" data-href="Measure" href="the-guide/mathematics/measure-theory/measure.html" class="internal-link" target="_self" rel="noopener nofollow">measure</a>  and a <a data-tooltip-position="top" aria-label="Markov Kernel" data-href="Markov Kernel" href="the-guide/computational-statistics/data-assimilation/markov-kernel.html" class="internal-link" target="_self" rel="noopener nofollow">Markov kernel</a>  ( can be <a data-tooltip-position="top" aria-label="Probability Density Function" data-href="Probability Density Function" href="the-guide/mathematics/probability-theory/probability-density-function.html" class="internal-link" target="_self" rel="noopener nofollow">pdf</a> or <a data-tooltip-position="top" aria-label="Measure" data-href="Measure" href="the-guide/mathematics/measure-theory/measure.html" class="internal-link" target="_self" rel="noopener nofollow">probability measure</a>) with  for every , we create a <a data-tooltip-position="top" aria-label="Markov Chain" data-href="Markov Chain" href="the-guide/computational-statistics/data-assimilation/markov-chain.html" class="internal-link" target="_self" rel="noopener nofollow">Markov chain</a>  that is invariant for the measure  by enforcing <a data-tooltip-position="top" aria-label="Detailed Balance Condition" data-href="Detailed Balance Condition" href="the-guide/computational-statistics/data-assimilation/detailed-balance-condition.html" class="internal-link" target="_self" rel="noopener nofollow">detailed balance</a> via an acceptance probabilitywhere  is the next step in the chain after  and the overall transition probability is the product of acceptance  and proposal . Rearranging the terms yields We then use a clever choice of the acceptance to ensure symmetry of the acceptances by setting <br>Corollary 3.6. - Invariant Measure via Metropolis Hastings
For the Metropolis Hastings MCMC approach, we have that the <a data-tooltip-position="top" aria-label="Detailed Balance Condition" data-href="Detailed Balance Condition" href="the-guide/computational-statistics/data-assimilation/detailed-balance-condition.html" class="internal-link" target="_self" rel="noopener nofollow">detailed balance</a> condition holds and hence  is invariant. To ensure this, we only require the <a data-tooltip-position="top" aria-label="Markov Chain" data-href="Markov Chain" href="the-guide/computational-statistics/data-assimilation/markov-chain.html" class="internal-link" target="_self" rel="noopener nofollow">Markov chain</a> to be <a data-tooltip-position="top" aria-label="Ergodicity" data-href="Ergodicity" href="the-guide/computational-statistics/data-assimilation/ergodicity.html" class="internal-link" target="_self" rel="noopener nofollow">ergodic</a>.
<br>Proof
We can simply verify that the above condition ensures detailed balance

<br>In cases where the acceptance is set to , which if inserted into both sides of detailed balance yields thereby satisfying the condition.
<br>Otherwise, we have thatyieldingagain satisfying the condition.

<br>Intuition
The first ratio involving  indicates if we went to a point with higher density, since we want to explore mainly high probability regions. This term is then additionally weighted by the second transition ratio, encoding how likely this transition was. This means there are two mechanisms that can intensify each other:

<br>A value higher of equal to  and thereby guaranteed acceptance can occur, if we went to a state with e.g. the same density, but the transition was improbable (and / or coming back the inverse way is very unlikely), we accept to ensure exploration of that part of the state space. 
<br>Inversely, if e.g. the densities were the same, but the transition was nearly guaranteed, we only accept with smaller and smaller probability to decorrelate the chain.

<br>Advantages
We only need the ratio , which eliminates the need to compute any normalization constants !
<br>Problems

<br>With increasing dimensions, almost all proposals will be outside of the <a data-tooltip-position="top" aria-label="Typical Sets" data-href="Typical Sets" href="the-guide/information-theory/information-theory-1/data-compression/typical-sets.html" class="internal-link" target="_self" rel="noopener nofollow">typical set</a> and have near-zero <a data-tooltip-position="top" aria-label="Probability Distribution" data-href="Probability Distribution" href="the-guide/mathematics/probability-theory/probability-distribution.html" class="internal-link" target="_self" rel="noopener nofollow">probability</a>, the <a data-tooltip-position="top" aria-label="Markov Chain" data-href="Markov Chain" href="the-guide/computational-statistics/data-assimilation/markov-chain.html" class="internal-link" target="_self" rel="noopener nofollow">Markov chain</a> will rarely move !
<br>Work against that by decreasing step size, but this will lead to very slow exploration

<br><img alt="center" src="lib/media/pasted-image-20231023101540.png" style="width: 350px; max-width: 100%;"><br>As mentioned above, this leaves us to specify a transition kernel  from the old sample  to a new one  and derive the respective acceptance criterion using our results. If the resulting chain is ergodic, we get a usable algorithm. Some example algorithms are ...<br><br>Random Walk Metropolis (RWM)
We pick a sample according to a conditioned <a data-tooltip-position="top" aria-label="Gaussian Distribution" data-href="Gaussian Distribution" href="the-guide/mathematics/probability-theory/gaussian-distribution.html" class="internal-link" target="_self" rel="noopener nofollow">Gaussian</a>, i.e. centered around the last sample  and . In most cases, the <a data-tooltip-position="top" aria-label="Covariance and Variance" data-href="Covariance and Variance" href="the-guide/mathematics/statistics/covariance-and-variance.html" class="internal-link" target="_self" rel="noopener nofollow">covariance</a> is chosen as identity or the <a data-tooltip-position="top" aria-label="Bayes Theorem" data-href="Bayes Theorem" href="the-guide/mathematics/statistics/bayes-theorem.html" class="internal-link" target="_self" rel="noopener nofollow">prior</a> covariance  of the <a data-tooltip-position="top" aria-label="Static and Dynamic Systems" data-href="Static and Dynamic Systems" href="the-guide/robotics,-dynamics-and-control/dynamics/static-and-dynamic-systems.html" class="internal-link" target="_self" rel="noopener nofollow">stochastic dynamical system</a>. The proposal is by design (Gaussians) symmetric (), and hence
<br>
<br>For the deterministic setting with initial <a data-tooltip-position="top" aria-label="Gaussian Distribution" data-href="Gaussian Distribution" href="the-guide/mathematics/probability-theory/gaussian-distribution.html" class="internal-link" target="_self" rel="noopener nofollow">Gaussians</a> as in the <a data-href="Data Assimilation" href="the-guide/computational-statistics/data-assimilation/data-assimilation.html" class="internal-link" target="_self" rel="noopener nofollow">Data Assimilation</a> lecture, we get  and all states are in . We move to the new point with probability  if it lowers the log-posterior. This term  is the sum of the prior and the model-data-misfit functional (likelihood). The algorithm thereby biases the chain towards samples that increase the fit to the model and the data.
<br>For the stochastic case, we sample whole trajectories by repeatedly sampling from the Gaussian, e.g.  

<br>No information is build into the proposal, all information is in the acceptance step, which considers the entire log-posterior


<br>The two key hyperparameters are the <a data-tooltip-position="top" aria-label="Covariance and Variance" data-href="Covariance and Variance" href="the-guide/mathematics/statistics/covariance-and-variance.html" class="internal-link" target="_self" rel="noopener nofollow">covariance</a> , encoding prior knowledge about correlations in the model and the step-scaling . The latter has to be chosen such that the acceptance probability is neither too close to  nor . If the acceptance is too low, the chain becomes highly correlated, increasing , while an acceptance close to  can stem from very small  (we always scale down the step), which also yields high correlations.
<br>Incorporates no information into proposal and posterior into acceptance.
<br><br>Independence Dynamics Sampler
In this version, we encode more information into the proposal step by considering trajectories sampled from the prior <a data-tooltip-position="top" aria-label="Measure" data-href="Measure" href="the-guide/mathematics/measure-theory/measure.html" class="internal-link" target="_self" rel="noopener nofollow">measure</a>  (dynamics) with density . Thereby, we generate independent draws () without considering the data We then incorporate the data into the acceptance via the fact that with the <a data-tooltip-position="top" aria-label="The Smoothing Problem" data-href="The Smoothing Problem" href="the-guide/computational-statistics/data-assimilation/smoothing-algorithms/the-smoothing-problem.html" class="internal-link" target="_self" rel="noopener nofollow">model-data misfit functional</a>  from the <a data-tooltip-position="top" aria-label="Likelihood Function" data-href="Likelihood Function" href="the-guide/mathematics/statistics/likelihood-function.html" class="internal-link" target="_self" rel="noopener nofollow">likelihood</a>, we can write yielding the criterion 
<br>Ergodicity of Independence Dynamics Sampler 
The independence dynamics samples is <a data-tooltip-position="top" aria-label="Ergodicity" data-href="Ergodicity" href="the-guide/computational-statistics/data-assimilation/ergodicity.html" class="internal-link" target="_self" rel="noopener nofollow">ergodic</a>, if the observation operator is bounded  such that  for every . The acceptance  is lower-bounded by , so we have ergodicity by .
<br>
<br>Acceptance probability  is reached, when the move to the new state  decreases , i.e. fits the observed data better.
<br>Incorporates prior into proposal and likelihood into acceptance.
<br>Disadvantages
Does not allow any hyperparameter-tuning, which makes it susceptible to bad acceptance probabilities (close to  or ):

<br>Relies entirely on prior samples matching the data well (small )
<br>Highly problematic, if ...

<br> is large (long trajectories) 
<br> is small (we trust data a lot, but method relies on prior), a larger covariance will scale down the exponentials, which reduces extremely high or low acceptances



<br><br>To overcome the issues of the Independence Dynamics Sampler, we have to introduce a way to affect local proposals. In this version, we consider the special case of a prior <a data-tooltip-position="top" aria-label="Measure" data-href="Measure" href="the-guide/mathematics/measure-theory/measure.html" class="internal-link" target="_self" rel="noopener nofollow">measure</a>  with <a data-tooltip-position="top" aria-label="Probability Density Function" data-href="Probability Density Function" href="the-guide/mathematics/probability-theory/probability-density-function.html" class="internal-link" target="_self" rel="noopener nofollow">density</a>  resulting from  described in <a data-tooltip-position="top" aria-label="The Smoothing Problem" data-href="The Smoothing Problem" href="the-guide/computational-statistics/data-assimilation/smoothing-algorithms/the-smoothing-problem.html" class="internal-link" target="_self" rel="noopener nofollow">special case 1</a>in order to make proposals. We then incorporate effects of  and  via the accept/reject step. <br>This way, we retrieve an algorithm sampling proposals via Gaussian perturbations while also introducing a parameter  allowing smaller steps to ensure good acceptance probabilities.<br>First, we derive a version of the Random Walk Sampler. For this, we notice that we can writewith<br>Intuition
First part tells us how likely a trajectory was given the dynamics prior, the second part is a correction term to enforce exploration. This results e.g. in a large positive summand in the exponential, if we sampled a likely trajectory given our current dynamics that produced different states.
<br>Subsequently, we can deriveThe resulting pCN method is a random-walk-type method based on the proposal in the following theorem.<br>pCN Sampler
As a Gaussian proposal, we use the <a data-tooltip-position="top" aria-label="Markov Chain" data-href="Markov Chain" href="the-guide/computational-statistics/data-assimilation/markov-chain.html" class="internal-link" target="_self" rel="noopener nofollow">Markov chain</a> where and  is assumed <a data-tooltip-position="top" aria-label="Statistical Independence" data-href="Statistical Independence" href="the-guide/mathematics/statistics/statistical-independence.html" class="internal-link" target="_self" rel="noopener nofollow">independent</a> of . Thereby, we create a Gaussian perturbation whose magnitude can be controlled via . Based on this, we can construct an acceptance criterion to the proposal above via  
<br>
<br>Allows us to make small moves when acceptance probability is high
<br>Theorem
The Markov chain  of the pCN algorithm satisfies the <a data-tooltip-position="top" aria-label="Detailed Balance Condition" data-href="Detailed Balance Condition" href="the-guide/computational-statistics/data-assimilation/detailed-balance-condition.html" class="internal-link" target="_self" rel="noopener nofollow">detailed balance condition</a> with respect to the measure  with density . 
<br>Proof
In order to plug it into the general acceptance formulation, we have to show that which is equivalent to the denominator being symmetric in , automatically fulfilling the detailed balance. The initial distribution is Gaussian and so is , which is why the resulting distribution has exponential form, withwhich is indeed symmetric in the states. The first term comes from , the second is contributed by the proposal in the theorem. We can now simply use the identity 
<br><br>Another sampling algorithm can be obtained by using the above and applying the <a data-tooltip-position="top" aria-label="Reformulation of Stochastic Dynamics" data-href="Reformulation of Stochastic Dynamics" href="the-guide/computational-statistics/data-assimilation/reformulation-of-stochastic-dynamics.html" class="internal-link" target="_self" rel="noopener nofollow">reformulation of stochastic dynamics</a> to instead consider the noise sequence and the initial condition as the unknowns. This way, we retrieve an algorithm sampling proposals from the dynamics as the Independence Dynamics Sampler does while also introducing a parameter  allowing smaller steps to ensure good acceptance probabilities.<br>pCN Dynamics Sampler
This algorithm implicitly (via the mapping ) proposes samples via the dynamics as in the Independence Dynamics Sampler, but also allows small steps to be taken via  to ensure good acceptance probabilities. Specifically, again using the dynamics-free  as a prior, the proposals are and the acceptance probability is given bywhere it can be shown that .
<br>
<br>Interpretation as Independence Dynamics Sampler, but expressed in terms of initial condition and noise.
<br>Introduces hyperparameter  to tweak variance of proposals, allowing better acceptance probabilities than the Independence Dynamics Sampler
<br>May perform better than pCN when effect of the dynamics is large or  is small

<br>Gaussian measure is far from actual dynamics
<br>Gaussian measure is precisely the measure governing noise and initial condition


<br>Acceptance does not need to incorporate dynamics only model-data misfit. This can increase the acceptance rate. 
]]></description><link>the-guide/computational-statistics/data-assimilation/smoothing-algorithms/metropolis-hastings-mcmc.html</link><guid isPermaLink="false">The Guide/Computational Statistics/Data Assimilation/Smoothing Algorithms/Metropolis-Hastings MCMC.md</guid><pubDate>Mon, 03 Mar 2025 15:30:25 GMT</pubDate><enclosure url="lib/media/pasted-image-20231023101540.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src=&quot;lib/media/pasted-image-20231023101540.png&quot;&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[The Smoothing Problem]]></title><description><![CDATA[ 
 <br>In a Nutshell
Offline setting for <a data-tooltip-position="top" aria-label="Data Assimilation" data-href="Data Assimilation" href="the-guide/computational-statistics/data-assimilation/data-assimilation.html" class="internal-link" target="_self" rel="noopener nofollow">data assimilation</a>, tries to incorporate all the probabilistic information of the whole dataset using <a data-tooltip-position="top" aria-label="Bayes Theorem" data-href="Bayes Theorem" href="the-guide/mathematics/statistics/bayes-theorem.html" class="internal-link" target="_self" rel="noopener nofollow">Bayes</a> formula. The process of smoothing is concerned with conditioning a signal  on given data  collected in a <a data-href="Data Assimilation" href="the-guide/computational-statistics/data-assimilation/data-assimilation.html" class="internal-link" target="_self" rel="noopener nofollow">Data Assimilation</a> window , i.e. finding the <a data-tooltip-position="top" aria-label="Probability Density Function" data-href="Probability Density Function" href="the-guide/mathematics/probability-theory/probability-density-function.html" class="internal-link" target="_self" rel="noopener nofollow">pdf</a> for full trajectories 
<br><br><br>
<br>Discrete Time

<br><a data-href="Kalman Smoother" href="the-guide/computational-statistics/data-assimilation/smoothing-algorithms/kalman-smoother.html" class="internal-link" target="_self" rel="noopener nofollow">Kalman Smoother</a> - linear dynamics, only Gaussian distributions

<br>Yields closed-form posterior


<br><a data-href="Markov-Chain Monte Carlo Methods" href="the-guide/computational-statistics/data-assimilation/smoothing-algorithms/markov-chain-monte-carlo-methods.html" class="internal-link" target="_self" rel="noopener nofollow">Markov-Chain Monte Carlo Methods</a> - general distributions

<br>Only samples from posterior and potentially needs many and / or expensive sampling steps


<br><a data-href="Variational Smoothing Methods" href="the-guide/computational-statistics/data-assimilation/smoothing-algorithms/variational-smoothing-methods.html" class="internal-link" target="_self" rel="noopener nofollow">Variational Smoothing Methods</a> - concentrate on maximal points of posterior density

<br>Performs <a data-tooltip-position="top" aria-label="Maximum A Posteriori Estimator" data-href="Maximum A Posteriori Estimator" href="the-guide/mathematics/statistics/maximum-a-posteriori-estimator.html" class="internal-link" target="_self" rel="noopener nofollow">MAP</a> <a data-tooltip-position="top" aria-label="Estimator" data-href="Estimator" href="the-guide/mathematics/statistics/estimator.html" class="internal-link" target="_self" rel="noopener nofollow">estimation</a>, more efficient, but loses information 




<br><br>With the <a data-tooltip-position="top" aria-label="Markov Chain" data-href="Markov Chain" href="the-guide/computational-statistics/data-assimilation/markov-chain.html" class="internal-link" target="_self" rel="noopener nofollow">Markovian</a> assumption, we can write the distribution of the states as <br><br>General Time-Discrete Setting
We consider the <a data-tooltip-position="top" aria-label="Static and Dynamic Systems" data-href="Static and Dynamic Systems" href="the-guide/robotics,-dynamics-and-control/dynamics/static-and-dynamic-systems.html" class="internal-link" target="_self" rel="noopener nofollow">stochastic dynamical system</a>, yielding a solution operator  of an underlying ODE and a <a data-tooltip-position="top" aria-label="Markov Chain" data-href="Markov Chain" href="the-guide/computational-statistics/data-assimilation/markov-chain.html" class="internal-link" target="_self" rel="noopener nofollow">Markov chain</a>  defined by the random mapwith an <a data-tooltip-position="top" aria-label="Statistical Independence" data-href="Statistical Independence" href="the-guide/mathematics/statistics/statistical-independence.html" class="internal-link" target="_self" rel="noopener nofollow">i.i.d.</a> sequence  with . The initial condition  is independent of , but possibly random.
This model is supplemented by observations / data  of the <a data-href="Static and Dynamic Systems" href="the-guide/robotics,-dynamics-and-control/dynamics/static-and-dynamic-systems.html" class="internal-link" target="_self" rel="noopener nofollow">Static and Dynamic Systems</a> as it evolves, typically reducing uncertainty. At each (discrete) time step, we observe a <a data-tooltip-position="top" aria-label="Mathematical Relations" data-href="Mathematical Relations" href="the-guide/mathematics/general-stuff/mathematical-relations.html" class="internal-link" target="_self" rel="noopener nofollow">function</a> of the signal with additive noise with the observation operator  and and <a data-tooltip-position="top" aria-label="Statistical Independence" data-href="Statistical Independence" href="the-guide/mathematics/statistics/statistical-independence.html" class="internal-link" target="_self" rel="noopener nofollow">i.i.d.</a> sequence  that is independent of . Here, we assume  with 
<br>If we sample trajectories by using these formulas, we denote the resulting <a data-tooltip-position="top" aria-label="Measure" data-href="Measure" href="the-guide/mathematics/measure-theory/measure.html" class="internal-link" target="_self" rel="noopener nofollow">measures</a> as  and  with <a data-tooltip-position="top" aria-label="Probability Density Function" data-href="Probability Density Function" href="the-guide/mathematics/probability-theory/probability-density-function.html" class="internal-link" target="_self" rel="noopener nofollow">densities</a>  and , respectively.<br>
<br>Prior - prediction based on model - In this simple scenario, we define the initial distribution to be a <a data-tooltip-position="top" aria-label="Gaussian Distribution" data-href="Gaussian Distribution" href="the-guide/mathematics/probability-theory/gaussian-distribution.html" class="internal-link" target="_self" rel="noopener nofollow">Gaussian</a>, yielding while the underlying ODE and the <a data-href="Markov Kernel" href="the-guide/computational-statistics/data-assimilation/markov-kernel.html" class="internal-link" target="_self" rel="noopener nofollow">Markov Kernel</a> yield the transition Combining these information, we obtain a distribution proportional to which determines a prior <a data-href="Measure" href="the-guide/mathematics/measure-theory/measure.html" class="internal-link" target="_self" rel="noopener nofollow">Measure</a>  on  with <a data-tooltip-position="top" aria-label="Probability Density Function" data-href="Probability Density Function" href="the-guide/mathematics/probability-theory/probability-density-function.html" class="internal-link" target="_self" rel="noopener nofollow">pdf</a> , generally via The distribution is not generally Gaussian, unless  is linear. In literature,  is referred to as the background <a data-tooltip-position="top" aria-label="Expectations" data-href="Expectations" href="the-guide/mathematics/statistics/expectations.html" class="internal-link" target="_self" rel="noopener nofollow">mean</a> and  as the background <a data-tooltip-position="top" aria-label="Covariance and Variance" data-href="Covariance and Variance" href="the-guide/mathematics/statistics/covariance-and-variance.html" class="internal-link" target="_self" rel="noopener nofollow">covariance</a>.
<br>Intuition
At every time step the distribution of the state gets blurred further and further, because there is a probabilistic transition starting from every possible state f the last time-step. 
The term  measures the quadratic distance from the new mean based on the transition of the last state, resulting in a probability according to a <a data-tooltip-position="top" aria-label="Gaussian Distribution" data-href="Gaussian Distribution" href="the-guide/mathematics/probability-theory/gaussian-distribution.html" class="internal-link" target="_self" rel="noopener nofollow">Gaussian</a> (if linear dynamics) when used with the rest of the formula. 
The transformation via the root of the <a data-tooltip-position="top" aria-label="Covariance and Variance" data-href="Covariance and Variance" href="the-guide/mathematics/statistics/covariance-and-variance.html" class="internal-link" target="_self" rel="noopener nofollow">precision</a> <a data-tooltip-position="top" aria-label="Matrices" data-href="Matrices" href="the-guide/mathematics/linear-algebra/matrices.html" class="internal-link" target="_self" rel="noopener nofollow">matrix</a>  (high precision = narrow, peaky Gaussian) encodes information about correlations and trust in the model. If the variance in one dimension is e.g. very high, the distance to the mean is scaled down, because even states that are far away from the mean are still higher in probability, because we do not trust our model enough.
<br>
<br>Likelihood - how likely is observation based on prior - The data likelihood can be expressed via a <a data-tooltip-position="top" aria-label="Gaussian Distribution" data-href="Gaussian Distribution" href="the-guide/mathematics/probability-theory/gaussian-distribution.html" class="internal-link" target="_self" rel="noopener nofollow">Gaussian</a> distribution on  of the form where  is denoted the model-data misfit functional.
<br>Intuition
The new state is <a data-tooltip-position="top" aria-label="Mathematical Relations" data-href="Mathematical Relations" href="the-guide/mathematics/general-stuff/mathematical-relations.html" class="internal-link" target="_self" rel="noopener nofollow">mapped</a> via the observation operator to only compare state dimensions that we can actually observe. Then, the distance from the mean and the transformation via the root of the <a data-tooltip-position="top" aria-label="Covariance and Variance" data-href="Covariance and Variance" href="the-guide/mathematics/statistics/covariance-and-variance.html" class="internal-link" target="_self" rel="noopener nofollow">precision</a> matrix work as above.
For low variances in a dimension, we trust the observation model a lot, which makes states far away in these dimensions less likely (by scaling the distance up). 
<br>Theorem 2.8 - Posterior Smoothing Distribution
Following from <a data-href="Bayes Theorem" href="the-guide/mathematics/statistics/bayes-theorem.html" class="internal-link" target="_self" rel="noopener nofollow">Bayes Theorem</a>, the <a data-tooltip-position="top" aria-label="Bayes Theorem" data-href="Bayes Theorem" href="the-guide/mathematics/statistics/bayes-theorem.html" class="internal-link" target="_self" rel="noopener nofollow">posterior</a> smoothing distribution on  for the <a data-tooltip-position="top" aria-label="Static and Dynamic Systems" data-href="Static and Dynamic Systems" href="the-guide/robotics,-dynamics-and-control/dynamics/static-and-dynamic-systems.html" class="internal-link" target="_self" rel="noopener nofollow">stochastic dynamics model</a> is a <a data-tooltip-position="top" aria-label="Measure" data-href="Measure" href="the-guide/mathematics/measure-theory/measure.html" class="internal-link" target="_self" rel="noopener nofollow">probability measure</a>  on  with <a data-tooltip-position="top" aria-label="Probability Density Function" data-href="Probability Density Function" href="the-guide/mathematics/probability-theory/probability-density-function.html" class="internal-link" target="_self" rel="noopener nofollow">pdf</a>  proportional to The normalized density could be obtained via This distribution is non-Gaussian unless  and  are linear.  is the negative log-posterior. For the pdfs, it holds that 
<br>Intuition
Combining the two terms above, we obtain an evolving probability cloud over the time -state <a data-href="Space" href="the-guide/mathematics/general-stuff/space.html" class="internal-link" target="_self" rel="noopener nofollow">Space</a>.<br>
At each step, the cloud could be obtained by evolving every possible state via the ODE solution operator and scaling it as explained for the prior (<a data-tooltip-position="top" aria-label="Data Assimilation" data-href="Data Assimilation" href="the-guide/computational-statistics/data-assimilation/data-assimilation.html" class="internal-link" target="_self" rel="noopener nofollow">relationship smoothing and filtering</a>.<br>
Then each state is again re-scaled by adding the summand in the exponent for the likelihood, which tells us how close this state is to the data we actually observed (scaled again with the precision).<br>
Because we are in the smoothing setting, we consider the distribution over the whole trajectory, which is why the effect of all of the above is over all the time steps, meaning e.g. an observation at  re-scales the probabilities at other time steps, most noticeably at  and so on.
<br><br><br>Only Noise, Prior with 
In some cases, it can be useful to define a <a data-tooltip-position="top" aria-label="Measure" data-href="Measure" href="the-guide/mathematics/measure-theory/measure.html" class="internal-link" target="_self" rel="noopener nofollow">measure</a>  with <a data-tooltip-position="top" aria-label="Probability Density Function" data-href="Probability Density Function" href="the-guide/mathematics/probability-theory/probability-density-function.html" class="internal-link" target="_self" rel="noopener nofollow">density</a>  based on ,  and  alone, meaning we have no assumption on the <a data-tooltip-position="top" aria-label="Static and Dynamic Systems" data-href="Static and Dynamic Systems" href="the-guide/robotics,-dynamics-and-control/dynamics/static-and-dynamic-systems.html" class="internal-link" target="_self" rel="noopener nofollow">dynamics</a> and deduce the <a data-tooltip-position="top" aria-label="Bayes Theorem" data-href="Bayes Theorem" href="the-guide/mathematics/statistics/bayes-theorem.html" class="internal-link" target="_self" rel="noopener nofollow">prior</a> distribution purely based on assumed initial distribution and <a data-tooltip-position="top" aria-label="Covariance and Variance" data-href="Covariance and Variance" href="the-guide/mathematics/statistics/covariance-and-variance.html" class="internal-link" target="_self" rel="noopener nofollow">covariances</a>. The density for the Gaussian case is which is a <a data-tooltip-position="top" aria-label="Gaussian Distribution" data-href="Gaussian Distribution" href="the-guide/mathematics/probability-theory/gaussian-distribution.html" class="internal-link" target="_self" rel="noopener nofollow">Gaussian</a> that is independent in each component , i.e. with  and block-diagonal , where the first block is  (covariance of initial distribution) and all others  (covariance of dynamics noise).
<br>
<br>By <a data-tooltip-position="top" aria-label="Reformulation of Stochastic Dynamics" data-href="Reformulation of Stochastic Dynamics" href="the-guide/computational-statistics/data-assimilation/reformulation-of-stochastic-dynamics.html" class="internal-link" target="_self" rel="noopener nofollow">Lemma 2.9</a> this can be used to derive the prior density of the noise
<br>Used to derive the <a data-tooltip-position="top" aria-label="Metropolis-Hastings MCMC" data-href="Metropolis-Hastings MCMC" href="the-guide/computational-statistics/data-assimilation/smoothing-algorithms/metropolis-hastings-mcmc.html" class="internal-link" target="_self" rel="noopener nofollow">pCN sampler</a>
<br>Deterministic Dynamics
In this scenario, we completely trust our dynamics model, yielding a distribution of only the initial condition , a probability <a data-href="Measure" href="the-guide/mathematics/measure-theory/measure.html" class="internal-link" target="_self" rel="noopener nofollow">Measure</a>  on  with density  given by using
]]></description><link>the-guide/computational-statistics/data-assimilation/smoothing-algorithms/the-smoothing-problem.html</link><guid isPermaLink="false">The Guide/Computational Statistics/Data Assimilation/Smoothing Algorithms/The Smoothing Problem.md</guid><pubDate>Wed, 23 Apr 2025 21:55:34 GMT</pubDate></item><item><title><![CDATA[Variational Smoothing Methods]]></title><description><![CDATA[<a class="tag" href="?query=tag:Data-Assimilation" style="background-color: rgb(4, 108, 116); color: white; font-weight: 700; border: none; border-radius: 1em; padding: 0.2em 0.5em;">#Data-Assimilation</a> 
 <br>In a Nutshell
Class of <a data-tooltip-position="top" aria-label="The Smoothing Problem" data-href="The Smoothing Problem" href="the-guide/computational-statistics/data-assimilation/smoothing-algorithms/the-smoothing-problem.html" class="internal-link" target="_self" rel="noopener nofollow">smoothing</a> algorithms that focuses on a <a data-tooltip-position="top" aria-label="Maximum A Posteriori Estimator" data-href="Maximum A Posteriori Estimator" href="the-guide/mathematics/statistics/maximum-a-posteriori-estimator.html" class="internal-link" target="_self" rel="noopener nofollow">MAP</a> <a data-tooltip-position="top" aria-label="Estimator" data-href="Estimator" href="the-guide/mathematics/statistics/estimator.html" class="internal-link" target="_self" rel="noopener nofollow">estimate</a> to reduce the complexity of the problem.
<br><br>Due to the required number of samples and the sampling procedure itself, <a data-tooltip-position="top" aria-label="Markov-Chain Monte Carlo Methods" data-href="Markov-Chain Monte Carlo Methods" href="the-guide/computational-statistics/data-assimilation/smoothing-algorithms/markov-chain-monte-carlo-methods.html" class="internal-link" target="_self" rel="noopener nofollow">MCMC</a> methods can be very expensive.<br>Variational methods aim to solve this problem by instead only focusing on the peaks of the distribution using a small (or one) number of samples.<br>Mathematically, the objective is <a data-href="Maximum A Posteriori Estimator" href="the-guide/mathematics/statistics/maximum-a-posteriori-estimator.html" class="internal-link" target="_self" rel="noopener nofollow">Maximum A Posteriori Estimator</a> based onwhere  is the <a data-tooltip-position="top" aria-label="The Smoothing Problem" data-href="The Smoothing Problem" href="the-guide/computational-statistics/data-assimilation/smoothing-algorithms/the-smoothing-problem.html" class="internal-link" target="_self" rel="noopener nofollow">negative log-posterior</a> of the smoothing formulation. Based on our <a data-tooltip-position="top" aria-label="Data Assimilation" data-href="Data Assimilation" href="the-guide/computational-statistics/data-assimilation/data-assimilation.html" class="internal-link" target="_self" rel="noopener nofollow">Gaussian setting of the</a> <a href=".?query=tag:Data-Assimilation" class="tag" target="_blank" rel="noopener nofollow">#Data-Assimilation</a> course, we can deduce thatThe resulting methods are denoted variational methods, sometimes 4DVAR due to applications in atmospheric sciences usually solving them over  space with an additional time dimension. <br>
<br>The method w4Dvar computes the minimum of  enforces dynamics in a weak sense 
<br>Deviations from deterministic dynamics and data are penalized
<br>In a strong version, dynamics and data are enforced, does not "allow" noise. Corresponds to deterministic case
<br>New minimizatio problem is lower dimensional ! How much depends on the data assimilation window
<br>In the following, we will only consider different formulations of the objective, as the resulting algorithms are more a topic of <a data-tooltip-position="top" aria-label="Optimization Problem" data-href="Optimization Problem" href="the-guide/mathematics/optimization/convex-optimization-lecture/optimization-problem.html" class="internal-link" target="_self" rel="noopener nofollow">optimization</a>.<br><br>Theorem 3.10 - Stochastic Dynamics
Consider the <a data-tooltip-position="top" aria-label="Data Assimilation" data-href="Data Assimilation" href="the-guide/computational-statistics/data-assimilation/data-assimilation.html" class="internal-link" target="_self" rel="noopener nofollow">data assimilation</a> problem for stochastic dynamics with  and . Then ...

<br>The <a data-tooltip-position="top" aria-label="Set" data-href="Set" href="the-guide/mathematics/general-stuff/set.html" class="internal-link" target="_self" rel="noopener nofollow">infimum</a> of  is attained in at least one point, the posterior density  (associated with measure ) is maximal there 
<br>Let  denote the <a data-tooltip-position="top" aria-label="Set" data-href="Set" href="the-guide/mathematics/general-stuff/set.html" class="internal-link" target="_self" rel="noopener nofollow">open ball</a> of radius  around . Then  

<br>
<br>Second statement basically restates the first, but makes no reference to Lebesque measure, can be generalized to infinite dimensions e.g. for continuous time (proof need adaption)
<br>Proof

<br> is continuous and non-negative, thus the infimum exists and is non-negativewith Assume without loss of generality that  for every  (we can relabel and throw out enough elements).
<br>Derive 

<br>We observe that the second term of  is non-negative. We can bound the term with  bounded and  continuous
<br>Induction over  up to  to show 
<br>Weierstraß says there is convergent subsequence denoted  limited by a 


<br>Probability of having sequence in open ball where 
<br>There exists a  that bounds the  above
<br>Using  and , we can now write , because the integrands are constant w.r.t. .

<br>
<br>Assumptions on  and  can be reduced at the cost of a more technical proof (min should be continuity)
<br>Using the same strategy as above, we may obtain a similar result for the <a data-tooltip-position="top" aria-label="The Smoothing Problem" data-href="The Smoothing Problem" href="the-guide/computational-statistics/data-assimilation/smoothing-algorithms/the-smoothing-problem.html" class="internal-link" target="_self" rel="noopener nofollow">special case of deterministic dynamics</a>.<br>Theorem 3.12 - Deterministic Dynamics
Consider the <a data-tooltip-position="top" aria-label="Data Assimilation" data-href="Data Assimilation" href="the-guide/computational-statistics/data-assimilation/data-assimilation.html" class="internal-link" target="_self" rel="noopener nofollow">data assimilation</a> problem for deterministi dynamics with  and . Then ...

<br>The <a data-tooltip-position="top" aria-label="Set" data-href="Set" href="the-guide/mathematics/general-stuff/set.html" class="internal-link" target="_self" rel="noopener nofollow">infimum</a> of  is attained in at least one point, the det. posterior density  (measure ) is maximal there 
<br>Let  denote the <a data-tooltip-position="top" aria-label="Set" data-href="Set" href="the-guide/mathematics/general-stuff/set.html" class="internal-link" target="_self" rel="noopener nofollow">open ball</a> of radius  around . Then  

<br>Problems 

<br>In contrast to the Bayesian perspective, where the posterior distribution is continuous with respect to small changes in the data, the maximizer may be discontinuous as a function of the data. Consider for example the two peaks example from the lecture, where small changes can lead to the estimate jumping back and forth between the peaks.
<br>The resulting optimization problem is not straight-forward. In fact, optimizers may get stuck in local minima, which make them very susceptible to bad initial conditions. 

]]></description><link>the-guide/computational-statistics/data-assimilation/smoothing-algorithms/variational-smoothing-methods.html</link><guid isPermaLink="false">The Guide/Computational Statistics/Data Assimilation/Smoothing Algorithms/Variational Smoothing Methods.md</guid><pubDate>Thu, 07 Nov 2024 09:11:47 GMT</pubDate></item><item><title><![CDATA[Assessing the Quality of Data-Assimilation Algorithms]]></title><description><![CDATA[ 
 <br>Let  denote a realization of data generated from a realization of the truth .  We want to state essential questions to asses the quality of <a data-tooltip-position="top" aria-label="The Smoothing Problem" data-href="The Smoothing Problem" href="the-guide/computational-statistics/data-assimilation/smoothing-algorithms/the-smoothing-problem.html" class="internal-link" target="_self" rel="noopener nofollow">smoothing</a> and <a data-tooltip-position="top" aria-label="The Filtering Problem" data-href="The Filtering Problem" href="the-guide/computational-statistics/data-assimilation/filtering-algorithms/the-filtering-problem.html" class="internal-link" target="_self" rel="noopener nofollow">filtering</a> algorithms.<br><br>Data Quality
How informative is the data we have ?
<br>Studying the properties of the <a data-tooltip-position="top" aria-label="Probability Distribution" data-href="Probability Distribution" href="the-guide/mathematics/probability-theory/probability-distribution.html" class="internal-link" target="_self" rel="noopener nofollow">probability distribution</a> of the prior and the <a data-tooltip-position="top" aria-label="Probability Density Function" data-href="Probability Density Function" href="the-guide/mathematics/probability-theory/probability-density-function.html" class="internal-link" target="_self" rel="noopener nofollow">pdf</a> of the <a data-tooltip-position="top" aria-label="Bayes Theorem" data-href="Bayes Theorem" href="the-guide/mathematics/statistics/bayes-theorem.html" class="internal-link" target="_self" rel="noopener nofollow">Bayesian posterior</a> itself without considering any algorithm.<br>
<br>
<a data-tooltip-position="top" aria-label="Bayes Theorem" data-href="Bayes Theorem" href="the-guide/mathematics/statistics/bayes-theorem.html" class="internal-link" target="_self" rel="noopener nofollow">Bayesian Posterior</a> Consistency

<br>Limiting behavior of  as either  (limit of infinite data) or  (limit of vanishing noise). The key question is whether the posterior  converges to the truth in any, none or both of these cases, e.g. getting closer and closer to a Dirac probability measure centered on . Deterministic dynamics an be discussed solely on the basis of  and a dirac measure on . In the filtering case, we may be concerned with the same behavior for  and a dirac measure on .


<br>
Model Error

<br>Mathematical model can be significantly different from reality. We assume an unknown true <a data-tooltip-position="top" aria-label="Static and Dynamic Systems" data-href="Static and Dynamic Systems" href="the-guide/robotics,-dynamics-and-control/dynamics/static-and-dynamic-systems.html" class="internal-link" target="_self" rel="noopener nofollow">solution operator</a>  that may exhibit stochastic or deterministic dynamics and / or act on a higher dimensional <a data-tooltip-position="top" aria-label="Space" data-href="Space" href="the-guide/mathematics/general-stuff/space.html" class="internal-link" target="_self" rel="noopener nofollow">space</a>.


<br><br>Algorithm Quality
How effective is the algorithm at extracting this information ?
<br>
<br>
Bayesian Quality Assessment

<br>Independent from the data quality. In the (in practice unlikely) case in which we know the true posterior  given a dataset , we can compare this to the solution  of a given algorithm using a <a data-tooltip-position="top" aria-label="Metrics on Spaces of Probability Measures" data-href="Metrics on Spaces of Probability Measures" href="the-guide/computational-statistics/data-assimilation/metrics-on-spaces-of-probability-measures.html" class="internal-link" target="_self" rel="noopener nofollow">metric</a> for <a data-tooltip-position="top" aria-label="Measure" data-href="Measure" href="the-guide/mathematics/measure-theory/measure.html" class="internal-link" target="_self" rel="noopener nofollow">probability measures</a>. We can also use the fact that any true <a data-tooltip-position="top" aria-label="Metric Space and Completeness" data-href="Metric Space and Completeness" href="the-guide/mathematics/general-stuff/metric-space-and-completeness.html" class="internal-link" target="_self" rel="noopener nofollow">metric</a> <a data-tooltip-position="top" aria-label="Metrics on Spaces of Probability Measures" data-href="Metrics on Spaces of Probability Measures" href="the-guide/computational-statistics/data-assimilation/metrics-on-spaces-of-probability-measures.html" class="internal-link" target="_self" rel="noopener nofollow">on spaces of probability measures</a> allows the use of the triangle inequalitywhere the term in red is concerned with the posterior consistency, while the green one is the Bayesian Quality Assessment.


<br>
Signal Estimation Quality Assessment

<br>Entangles algorithm properties with data quality. We assume that the algorithm at hand provides the signal  for a given dataset. How close is this to the signal  we would get if the algorithm was applied to  ? This  depends on both Posterior and Bayesian Quality Assessment, measuring the combined effectNote however, that metrics need proper measures and we need to use something like the <a data-tooltip-position="top" aria-label="Wasserstein Distance" data-href="Wasserstein Distance" href="the-guide/mathematics/optimal-transport/wasserstein-distance.html" class="internal-link" target="_self" rel="noopener nofollow">Wasserstein distance</a> to compare with the dirac of . If the algorithm provides a distribution we compare based on e.g. moments like <a data-tooltip-position="top" aria-label="Expectations" data-href="Expectations" href="the-guide/mathematics/statistics/expectations.html" class="internal-link" target="_self" rel="noopener nofollow">mean</a> / <a data-tooltip-position="top" aria-label="Covariance and Variance" data-href="Covariance and Variance" href="the-guide/mathematics/statistics/covariance-and-variance.html" class="internal-link" target="_self" rel="noopener nofollow">variance</a>. 


<br>
Forecast Skill

<br>Ability to make forecasts. In the Bayesian Quality Assessment case, we may study the k-lag distance by pushing both  and  forward  times and compare them. For the Signal Estimation Assessment case, we may apply our model  times while using  as the initial condition and then compare to .


<br><br>Algorithm Computational Efficiency 
At what computational cost does the algorithm reach this ?
]]></description><link>the-guide/computational-statistics/data-assimilation/assessing-the-quality-of-data-assimilation-algorithms.html</link><guid isPermaLink="false">The Guide/Computational Statistics/Data Assimilation/Assessing the Quality of Data-Assimilation Algorithms.md</guid><pubDate>Mon, 03 Mar 2025 15:30:25 GMT</pubDate></item><item><title><![CDATA[Data Assimilation]]></title><description><![CDATA[<a class="tag" href="?query=tag:Data-Assimilation" style="background-color: rgb(4, 108, 116); color: white; font-weight: 700; border: none; border-radius: 1em; padding: 0.2em 0.5em;">#Data-Assimilation</a> <a class="tag" href="?query=tag:Data-Assimilation" style="background-color: rgb(4, 108, 116); color: white; font-weight: 700; border: none; border-radius: 1em; padding: 0.2em 0.5em;">#Data-Assimilation</a> <a class="tag" href="?query=tag:Data-Assimilation" style="background-color: rgb(4, 108, 116); color: white; font-weight: 700; border: none; border-radius: 1em; padding: 0.2em 0.5em;">#Data-Assimilation</a> 
 <br>In a Nutshell
Mathematical discipline that seeks to combine theoretical models in form of <a data-href="ODEs - Ordinary Differential Equations" href="the-guide/mathematics/differential-equations/odes-ordinary-differential-equations.html" class="internal-link" target="_self" rel="noopener nofollow">ODEs - Ordinary Differential Equations</a> and numerics with observations given by data. Notation and settings follow the <a href=".?query=tag:Data-Assimilation" class="tag" target="_blank" rel="noopener nofollow">#Data-Assimilation</a> course based on the book by Law and Stuart.
<br><br><br>Throughout the course, we will use a discrete time setting with <a data-tooltip-position="top" aria-label="Gaussian Distribution" data-href="Gaussian Distribution" href="the-guide/mathematics/probability-theory/gaussian-distribution.html" class="internal-link" target="_self" rel="noopener nofollow">Gaussians</a> in almost all cases, because this allows closed-form solutions. Some of the resulting algorithms will be constrained to this setting, while others can be extend beyond it.<br>General Time-Discrete Setting
We consider the <a data-tooltip-position="top" aria-label="Static and Dynamic Systems" data-href="Static and Dynamic Systems" href="the-guide/robotics,-dynamics-and-control/dynamics/static-and-dynamic-systems.html" class="internal-link" target="_self" rel="noopener nofollow">stochastic dynamical system</a>, yielding a solution operator  of an underlying ODE and a <a data-tooltip-position="top" aria-label="Markov Chain" data-href="Markov Chain" href="the-guide/computational-statistics/data-assimilation/markov-chain.html" class="internal-link" target="_self" rel="noopener nofollow">Markov chain</a>  defined by the random mapwith an <a data-tooltip-position="top" aria-label="Statistical Independence" data-href="Statistical Independence" href="the-guide/mathematics/statistics/statistical-independence.html" class="internal-link" target="_self" rel="noopener nofollow">i.i.d.</a> sequence  with . The initial condition  is independent of , but possibly random.
This model is supplemented by observations / data  of the <a data-href="Static and Dynamic Systems" href="the-guide/robotics,-dynamics-and-control/dynamics/static-and-dynamic-systems.html" class="internal-link" target="_self" rel="noopener nofollow">Static and Dynamic Systems</a> as it evolves, typically reducing uncertainty. At each (discrete) time step, we observe a <a data-tooltip-position="top" aria-label="Mathematical Relations" data-href="Mathematical Relations" href="the-guide/mathematics/general-stuff/mathematical-relations.html" class="internal-link" target="_self" rel="noopener nofollow">function</a> of the signal with additive noise with the observation operator  and and <a data-tooltip-position="top" aria-label="Statistical Independence" data-href="Statistical Independence" href="the-guide/mathematics/statistics/statistical-independence.html" class="internal-link" target="_self" rel="noopener nofollow">i.i.d.</a> sequence  that is independent of . For the most part, we assume  with .
<br>Deterministic Setting
There is also a simpler setting, where we assume deterministic dynamics and completely trust our model. In this case, we only want to update our beliefs about the initial value given observationsNote that the observations are still suspect to noise.
<br>This provides a <a data-tooltip-position="top" aria-label="Probabilistic View of Dynamical Systems" data-href="Probabilistic View of Dynamical Systems" href="the-guide/computational-statistics/data-assimilation/probabilistic-view-of-dynamical-systems.html" class="internal-link" target="_self" rel="noopener nofollow">probabilistic view on the system</a> via the model of the <a data-tooltip-position="top" aria-label="Probability Distribution" data-href="Probability Distribution" href="the-guide/mathematics/probability-theory/probability-distribution.html" class="internal-link" target="_self" rel="noopener nofollow">jointly varying</a> <a data-tooltip-position="top" aria-label="Random Variable" data-href="Random Variable" href="the-guide/mathematics/probability-theory/random-variable.html" class="internal-link" target="_self" rel="noopener nofollow">random variable</a>  ( in deterministic case). We want to discover information about the signal, which equates to find the <a data-tooltip-position="top" aria-label="Measure" data-href="Measure" href="the-guide/mathematics/measure-theory/measure.html" class="internal-link" target="_self" rel="noopener nofollow">probability measure</a> for the <a data-tooltip-position="top" aria-label="Probability Distribution" data-href="Probability Distribution" href="the-guide/mathematics/probability-theory/probability-distribution.html" class="internal-link" target="_self" rel="noopener nofollow">conditional</a> <a data-tooltip-position="top" aria-label="Probability Distribution" data-href="Probability Distribution" href="the-guide/mathematics/probability-theory/probability-distribution.html" class="internal-link" target="_self" rel="noopener nofollow">distribution</a> . <br>Attention
Throughout the course <a href=".?query=tag:Data-Assimilation" class="tag" target="_blank" rel="noopener nofollow">#Data-Assimilation</a> ,  denotes both the <a data-tooltip-position="top" aria-label="Measure" data-href="Measure" href="the-guide/mathematics/measure-theory/measure.html" class="internal-link" target="_self" rel="noopener nofollow">probability measure</a> whenever its argument is a <a data-tooltip-position="top" aria-label="Borel Set and Borel Sigma Algebra" data-href="Borel Set and Borel Sigma Algebra" href="the-guide/mathematics/probability-theory/borel-set-and-borel-sigma-algebra.html" class="internal-link" target="_self" rel="noopener nofollow">Borel</a> set and a <a data-tooltip-position="top" aria-label="Probability Density Function" data-href="Probability Density Function" href="the-guide/mathematics/probability-theory/probability-density-function.html" class="internal-link" target="_self" rel="noopener nofollow">density</a> whenever its argument is a point in .
<br>In general, we consider two distinct settings for data assimilation, one on- and one offline.<br><br><br>Offline setting for data assimilation, tries to incorporate all the probabilistic information of the whole dataset using <a data-tooltip-position="top" aria-label="Bayes Theorem" data-href="Bayes Theorem" href="the-guide/mathematics/statistics/bayes-theorem.html" class="internal-link" target="_self" rel="noopener nofollow">Bayes</a> formula, resulting in a posterior for the whole trajectory . <br><br>
<br>Discrete Time

<br><a data-href="Kalman Smoother" href="the-guide/computational-statistics/data-assimilation/smoothing-algorithms/kalman-smoother.html" class="internal-link" target="_self" rel="noopener nofollow">Kalman Smoother</a> - linear dynamics, only Gaussian distributions

<br>Yields closed-form posterior


<br><a data-href="Markov-Chain Monte Carlo Methods" href="the-guide/computational-statistics/data-assimilation/smoothing-algorithms/markov-chain-monte-carlo-methods.html" class="internal-link" target="_self" rel="noopener nofollow">Markov-Chain Monte Carlo Methods</a> - general distributions

<br>Only samples from posterior and potentially expensive


<br><a data-href="Variational Smoothing Methods" href="the-guide/computational-statistics/data-assimilation/smoothing-algorithms/variational-smoothing-methods.html" class="internal-link" target="_self" rel="noopener nofollow">Variational Smoothing Methods</a> - concentrate on extremal points of posterior

<br>Performs <a data-tooltip-position="top" aria-label="Maximum A Posteriori Estimator" data-href="Maximum A Posteriori Estimator" href="the-guide/mathematics/statistics/maximum-a-posteriori-estimator.html" class="internal-link" target="_self" rel="noopener nofollow">MAP</a> <a data-tooltip-position="top" aria-label="Estimator" data-href="Estimator" href="the-guide/mathematics/statistics/estimator.html" class="internal-link" target="_self" rel="noopener nofollow">estimation</a>




<br><br><br>Online setting for data assimilation, determine information on the state of the signal at any specific point, therefore only considering the accumulated data up until time-step , which we denote . The process of filtering is concerned with determining the <a data-tooltip-position="top" aria-label="Probability Density Function" data-href="Probability Density Function" href="the-guide/mathematics/probability-theory/probability-density-function.html" class="internal-link" target="_self" rel="noopener nofollow">pdf</a>  associated with the probability <a data-href="Measure" href="the-guide/mathematics/measure-theory/measure.html" class="internal-link" target="_self" rel="noopener nofollow">Measure</a> of the <a data-tooltip-position="top" aria-label="Random Variable" data-href="Random Variable" href="the-guide/mathematics/probability-theory/random-variable.html" class="internal-link" target="_self" rel="noopener nofollow">random variable</a>  and the sequential update of this as the index increments.<br><br>
<br>Discrete Time

<br><a data-href="Kalman Filter" href="the-guide/computational-statistics/data-assimilation/filtering-algorithms/kalman-filter.html" class="internal-link" target="_self" rel="noopener nofollow">Kalman Filter</a> - linear dynamics, only Gaussian distributions

<br>Yields closed-form posterior


<br><a data-href="Approximate Gaussian Filters" href="the-guide/computational-statistics/data-assimilation/filtering-algorithms/approximate-gaussian-filters.html" class="internal-link" target="_self" rel="noopener nofollow">Approximate Gaussian Filters</a> - Variational filtering methods based on a Gaussian assumption
<br><a data-href="The Particle Filter" href="the-guide/computational-statistics/data-assimilation/filtering-algorithms/the-particle-filter.html" class="internal-link" target="_self" rel="noopener nofollow">The Particle Filter</a> - 


<br><br><br>Naturally, the solution both settings compute after the whole interval should coincide, while the filtering at any intermediate step only incorporates a subset of data-points.<br>Theorem 2.12
Let  denote the smoothing distribution on the discrete time interval  and  the filtering distribution at time  for the stochastic dynamics. Then the marginal of the smoothing distribution on  is the same as the filtering distribution at time : 
<br>Intuition
Smoothing handles whole trajectories, filtering single steps. The transition at every time-step is stochastic, we need to integrate out all possible sub-trajectories weighted by their probability .
<br>Remark
The marginal of the smoothing distribution at any time step  is not the same as the filter , because smoothing also incorporates the data observed after . 
<br>Theorem 2.14 - Smoothing and Filtering for Deterministic Dynamics
In this case, we can replace the integral of the preceding theorem by  <a data-tooltip-position="top" aria-label="Push-Forward Measure" data-href="Push-Forward Measure" href="the-guide/mathematics/measure-theory/push-forward-measure.html" class="internal-link" target="_self" rel="noopener nofollow">push-forwards</a>As before, this does not hold for arbitrary time-steps.
<br><br><br>Throughout the lecture <a href=".?query=tag:Data-Assimilation" class="tag" target="_blank" rel="noopener nofollow">#Data-Assimilation</a> we used very simple examples to illustrate some basic ideas.<br>
<br>Linear - Consider the <a data-tooltip-position="top" aria-label="Static and Dynamic Systems" data-href="Static and Dynamic Systems" href="the-guide/robotics,-dynamics-and-control/dynamics/static-and-dynamic-systems.html" class="internal-link" target="_self" rel="noopener nofollow">dynamical system</a> The influence of noise on that system depends on the parameter , as is shown in the plots below.<img alt="center" src="lib/media/pasted-image-20240422090504.png" style="width: 600px; max-width: 100%;">If the system is linear and the noise <a data-tooltip-position="top" aria-label="Gaussian Distribution" data-href="Gaussian Distribution" href="the-guide/mathematics/probability-theory/gaussian-distribution.html" class="internal-link" target="_self" rel="noopener nofollow">Gaussian</a> as in our initial setup, we can compute For  and , the <a data-tooltip-position="top" aria-label="Covariance and Variance" data-href="Covariance and Variance" href="the-guide/mathematics/statistics/covariance-and-variance.html" class="internal-link" target="_self" rel="noopener nofollow">variance</a> explodes, which matches the intuition from the right plot. If , we see that the variance approaches . With the formulas above, we reach a <a data-tooltip-position="top" aria-label="Gaussian Distribution" data-href="Gaussian Distribution" href="the-guide/mathematics/probability-theory/gaussian-distribution.html" class="internal-link" target="_self" rel="noopener nofollow">Gaussian</a> <a data-tooltip-position="top" aria-label="Probability Distribution" data-href="Probability Distribution" href="the-guide/mathematics/probability-theory/probability-distribution.html" class="internal-link" target="_self" rel="noopener nofollow">distribution</a> for  with and zero <a data-tooltip-position="top" aria-label="Expectations" data-href="Expectations" href="the-guide/mathematics/statistics/expectations.html" class="internal-link" target="_self" rel="noopener nofollow">mean</a>.
<br>Nonlinear - consider 

<br>
<br>Adding noise can result in chaotic behavior depending on 


]]></description><link>the-guide/computational-statistics/data-assimilation/data-assimilation.html</link><guid isPermaLink="false">The Guide/Computational Statistics/Data Assimilation/Data Assimilation.md</guid><pubDate>Wed, 23 Apr 2025 21:55:31 GMT</pubDate><enclosure url="lib/media/pasted-image-20240422090504.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src=&quot;lib/media/pasted-image-20240422090504.png&quot;&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[Detailed Balance Condition]]></title><description><![CDATA[ 
 <br>In a Nutshell
Sufficient, but not necessary condition for the existence of a <a data-tooltip-position="top" aria-label="Markov Chain" data-href="Markov Chain" href="the-guide/computational-statistics/data-assimilation/markov-chain.html" class="internal-link" target="_self" rel="noopener nofollow">stationary distribution</a> of a <a data-href="Markov Chain" href="the-guide/computational-statistics/data-assimilation/markov-chain.html" class="internal-link" target="_self" rel="noopener nofollow">Markov Chain</a>. Is used as a stronger assumption on the underlying <a data-tooltip-position="top" aria-label="Probability Distribution" data-href="Probability Distribution" href="the-guide/mathematics/probability-theory/probability-distribution.html" class="internal-link" target="_self" rel="noopener nofollow">distribution</a>to derive the acceptance criterion in <a data-tooltip-position="top" aria-label="Markov-Chain Monte Carlo Methods" data-href="Markov-Chain Monte Carlo Methods" href="the-guide/computational-statistics/data-assimilation/smoothing-algorithms/markov-chain-monte-carlo-methods.html" class="internal-link" target="_self" rel="noopener nofollow">MCMC</a> methods, i.e. Metropolis-Hastings. 
<br><br>Invariance of a distribution with respect to a <a data-tooltip-position="top" aria-label="Measure" data-href="Measure" href="the-guide/mathematics/measure-theory/measure.html" class="internal-link" target="_self" rel="noopener nofollow">measure</a> relies on <a data-tooltip-position="top" aria-label="Expectations" data-href="Expectations" href="the-guide/mathematics/statistics/expectations.html" class="internal-link" target="_self" rel="noopener nofollow">expectations</a> in the sample <a data-tooltip-position="top" aria-label="Space" data-href="Space" href="the-guide/mathematics/general-stuff/space.html" class="internal-link" target="_self" rel="noopener nofollow">space</a>, e.g. , which can be costly to compute. Detailed balance is a stricter, point-wise condition that requires time reversibility via where  is the distribution in question and  denotes transition probabilities, e.g. via a <a data-href="Markov Kernel" href="the-guide/computational-statistics/data-assimilation/markov-kernel.html" class="internal-link" target="_self" rel="noopener nofollow">Markov Kernel</a>. <br>Proof
Because  follow the same <a data-tooltip-position="top" aria-label="Probability Distribution" data-href="Probability Distribution" href="the-guide/mathematics/probability-theory/probability-distribution.html" class="internal-link" target="_self" rel="noopener nofollow">distribution</a>, and integrating out  yields (probability of going from a fixed  to any state in the state space is ), integrating the condition yields which is precisely the definition of an <a data-tooltip-position="top" aria-label="Markov Chain" data-href="Markov Chain" href="the-guide/computational-statistics/data-assimilation/markov-chain.html" class="internal-link" target="_self" rel="noopener nofollow">invariant density</a> for .
]]></description><link>the-guide/computational-statistics/data-assimilation/detailed-balance-condition.html</link><guid isPermaLink="false">The Guide/Computational Statistics/Data Assimilation/Detailed Balance Condition.md</guid><pubDate>Sat, 14 Sep 2024 08:05:41 GMT</pubDate></item><item><title><![CDATA[Ergodicity]]></title><description><![CDATA[ 
 <br>In a Nutshell
If a system is ergodic, a particle in a <a data-tooltip-position="top" aria-label="Static and Dynamic Systems" data-href="Static and Dynamic Systems" href="the-guide/robotics,-dynamics-and-control/dynamics/static-and-dynamic-systems.html" class="internal-link" target="_self" rel="noopener nofollow">dynamic</a> or <a data-tooltip-position="top" aria-label="Stochastic Process" data-href="Stochastic Process" href="the-guide/mathematics/probability-theory/stochastic-process.html" class="internal-link" target="_self" rel="noopener nofollow">stochastic</a> process will eventually visit all parts of the state <a data-tooltip-position="top" aria-label="Space" data-href="Space" href="the-guide/mathematics/general-stuff/space.html" class="internal-link" target="_self" rel="noopener nofollow">space</a>, in other words the system cannot be reduced or factored into smaller components.<br>
Ergodicity ensures the existence of a unique <a data-tooltip-position="top" aria-label="Markov Chain" data-href="Markov Chain" href="the-guide/computational-statistics/data-assimilation/markov-chain.html" class="internal-link" target="_self" rel="noopener nofollow">invariant measure</a> and guarantees convergence towards it.
<br>
<br>Implies that a large collection of samples can represent the average statistical properties of the <a data-tooltip-position="top" aria-label="Static and Dynamic Systems" data-href="Static and Dynamic Systems" href="the-guide/robotics,-dynamics-and-control/dynamics/static-and-dynamic-systems.html" class="internal-link" target="_self" rel="noopener nofollow">stochastic dynamic system</a>
<br>Implies that average behavior can be deduced from a typical point
<br>Captures the common-sense notion of randomness
<br><br>Ergodicity
An ergodic <a data-href="Static and Dynamic Systems" href="the-guide/robotics,-dynamics-and-control/dynamics/static-and-dynamic-systems.html" class="internal-link" target="_self" rel="noopener nofollow">Static and Dynamic Systems</a> is one for which, for a suitable class of test <a data-tooltip-position="top" aria-label="Mathematical Relations" data-href="Mathematical Relations" href="the-guide/mathematics/general-stuff/mathematical-relations.html" class="internal-link" target="_self" rel="noopener nofollow">functions</a> , the Markov chain above with initial condition  satisfies almost surely with respect to the <a data-tooltip-position="top" aria-label="Markov Chain" data-href="Markov Chain" href="the-guide/computational-statistics/data-assimilation/markov-chain.html" class="internal-link" target="_self" rel="noopener nofollow">invariant measure</a> . For this to be true, the system has to be irreducible (reach very state) and aperiodic (no loops).
<br>Quote
"The time average equals the space average."
<br>
<br>Important concept e.g. in <a data-href="Markov-Chain Monte Carlo Methods" href="the-guide/computational-statistics/data-assimilation/smoothing-algorithms/markov-chain-monte-carlo-methods.html" class="internal-link" target="_self" rel="noopener nofollow">Markov-Chain Monte Carlo Methods</a>
<br>Intuition
After sufficient time, the chain "forgets" ist initialization and, in the long run, approaches a time-averaged behavior encoded in the invariant <a data-href="Measure" href="the-guide/mathematics/measure-theory/measure.html" class="internal-link" target="_self" rel="noopener nofollow">Measure</a>.
<br><br><br>Consider the <a data-tooltip-position="top" aria-label="Static and Dynamic Systems" data-href="Static and Dynamic Systems" href="the-guide/robotics,-dynamics-and-control/dynamics/static-and-dynamic-systems.html" class="internal-link" target="_self" rel="noopener nofollow">dynamical system</a>  with some . The influence of noise on that system depends on the parameter , as is shown in the plots below.<img alt="center" src="lib/media/pasted-image-20240422090504.png" style="width: 600px; max-width: 100%;">If the system is linear and the noise <a data-tooltip-position="top" aria-label="Gaussian Distribution" data-href="Gaussian Distribution" href="the-guide/mathematics/probability-theory/gaussian-distribution.html" class="internal-link" target="_self" rel="noopener nofollow">Gaussian</a> as in our initial setup, we can compute For  and , the <a data-tooltip-position="top" aria-label="Covariance and Variance" data-href="Covariance and Variance" href="the-guide/mathematics/statistics/covariance-and-variance.html" class="internal-link" target="_self" rel="noopener nofollow">variance</a> explodes, which matches the intuition from the right plot. If , we see that the variance approaches . With the formulas above, we reach a <a data-tooltip-position="top" aria-label="Gaussian Distribution" data-href="Gaussian Distribution" href="the-guide/mathematics/probability-theory/gaussian-distribution.html" class="internal-link" target="_self" rel="noopener nofollow">Gaussian</a> <a data-tooltip-position="top" aria-label="Probability Distribution" data-href="Probability Distribution" href="the-guide/mathematics/probability-theory/probability-distribution.html" class="internal-link" target="_self" rel="noopener nofollow">distribution</a> for  with and zero <a data-tooltip-position="top" aria-label="Expectations" data-href="Expectations" href="the-guide/mathematics/statistics/expectations.html" class="internal-link" target="_self" rel="noopener nofollow">mean</a>. The <a data-tooltip-position="top" aria-label="Markov Chain" data-href="Markov Chain" href="the-guide/computational-statistics/data-assimilation/markov-chain.html" class="internal-link" target="_self" rel="noopener nofollow">invariant</a> <a data-tooltip-position="top" aria-label="Measure" data-href="Measure" href="the-guide/mathematics/measure-theory/measure.html" class="internal-link" target="_self" rel="noopener nofollow">measure</a> is therefore  and we have that ]]></description><link>the-guide/computational-statistics/data-assimilation/ergodicity.html</link><guid isPermaLink="false">The Guide/Computational Statistics/Data Assimilation/Ergodicity.md</guid><pubDate>Wed, 23 Apr 2025 21:55:32 GMT</pubDate><enclosure url="lib/media/pasted-image-20240422090504.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src=&quot;lib/media/pasted-image-20240422090504.png&quot;&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[Long Time Behavior of Dynamical Systems]]></title><description><![CDATA[ 
 <br>For a (discrete, but generalizable to continuous) <a data-tooltip-position="top" aria-label="Static and Dynamic Systems" data-href="Static and Dynamic Systems" href="the-guide/robotics,-dynamics-and-control/dynamics/static-and-dynamic-systems.html" class="internal-link" target="_self" rel="noopener nofollow">dynamical system</a>, we can extend the operator  to act on Borel subsets of  by <br>Bounded Absorbing <a data-href="Set" href="the-guide/mathematics/general-stuff/set.html" class="internal-link" target="_self" rel="noopener nofollow">Set</a>
For a <a data-tooltip-position="top" aria-label="Set" data-href="Set" href="the-guide/mathematics/general-stuff/set.html" class="internal-link" target="_self" rel="noopener nofollow">ball</a>  in  with radius  in euclidean norm centered at the origin, we denote the bounding absorbing set  to be the set for which there is an index , for which where .
<br>If such a bounded absorbing set exists, we can define the global attractor as the intersection of all absorbing sets]]></description><link>the-guide/computational-statistics/data-assimilation/long-time-behavior-of-dynamical-systems.html</link><guid isPermaLink="false">The Guide/Computational Statistics/Data Assimilation/Long Time Behavior of Dynamical Systems.md</guid><pubDate>Sat, 12 Apr 2025 10:18:01 GMT</pubDate></item><item><title><![CDATA[Markov Chain]]></title><description><![CDATA[ 
 <br>Definition
<a data-tooltip-position="top" aria-label="Stochastic Process" data-href="Stochastic Process" href="the-guide/mathematics/probability-theory/stochastic-process.html" class="internal-link" target="_self" rel="noopener nofollow">Stochastic process</a> with dependency of the <a data-tooltip-position="top" aria-label="Random Variable" data-href="Random Variable" href="the-guide/mathematics/probability-theory/random-variable.html" class="internal-link" target="_self" rel="noopener nofollow">random variables</a>, standing in contrast to random walks. In a Markov chain  the <a data-tooltip-position="top" aria-label="Random Variable" data-href="Random Variable" href="the-guide/mathematics/probability-theory/random-variable.html" class="internal-link" target="_self" rel="noopener nofollow">random variables</a>  and  are <a data-tooltip-position="top" aria-label="Statistical Independence" data-href="Statistical Independence" href="the-guide/mathematics/statistics/statistical-independence.html" class="internal-link" target="_self" rel="noopener nofollow">independent</a> if <a data-tooltip-position="top" aria-label="Probability Distribution" data-href="Probability Distribution" href="the-guide/mathematics/probability-theory/probability-distribution.html" class="internal-link" target="_self" rel="noopener nofollow">conditioned</a> on . Mathematically,where the second step is possible because the conditioning on  does not have any extra information.
<br><img alt="center" src="lib/media/pasted-image-20240207103418.png" style="width: 300px; max-width: 100%;"><br>
<br>Markov Property

<br>Transition dynamics depend only on the current time step


<br>Other Properties

<br>Time homogeneous, if <a data-tooltip-position="top" aria-label="Markov Decision Process" data-href="Markov Decision Process" href="the-guide/machine-learning/reinforcement-learning/markov-decision-process.html" class="internal-link" target="_self" rel="noopener nofollow">transition matrix</a> is the same after each step
<br>...


<br>Quote

<br>"There is no extra information about  and  contained in ." 
<br>"The future is independent of the past given the present."

<br><br>For a discrete setting, the chain can be described using the State Transition Matrix . Given Markov state , its successor , the state transition probability  is defined by the <a data-tooltip-position="top" aria-label="Matrices" data-href="Matrices" href="the-guide/mathematics/linear-algebra/matrices.html" class="internal-link" target="_self" rel="noopener nofollow">matrix</a> with  being the probability to go from state  to state . For the continuous case, we get the more general <a data-href="Markov Kernel" href="the-guide/computational-statistics/data-assimilation/markov-kernel.html" class="internal-link" target="_self" rel="noopener nofollow">Markov Kernel</a> <a data-tooltip-position="top" aria-label="Mathematical Relations" data-href="Mathematical Relations" href="the-guide/mathematics/general-stuff/mathematical-relations.html" class="internal-link" target="_self" rel="noopener nofollow">function</a>.<br><br><br>Given (discrete case) the <a data-tooltip-position="top" aria-label="Markov Decision Process" data-href="Markov Decision Process" href="the-guide/machine-learning/reinforcement-learning/markov-decision-process.html" class="internal-link" target="_self" rel="noopener nofollow">transition</a> <a data-tooltip-position="top" aria-label="Matrices" data-href="Matrices" href="the-guide/mathematics/linear-algebra/matrices.html" class="internal-link" target="_self" rel="noopener nofollow">matrix</a> , a stationary or equilibrium <a data-tooltip-position="top" aria-label="Probability Distribution" data-href="Probability Distribution" href="the-guide/mathematics/probability-theory/probability-distribution.html" class="internal-link" target="_self" rel="noopener nofollow">distribution</a>  (here a row vector) is defined by It is therefore related to the <a data-tooltip-position="top" aria-label="Eigenvalue Problem" data-href="Eigenvalue Problem" href="the-guide/mathematics/linear-algebra/eigenvalue-problem.html" class="internal-link" target="_self" rel="noopener nofollow">eigenvalue problem</a>. <br>Theorem
If the Markov chain is time homogeneous, has a finite state space and is irreducible and aperiodic, then the stationary <a data-tooltip-position="top" aria-label="Probability Distribution" data-href="Probability Distribution" href="the-guide/mathematics/probability-theory/probability-distribution.html" class="internal-link" target="_self" rel="noopener nofollow">distribution</a> is unique and This follows from the <a data-href="Perron-Frobenius Theorem" href="the-guide/mathematics/linear-algebra/perron-frobenius-theorem.html" class="internal-link" target="_self" rel="noopener nofollow">Perron-Frobenius Theorem</a> (intuition <a data-href="Power Iteration" href="the-guide/mathematics/linear-algebra/power-iteration.html" class="internal-link" target="_self" rel="noopener nofollow">Power Iteration</a>). For densities, a Markov chain with unique invariant density  equivalently satisfies 
<br>
<br>Probability of being at  is the sum of all probabilities to reach  from any other point.
<br>A sufficient, but not necessary condition for the existence of the stationary distribution is <a data-tooltip-position="top" aria-label="Detailed Balance Condition" data-href="Detailed Balance Condition" href="the-guide/computational-statistics/data-assimilation/detailed-balance-condition.html" class="internal-link" target="_self" rel="noopener nofollow">detailed balance</a> 
]]></description><link>the-guide/computational-statistics/data-assimilation/markov-chain.html</link><guid isPermaLink="false">The Guide/Computational Statistics/Data Assimilation/Markov Chain.md</guid><pubDate>Sat, 25 Jan 2025 16:06:16 GMT</pubDate><enclosure url="lib/media/pasted-image-20240207103418.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src=&quot;lib/media/pasted-image-20240207103418.png&quot;&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[Markov Kernel]]></title><description><![CDATA[ 
 <br>In a Nutshell
<a data-tooltip-position="top" aria-label="Mathematical Relations" data-href="Mathematical Relations" href="the-guide/mathematics/general-stuff/mathematical-relations.html" class="internal-link" target="_self" rel="noopener nofollow">Map</a> that generalizes the <a data-tooltip-position="top" aria-label="Markov Decision Process" data-href="Markov Decision Process" href="the-guide/machine-learning/reinforcement-learning/markov-decision-process.html" class="internal-link" target="_self" rel="noopener nofollow">transition matrix</a> of a discrete MDP.
<br><br>Definition
A <a data-tooltip-position="top" aria-label="Mathematical Relations" data-href="Mathematical Relations" href="the-guide/mathematics/general-stuff/mathematical-relations.html" class="internal-link" target="_self" rel="noopener nofollow">function</a>  is a Markov Kernel, if the following condition hold

<br>For each ,  is a <a data-tooltip-position="top" aria-label="Measure" data-href="Measure" href="the-guide/mathematics/measure-theory/measure.html" class="internal-link" target="_self" rel="noopener nofollow">probability measure</a> on ;
<br> is -measurable for all ;

<br>The first condition implies that at a fixed , the Markov kernel describes a <a data-tooltip-position="top" aria-label="Probability Distribution" data-href="Probability Distribution" href="the-guide/mathematics/probability-theory/probability-distribution.html" class="internal-link" target="_self" rel="noopener nofollow">probability distribution</a> of a new point .]]></description><link>the-guide/computational-statistics/data-assimilation/markov-kernel.html</link><guid isPermaLink="false">The Guide/Computational Statistics/Data Assimilation/Markov Kernel.md</guid><pubDate>Fri, 13 Sep 2024 06:58:58 GMT</pubDate></item><item><title><![CDATA[Metrics on Spaces of Probability Measures]]></title><description><![CDATA[ 
 <br>In a Nutshell
While easier to find and interpret, <a data-tooltip-position="top" aria-label="Divergence" data-href="Divergence" href="the-guide/information-theory/information-geometry/divergence.html" class="internal-link" target="_self" rel="noopener nofollow">divergences</a> may be restricted in use due to them not fulfilling the triangle inequality. However, under certain assumptions there exist <a data-tooltip-position="top" aria-label="Metric Space and Completeness" data-href="Metric Space and Completeness" href="the-guide/mathematics/general-stuff/metric-space-and-completeness.html" class="internal-link" target="_self" rel="noopener nofollow">metrics</a> on <a data-tooltip-position="top" aria-label="Measure" data-href="Measure" href="the-guide/mathematics/measure-theory/measure.html" class="internal-link" target="_self" rel="noopener nofollow">probability measures</a>.
<br><br>Consider the <a data-tooltip-position="top" aria-label="Space" data-href="Space" href="the-guide/mathematics/general-stuff/space.html" class="internal-link" target="_self" rel="noopener nofollow">space</a>  of probability measures on  with strictly positive Lebesgue density. Let  and  be two <a data-tooltip-position="top" aria-label="Measure" data-href="Measure" href="the-guide/mathematics/measure-theory/measure.html" class="internal-link" target="_self" rel="noopener nofollow">probability measures</a> on  with corresponding <a data-tooltip-position="top" aria-label="Probability Density Function" data-href="Probability Density Function" href="the-guide/mathematics/probability-theory/probability-density-function.html" class="internal-link" target="_self" rel="noopener nofollow">densities</a>  and  ...<br>Total Variation Distance
The total variation distance on  is defined by 
<br>Hellinger Distance
The Hellinger distance on  is defined by 
<br>Lemma
The Hellinger distance and the <a data-tooltip-position="top" aria-label="Kullback-Leibler Divergence" data-href="Kullback-Leibler Divergence" href="the-guide/information-theory/information-theory-1/information/kullback-leibler-divergence.html" class="internal-link" target="_self" rel="noopener nofollow">KL-divergence</a> satisfywhile for the total variation it holds that
<br>Lemma
Total Variation and Hellinger Distance satisfy 
<br>Root Mean Square Distance
Let  denote for each  an element of the space of prob measures over  denoted  and let  be a <a data-tooltip-position="top" aria-label="Random Variable" data-href="Random Variable" href="the-guide/mathematics/probability-theory/random-variable.html" class="internal-link" target="_self" rel="noopener nofollow">random variable</a>. We can define a root mean square distance between two random prob measures  and  via using the convention  for measurable  and 
<br>
<br>Without randomness in , it reduces to <a data-tooltip-position="top" aria-label="Total Variation" data-href="Total Variation" href="the-guide/integral-transforms-and-signals/total-variation.html" class="internal-link" target="_self" rel="noopener nofollow">total variation</a>
]]></description><link>the-guide/computational-statistics/data-assimilation/metrics-on-spaces-of-probability-measures.html</link><guid isPermaLink="false">The Guide/Computational Statistics/Data Assimilation/Metrics on Spaces of Probability Measures.md</guid><pubDate>Wed, 23 Apr 2025 09:23:22 GMT</pubDate></item><item><title><![CDATA[Probabilistic View of Dynamical Systems]]></title><description><![CDATA[ 
 <br>In a Nutshell
There is a natural connection between <a data-tooltip-position="top" aria-label="Static and Dynamic Systems" data-href="Static and Dynamic Systems" href="the-guide/robotics,-dynamics-and-control/dynamics/static-and-dynamic-systems.html" class="internal-link" target="_self" rel="noopener nofollow">dynamical systems</a> and the underlying dynamical systems they produce on <a data-tooltip-position="top" aria-label="Measure" data-href="Measure" href="the-guide/mathematics/measure-theory/measure.html" class="internal-link" target="_self" rel="noopener nofollow">probability measures</a>. The <a data-tooltip-position="top" aria-label="Markov Chain" data-href="Markov Chain" href="the-guide/computational-statistics/data-assimilation/markov-chain.html" class="internal-link" target="_self" rel="noopener nofollow">Markovian</a> propagations of this measure via <a data-tooltip-position="top" aria-label="Markov Kernel" data-href="Markov Kernel" href="the-guide/computational-statistics/data-assimilation/markov-kernel.html" class="internal-link" target="_self" rel="noopener nofollow">Markov Kernels</a> are linear, even if the underlying dynamics are not. However, the dynamic on <a data-tooltip-position="top" aria-label="Probability Distribution" data-href="Probability Distribution" href="the-guide/mathematics/probability-theory/probability-distribution.html" class="internal-link" target="_self" rel="noopener nofollow">probability distributions</a> is infinite dimensional.
<br><br>We consider the <a data-tooltip-position="top" aria-label="Static and Dynamic Systems" data-href="Static and Dynamic Systems" href="the-guide/robotics,-dynamics-and-control/dynamics/static-and-dynamic-systems.html" class="internal-link" target="_self" rel="noopener nofollow">stochastic dynamical system</a> with an <a data-tooltip-position="top" aria-label="Statistical Independence" data-href="Statistical Independence" href="the-guide/mathematics/statistics/statistical-independence.html" class="internal-link" target="_self" rel="noopener nofollow">i.i.d.</a> sequence  <a data-tooltip-position="top" aria-label="Probability Distribution" data-href="Probability Distribution" href="the-guide/mathematics/probability-theory/probability-distribution.html" class="internal-link" target="_self" rel="noopener nofollow">distributed</a> according to a <a data-tooltip-position="top" aria-label="Measure" data-href="Measure" href="the-guide/mathematics/measure-theory/measure.html" class="internal-link" target="_self" rel="noopener nofollow">probability measure</a> on  with density . The initial condition  is independent of , but possibly random.<br>Warning
In the following,  denotes both the <a data-tooltip-position="top" aria-label="Measure" data-href="Measure" href="the-guide/mathematics/measure-theory/measure.html" class="internal-link" target="_self" rel="noopener nofollow">probability measure</a> whenever its argument is a Borel set and a <a data-tooltip-position="top" aria-label="Probability Density Function" data-href="Probability Density Function" href="the-guide/mathematics/probability-theory/probability-density-function.html" class="internal-link" target="_self" rel="noopener nofollow">density</a> whenever its argument is a point on .
<br><br><br>Under these assumptions  forms a <a data-tooltip-position="top" aria-label="Markov Chain" data-href="Markov Chain" href="the-guide/computational-statistics/data-assimilation/markov-chain.html" class="internal-link" target="_self" rel="noopener nofollow">Markov chain</a> with To determine the probability that the next iterate is located inside a <a data-tooltip-position="top" aria-label="Measure" data-href="Measure" href="the-guide/mathematics/measure-theory/measure.html" class="internal-link" target="_self" rel="noopener nofollow">measurable</a> <a data-tooltip-position="top" aria-label="Set" data-href="Set" href="the-guide/mathematics/general-stuff/set.html" class="internal-link" target="_self" rel="noopener nofollow">subset</a> (Borel set  or event), we can thus compute and define the <a data-href="Markov Kernel" href="the-guide/computational-statistics/data-assimilation/markov-kernel.html" class="internal-link" target="_self" rel="noopener nofollow">Markov Kernel</a>If  with <a data-tooltip-position="top" aria-label="Probability Density Function" data-href="Probability Density Function" href="the-guide/mathematics/probability-theory/probability-density-function.html" class="internal-link" target="_self" rel="noopener nofollow">pdf</a> , we can write and then <br>Dynamical Evolution of the Density
With this, we have a linear dynamical system for the evolution of the <a data-tooltip-position="top" aria-label="Probability Density Function" data-href="Probability Density Function" href="the-guide/mathematics/probability-theory/probability-density-function.html" class="internal-link" target="_self" rel="noopener nofollow">pdf</a> of the form where  is the integral operator (convolution) 
]]></description><link>the-guide/computational-statistics/data-assimilation/probabilistic-view-of-dynamical-systems.html</link><guid isPermaLink="false">The Guide/Computational Statistics/Data Assimilation/Probabilistic View of Dynamical Systems.md</guid><pubDate>Mon, 03 Mar 2025 15:30:25 GMT</pubDate></item><item><title><![CDATA[Reformulation of Stochastic Dynamics]]></title><description><![CDATA[ 
 <br>For <a data-tooltip-position="top" aria-label="Static and Dynamic Systems" data-href="Static and Dynamic Systems" href="the-guide/robotics,-dynamics-and-control/dynamics/static-and-dynamic-systems.html" class="internal-link" target="_self" rel="noopener nofollow">stochastic dynamical systems</a>, it can be useful to reformulate the distribution to instead take the noise as an input, e.g. for derivation of <a data-tooltip-position="top" aria-label="Metropolis-Hastings MCMC" data-href="Metropolis-Hastings MCMC" href="the-guide/computational-statistics/data-assimilation/smoothing-algorithms/metropolis-hastings-mcmc.html" class="internal-link" target="_self" rel="noopener nofollow">pCN Dynamics Sampler</a> or even the simplified objective for <a data-tooltip-position="top" aria-label="Diffusion Models" data-href="Diffusion Models" href="the-guide/machine-learning/generative-models/diffusion-models.html" class="internal-link" target="_self" rel="noopener nofollow">Diffusion models</a>. We define the vector <br>Lemma 2.9
Define the <a data-tooltip-position="top" aria-label="Mathematical Relations" data-href="Mathematical Relations" href="the-guide/mathematics/general-stuff/mathematical-relations.html" class="internal-link" target="_self" rel="noopener nofollow">mapping</a>  that transforms the initial condition and all observed noise to reconstruct the signal bywhere  is defined as in the stochastic part of <a data-tooltip-position="top" aria-label="Static and Dynamic Systems" data-href="Static and Dynamic Systems" href="the-guide/robotics,-dynamics-and-control/dynamics/static-and-dynamic-systems.html" class="internal-link" target="_self" rel="noopener nofollow">dynamical systems</a>. This mapping is invertible. For , it yields the identity.
<br>Once the initial condition is specified, we can reformulate the stochastic dynamics easily via Using <a data-tooltip-position="top" aria-label="Push-Forward Measure" data-href="Push-Forward Measure" href="the-guide/mathematics/measure-theory/push-forward-measure.html" class="internal-link" target="_self" rel="noopener nofollow">push-forward</a> <a data-tooltip-position="top" aria-label="Measure" data-href="Measure" href="the-guide/mathematics/measure-theory/measure.html" class="internal-link" target="_self" rel="noopener nofollow">measures</a>, we can define <br><br>With the above, we can design algorithms that generate samples of  and then convert them into samples of . To derive an expression for , we first consider the <a data-tooltip-position="top" aria-label="The Smoothing Problem" data-href="The Smoothing Problem" href="the-guide/computational-statistics/data-assimilation/smoothing-algorithms/the-smoothing-problem.html" class="internal-link" target="_self" rel="noopener nofollow">dynamics free special case</a> as a prior on the noise , i.e. based on the prior in this case has the formTo derive the <a data-tooltip-position="top" aria-label="Likelihood Function" data-href="Likelihood Function" href="the-guide/mathematics/statistics/likelihood-function.html" class="internal-link" target="_self" rel="noopener nofollow">likelihood</a> , we define  and can now concatenate the data via where  is the <a data-tooltip-position="top" aria-label="Gaussian Distribution" data-href="Gaussian Distribution" href="the-guide/mathematics/probability-theory/gaussian-distribution.html" class="internal-link" target="_self" rel="noopener nofollow">Gaussian</a> <a data-tooltip-position="top" aria-label="Random Variable" data-href="Random Variable" href="the-guide/mathematics/probability-theory/random-variable.html" class="internal-link" target="_self" rel="noopener nofollow">random variable</a>  with The likelihood is therefore determined by . <br>Reformulation of Stochastic Dynamics
By <a data-href="Bayes Theorem" href="the-guide/mathematics/statistics/bayes-theorem.html" class="internal-link" target="_self" rel="noopener nofollow">Bayes Theorem</a>, the <a data-tooltip-position="top" aria-label="Data Assimilation" data-href="Data Assimilation" href="the-guide/computational-statistics/data-assimilation/data-assimilation.html" class="internal-link" target="_self" rel="noopener nofollow">posterior smoothing</a> <a data-tooltip-position="top" aria-label="Probability Distribution" data-href="Probability Distribution" href="the-guide/mathematics/probability-theory/probability-distribution.html" class="internal-link" target="_self" rel="noopener nofollow">distribution</a> on  for the stochastic dynamics model is a probability measure  on  with <a data-tooltip-position="top" aria-label="Probability Density Function" data-href="Probability Density Function" href="the-guide/mathematics/probability-theory/probability-density-function.html" class="internal-link" target="_self" rel="noopener nofollow">pdf</a>  proportional to with  is again denoted the negative log posterior.
<br>Intuition
The term  determines again the trust- / precision-weighted squared distance of observed data and the noise vector we are evaluating. For this, it is first transformed via , so that we can compare it with the actual data.<br>
The second term simply adds how likely each step (initial plus all the noise values in ) was to begin with. Imagine e.g. a diagonal covariance . A high variance (=low trust) in one dimension scales down large perturbations, yielding higher posterior values.
]]></description><link>the-guide/computational-statistics/data-assimilation/reformulation-of-stochastic-dynamics.html</link><guid isPermaLink="false">The Guide/Computational Statistics/Data Assimilation/Reformulation of Stochastic Dynamics.md</guid><pubDate>Mon, 03 Mar 2025 15:30:25 GMT</pubDate></item><item><title><![CDATA[Well-Posedness of Data Assimilation Problems]]></title><description><![CDATA[<a class="tag" href="?query=tag:Data-Assimilation" style="background-color: rgb(4, 108, 116); color: white; font-weight: 700; border: none; border-radius: 1em; padding: 0.2em 0.5em;">#Data-Assimilation</a> 
 <br>In a Nutshell
With well-posedness, we refer to the existence of a unique solution that depends continuously on the parameters. Intuitively, this guarantees that small changes in the data (which is especially important given our stochastic setting) will not yield unbounded changes to the solution.
<br>In the following, we will therefore derive well-posedness of our <a href=".?query=tag:Data-Assimilation" class="tag" target="_blank" rel="noopener nofollow">#Data-Assimilation</a> distributions with regard to the data by showing continuous dependence.<br><br><br>Throughout all of the following proofs we will repeatedly use the fact that we can rewrite integrals weighted by the exponentials of the <a data-tooltip-position="top" aria-label="The Smoothing Problem" data-href="The Smoothing Problem" href="the-guide/computational-statistics/data-assimilation/smoothing-algorithms/the-smoothing-problem.html" class="internal-link" target="_self" rel="noopener nofollow">smoothing</a> <a data-tooltip-position="top" aria-label="Probability Density Function" data-href="Probability Density Function" href="the-guide/mathematics/probability-theory/probability-density-function.html" class="internal-link" target="_self" rel="noopener nofollow">pdfs</a>  and  as <a data-tooltip-position="top" aria-label="Expectations" data-href="Expectations" href="the-guide/mathematics/statistics/expectations.html" class="internal-link" target="_self" rel="noopener nofollow">expectations</a> with respect to their measures . Formally, we have Because we know the terms to be constants, we can insert the definitions of the pdfs back into the integrals, which yields e.g. for the priorNote that from the integrals it follows that  and . Intuitively, this is a result of the Gaussians spreading the distribution more and more. <br><br><br>In the following, we assume that we have two datasets  that are both contained in a euclidean <a data-tooltip-position="top" aria-label="Measure" data-href="Measure" href="the-guide/mathematics/measure-theory/measure.html" class="internal-link" target="_self" rel="noopener nofollow">ball</a> of radius  in the euclidean norm on  with  and we define with <a data-tooltip-position="top" aria-label="Data Assimilation" data-href="Data Assimilation" href="the-guide/computational-statistics/data-assimilation/data-assimilation.html" class="internal-link" target="_self" rel="noopener nofollow">observation operator</a> . <br>We further assume that , our expected observations based on the prior measure of the whole trajectory are bounded. <br> is any function that collects terms that do not depend on , but possibly on  and / or . Based on the definition of our <a data-tooltip-position="top" aria-label="The Smoothing Problem" data-href="The Smoothing Problem" href="the-guide/computational-statistics/data-assimilation/smoothing-algorithms/the-smoothing-problem.html" class="internal-link" target="_self" rel="noopener nofollow">smoothing setting</a>, we consider the model-data misfitand want to find a lower bound on  and  that depends on the data only through . We can boundwhere we used  <a data-tooltip-position="top" aria-label="Norms" data-href="Norms" href="the-guide/mathematics/linear-algebra/norms.html" class="internal-link" target="_self" rel="noopener nofollow">norm equivalence on finite dimensional spaces</a>,  triangle inequality and  the assumption that .<br>We first consider the ratio , since we know that  is independent of the data and we can thereby frame our problem to derive a lower bound of this ratio. Based on the bound above, we can writeWith the inequality above, we can assume any  sufficiently large, such that This enables us to insert <a data-tooltip-position="top" aria-label="Chebyshev or Markov Inequality" data-href="Chebyshev or Markov Inequality" href="the-guide/mathematics/probability-theory/chebyshev-or-markov-inequality.html" class="internal-link" target="_self" rel="noopener nofollow">the Markov Inequality</a> to obtain which has the desired property of only depending on  and  through .<br>Again, since <a data-tooltip-position="top" aria-label="Norms" data-href="Norms" href="the-guide/mathematics/linear-algebra/norms.html" class="internal-link" target="_self" rel="noopener nofollow">all norms are equivalent on finite dimensional vector spaces</a>, there is a constant , such that <br>Theorem 2.15 - Well-Posedness of Smoothing Problem
Consider the <a data-tooltip-position="top" aria-label="Data Assimilation" data-href="Data Assimilation" href="the-guide/computational-statistics/data-assimilation/data-assimilation.html" class="internal-link" target="_self" rel="noopener nofollow">smoothing problem</a> arising from <a data-tooltip-position="top" aria-label="Static and Dynamic Systems" data-href="Static and Dynamic Systems" href="the-guide/robotics,-dynamics-and-control/dynamics/static-and-dynamic-systems.html" class="internal-link" target="_self" rel="noopener nofollow">stochastic dynamics</a>. Consider two posterior <a data-tooltip-position="top" aria-label="Probability Distribution" data-href="Probability Distribution" href="the-guide/mathematics/probability-theory/probability-distribution.html" class="internal-link" target="_self" rel="noopener nofollow">probability distributions</a>  and  resulting from two distinct datasets  and . Assuming that the <a data-tooltip-position="top" aria-label="Expectations" data-href="Expectations" href="the-guide/mathematics/statistics/expectations.html" class="internal-link" target="_self" rel="noopener nofollow">expectation</a> is bounded , then there exists , such that for all  the <a data-tooltip-position="top" aria-label="Metrics on Spaces of Probability Measures" data-href="Metrics on Spaces of Probability Measures" href="the-guide/computational-statistics/data-assimilation/metrics-on-spaces-of-probability-measures.html" class="internal-link" target="_self" rel="noopener nofollow">Hellinger distance</a> fulfills The posterior measure is Lipschitz continuous in the <a data-tooltip-position="top" aria-label="Metrics on Spaces of Probability Measures" data-href="Metrics on Spaces of Probability Measures" href="the-guide/computational-statistics/data-assimilation/metrics-on-spaces-of-probability-measures.html" class="internal-link" target="_self" rel="noopener nofollow">Hellinger metric</a>.
<br>Follows mainly from educated extending etc. By inserting the two densities and the rewriting trick, we can derive Both terms can be bounded above by  using the ideas from above. <br><br>Defining we can use the results above for the <a data-tooltip-position="top" aria-label="The Smoothing Problem" data-href="The Smoothing Problem" href="the-guide/computational-statistics/data-assimilation/smoothing-algorithms/the-smoothing-problem.html" class="internal-link" target="_self" rel="noopener nofollow">deterministic dynamics</a> case with <a data-tooltip-position="top" aria-label="Bayes Theorem" data-href="Bayes Theorem" href="the-guide/mathematics/statistics/bayes-theorem.html" class="internal-link" target="_self" rel="noopener nofollow">posterior</a>  and posterior distributions with measures  for two different datasets .<br>Theorem 2.18 - Deterministic Dynamics
For the <a data-tooltip-position="top" aria-label="Data Assimilation" data-href="Data Assimilation" href="the-guide/computational-statistics/data-assimilation/data-assimilation.html" class="internal-link" target="_self" rel="noopener nofollow">deterministic dynamics case</a>, we can define Assuming that then , there is a , such that for all 
<br><br>Corollary 2.16 - Expectation Bound for Smoothing Problem
Assuming the smoothing problem arising from a stochastic dynamics model results in two posterior distributions  for two different datasets . Assuming <a data-tooltip-position="top" aria-label="Expectations" data-href="Expectations" href="the-guide/mathematics/statistics/expectations.html" class="internal-link" target="_self" rel="noopener nofollow">expectation</a>  and <a data-tooltip-position="top" aria-label="Covariance and Variance" data-href="Covariance and Variance" href="the-guide/mathematics/statistics/covariance-and-variance.html" class="internal-link" target="_self" rel="noopener nofollow">variance</a>  are bounded for , there is a , such that for all  it holds that 
<br>Proof

<br>Using the fact that  and , we can reformulate 
<br>For the left side of<a class="internal-link" data-href="Covariance and Variance.md" href="the-guide/mathematics/statistics/covariance-and-variance.html" target="_self" rel="noopener nofollow"></a> use the binomial equations via With the <a data-href="Cauchy-Schwartz Inequality" href="Cauchy-Schwartz Inequality" class="internal-link" target="_self" rel="noopener nofollow">Cauchy-Schwartz Inequality</a>, we obtain 
<br>...

<br><br><br>For filtering, the situation is more straight-forward, because we were able to derive the operatorsbased on the <a data-tooltip-position="top" aria-label="The Filtering Problem" data-href="The Filtering Problem" href="the-guide/computational-statistics/data-assimilation/filtering-algorithms/the-filtering-problem.html" class="internal-link" target="_self" rel="noopener nofollow">filtering formulation</a>. These are closed-form and thereby guarantee continuity. Note that while  is independent of  and linear based on <a data-tooltip-position="top" aria-label="Probabilistic View of Dynamical Systems" data-href="Probabilistic View of Dynamical Systems" href="the-guide/computational-statistics/data-assimilation/probabilistic-view-of-dynamical-systems.html" class="internal-link" target="_self" rel="noopener nofollow">probabilistic view of dynamical systems</a> while  depends on  because we see different data and generally non-linear-<br>Additionally, a corollary can be derived based on the <a data-tooltip-position="top" aria-label="Data Assimilation" data-href="Data Assimilation" href="the-guide/computational-statistics/data-assimilation/data-assimilation.html" class="internal-link" target="_self" rel="noopener nofollow">relationship between smoothing and filtering</a>.<br>Corollary 2.17 - Expectation Bound for Filtering Problem
Assuming the filtering problem at time  arising from a stochastic dynamics model results in two posterior distributions with measures  for two different datasets . Assuming <a data-tooltip-position="top" aria-label="Expectations" data-href="Expectations" href="the-guide/mathematics/statistics/expectations.html" class="internal-link" target="_self" rel="noopener nofollow">expectation</a>  and <a data-tooltip-position="top" aria-label="Covariance and Variance" data-href="Covariance and Variance" href="the-guide/mathematics/statistics/covariance-and-variance.html" class="internal-link" target="_self" rel="noopener nofollow">variance</a>  are bounded for , there is a , such that for all  it holds that where  are the partial datasets at time . Note that the measures with subscript  are the <a data-tooltip-position="top" aria-label="Marginal Distribution" data-href="Marginal Distribution" href="the-guide/mathematics/probability-theory/marginal-distribution.html" class="internal-link" target="_self" rel="noopener nofollow">marginals</a> of  and  up until . 
]]></description><link>the-guide/computational-statistics/data-assimilation/well-posedness-of-data-assimilation-problems.html</link><guid isPermaLink="false">The Guide/Computational Statistics/Data Assimilation/Well-Posedness of Data Assimilation Problems.md</guid><pubDate>Mon, 03 Mar 2025 15:30:25 GMT</pubDate></item><item><title><![CDATA[Approximate Bayesian Computation]]></title><description><![CDATA[ 
 <br><img alt="center" src="lib/media/pasted-image-20230916085109.png" style="width: 600px; max-width: 100%;">]]></description><link>the-guide/computational-statistics/approximate-bayesian-computation.html</link><guid isPermaLink="false">The Guide/Computational Statistics/Approximate Bayesian Computation.md</guid><pubDate>Fri, 20 Dec 2024 11:50:52 GMT</pubDate><enclosure url="lib/media/pasted-image-20230916085109.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src=&quot;lib/media/pasted-image-20230916085109.png&quot;&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[Gibbs Sampling]]></title><description><![CDATA[ 
 <br>In a Nutshell
<a data-href="Markov-Chain Monte Carlo Methods" href="the-guide/computational-statistics/data-assimilation/smoothing-algorithms/markov-chain-monte-carlo-methods.html" class="internal-link" target="_self" rel="noopener nofollow">Markov-Chain Monte Carlo Methods</a> algorithm for obtaining a sequence of observations approximated from a specified multivariate <a data-tooltip-position="top" aria-label="Probability Distribution" data-href="Probability Distribution" href="the-guide/mathematics/probability-theory/probability-distribution.html" class="internal-link" target="_self" rel="noopener nofollow">distribution</a> from which direct sampling is difficult. In its base version, it is a special case of the <a data-tooltip-position="top" aria-label="Markov-Chain Monte Carlo Methods" data-href="Markov-Chain Monte Carlo Methods" href="the-guide/computational-statistics/data-assimilation/smoothing-algorithms/markov-chain-monte-carlo-methods.html" class="internal-link" target="_self" rel="noopener nofollow">Metropolis-Hastings algorithm</a>. The motivation is that given a multivariate distribution is is simpler to sample from a conditional than to marginalize by integrating over a <a data-tooltip-position="top" aria-label="Probability Distribution" data-href="Probability Distribution" href="the-guide/mathematics/probability-theory/probability-distribution.html" class="internal-link" target="_self" rel="noopener nofollow">joint distribution</a>.
<br><br>Algorithm
We want  samples of  from .

<br>Begin with initial sample 
<br>Sample  by cycling through the components in order, conditioned on the others. If the component was already resampled, condition on the new sample leading to 
<br>Repeat  times

<br>Additional Information
Categories as a random algorithm, opposed to deterministic algorithms for statistical inferenec, e.g. <a data-tooltip-position="top" aria-label="Expectation Maximization Algorithm" data-href="Expectation Maximization Algorithm" href="the-guide/machine-learning/generative-models/expectation-maximization-algorithm.html" class="internal-link" target="_self" rel="noopener nofollow">EM</a>.
<br><br><br>Assume observations  generated by a sampling distribution  and a prior  on the parameter space . Define <br>
<br>, where  is the collection of all components of the parameter space except .<br>
For each of the  components, the <a data-tooltip-position="top" aria-label="Bayes Theorem" data-href="Bayes Theorem" href="the-guide/mathematics/statistics/bayes-theorem.html" class="internal-link" target="_self" rel="noopener nofollow">posterior</a> <a data-tooltip-position="top" aria-label="Mutual Information" data-href="Mutual Information" href="the-guide/information-theory/information-theory-1/information/mutual-information.html" class="internal-link" target="_self" rel="noopener nofollow">mutual information</a> fulfills This mutual information is the amount of information that is passed from the -th to the -th step in a cycle. It vanishes, if the parameters are marginally independent. 
<br><br><br>
<br><a rel="noopener nofollow" class="external-link" href="https://en.wikipedia.org/wiki/Gibbs_sampling" target="_blank">https://en.wikipedia.org/wiki/Gibbs_sampling</a>
]]></description><link>the-guide/computational-statistics/gibbs-sampling.html</link><guid isPermaLink="false">The Guide/Computational Statistics/Gibbs Sampling.md</guid><pubDate>Wed, 23 Apr 2025 09:53:52 GMT</pubDate></item><item><title><![CDATA[Hamiltonian Monte Carlo]]></title><description><![CDATA[ 
 <br>In a Nutshell
Variant of <a data-tooltip-position="top" aria-label="Markov-Chain Monte Carlo Methods" data-href="Markov-Chain Monte Carlo Methods" href="the-guide/computational-statistics/data-assimilation/smoothing-algorithms/markov-chain-monte-carlo-methods.html" class="internal-link" target="_self" rel="noopener nofollow">Metropolis-Hastings</a> <a data-tooltip-position="top" aria-label="Markov-Chain Monte Carlo Methods" data-href="Markov-Chain Monte Carlo Methods" href="the-guide/computational-statistics/data-assimilation/smoothing-algorithms/markov-chain-monte-carlo-methods.html" class="internal-link" target="_self" rel="noopener nofollow">MCMC</a> algorithm that exploits the geometry of the <a data-tooltip-position="top" aria-label="Typical Sets" data-href="Typical Sets" href="the-guide/information-theory/information-theory-1/data-compression/typical-sets.html" class="internal-link" target="_self" rel="noopener nofollow">typical set</a> of a parameter space  by letting the particle drift along a trajectory enforced by a vector field. Because the simple gradient of the target <a data-tooltip-position="top" aria-label="Probability Distribution" data-href="Probability Distribution" href="the-guide/mathematics/probability-theory/probability-distribution.html" class="internal-link" target="_self" rel="noopener nofollow">distribution</a>  will simply drift towards the highest densities (modes), we twist this behavior towards exploring the typical set by auxiliary momentum parameters  and use <a data-tooltip-position="top" aria-label="Hamiltonian Mechanics" data-href="Hamiltonian Mechanics" href="the-guide/robotics,-dynamics-and-control/dynamics/hamiltonian-mechanics.html" class="internal-link" target="_self" rel="noopener nofollow">Hamiltonian mechanics</a> to compute the desired vector field. 
<br>Generally, there are 3 steps:<br>
<br>Use current position  and sample momentum , usually from a <a data-tooltip-position="top" aria-label="Gaussian Distribution" data-href="Gaussian Distribution" href="the-guide/mathematics/probability-theory/gaussian-distribution.html" class="internal-link" target="_self" rel="noopener nofollow">Gaussian</a>
<br>Compute trajectory in phase space for particle via (symplectic) integration of Hamilton's equations over fixed horizon
<br>Arrive at , accept new location according to a computed <a data-tooltip-position="top" aria-label="Probability Distribution" data-href="Probability Distribution" href="the-guide/mathematics/probability-theory/probability-distribution.html" class="internal-link" target="_self" rel="noopener nofollow">probability</a><br>
<img alt="center" src="lib/media/pasted-image-20231101144152.png" style="width: 250px; max-width: 100%;">
<br><br><br>
<br>Introduce auxiliary momentum variable  and draw from a <a data-tooltip-position="top" aria-label="Probability Distribution" data-href="Probability Distribution" href="the-guide/mathematics/probability-theory/probability-distribution.html" class="internal-link" target="_self" rel="noopener nofollow">joint density</a> In most cases, a multivariate <a data-tooltip-position="top" aria-label="Gaussian Distribution" data-href="Gaussian Distribution" href="the-guide/mathematics/probability-theory/gaussian-distribution.html" class="internal-link" target="_self" rel="noopener nofollow">normal</a> that is independent of  is used 
<br>This leads to the Hamiltonian composed of kinetic and potential energyyielding the equations where the second equation results from the momentum density being independent of the target density.
<br><br><br><br>
<br>By design, the complementary momentum dimensions have to be dual to the target parameters (invariant phase space volumes). Constructing a <a data-tooltip-position="top" aria-label="Probability Distribution" data-href="Probability Distribution" href="the-guide/mathematics/probability-theory/probability-distribution.html" class="internal-link" target="_self" rel="noopener nofollow">distribution</a> on phase space that marginalizes to the target distribution ensures that the typical set of the phase space is projected onto the typical set of the target

<br>Statistical mechanics, probability of energy levels


<br>Performance of exploration is determined by how quickly the relevant (typical) energy levels are explored,  has to be similar to : <img alt="center" src="lib/media/pasted-image-20231101150223.png" style="width: 500px; max-width: 100%;">
<br>Empirically, <a data-tooltip-position="top" aria-label="Gaussian Distribution" data-href="Gaussian Distribution" href="the-guide/mathematics/probability-theory/gaussian-distribution.html" class="internal-link" target="_self" rel="noopener nofollow">Gaussian</a> kinetic energies perform well (Central Limit Theorem ?), need to adapt <a data-tooltip-position="top" aria-label="Covariance and Variance" data-href="Covariance and Variance" href="the-guide/mathematics/statistics/covariance-and-variance.html" class="internal-link" target="_self" rel="noopener nofollow">covariance</a> matrix in order to adapt to structure of the target distribution, best case would be uniform level sets to explore all energies. Both of the strategies alter how the kinetic energeis are computed !

<br>Euclidean-Gaussian Kinetic Energies

<br>Given a metric in parameter space we can modify this via rotations and scalings to a family of metrics This implies an inverse structure on momentum spaceresulting in the distirbution 
<br>Global transformation will only result in uniform level sets if target is gaussian.


<br>Riemannian-Gaussian Kinetic Energies

<br>Construct a state-dependent <a data-tooltip-position="top" aria-label="Covariance and Variance" data-href="Covariance and Variance" href="the-guide/mathematics/statistics/covariance-and-variance.html" class="internal-link" target="_self" rel="noopener nofollow">covariance</a> <a data-tooltip-position="top" aria-label="Matrices" data-href="Matrices" href="the-guide/mathematics/linear-algebra/matrices.html" class="internal-link" target="_self" rel="noopener nofollow">matrix</a> to obtain Allows for local corrections. The resulting level sets are uniform, if the metric resembles the Hessian of the target distirbution.




<br><br>
<br>First transition through orbit is more informative than revisiting states close to previously explored ones, longer integration times yield diminishing returns
<br>General distributions need dynamically adjusted integration times
<br>Heuristic No-U-Turn termination criterion as in <a data-tooltip-position="top" aria-label="No-U-Turn Sampler (NUTS)" data-href="No-U-Turn Sampler (NUTS)" href="the-guide/computational-statistics/no-u-turn-sampler-(nuts).html" class="internal-link" target="_self" rel="noopener nofollow">NUTS</a>
<br><br><br>
<br>Symplectic integrators by design preserve phase space volumes and avoid drifting 
<br>E.g. Leapfrog 

<br>
<br>for  do

<br>
<br>
<br>


<br>end for


<br>Can still diverge in areas of high curvature, but this is easily detectable as this will lead to divergence of energies 
<br>Treat last point of each trajectory as proposal in a <a data-tooltip-position="top" aria-label="Markov-Chain Monte Carlo Methods" data-href="Markov-Chain Monte Carlo Methods" href="the-guide/computational-statistics/data-assimilation/smoothing-algorithms/markov-chain-monte-carlo-methods.html" class="internal-link" target="_self" rel="noopener nofollow">Metropolis-Hastings</a> context.

<br>Need momentum flip to reach non-zero acceptance probabilities, as each momentum restricts direction to move<img alt="center" src="lib/media/pasted-image-20231101164303.png" style="width: 350px; max-width: 100%;">
<br>Acceptance probability 

<br>If integrator exact, acceptance would be guaranteed, difference in energy levels is only due to drift / divergence / numerical errors




<br>For performance, use each point in trajectory instead of endpoints. Because checking acceptance independently can lead to not checking good states at all, roll out trajectory  and then sample a point from it with probability 
<br>Optimal Choice of Symplectic Integrator

<br>Find optimal stepsize  and order  (Runge-Kutta, Euler, ...), need to find balance


<br><br><br>
<br><a rel="noopener nofollow" class="external-link" href="https://www.youtube.com/watch?v=a-wydhEuAm0&amp;t=1257s" target="_blank">https://www.youtube.com/watch?v=a-wydhEuAm0&amp;t=1257s</a>
]]></description><link>the-guide/computational-statistics/hamiltonian-monte-carlo.html</link><guid isPermaLink="false">The Guide/Computational Statistics/Hamiltonian Monte Carlo.md</guid><pubDate>Fri, 20 Dec 2024 11:50:52 GMT</pubDate><enclosure url="lib/media/pasted-image-20231101144152.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src=&quot;lib/media/pasted-image-20231101144152.png&quot;&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[Inverse Transform Sampling]]></title><description><![CDATA[ 
 <br>Also called <a data-tooltip-position="top" aria-label="Cumulative Distribution Function" data-href="Cumulative Distribution Function" href="the-guide/mathematics/probability-theory/cumulative-distribution-function.html" class="internal-link" target="_self" rel="noopener nofollow">cdf</a> sampling.<br><br>Info
Concept to generate samples from a discrete <a data-tooltip-position="top" aria-label="Probability Distribution" data-href="Probability Distribution" href="the-guide/mathematics/probability-theory/probability-distribution.html" class="internal-link" target="_self" rel="noopener nofollow">distribution</a> by using its <a data-tooltip-position="top" aria-label="Cumulative Distribution Function" data-href="Cumulative Distribution Function" href="the-guide/mathematics/probability-theory/cumulative-distribution-function.html" class="internal-link" target="_self" rel="noopener nofollow">cdf</a> . Based on this, we generate uniform  samples in the range  and take the sample whose bar intersects the horizontal line that results from 
<br><img alt="center" src="lib/media/pasted-image-20240301202028.png" style="width: 300px; max-width: 100%;">]]></description><link>the-guide/computational-statistics/inverse-transform-sampling.html</link><guid isPermaLink="false">The Guide/Computational Statistics/Inverse Transform Sampling.md</guid><pubDate>Mon, 09 Sep 2024 15:37:42 GMT</pubDate><enclosure url="lib/media/pasted-image-20240301202028.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src=&quot;lib/media/pasted-image-20240301202028.png&quot;&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[Kernel Density Estimation]]></title><description><![CDATA[ 
 <br>Non-parametric method to estimate the <a data-tooltip-position="top" aria-label="Probability Density Function" data-href="Probability Density Function" href="the-guide/mathematics/probability-theory/probability-density-function.html" class="internal-link" target="_self" rel="noopener nofollow">pdf</a> of a <a data-tooltip-position="top" aria-label="Random Variable" data-href="Random Variable" href="the-guide/mathematics/probability-theory/random-variable.html" class="internal-link" target="_self" rel="noopener nofollow">random variable</a> based on <a data-tooltip-position="top" aria-label="Statistical Independence" data-href="Statistical Independence" href="the-guide/mathematics/statistics/statistical-independence.html" class="internal-link" target="_self" rel="noopener nofollow">i.i.d.</a> samples . The kernel <a data-tooltip-position="top" aria-label="Estimator" data-href="Estimator" href="the-guide/mathematics/statistics/estimator.html" class="internal-link" target="_self" rel="noopener nofollow">estimator</a> is <br>
<br>One wants do choose smoothing parameter  as small as data allows
]]></description><link>the-guide/computational-statistics/kernel-density-estimation.html</link><guid isPermaLink="false">The Guide/Computational Statistics/Kernel Density Estimation.md</guid><pubDate>Mon, 09 Sep 2024 15:33:49 GMT</pubDate></item><item><title><![CDATA[Metropolis-Adjusted Langevin Algorithm]]></title><description><![CDATA[ 
 <br>In a Nutshell
Class of <a data-tooltip-position="top" aria-label="Markov-Chain Monte Carlo Methods" data-href="Markov-Chain Monte Carlo Methods" href="the-guide/computational-statistics/data-assimilation/smoothing-algorithms/markov-chain-monte-carlo-methods.html" class="internal-link" target="_self" rel="noopener nofollow">Markov-Chain Monte Carlo</a> algorithms to obtain random samples from a <a data-tooltip-position="top" aria-label="Probability Distribution" data-href="Probability Distribution" href="the-guide/mathematics/probability-theory/probability-distribution.html" class="internal-link" target="_self" rel="noopener nofollow">distribution</a>, where direct sampling is difficult. The basic idea is to simulate the <a data-tooltip-position="top" aria-label="Langevin Dynamics" data-href="Langevin Dynamics" href="the-guide/robotics,-dynamics-and-control/dynamics/langevin-dynamics.html" class="internal-link" target="_self" rel="noopener nofollow">Langevin equation</a> with the steady state distribution according to the desired one. Suitable for cases, where the full <a data-tooltip-position="top" aria-label="Derivative, Gradient, Jacobian and Hessian > The Gradient" data-href="Derivative, Gradient, Jacobian and Hessian#The Gradient" href="the-guide/mathematics/analysis-and-calculus/derivative,-gradient,-jacobian-and-hessian.html#The_Gradient" class="internal-link" target="_self" rel="noopener nofollow">gradient</a> is feasible to compute, otherwise consider <a data-tooltip-position="top" aria-label="Stochastic Gradient Langevin Dynamics" data-href="Stochastic Gradient Langevin Dynamics" href="the-guide/computational-statistics/stochastic-gradient-langevin-dynamics.html" class="internal-link" target="_self" rel="noopener nofollow">stochastic gradient Langevin dynamics</a>.
<br>Metropolis-Adjusted Langevin Algorithm (MALA) uses two mechanisms to generate a random walk with the target distribution  as its <a data-tooltip-position="top" aria-label="Markov Chain" data-href="Markov Chain" href="the-guide/computational-statistics/data-assimilation/markov-chain.html" class="internal-link" target="_self" rel="noopener nofollow">stationary distribution</a> :<br>
<br>New states are proposed using (overdamped) <a data-tooltip-position="top" aria-label="Langevin Dynamics" data-href="Langevin Dynamics" href="the-guide/robotics,-dynamics-and-control/dynamics/langevin-dynamics.html" class="internal-link" target="_self" rel="noopener nofollow">Langevin dynamics</a> (gradient-based)
<br>States are treated as proposals in a <a data-tooltip-position="top" aria-label="Markov-Chain Monte Carlo Methods" data-href="Markov-Chain Monte Carlo Methods" href="the-guide/computational-statistics/data-assimilation/smoothing-algorithms/markov-chain-monte-carlo-methods.html" class="internal-link" target="_self" rel="noopener nofollow">Metropolis-Hastings</a> context
<br><br>The Langevin Equation with particle position  in a potential  under the influence of noise . This can be written as an (overdamped) <a data-href="Itô Diffusion" href="the-guide/mathematics/probability-theory/itô-diffusion.html" class="internal-link" target="_self" rel="noopener nofollow">Itô Diffusion</a> in form of an <a data-tooltip-position="top" aria-label="Stochastic Differential Equation" data-href="Stochastic Differential Equation" href="the-guide/mathematics/probability-theory/stochastic-differential-equation.html" class="internal-link" target="_self" rel="noopener nofollow">SDE</a> using <a data-tooltip-position="top" aria-label="Wiener Process" data-href="Wiener Process" href="the-guide/mathematics/probability-theory/wiener-process.html" class="internal-link" target="_self" rel="noopener nofollow">Brownian motion</a>  as noise. For this equation, the <a data-tooltip-position="top" aria-label="Markov Chain" data-href="Markov Chain" href="the-guide/computational-statistics/data-assimilation/markov-chain.html" class="internal-link" target="_self" rel="noopener nofollow">stationary distribution</a> does not change along the trajectory, which is why we can choose  in such a way that the process generates samples from a desired distribution .<br><br>The <a data-href="Fokker-Planck Equation" href="the-guide/mathematics/probability-theory/fokker-planck-equation.html" class="internal-link" target="_self" rel="noopener nofollow">Fokker-Planck Equation</a> for the SDE above yields We want a steady-state solution, denoted  and therefore Plugging this into the equation above for  yields the new equation which means the flux  must be constant. Since there is a boundary condition of  vanishing at infinity (see <a data-href="Fokker-Planck Equation" href="the-guide/mathematics/probability-theory/fokker-planck-equation.html" class="internal-link" target="_self" rel="noopener nofollow">Fokker-Planck Equation</a>), it must be zero everywhere, yielding the condition which is called a <a data-tooltip-position="top" aria-label="Boltzmann Distribution" data-href="Boltzmann Distribution" href="Boltzmann Distribution" class="internal-link" target="_self" rel="noopener nofollow">Gibbs / Boltzmann distribution</a>. We can now sample from energy-based models of the form  by setting the potential to be . Writing , we arrive at the potential because in the end, the normalization constant  will vanish when computing the gradient for the SDE.<br><br><br>This leads us to the objective to simulate the SDE in order to generate samples from its <a data-tooltip-position="top" aria-label="Markov Chain" data-href="Markov Chain" href="the-guide/computational-statistics/data-assimilation/markov-chain.html" class="internal-link" target="_self" rel="noopener nofollow">steady-state distribution</a> . An approximate solution can be computed by e.g. employing the <a data-tooltip-position="top" aria-label="Euler-Maruyama Method" data-href="Euler-Maruyama Method" href="the-guide/mathematics/probability-theory/euler-maruyama-method.html" class="internal-link" target="_self" rel="noopener nofollow">Euler-Maruyama method</a> with a fixed time step . This results in the update which already provides an algorithm to sample from the posterior, the unadjusted Langevin Algorithm (ULA).<br><br>Additionally, MALA introduces an acceptance / rejection step according to the <a data-href="Markov-Chain Monte Carlo Methods" href="the-guide/computational-statistics/data-assimilation/smoothing-algorithms/markov-chain-monte-carlo-methods.html" class="internal-link" target="_self" rel="noopener nofollow">Markov-Chain Monte Carlo Methods</a> in order to guarantee <a data-tooltip-position="top" aria-label="Detailed Balance Condition" data-href="Detailed Balance Condition" href="the-guide/computational-statistics/data-assimilation/detailed-balance-condition.html" class="internal-link" target="_self" rel="noopener nofollow">detailed balance</a>. <br>Update and Proposal Mechanism
The above update is initially treated as a proposal for a new state. Then, a point  is drawn from the uniform distribution on  and the sample is accepted if  withwhere If the sample is rejected, we set .
<br><br><br>To sample from the <a data-tooltip-position="top" aria-label="Bayes Theorem" data-href="Bayes Theorem" href="the-guide/mathematics/statistics/bayes-theorem.html" class="internal-link" target="_self" rel="noopener nofollow">posterior</a> <a data-tooltip-position="top" aria-label="Probability Distribution" data-href="Probability Distribution" href="the-guide/mathematics/probability-theory/probability-distribution.html" class="internal-link" target="_self" rel="noopener nofollow">distribution</a> , we can use the fact that Given observations  and the particles , using the gradient of these distributions in the SDE above yields particles that are distributed as the desired posterior. Note that in this context  is usually the dataset, while the particles are .<br><br><br>
<br><a rel="noopener nofollow" class="external-link" href="https://en.wikipedia.org/wiki/Metropolis-adjusted_Langevin_algorithm" target="_blank">https://en.wikipedia.org/wiki/Metropolis-adjusted_Langevin_algorithm</a>
<br><a rel="noopener nofollow" class="external-link" href="https://abdulfatir.com/blog/2020/Langevin-Monte-Carlo/" target="_blank">https://abdulfatir.com/blog/2020/Langevin-Monte-Carlo/</a>
]]></description><link>the-guide/computational-statistics/metropolis-adjusted-langevin-algorithm.html</link><guid isPermaLink="false">The Guide/Computational Statistics/Metropolis-Adjusted Langevin Algorithm.md</guid><pubDate>Thu, 17 Apr 2025 16:23:34 GMT</pubDate></item><item><title><![CDATA[Multidimensional Scaling]]></title><description><![CDATA[ 
 <br>Statistical methods that given pairwise dissimilarities, construct a <a data-tooltip-position="top" aria-label="Mathematical Relations" data-href="Mathematical Relations" href="the-guide/mathematics/general-stuff/mathematical-relations.html" class="internal-link" target="_self" rel="noopener nofollow">map</a> that preserves distances. The dissimilarity does not need to be a <a data-tooltip-position="top" aria-label="Metric Space and Completeness" data-href="Metric Space and Completeness" href="the-guide/mathematics/general-stuff/metric-space-and-completeness.html" class="internal-link" target="_self" rel="noopener nofollow">metric</a>, but the reconstructed map will be endowed with the natural distance .<img alt="center" src="lib/media/pasted-image-20231130100309.png"><br><br>
<br>Classical MDS

<br>...


<br>Metric MDS

<br>Given dissimiliarities in a <a data-tooltip-position="top" aria-label="Matrices" data-href="Matrices" href="the-guide/mathematics/linear-algebra/matrices.html" class="internal-link" target="_self" rel="noopener nofollow">matrix</a>  with entry , representing a set of  samples in an unknown <a data-tooltip-position="top" aria-label="Space" data-href="Space" href="the-guide/mathematics/general-stuff/space.html" class="internal-link" target="_self" rel="noopener nofollow">space</a> , find coordinates  that preserves the dissimiliarities by solving for a given weight <a data-tooltip-position="top" aria-label="Matrices" data-href="Matrices" href="the-guide/mathematics/linear-algebra/matrices.html" class="internal-link" target="_self" rel="noopener nofollow">matrix</a> .

<br>Intuition - minimize weighted deviation of dissimiliarity and <a data-tooltip-position="top" aria-label="Metric Space and Completeness" data-href="Metric Space and Completeness" href="the-guide/mathematics/general-stuff/metric-space-and-completeness.html" class="internal-link" target="_self" rel="noopener nofollow">distance</a> in new <a data-tooltip-position="top" aria-label="Space" data-href="Space" href="the-guide/mathematics/general-stuff/space.html" class="internal-link" target="_self" rel="noopener nofollow">space</a>




<br>Non-Metric MDS

<br>...


]]></description><link>the-guide/computational-statistics/multidimensional-scaling.html</link><guid isPermaLink="false">The Guide/Computational Statistics/Multidimensional Scaling.md</guid><pubDate>Mon, 24 Feb 2025 23:49:44 GMT</pubDate><enclosure url="lib/media/pasted-image-20231130100309.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src=&quot;lib/media/pasted-image-20231130100309.png&quot;&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[No-U-Turn Sampler (NUTS)]]></title><description><![CDATA[ 
 <br>Dynamic implementation of <a data-href="Hamiltonian Monte Carlo" href="the-guide/computational-statistics/hamiltonian-monte-carlo.html" class="internal-link" target="_self" rel="noopener nofollow">Hamiltonian Monte Carlo</a> that uses No-U-Turn heuristic for integration time in combination with a multiplicative expansion of each trajectory.<br><br>Problem in <a data-tooltip-position="top" aria-label="Hamiltonian Monte Carlo" data-href="Hamiltonian Monte Carlo" href="the-guide/computational-statistics/hamiltonian-monte-carlo.html" class="internal-link" target="_self" rel="noopener nofollow">HMC</a><br>
<br>Initial exploration when orbiting the energy level set for the first time yields new and informative state at each step
<br>This has highly diminishing returns once we complete first orbit and return to neighborhoods we already explored
<br><br><br>Algorithm needs time-reversibility to guarantee convergence to the correct <a data-tooltip-position="top" aria-label="Probability Distribution" data-href="Probability Distribution" href="the-guide/mathematics/probability-theory/probability-distribution.html" class="internal-link" target="_self" rel="noopener nofollow">distribution</a>.<br><br><br>Consider termination criterion for each trajectory  that stops as soon as the particle starts to return towards the last sample (makes a u-turn). For this, sample forwards and backwards in time <img alt="center" src="lib/media/pasted-image-20231108110623.png" style="width: 500px; max-width: 100%;"><br>
<img alt="center" src="lib/media/pasted-image-20231108110246.png" style="width: 400px; max-width: 100%;"> In euclidean space, the criteria are <br>
<br><a data-tooltip-position="top" aria-label="Vector Space" data-href="Vector Space" href="the-guide/mathematics/general-stuff/vector-space.html" class="internal-link" target="_self" rel="noopener nofollow">Inner product</a> between difference <a data-tooltip-position="top" aria-label="Vector Space" data-href="Vector Space" href="the-guide/mathematics/general-stuff/vector-space.html" class="internal-link" target="_self" rel="noopener nofollow">vector</a> of trajectories and momentum, will be only positive if momentum starts to point "inwards", once particle returns in parameter space.<br>
- Above criterion is only valid for <a data-tooltip-position="top" aria-label="Hamiltonian Monte Carlo" data-href="Hamiltonian Monte Carlo" href="the-guide/computational-statistics/hamiltonian-monte-carlo.html" class="internal-link" target="_self" rel="noopener nofollow">Euclidean manifolds</a>, but can be generalized using the fact that in general which lets us define 
<br>In practice, we can approximate  via and arrive at the general termination criteria
]]></description><link>the-guide/computational-statistics/no-u-turn-sampler-(nuts).html</link><guid isPermaLink="false">The Guide/Computational Statistics/No-U-Turn Sampler (NUTS).md</guid><pubDate>Mon, 24 Feb 2025 23:32:25 GMT</pubDate><enclosure url="lib/media/pasted-image-20231108110623.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src=&quot;lib/media/pasted-image-20231108110623.png&quot;&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[Score Matching]]></title><description><![CDATA[ 
 <br>In a Nutshell
Alternative approach to e.g. <a data-tooltip-position="top" aria-label="Maximum Likelihood Estimator" data-href="Maximum Likelihood Estimator" href="the-guide/mathematics/statistics/maximum-likelihood-estimator.html" class="internal-link" target="_self" rel="noopener nofollow">maximum likelihood</a> for <a data-tooltip-position="top" aria-label="Estimator" data-href="Estimator" href="the-guide/mathematics/statistics/estimator.html" class="internal-link" target="_self" rel="noopener nofollow">estimating</a> parameters of a parameterized <a data-tooltip-position="top" aria-label="Probability Distribution" data-href="Probability Distribution" href="the-guide/mathematics/probability-theory/probability-distribution.html" class="internal-link" target="_self" rel="noopener nofollow">probability distribution</a> up to a constant of proportionality.
<br><br><br>We observed a <a data-tooltip-position="top" aria-label="Set" data-href="Set" href="the-guide/mathematics/general-stuff/set.html" class="internal-link" target="_self" rel="noopener nofollow">set</a> of  data samples  whose underlying distribution  over the <a data-tooltip-position="top" aria-label="Space" data-href="Space" href="the-guide/mathematics/general-stuff/space.html" class="internal-link" target="_self" rel="noopener nofollow">space</a>  is unknown. Therefore, we want to model it via another parameterized distribution . In the usual <a data-tooltip-position="top" aria-label="Maximum Likelihood Estimator" data-href="Maximum Likelihood Estimator" href="the-guide/mathematics/statistics/maximum-likelihood-estimator.html" class="internal-link" target="_self" rel="noopener nofollow">maximum likelihood</a> sense, we'd have to perform the <a data-tooltip-position="top" aria-label="Optimization Problem" data-href="Optimization Problem" href="the-guide/mathematics/optimization/convex-optimization-lecture/optimization-problem.html" class="internal-link" target="_self" rel="noopener nofollow">optimization</a>In theory, this would require computing the normalization constant , where  is the un-normalized distribution. <br><br><br>The idea of score matching is to find a , such that the <a data-tooltip-position="top" aria-label="Derivative, Gradient, Jacobian and Hessian > The Gradient" data-href="Derivative, Gradient, Jacobian and Hessian#The Gradient" href="the-guide/mathematics/analysis-and-calculus/derivative,-gradient,-jacobian-and-hessian.html#The_Gradient" class="internal-link" target="_self" rel="noopener nofollow">gradient</a> of the models <a data-tooltip-position="top" aria-label="Likelihood Function" data-href="Likelihood Function" href="the-guide/mathematics/statistics/likelihood-function.html" class="internal-link" target="_self" rel="noopener nofollow">log-likelihood</a> is close to the one of the data distribution. Importantly, in contrast to the usual <a data-tooltip-position="top" aria-label="Likelihood Function" data-href="Likelihood Function" href="the-guide/mathematics/statistics/likelihood-function.html" class="internal-link" target="_self" rel="noopener nofollow">score function</a> that is the gradient of the log-likelihood w.r.t. the parameters, score matching instead considers the gradient of the log-likelihood w.r.t. the dataTo avoid confusion, this is sometimes referred to as the Stein score. The motivation follows from the equality where the last term vanishes. The initial aim of score matching is to minimize the Fisher <a data-tooltip-position="top" aria-label="Divergence" data-href="Divergence" href="the-guide/information-theory/information-geometry/divergence.html" class="internal-link" target="_self" rel="noopener nofollow">divergence</a>  between the Stein score and the unknown distribution<br>Intuition
Instead of working directly on the data distribution, we extract information via the above formula. This tells us the gradient of the distribution for given values of the parameters. We thereby extract information about the first-order structure.
<br><br><br>The objective still requires the unknown distribution , but we can reformulate the objective to make it tractable. Writing out the norm in the objective above yieldsthe first term does not depend on  and the last one is already tractable. The only difficulty results from the mixed term. Writing out the <a data-tooltip-position="top" aria-label="Expectations" data-href="Expectations" href="the-guide/mathematics/statistics/expectations.html" class="internal-link" target="_self" rel="noopener nofollow">expectation</a> only for this term, we can apply the <a data-tooltip-position="top" aria-label="Logarithm and Exponential" data-href="Logarithm and Exponential" href="the-guide/mathematics/general-stuff/logarithm-and-exponential.html" class="internal-link" target="_self" rel="noopener nofollow">log-ratio trick</a> to the red term involving  to obtain<br>Assumption
The following requires an additional assumption that is mostly attained in  or . As a regularity condition, we require that for any , it holds that 
<br>We then use <a data-tooltip-position="top" aria-label="Logarithm and Exponential > Integration" data-href="Logarithm and Exponential#Integration" href="the-guide/mathematics/general-stuff/logarithm-and-exponential.html#Integration" class="internal-link" target="_self" rel="noopener nofollow">integration by parts</a> / <a data-tooltip-position="top" aria-label="Green's Identities" data-href="Green's Identities" href="the-guide/mathematics/analysis-and-calculus/green's-identities.html" class="internal-link" target="_self" rel="noopener nofollow">Green's first identity</a> with  and , obtainingwhere  denotes the <a data-tooltip-position="top" aria-label="Derivative, Gradient, Jacobian and Hessian > The Hessian" data-href="Derivative, Gradient, Jacobian and Hessian#The Hessian" href="the-guide/mathematics/analysis-and-calculus/derivative,-gradient,-jacobian-and-hessian.html#The_Hessian" class="internal-link" target="_self" rel="noopener nofollow">Hessian</a>, whose <a data-tooltip-position="top" aria-label="Matrices" data-href="Matrices" href="the-guide/mathematics/linear-algebra/matrices.html" class="internal-link" target="_self" rel="noopener nofollow">trace</a> equals the <a data-href="Laplacian" href="Laplacian" class="internal-link" target="_self" rel="noopener nofollow">Laplacian</a> for euclidean geometry. Both boundary terms vanish based on the given assumption. We can now again write this term as an <a data-tooltip-position="top" aria-label="Expectations" data-href="Expectations" href="the-guide/mathematics/statistics/expectations.html" class="internal-link" target="_self" rel="noopener nofollow">expectation</a> w.r.t. . We have thereby eliminated all terms of the unknown distribution and only require samples from it. <br>Tractable Loss Function 
The final objective isor when approximated via <a data-tooltip-position="top" aria-label="Monte Carlo Integration" data-href="Monte Carlo Integration" href="the-guide/mathematics/probability-theory/monte-carlo-integration.html" class="internal-link" target="_self" rel="noopener nofollow">Monte Carlo integration</a>
<br>Intuition
The second term is small, if the current  yields a distribution that explains the data well, because small perturbations do not change the gradient much (first order information). The first term can be understood as a sharpness, because tend to want a sharper minimum instead of a flat one, where surrounding points in parameter space explain the data almost equally well.  
<br>Example<br>
<br>For a simple <a data-tooltip-position="top" aria-label="Gaussian Distribution" data-href="Gaussian Distribution" href="the-guide/mathematics/probability-theory/gaussian-distribution.html" class="internal-link" target="_self" rel="noopener nofollow">Gaussian</a> , the objective is , which can be solved analytically. In this simple case, we get the usual <a data-tooltip-position="top" aria-label="Maximum Likelihood Estimator" data-href="Maximum Likelihood Estimator" href="the-guide/mathematics/statistics/maximum-likelihood-estimator.html" class="internal-link" target="_self" rel="noopener nofollow">maximum likelihood</a> estimators of empirical mean and variance.
<br><br><br>
<br><a rel="noopener nofollow" class="external-link" href="https://andrewcharlesjones.github.io/journal/21-score-matching.html" target="_blank">https://andrewcharlesjones.github.io/journal/21-score-matching.html</a>, very messy notaiton
]]></description><link>the-guide/computational-statistics/score-matching.html</link><guid isPermaLink="false">The Guide/Computational Statistics/Score Matching.md</guid><pubDate>Sat, 25 Jan 2025 17:52:28 GMT</pubDate></item><item><title><![CDATA[Simulation-Based Inference]]></title><description><![CDATA[ 
 <br>Info
We want to perform inference on a model's parameters  given observations  to obtain In this setting, we don't know the likelihood function , which is required to compute the <a data-tooltip-position="top" aria-label="Probability Distribution" data-href="Probability Distribution" href="the-guide/mathematics/probability-theory/probability-distribution.html" class="internal-link" target="_self" rel="noopener nofollow">probability</a> using the usual approach via <a data-href="Bayes Theorem" href="the-guide/mathematics/statistics/bayes-theorem.html" class="internal-link" target="_self" rel="noopener nofollow">Bayes Theorem</a>. Instead, we want to use a simulation to generate outputs
<br>In the setting of <a data-href="Approximate Bayesian Computation" href="the-guide/computational-statistics/approximate-bayesian-computation.html" class="internal-link" target="_self" rel="noopener nofollow">Approximate Bayesian Computation</a>, we would measure frequency of "good" results given parameters. If the simulation is very expensive, e.g. with high dimensional data or complex models, we turn to Likelihood-Free Inference (LFI), where we fit a model based on a few runs of the simulation, maximizing the objective Since we want the result to be a <a data-tooltip-position="top" aria-label="Probability Distribution" data-href="Probability Distribution" href="the-guide/mathematics/probability-theory/probability-distribution.html" class="internal-link" target="_self" rel="noopener nofollow">distribution</a>, this leads to density estimation-based LFI (DELFI), e.g. by using a <a data-tooltip-position="top" aria-label="Gaussian Distribution" data-href="Gaussian Distribution" href="the-guide/mathematics/probability-theory/gaussian-distribution.html" class="internal-link" target="_self" rel="noopener nofollow">Gaussian</a>For general settings, employ <a data-href="Normalizing Flows" href="the-guide/machine-learning/generative-models/normalizing-flows.html" class="internal-link" target="_self" rel="noopener nofollow">Normalizing Flows</a>.<br><br><br>
<br><a rel="noopener nofollow" class="external-link" href="https://www.pnas.org/doi/10.1073/pnas.1912789117" target="_blank">https://www.pnas.org/doi/10.1073/pnas.1912789117</a>
]]></description><link>the-guide/computational-statistics/simulation-based-inference.html</link><guid isPermaLink="false">The Guide/Computational Statistics/Simulation-Based Inference.md</guid><pubDate>Fri, 20 Dec 2024 11:50:52 GMT</pubDate></item><item><title><![CDATA[Stochastic Gradient Langevin Dynamics]]></title><description><![CDATA[ 
 <br>In a Nutshell
Class of <a data-tooltip-position="top" aria-label="Markov-Chain Monte Carlo Methods" data-href="Markov-Chain Monte Carlo Methods" href="the-guide/computational-statistics/data-assimilation/smoothing-algorithms/markov-chain-monte-carlo-methods.html" class="internal-link" target="_self" rel="noopener nofollow">Markov-Chain Monte Carlo</a> algorithms to obtain random samples from a <a data-tooltip-position="top" aria-label="Probability Distribution" data-href="Probability Distribution" href="the-guide/mathematics/probability-theory/probability-distribution.html" class="internal-link" target="_self" rel="noopener nofollow">distribution</a>, where direct sampling is difficult. The basic idea is to simulate the <a data-tooltip-position="top" aria-label="Langevin Dynamics" data-href="Langevin Dynamics" href="the-guide/robotics,-dynamics-and-control/dynamics/langevin-dynamics.html" class="internal-link" target="_self" rel="noopener nofollow">Langevin equation</a> with the steady state distribution according to the desired one. Suitable for cases where the full <a data-tooltip-position="top" aria-label="Derivative, Gradient, Jacobian and Hessian > The Gradient" data-href="Derivative, Gradient, Jacobian and Hessian#The Gradient" href="the-guide/mathematics/analysis-and-calculus/derivative,-gradient,-jacobian-and-hessian.html#The_Gradient" class="internal-link" target="_self" rel="noopener nofollow">gradient</a> is infeasible to compute at the cost of more challenging convergence properties. if the gradient is feasible consider <a data-tooltip-position="top" aria-label="Metropolis-Adjusted Langevin Algorithm" data-href="Metropolis-Adjusted Langevin Algorithm" href="the-guide/computational-statistics/metropolis-adjusted-langevin-algorithm.html" class="internal-link" target="_self" rel="noopener nofollow">MALA</a>.
<br><br>
<br>Basically the setting of first HiWi semester at IAS<br>
<img alt="Langevin_OT 1.gif" src="lib/media/langevin_ot-1.gif">On the same theoretical basis of <a data-tooltip-position="top" aria-label="Langevin Dynamics" data-href="Langevin Dynamics" href="the-guide/robotics,-dynamics-and-control/dynamics/langevin-dynamics.html" class="internal-link" target="_self" rel="noopener nofollow">Langevin dynamics</a> as <a data-tooltip-position="top" aria-label="Metropolis-Adjusted Langevin Algorithm" data-href="Metropolis-Adjusted Langevin Algorithm" href="the-guide/computational-statistics/metropolis-adjusted-langevin-algorithm.html" class="internal-link" target="_self" rel="noopener nofollow">MALA</a> in the context of <a data-tooltip-position="top" aria-label="- Bayesian Statistics -" data-href="- Bayesian Statistics -" href="the-guide/mathematics/statistics/-bayesian-statistics-.html" class="internal-link" target="_self" rel="noopener nofollow">posterior approximation</a>, SGLD aims to update particles to resemble a <a data-tooltip-position="top" aria-label="Bayes Theorem" data-href="Bayes Theorem" href="the-guide/mathematics/statistics/bayes-theorem.html" class="internal-link" target="_self" rel="noopener nofollow">posterior</a>given a dataset . Instead of evaluating the full gradient, it uses a minibatch approximation based on the samples, yielding the chain updatewith , . In addition, for convergence, we have to re<br>
quire the <a data-tooltip-position="top" aria-label="Stochastic Optimization" data-href="Stochastic Optimization" href="the-guide/mathematics/optimization/stochastic-optimization.html" class="internal-link" target="_self" rel="noopener nofollow">Robbins-Monro</a> condition
]]></description><link>the-guide/computational-statistics/stochastic-gradient-langevin-dynamics.html</link><guid isPermaLink="false">The Guide/Computational Statistics/Stochastic Gradient Langevin Dynamics.md</guid><pubDate>Thu, 17 Apr 2025 16:23:34 GMT</pubDate><enclosure url="lib/media/langevin_ot-1.gif" length="0" type="image/gif"/><content:encoded>&lt;figure&gt;&lt;img src=&quot;lib/media/langevin_ot-1.gif&quot;&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[Variational Inference]]></title><description><![CDATA[ 
 <br>In a Nutshell
Approach to solve the problem of intractable distributions in <a data-href="- Bayesian Statistics -" href="the-guide/mathematics/statistics/-bayesian-statistics-.html" class="internal-link" target="_self" rel="noopener nofollow">- Bayesian Statistics -</a> by turning inference into an <a data-tooltip-position="top" aria-label="Optimization Problem" data-href="Optimization Problem" href="the-guide/mathematics/optimization/convex-optimization-lecture/optimization-problem.html" class="internal-link" target="_self" rel="noopener nofollow">optimization problem</a> that brings an approximate model close to the true distribution.<br>
The name is derived from the fact that we are technically optimizing a <a data-tooltip-position="top" aria-label="Calculus of Variations" data-href="Calculus of Variations" href="the-guide/mathematics/functional-analysis-and-calculus-of-variations/calculus-of-variations.html" class="internal-link" target="_self" rel="noopener nofollow">functional</a> to best approximate the distribution.
<br><br>Category of algorithms in statistics and Bayesian machine learning that tries to solve the problem of dealing with intractable <a data-tooltip-position="top" aria-label="Probability Distribution" data-href="Probability Distribution" href="the-guide/mathematics/probability-theory/probability-distribution.html" class="internal-link" target="_self" rel="noopener nofollow">distributions</a>. Pick a family of <a data-tooltip-position="top" aria-label="Probability Distribution" data-href="Probability Distribution" href="the-guide/mathematics/probability-theory/probability-distribution.html" class="internal-link" target="_self" rel="noopener nofollow">distributions</a> over the latent variables  with its own parameters and try to find parameters  that move  as close as possible to the <a data-tooltip-position="top" aria-label="Bayes Theorem" data-href="Bayes Theorem" href="the-guide/mathematics/statistics/bayes-theorem.html" class="internal-link" target="_self" rel="noopener nofollow">posterior</a>  of interest. This "closeness" is mostly measured using the <a data-tooltip-position="top" aria-label="Kullback-Leibler Divergence" data-href="Kullback-Leibler Divergence" href="the-guide/information-theory/information-theory-1/information/kullback-leibler-divergence.html" class="internal-link" target="_self" rel="noopener nofollow">KL-Divergence</a> <br>
<br>If  is high where  is high, the log is very small and we are happy
<br>If q is high where  is low we pay a (logarithmically increasing) price in form of increased divergence
<br>If  is low we do not care, since we take the <a data-tooltip-position="top" aria-label="Expectations" data-href="Expectations" href="the-guide/mathematics/statistics/expectations.html" class="internal-link" target="_self" rel="noopener nofollow">expectation</a> over 
<br><br>We can't minimize the KL term above, because it contains the posterior. However, we can reformulate it to where the term in brackets is exactly the negative <a data-tooltip-position="top" aria-label="Evidence Lower Bound" data-href="Evidence Lower Bound" href="the-guide/mathematics/probability-theory/evidence-lower-bound.html" class="internal-link" target="_self" rel="noopener nofollow">ELBO</a>. Since the remaining term does not depend on  we can instead maximize the ELBO.<br>
...]]></description><link>the-guide/computational-statistics/variational-inference.html</link><guid isPermaLink="false">The Guide/Computational Statistics/Variational Inference.md</guid><pubDate>Thu, 17 Apr 2025 16:23:35 GMT</pubDate></item><item><title><![CDATA[The Camera Matrix]]></title><description><![CDATA[ 
 <br>
<br>Overall  <a rel="noopener nofollow" class="external-link" href="https://ksimek.github.io/2012/08/14/decompose/" target="_blank">https://ksimek.github.io/2012/08/14/decompose/</a>
<br>Extrinsic  <a rel="noopener nofollow" class="external-link" href="https://ksimek.github.io/2012/08/22/extrinsic/" target="_blank">https://ksimek.github.io/2012/08/22/extrinsic/</a>
<br>Intrinsic  <a rel="noopener nofollow" class="external-link" href="https://ksimek.github.io/2013/08/13/intrinsic/" target="_blank">https://ksimek.github.io/2013/08/13/intrinsic/</a>
]]></description><link>the-guide/computer-science/computer-vision/the-camera-matrix.html</link><guid isPermaLink="false">The Guide/Computer Science/Computer Vision/The Camera Matrix.md</guid><pubDate>Wed, 23 Apr 2025 10:58:19 GMT</pubDate></item><item><title><![CDATA[HTTP and HTTPS - Hypertext Transfer Protocol]]></title><description><![CDATA[ 
 <br>In a Nutshell
Foundation of data communication for the World Wide Web. It is an <a data-tooltip-position="top" aria-label="Internet Protocol Suite (TCP-IP)" data-href="Internet Protocol Suite (TCP-IP)" href="the-guide/computer-science/networks/internet-protocol-suite-(tcp-ip).html" class="internal-link" target="_self" rel="noopener nofollow">application-layer protocol in the Internet protocol suite model</a>. 
<br><br>HTTP defines how messages are formatted and transmitted on the internet, and what actions web servers and browsers should take in response to various commands.<br>
<br>Application Layer Protocol: Operates at the highest level of the TCP/IP model.
<br>Client-Server Model: Involves a client (typically a web browser) sending requests to a server (typically a web server hosting website data).
<br>Stateless Protocol: Each request from a client to a server is treated independently. The server does not retain any information about previous client requests. This simplifies server design but requires mechanisms like cookies and sessions to maintain user state.
<br>Request-Response Paradigm: Clients initiate communication by sending requests, and servers respond with data or status information.
<br>The HTTP protocol operates on a client-server architecture:<br>
<br>Client: Usually a web browser (Chrome, Firefox, Safari, etc.) or any application that needs to access resources on a server. The client initiates an HTTP request to the server.
<br>Server: A computer program (like Apache, Nginx, IIS) that listens for HTTP requests from clients and responds with the requested resources (HTML pages, images, videos, etc.) or an error message.
<br><br><br>An HTTP request is sent by the client to the server. It consists of the following components:<br>Request Line<br>GET/products/123 HTTP/1.1     # method / target / version
<br>
<br>HTTP Method: Indicates the action the client wants to perform on the resource. Common methods include:

<br>GET: Requests a specific resource.
<br>POST: Submits data to be processed by the server.
<br>PUT: Updates an existing resource or creates a new resource at a specified URI.
<br>DELETE: Deletes a specific resource.
<br>HEAD: Similar to GET, but only retrieves the headers, not the body.
<br>OPTIONS: Describes the communication options for the target resource.
<br>CONNECT: Establishes a tunnel to the server identified by the target resource.
<br>TRACE: Performs a message loop-back test along the path to the target resource.
<br>PATCH: Applies partial modifications to a resource.


<br>Request Target (URI): Identifies the resource on the server that the client wants to access. It can be in various forms:

<br>Origin Form: /index.html (most common form for GET requests).
<br>Absolute Form: http://www.example.com/index.html (used in proxy requests).
<br>Authority Form: www.example.com:80 (used for CONNECT requests).
<br>Asterisk Form: * (used for OPTIONS requests to the server as a whole).


<br>HTTP Version: Indicates the version of the HTTP protocol being used (e.g., HTTP/1.1, HTTP/2, HTTP/3).
<br>Request Headers <br>Host: https://www.google.com/search?q=api.example.com User-Agent: Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) 1 Chrome/112.0.0.0 Safari/537.36 Accept: application/json
<br>Provide additional information about the request to the server. They are key-value pairs, with each header on a separate line. Common request headers include: <br>
<br>

<br>Host: Specifies the domain name of the server. Essential for virtual hosting. 


<br>
<br>User-Agent: Identifies the client software making the request (e.g., browser name and version).
<br>Accept: Indicates the content types the client can understand. 
<br>Accept-Language: Specifies the preferred language(s) for the response. 
<br>Accept-Encoding: Lists the content encodings the client can handle (e.g., gzip, deflate). 
<br>Connection: Controls options for the current connection (e.g., keep-alive to reuse the connection). 
<br>Cookie: Contains cookies previously sent by the server. 
<br>Referer: Indicates the URL of the page that linked to the requested resource. 
<br>Request Body (Optional)<br>
Contains data sent to the server, typically used with POST, PUT, and PATCH requests. The format of the body is specified by the Content-Type header. Examples include form data, JSON, XML, etc.<br><br><br>An HTTP response is sent by the server back to the client after receiving and processing a request. It consists of the following components:<br>Status Line <br>HTTP/1.1 200 OK
<br>
<br>HTTP Version: Indicates the version of the HTTP protocol used by the server.
<br>Status Code: A three-digit integer code that indicates the outcome of the request. 
<br><br>
<br>Reason Phrase: A human-readable description of the status code.
<br>Response Headers <br>Content-Type: application/json 
Content-Length: 150 
Date: Thu, 17 Apr 2025 14:30:00 GMT 
Server: Apache/2.4.54 (Unix)
<br>Provide additional information about the response. They are also key-value pairs. Common response headers include: <br>
<br>Content-Type: Specifies the media type of the response body (e.g., text/html, application/json, image/jpeg). 
<br>
<br>Content-Length: Indicates the size of the response body in bytes. 
<br>Date: The date and time at which the response was generated by the server. 
<br>Server: Identifies the web server software used by the server. 
<br>Location: Used in redirects (3xx status codes) to specify the new URL. 
<br>Set-Cookie: Instructs the client to store a cookie. 
<br>Cache-Control: Specifies directives for caching the response. 
<br>ETag: An entity tag representing a specific version of the resource. Used for cache validation. 
<br>Last-Modified: Indicates the date and time the resource was last modified.
<br>Response Body (Optional):<br>
Contains the data requested by the client. The format is determined by the Content-Type header. For example, for a request for an HTML page, the body would contain the HTML markup. <br><br><br>HTTPS is the secure version of HTTP. It uses TLS (Transport Layer Security) or SSL (Secure Sockets Layer) to encrypt the communication between the client and the server. This provides: <br>
<br>Confidentiality: Prevents eavesdropping on the data being transmitted. 
<br>Integrity: Ensures that the data has not been tampered with during transmission. * Authentication: Verifies the identity of the server (and optionally the client). HTTPS typically uses port 443, while HTTP uses port 80. The presence of a valid SSL/TLS certificate on the server is essential for establishing an HTTPS connection.
]]></description><link>the-guide/computer-science/networks/http-and-https-hypertext-transfer-protocol.html</link><guid isPermaLink="false">The Guide/Computer Science/Networks/HTTP and HTTPS - Hypertext Transfer Protocol.md</guid><pubDate>Thu, 17 Apr 2025 15:50:47 GMT</pubDate></item><item><title><![CDATA[Internet Protocol Suite (TCP-IP)]]></title><description><![CDATA[ 
 <br>In a Nutshell
Conceptual communication protocol framework derived from the more general <a data-tooltip-position="top" aria-label="OSI Model - Open Systems Interconnection" data-href="OSI Model - Open Systems Interconnection" href="the-guide/computer-science/networks/osi-model-open-systems-interconnection.html" class="internal-link" target="_self" rel="noopener nofollow">OSI model</a>. Widely used in modern communication, e.g. internet.
<br><img alt="Pasted image 20250417155348.png" src="lib/media/pasted-image-20250417155348.png"><br><br>Application Layer
The application layer is the highest layer of the model. It’s what you, as a user, see and interact with when sending and receiving data. 
It is responsible for ...

<br>Generating the data
<br>Requesting connections

Some example protocols are ...

<br><a data-href="HTTP and HTTPS - Hypertext Transfer Protocol" href="the-guide/computer-science/networks/http-and-https-hypertext-transfer-protocol.html" class="internal-link" target="_self" rel="noopener nofollow">HTTP and HTTPS - Hypertext Transfer Protocol</a>
<br><a data-href="SSH - Secure Shell" href="the-guide/computer-science/networks/ssh-secure-shell.html" class="internal-link" target="_self" rel="noopener nofollow">SSH - Secure Shell</a>
<br><a data-href="LCM - Lightweight Communications and Marshalling" href="the-guide/computer-science/networks/lcm-lightweight-communications-and-marshalling.html" class="internal-link" target="_self" rel="noopener nofollow">LCM - Lightweight Communications and Marshalling</a>
<br>...

<br>Transport Layer
It establishes a reliable and error-free data connection between the application or device and the destination. 
It is responsible for ...

<br>Establishing the connection (error-free)
<br>Splitting the data into smaller packets and numbering them into a sequence. This is based on the amount of data, the destinations and the transmission rate
<br>Obtaining acknowledgment that packets have been received

The protocols in this layer are ...  

<br><a data-href="TCP - Transmission Control Protocol" href="the-guide/computer-science/networks/tcp-transmission-control-protocol.html" class="internal-link" target="_self" rel="noopener nofollow">TCP - Transmission Control Protocol</a>
<br><a data-href="UDP - User Datagram Protocol" href="the-guide/computer-science/networks/udp-user-datagram-protocol.html" class="internal-link" target="_self" rel="noopener nofollow">UDP - User Datagram Protocol</a>

<br>Internet Layer
The internet layer, also called the IP or network layer (though not to be confused with the network access layer, see below), is responsible for ...

<br>Sending the packets 
<br>Ensuring that the data is transferred as accurately as possible.
<br>Routing the data to the correct network 

The protocols belonging to this layer are:

<br>Internet protocol versions 4 and 6 (IPv4/IPv6). Used for routing the data across the network. It’s responsible for packets delivery from the host source to the destination host by looking at the IP addresses in the packet.
<br><a data-tooltip-position="top" aria-label="https://www.cloudflare.com/en-gb/learning/ddos/glossary/internet-control-message-protocol-icmp/" rel="noopener nofollow" class="external-link" href="https://www.cloudflare.com/en-gb/learning/ddos/glossary/internet-control-message-protocol-icmp/" target="_blank"><strong></strong></a>Internet control message protocol&nbsp;(ICMP). Used to provide information to the hosts in case of network problems.
<br><a data-tooltip-position="top" aria-label="https://blog.cloudflare.com/ddos-attack-trends-for-2021-q4/" rel="noopener nofollow" class="external-link" href="https://blog.cloudflare.com/ddos-attack-trends-for-2021-q4/" target="_blank"><strong></strong></a>Address resolution protocol&nbsp;(ARP). Used to find the hardware address of a host from a known IP address.

<br>Network Access Layer
All protocols and technologies required to deliver data across a physical medium, also refered to as physical layer or data link layer.
It is responsible for ...

<br>Adding a destination <a data-tooltip-position="top" aria-label="MAC - Media Access Control Address" data-href="MAC - Media Access Control Address" href="the-guide/computer-science/networks/mac-media-access-control-address.html" class="internal-link" target="_self" rel="noopener nofollow">MAC address</a>

Examples are ...

<br>Ethernet Cables
<br>Wi-Fi networks
<br>...

]]></description><link>the-guide/computer-science/networks/internet-protocol-suite-(tcp-ip).html</link><guid isPermaLink="false">The Guide/Computer Science/Networks/Internet Protocol Suite (TCP-IP).md</guid><pubDate>Thu, 17 Apr 2025 15:28:14 GMT</pubDate><enclosure url="lib/media/pasted-image-20250417155348.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src=&quot;lib/media/pasted-image-20250417155348.png&quot;&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[LCM - Lightweight Communications and Marshalling]]></title><description><![CDATA[<a class="tag" href="?query=tag:Robotics" style="background-color: rgb(4, 108, 116); color: white; font-weight: 700; border: none; border-radius: 1em; padding: 0.2em 0.5em;">#Robotics</a> <a class="tag" href="?query=tag:Networks" style="background-color: rgb(4, 108, 116); color: white; font-weight: 700; border: none; border-radius: 1em; padding: 0.2em 0.5em;">#Networks</a> 
 <br>In a Nutshell
Framework for asynchronous communication, e.g. for <a href=".?query=tag:Robotics" class="tag" target="_blank" rel="noopener nofollow">#Robotics</a> and embedded systems. In <a href=".?query=tag:Networks" class="tag" target="_blank" rel="noopener nofollow">#Networks</a> , it an be considered part of the Application Layer of the <a data-tooltip-position="top" aria-label="OSI Model - Open Systems Interconnection" data-href="OSI Model - Open Systems Interconnection" href="the-guide/computer-science/networks/osi-model-open-systems-interconnection.html" class="internal-link" target="_self" rel="noopener nofollow">OSI</a> or <a data-tooltip-position="top" aria-label="Internet Protocol Suite (TCP-IP)" data-href="Internet Protocol Suite (TCP-IP)" href="the-guide/computer-science/networks/internet-protocol-suite-(tcp-ip).html" class="internal-link" target="_self" rel="noopener nofollow">TCP-IP</a> models.
<br><br><br>
<br>Serialization / Marshalling - LCM converts structured data into compact binary forms for space-efficient transmission
<br>Language Independence - LCM files can be used to generate source code in multiple languages
<br>Lightweight - low latency and high throughput, enabling e.g. real-time control
<br>Event-Driven - subscribers don't actively poll for messages, system notifies when new messages available
<br>Various <a data-tooltip-position="top" aria-label="Internet Protocol Suite (TCP-IP)" data-href="Internet Protocol Suite (TCP-IP)" href="the-guide/computer-science/networks/internet-protocol-suite-(tcp-ip).html" class="internal-link" target="_self" rel="noopener nofollow">Transport Layer Mechanisms</a> - can use shared memory (local), <a data-tooltip-position="top" aria-label="UDP - User Datagram Protocol" data-href="UDP - User Datagram Protocol" href="the-guide/computer-science/networks/udp-user-datagram-protocol.html" class="internal-link" target="_self" rel="noopener nofollow">UDP</a> or (less commmon) <a data-tooltip-position="top" aria-label="TCP - Transmission Control Protocol" data-href="TCP - Transmission Control Protocol" href="the-guide/computer-science/networks/tcp-transmission-control-protocol.html" class="internal-link" target="_self" rel="noopener nofollow">TCP</a> 
<br><br><br>An LCM file is used to describe the schema for a message in human readable form as a struct with fields and data types. This is then used as a blueprint to generate code in other languages<br>package example # defines namespace

struct ExampleMessage {
	int32 id;
	float value;
	string name;
	int length;
	float numberlist[length];
}
<br>LCM supports a number of primitive types:<br><br>The LCM workflow is the following<br>
<br>Definition - define .lcm file 
<br>Code Generation - use a code generation tool to generate source code for targeted programming language
<br>lcm-gen -p example.lcm
<br>
<br>Serialization and Communication - the generated code above provides functions for serializing, de-serializing and message handling
<br>Integration - Use generated code in application to exchange messages 
<br><br><br>The lcm structs defined and generated above can be imported as python classes. IN the code, we use lcm instances to handle channels<br>lc = lcm.LCM()
<br>We can use the following methods:<br><br>In order to trigger message processing (typically checking and reading out <a data-tooltip-position="top" aria-label="UDP - User Datagram Protocol" data-href="UDP - User Datagram Protocol" href="the-guide/computer-science/networks/udp-user-datagram-protocol.html" class="internal-link" target="_self" rel="noopener nofollow">UDP</a> <a data-tooltip-position="top" aria-label="Network Socket" data-href="Network Socket" href="the-guide/computer-science/networks/network-socket.html" class="internal-link" target="_self" rel="noopener nofollow">socket</a> buffer), we can use ...<br><br><br><br>LCM acts as the library/interface that serializes, deserializes, and publishes messages. The serialized message data (binary format) is handed off to the LCM framework, which determines the transport mechanism (e.g., UDP, multicast, shared memory).<br><br>If LCM is configured to use UDP or multicast, it creates a socket on the operating system to send messages over the network stack.<br>
<br>Physical Path:

<br>Messages are encapsulated in IP packets and <a data-tooltip-position="top" aria-label="UDP - User Datagram Protocol" data-href="UDP - User Datagram Protocol" href="the-guide/computer-science/networks/udp-user-datagram-protocol.html" class="internal-link" target="_self" rel="noopener nofollow">UDP</a> datagrams.
<br>Packets travel through your NIC (Network Interface Card) if multicast is used over a physical network.
<br>For local communication, the messages are looped back within the OS (via the loopback interface, typically 127.0.0.1).


<br><br>If LCM is configured to use shared memory for local processes, the message is written directly into a shared memory segment.<br>
<br>Physical Path:

<br>Shared memory resides in the RAM of the system.
<br>LCM maps a portion of the RAM into the address space of both the sending and receiving processes.


<br><br><br>
<br>
Install MYSYS2 as shown in <a rel="noopener nofollow" class="external-link" href="https://www.msys2.org/" target="_blank">https://www.msys2.org/</a> (.msi !), make sure to do update at the end (<a rel="noopener nofollow" class="external-link" href="https://www.msys2.org/docs/updating/" target="_blank">https://www.msys2.org/docs/updating/</a>)
pacman -Suy
pacman -Su


<br>
Install required tools
# 
pacman -S base-devel git mingw-w64-x86_64-gcc mingw-w64-x86_64-cmake
# 
pacman -S mingw-w64-x86_64-python3 mingw-w64-x86_64-python3-pip 
# 
pacman -S mingw-w64-x86_64-glib2


<br>
Double-Check if pip, python, cmake, make and ninja are installed
make --version
cmake --version
ninja --version
which python # global path, not usr
which pip # global path, not usr


<br>
Download lcm 
git clone https://github.com/lcm-proj/lcm.git


<br>
Create new build directory, cd into it and run 
cd lcm
mkdir build
cd build
cmake -S . -G Ninja .. -DCMAKE_BUILD_TYPE=Release -DLCM_ENABLE_GO=OFF -DLCM_ENABLE_LUA=OFF


<br>
In the build dir run 
ninja


<br>
Go back up to the .lcm dir and install it via pip

<br># look for pyptoject.toml file
cd ..
pip install .
<br>
<br>Try basic example in <a rel="noopener nofollow" class="external-link" href="https://lcm-proj.github.io/lcm/content/tutorial-python.html" target="_blank">https://lcm-proj.github.io/lcm/content/tutorial-python.html</a>
<br><br>
<br>
GLIB
pacman -S mingw-w64-x86_64-glib2


<br>
Java<br>
Install on your machine via <a rel="noopener nofollow" class="external-link" href="https://learn.microsoft.com/en-us/java/openjdk/download" target="_blank">https://learn.microsoft.com/en-us/java/openjdk/download</a>
nano ~/.bashrc

In your file, add (&lt;your-actual-path&gt; in windows dir) ...
export JAVA_HOME="/c/Program Files/Microsoft/&lt;your-actual-path&gt;"
export PATH="$JAVA_HOME/bin:$PATH"

Save and close, reload profile
source ~/.bashrc


]]></description><link>the-guide/computer-science/networks/lcm-lightweight-communications-and-marshalling.html</link><guid isPermaLink="false">The Guide/Computer Science/Networks/LCM - Lightweight Communications and Marshalling.md</guid><pubDate>Mon, 31 Mar 2025 17:59:24 GMT</pubDate></item><item><title><![CDATA[MAC - Media Access Control Address]]></title><description><![CDATA[ 
 <br>In a Nutshell
.Unique, hardcoded identifier for a device in a network.
<br><br>Every network device (such as a computer, smartphone, router, or switch) is assigned a unique MAC address by the manufacturer.<br>
Address is hardcoded into the devices hardware (NIC - network interface controller). The format is a -digit hexadecimal number, separated by colons or hyphens<br>00:14:22:01:23:45
00-14-22-01-23-45
<br>The first three bytes (24 bits) of the MAC address represent the Organizationally Unique Identifier (OUI), which is assigned to the manufacturer of the device. The remaining three bytes are used to uniquely identify the specific NIC.<br><br><br>Some devices (e.g., smartphones and laptops) implement MAC address randomization for Wi-Fi connections, where a random MAC address is used when connecting to different networks to prevent the device from being tracked by its permanent MAC address.]]></description><link>the-guide/computer-science/networks/mac-media-access-control-address.html</link><guid isPermaLink="false">The Guide/Computer Science/Networks/MAC - Media Access Control Address.md</guid><pubDate>Thu, 17 Apr 2025 15:27:24 GMT</pubDate></item><item><title><![CDATA[Network Socket]]></title><description><![CDATA[ 
 <br>In a Nutshell
Used to describe the abstract concept as well as the software structure for endpoints (nodes) of a computer network. In the standardized <a data-href="Internet Protocol Suite (TCP-IP)" href="the-guide/computer-science/networks/internet-protocol-suite-(tcp-ip).html" class="internal-link" target="_self" rel="noopener nofollow">Internet Protocol Suite (TCP-IP)</a>, a socket is identified by the triplet (transport protocoll, <a data-tooltip-position="top" aria-label="Internet Protocol Suite (TCP-IP)" data-href="Internet Protocol Suite (TCP-IP)" href="the-guide/computer-science/networks/internet-protocol-suite-(tcp-ip).html" class="internal-link" target="_self" rel="noopener nofollow">IP address</a>, <a data-tooltip-position="top" aria-label="Port" data-href="Port" href="the-guide/computer-science/networks/port.html" class="internal-link" target="_self" rel="noopener nofollow">port number</a>), the protocolls usually being <a data-tooltip-position="top" aria-label="TCP - Transmission Control Protocol" data-href="TCP - Transmission Control Protocol" href="the-guide/computer-science/networks/tcp-transmission-control-protocol.html" class="internal-link" target="_self" rel="noopener nofollow">TCP</a> (connection-oriented) or <a data-tooltip-position="top" aria-label="UDP - User Datagram Protocol" data-href="UDP - User Datagram Protocol" href="the-guide/computer-science/networks/udp-user-datagram-protocol.html" class="internal-link" target="_self" rel="noopener nofollow">UDP</a> (connection-less).
<br>Quote
"The message passing API for the internet."
<br><br><br>The API that programs use to communicate with the protocol stack, using network sockets, is called a&nbsp;socket API. Internet socket APIs are usually based on the&nbsp;<a data-tooltip-position="top" aria-label="https://en.wikipedia.org/wiki/Berkeley_sockets" rel="noopener nofollow" class="external-link" title="Berkeley sockets" href="https://en.wikipedia.org/wiki/Berkeley_sockets" target="_blank">Berkeley sockets</a>&nbsp;standard (originally written in C, e.g. wrapped by <a data-tooltip-position="top" aria-label="Socket Module (Python Standard Library)" data-href="Socket Module (Python Standard Library)" href="life,-universe-and-everything/python/socket-module-(python-standard-library).html" class="internal-link" target="_self" rel="noopener nofollow">socket</a> in <a data-tooltip-position="top" aria-label="- Python Programming Language -" data-href="- Python Programming Language -" href="life,-universe-and-everything/python/-python-programming-language-.html" class="internal-link" target="_self" rel="noopener nofollow">Python</a>).<br>
In the Berkeley sockets standard, sockets are a form of&nbsp;<a data-tooltip-position="top" aria-label="https://en.wikipedia.org/wiki/File_descriptor" rel="noopener nofollow" class="external-link" title="File descriptor" href="https://en.wikipedia.org/wiki/File_descriptor" target="_blank">file descriptor</a>, due to the&nbsp;<a data-tooltip-position="top" aria-label="https://en.wikipedia.org/wiki/Unix_philosophy" rel="noopener nofollow" class="external-link" title="Unix philosophy" href="https://en.wikipedia.org/wiki/Unix_philosophy" target="_blank">Unix philosophy</a>&nbsp;that "everything is a file", and the analogies between sockets and files. Both have functions to read, write, open, and close. In practice, the differences strain the analogy, and different interfaces (send and receive) are used on a socket. In&nbsp;<a data-tooltip-position="top" aria-label="https://en.wikipedia.org/wiki/Inter-process_communication" rel="noopener nofollow" class="external-link" title="Inter-process communication" href="https://en.wikipedia.org/wiki/Inter-process_communication" target="_blank">inter-process communication</a>, each end generally has its own socket.<br><img alt="center" src="lib/media/pasted-image-20250103100148.png"><br><br>While the Berkeley socket interface and its derivatives is still used in low level tasks, higher-level libraries and frameworks like Boost.Asio (C++), socket.io (JavaScript), or <a data-tooltip-position="top" aria-label="Asynchio Library (Python Standard Library)" data-href="Asynchio Library (Python Standard Library)" href="life,-universe-and-everything/python/asynchio-library-(python-standard-library).html" class="internal-link" target="_self" rel="noopener nofollow">asyncio</a> (Python) offer more convenient abstractions for socket and asynchronous programming.]]></description><link>the-guide/computer-science/networks/network-socket.html</link><guid isPermaLink="false">The Guide/Computer Science/Networks/Network Socket.md</guid><pubDate>Wed, 23 Apr 2025 22:23:12 GMT</pubDate><enclosure url="lib/media/pasted-image-20250103100148.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src=&quot;lib/media/pasted-image-20250103100148.png&quot;&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[OSI Model - Open Systems Interconnection]]></title><description><![CDATA[ 
 <br>In a Nutshell
Conceptual framework used to describe how data is transmitted over a network. It is the foundation for most modern computer networks, including the internet.<br>
In actual real world applications, the most widely used practical implementation is the <a data-href="Internet Protocol Suite (TCP-IP)" href="the-guide/computer-science/networks/internet-protocol-suite-(tcp-ip).html" class="internal-link" target="_self" rel="noopener nofollow">Internet Protocol Suite (TCP-IP)</a>. The OSI is more general and theoretical.
<br><br><br><br><br><br><br><br><br><br>]]></description><link>the-guide/computer-science/networks/osi-model-open-systems-interconnection.html</link><guid isPermaLink="false">The Guide/Computer Science/Networks/OSI Model - Open Systems Interconnection.md</guid><pubDate>Thu, 17 Apr 2025 14:50:13 GMT</pubDate></item><item><title><![CDATA[Port]]></title><description><![CDATA[ 
 <br>In a Nutshell
Logical endpoint for communication between computers or devices on a network using the <a data-href="Internet Protocol Suite (TCP-IP)" href="the-guide/computer-science/networks/internet-protocol-suite-(tcp-ip).html" class="internal-link" target="_self" rel="noopener nofollow">Internet Protocol Suite (TCP-IP)</a>. It acts as an address for specific services or applications running on a device, allowing different types of network traffic to be directed to the appropriate software process.
<br><br>A port is identified by a port number, which is a 16-bit unsigned integer. Ports are generally classified into three categories:<br>
<br>
Well-Known Ports (0–1023): These are reserved for specific, commonly used services and protocols. They are assigned by the Internet Assigned Numbers Authority (IANA) and are standardized for widespread use. Examples include:

<br>Port 80: HTTP (Hypertext Transfer Protocol) – Used by web servers for serving websites.
<br>Port 443: HTTPS (HTTP Secure) – Used for secure web traffic.
<br>Port 22: SSH (Secure Shell) – Used for secure remote access to systems.
<br>Port 25: SMTP (Simple Mail Transfer Protocol) – Used for sending emails.


<br>
Registered Ports (1024–49151): These ports are assigned to applications or services that are not as universally standardized as well-known ports. They are typically used by software applications to allow communication with other services or devices. Examples include:    

<br>Port 3306: MySQL database service.
<br>Port 5432: PostgreSQL database service.


<br>
Dynamic or Private Ports (49152–65535): These ports are not assigned to specific services and are typically used by client applications dynamically when establishing connections. For instance, when your web browser connects to a web server, it might use a dynamic port (e.g., 49152) to communicate with the server’s HTTP service on port 80. These ports are also used for ephemeral (temporary) connections.

]]></description><link>the-guide/computer-science/networks/port.html</link><guid isPermaLink="false">The Guide/Computer Science/Networks/Port.md</guid><pubDate>Fri, 17 Jan 2025 09:17:36 GMT</pubDate></item><item><title><![CDATA[SSH - Secure Shell]]></title><description><![CDATA[ 
 <br>In a Nutshell
Network protocol that allows communication between two systems that is both encrypted (so no one can easily eavesdrop) and authenticated (so you can be sure you're connecting to the right machine).<br>
SSH provides a way to remotely access another machine, execute commands, and transfer files, all while encrypting the data in transit.
<br><br><br><img alt="center" src="lib/media/pasted-image-20241205130653.png"><br>
<br>Client Initiates Connection: The SSH client, like ssh on a Linux or macOS terminal, connects to the remote machine.
<br>Key Exchange: The SSH protocol uses public-key cryptography for secure key exchange. The client and server agree on encryption methods.
<br>Authentication: The client authenticates itself using one of several methods:

<br>Password Authentication: The user provides a password that the server verifies.
<br>Public Key Authentication: The client uses a private key, and the server checks it against a pre-configured list of authorized public keys.


<br>Session Established: Once authenticated, a secure, encrypted channel is opened, and the user can run commands or transfer files.
<br><br>
<br>Remote Access: SSH is commonly used to access remote servers in a secure manner.
<br>Encryption: SSH encrypts the entire session, ensuring that any data sent, including passwords and commands, is protected from interception.
<br>Authentication: SSH can use various methods to verify that the user is who they say they are, such as password-based authentication or more secure public key authentication.
<br>Port Forwarding: SSH can securely tunnel other network protocols, allowing for things like accessing a database behind a firewall.
<br>File Transfer: With tools like SCP (Secure Copy) or SFTP (Secure File Transfer Protocol), SSH allows for secure file transfers between machines.
<br>Implementation on Linux
A widely-used open source implementation of the SSH protocoll is <a data-href="OpenSSH" href="life,-universe-and-everything/linux-and-ubuntu/openssh.html" class="internal-link" target="_self" rel="noopener nofollow">OpenSSH</a>, e.g. used in <a data-href="- Linux -" href="life,-universe-and-everything/linux-and-ubuntu/-linux-.html" class="internal-link" target="_self" rel="noopener nofollow">- Linux -</a> systems.
]]></description><link>the-guide/computer-science/networks/ssh-secure-shell.html</link><guid isPermaLink="false">The Guide/Computer Science/Networks/SSH - Secure Shell.md</guid><pubDate>Thu, 13 Feb 2025 22:18:22 GMT</pubDate><enclosure url="lib/media/pasted-image-20241205130653.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src=&quot;lib/media/pasted-image-20241205130653.png&quot;&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[TCP - Transmission Control Protocol]]></title><description><![CDATA[ 
 <br>In a Nutshell
Core Transport Layer protocol of the <a data-tooltip-position="top" aria-label="Internet Protocol Suite (TCP-IP)" data-href="Internet Protocol Suite (TCP-IP)" href="the-guide/computer-science/networks/internet-protocol-suite-(tcp-ip).html" class="internal-link" target="_self" rel="noopener nofollow">Internet Protocol Suite</a>. TCP is a connection-oriented protocol, meaning it establishes a reliable connection between the sender and receiver before data is exchanged.
<br><br><img alt="Pasted image 20250417155316.png" src="lib/media/pasted-image-20250417155316.png"><br><br><br>]]></description><link>the-guide/computer-science/networks/tcp-transmission-control-protocol.html</link><guid isPermaLink="false">The Guide/Computer Science/Networks/TCP - Transmission Control Protocol.md</guid><pubDate>Thu, 17 Apr 2025 14:53:18 GMT</pubDate><enclosure url="lib/media/pasted-image-20250417155316.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src=&quot;lib/media/pasted-image-20250417155316.png&quot;&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[UDP - User Datagram Protocol]]></title><description><![CDATA[ 
 <br>In a Nutshell
UDP is a connection-less transport layer protocol in the <a data-href="Internet Protocol Suite (TCP-IP)" href="the-guide/computer-science/networks/internet-protocol-suite-(tcp-ip).html" class="internal-link" target="_self" rel="noopener nofollow">Internet Protocol Suite (TCP-IP)</a>. Unlike <a data-tooltip-position="top" aria-label="TCP - Transmission Control Protocol" data-href="TCP - Transmission Control Protocol" href="the-guide/computer-science/networks/tcp-transmission-control-protocol.html" class="internal-link" target="_self" rel="noopener nofollow">TCP</a>, UDP does not establish a connection before sending data and does not guarantee delivery or ordering. It’s faster (less overhead) but less reliable, e.g. for time-sensitive applications where loss of a small amount of data is acceptable.
<br><img alt="center" src="lib/media/pasted-image-20250411115414.png" style="width: 400px; max-width: 100%;"><br><br><br>
<br>Connectionless: No need to establish a connection before sending data.
<br>Unreliable: Data packets may be lost, duplicated, or arrive out of order.
<br>Low Overhead: Since it does not require connection establishment or error checking, UDP has minimal overhead, making it faster.
<br>No Flow Control: It doesn’t manage data flow between sender and receiver.
<br>No Acknowledgements: Unlike TCP, UDP doesn’t acknowledge received packets or request retransmissions for lost packets.
<br>Multicast Support
<br><br><br>]]></description><link>the-guide/computer-science/networks/udp-user-datagram-protocol.html</link><guid isPermaLink="false">The Guide/Computer Science/Networks/UDP - User Datagram Protocol.md</guid><pubDate>Fri, 11 Apr 2025 10:54:23 GMT</pubDate><enclosure url="lib/media/pasted-image-20250411115414.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src=&quot;lib/media/pasted-image-20250411115414.png&quot;&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[URL - Uniform Resource Locator]]></title><description><![CDATA[ 
 <br>In a Nutshell
Address used to access resources in a network, e.g. the internet. It specifies the location of a resource and how to retrieve it.<br>
It is used as a standardized way of referring to websites, files, services, or other types of data available in the network.
<br><br><br>A typical URL looks like this:<br>scheme://host:port/path?query#fragment
<br>For example, a full URL could look like this<br>https://www.example.com:8080/products/search?category=electronics&amp;sort=price#reviews
<br><br><br><br>
<br>This specifies the protocol or method used to access the resource.
<br>Examples:

<br>http: HyperText Transfer Protocol, used for standard web pages.
<br>https: Secure version of HTTP, where the data is encrypted.
<br>ftp: File Transfer Protocol, used to transfer files between computers.
<br>file: Refers to a local file on the system.
<br>mailto: Opens an email client to send an email.
<br>udp: User Datagram Protocol, used for low-latency communications (e.g., <a data-tooltip-position="top" aria-label="LCM - Lightweight Communications and Marshalling" data-href="LCM - Lightweight Communications and Marshalling" href="the-guide/computer-science/networks/lcm-lightweight-communications-and-marshalling.html" class="internal-link" target="_self" rel="noopener nofollow">|LCM</a>).
<br>ws: WebSocket, used for bidirectional communication in web applications.


<br><br>
<br>This is the address of the server where the resource is hosted. It can be a domain name (e.g., www.example.com) or an IP address (e.g., 192.168.1.1).
<br><br>
<br>This specifies the <a data-tooltip-position="top" aria-label="Port" data-href="Port" href="the-guide/computer-science/networks/port.html" class="internal-link" target="_self" rel="noopener nofollow">port number</a> on the host that is used for communication.
<br>The default ports are often implied based on the protocol:

<br>http uses port 80.
<br>https uses port 443.
<br>ftp uses port 21.


<br><br>
<br>The path identifies the specific resource or location on the server you want to access (like file path in a file system)
<br><br>
<br>The query part of the URL contains parameters or arguments that are passed to the resource. It usually follows a question mark (?).
<br>Multiple parameters are separated by an ampersand (&amp;).
<br>Often used in web applications to send data to the server (e.g., search queries, user identifiers, etc.).
<br><br>
<br>The fragment is an optional part of the URL, usually following a hash symbol (#), which points to a specific section or element within the resource.
<br>It’s commonly used in web pages to navigate to a specific part of the document, such as a heading or a div.
]]></description><link>the-guide/computer-science/networks/url-uniform-resource-locator.html</link><guid isPermaLink="false">The Guide/Computer Science/Networks/URL - Uniform Resource Locator.md</guid><pubDate>Fri, 17 Jan 2025 09:17:36 GMT</pubDate></item><item><title><![CDATA[CUDA]]></title><description><![CDATA[ 
 <br>
<br>Compute Unified Device Architecture for GPGUs

<br><img alt="center" src="lib/media/pasted-image-20230206155736.png" style="width: 550px; max-width: 100%;">
<br>Copying is bottleneck
<br><a data-tooltip-position="top" aria-label="GPU - Graphics Processing Unit" data-href="GPU - Graphics Processing Unit" href="the-guide/computer-science/gpu-graphics-processing-unit.html" class="internal-link" target="_self" rel="noopener nofollow">GPU</a> as device for host CPU

<br>Own DRAM
<br>Many threads in parallel

<br>Extremely lightweight with very little overhead in comparison to CPU




<br>Parallel portions of applications are called kernel

<br>Code in loop is called loop kernel, therefore the naming




<br>Problem Decomposition with CUDA

<br><img alt="center" src="lib/media/pasted-image-20230206155931.png" style="width: 500px; max-width: 100%;">

<br>Kernel computes grid
<br>Every block of grid and every element in block independent !

<br>Block  SM
<br>Elements  Threads

<br>Thread can compute multiple elements








<br>Software - Hardware Mapping

<br><img alt="center" src="lib/media/pasted-image-20230206160233.png" style="width: 600px; max-width: 100%;"> <img alt="center" src="lib/media/pasted-image-20230206161059.png" style="width: 550px; max-width: 100%;">

<br>Local memory is in global memory (no register, compiler managed), same speed 
<br>CUDA virtualizes multiprocessors, allows for scaling

<br>SM creates, manages, schedules and executes threads in groups of 32 - <a data-tooltip-position="top" aria-label="SIMT Architecture" data-href="SIMT Architecture" href="the-guide/computer-science/parallel-computing/simt-architecture.html" class="internal-link" target="_self" rel="noopener nofollow">warps</a>






<br>Scheduling

<br><img alt="center" src="lib/media/pasted-image-20230206160551.png" style="width: 400px; max-width: 100%;">

<br>Blocks must be able to execute independently
<br>Independent grid may execute concurrently given enough ressources
<br>Dependent grids sequentially, given by programming


<br>Parallel Execution

<br>Thread creation, scheduling and termination is handled by system
<br>Threads is a block execute concurrently and may synch at barriers

<br>Enables communication via shared memory


<br>Task Parallelism  independent grids
<br>Coarse Data Parallelism  independent blocks
<br>Fine Data Parallelism  concurrent threads




<br>Compilation

<br><img alt="center" src="lib/media/pasted-image-20230206163707.png" style="width: 400px; max-width: 100%;">
<br>Seperation of host and device code, compiled into binary or PTX

<br>Just-in-time

<br>PTX code loaded by application at runtime and compiled further
<br>Increases loading time
<br>Allows benefit from latest drivers


<br>Offline

<br>Direct compilation into binary




<br>Compatibility

<br>Binary

<br>Only between minor revisions of architecture


<br>PTX

<br>Between different architectures  generations
<br>Some instructions require high capabilities


<br>Application




<br>Synchronization CPU-GPU

<br><img alt="center" src="lib/media/pasted-image-20230206171133.png" style="width: 500px; max-width: 100%;">


<br>Barrier Synchronization

<br><img alt="center" src="lib/media/pasted-image-20230219182552.png" style="width: 500px; max-width: 100%;">

<br>Without synch race conditions between threads in different warps can occur




<br>Memory

<br><img alt="center" src="lib/media/pasted-image-20230206171540.png"><img alt="center" src="lib/media/pasted-image-20230206171554.png">


]]></description><link>the-guide/computer-science/parallel-computing/cuda.html</link><guid isPermaLink="false">The Guide/Computer Science/Parallel Computing/CUDA.md</guid><pubDate>Wed, 15 Jan 2025 14:11:06 GMT</pubDate><enclosure url="lib/media/pasted-image-20230206155736.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src=&quot;lib/media/pasted-image-20230206155736.png&quot;&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[Executing a CUDA Kernel]]></title><description><![CDATA[ 
 <br>In a Nutshell
Example of <a data-href="CUDA" href="the-guide/computer-science/parallel-computing/cuda.html" class="internal-link" target="_self" rel="noopener nofollow">CUDA</a>-Kernel running on a <a data-tooltip-position="top" aria-label="GPU - Graphics Processing Unit" data-href="GPU - Graphics Processing Unit" href="the-guide/computer-science/gpu-graphics-processing-unit.html" class="internal-link" target="_self" rel="noopener nofollow">GPU</a>.
<br><br><img alt="center" src="lib/media/pasted-image-20230206162134.png" style="width: 450px; max-width: 100%;"><br>
<img alt="center" src="lib/media/pasted-image-20230206162142.png" style="width: 450px; max-width: 100%;"><br>
<img alt="center" src="lib/media/pasted-image-20230206162150.png" style="width: 450px; max-width: 100%;"><br>
<img alt="center" src="lib/media/pasted-image-20230206162219.png" style="width: 450px; max-width: 100%;"><br>
<img alt="center" src="lib/media/pasted-image-20230206162233.png" style="width: 450px; max-width: 100%;"><br>
<img alt="center" src="lib/media/pasted-image-20230206162241.png" style="width: 450px; max-width: 100%;">]]></description><link>the-guide/computer-science/parallel-computing/executing-a-cuda-kernel.html</link><guid isPermaLink="false">The Guide/Computer Science/Parallel Computing/Executing a CUDA Kernel.md</guid><pubDate>Tue, 22 Apr 2025 15:11:18 GMT</pubDate><enclosure url="lib/media/pasted-image-20230206162134.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src=&quot;lib/media/pasted-image-20230206162134.png&quot;&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[Multiprocessing]]></title><description><![CDATA[ 
 <br>In a Nutshell
Use of two or more CPU cores to execute multiple processes simultaneously. Each process has its own memory space and operates independently.
<br><br>Advantages

<br>Fault isolation: A crash in one process does not affect others. 
<br>Avoids synchronization issues seen in multithreading.

<br>Disadvantages

<br>Higher memory usage due to separate memory spaces.
<br>Slower inter-process communication (IPC).
<br>More overhead for context switching.

]]></description><link>the-guide/computer-science/parallel-computing/multiprocessing.html</link><guid isPermaLink="false">The Guide/Computer Science/Parallel Computing/Multiprocessing.md</guid><pubDate>Sun, 26 Jan 2025 21:12:11 GMT</pubDate></item><item><title><![CDATA[Multithreading]]></title><description><![CDATA[ 
 <br>In a Nutshell
Ability of a <a data-tooltip-position="top" aria-label="CPU - Central Processing Unit" data-href="CPU - Central Processing Unit" href="the-guide/computer-science/cpu-central-processing-unit.html" class="internal-link" target="_self" rel="noopener nofollow">CPU</a> / a single core to execute multiple <a data-tooltip-position="top" aria-label="Threads vs. Processes" data-href="Threads vs. Processes" href="the-guide/computer-science/parallel-computing/threads-vs.-processes.html" class="internal-link" target="_self" rel="noopener nofollow">threads</a> concurrently within a single <a data-tooltip-position="top" aria-label="Threads vs. Processes" data-href="Threads vs. Processes" href="the-guide/computer-science/parallel-computing/threads-vs.-processes.html" class="internal-link" target="_self" rel="noopener nofollow">process</a>.
<br>]]></description><link>the-guide/computer-science/parallel-computing/multithreading.html</link><guid isPermaLink="false">The Guide/Computer Science/Parallel Computing/Multithreading.md</guid><pubDate>Fri, 20 Dec 2024 11:50:53 GMT</pubDate></item><item><title><![CDATA[SIMT Architecture]]></title><description><![CDATA[ 
 <br><br>
<br><img alt="center" src="lib/media/pasted-image-20230206163151.png">

<br>Branching execution can create holes  Divergence

<br>Full efficiency if all threads in a warp take same execution path
<br>First if, then else  equal length paths  effficient




<br><img alt="center" src="lib/media/pasted-image-20230206163201.png">

<br>Scheduling of threads by <a data-tooltip-position="top" aria-label="GPU - Graphics Processing Unit" data-href="GPU - Graphics Processing Unit" href="the-guide/computer-science/gpu-graphics-processing-unit.html" class="internal-link" target="_self" rel="noopener nofollow">MT-Scheduler</a>

<br>Status of waiting threads in register files
<br>Extremely fast scheduling of warps (in hardware)
<br>Thread blocks partitioned into warps based on thread id
<br>last warp can be padded with extra threads if not divisibe




<br><img alt="Pasted image 20230206171657.png" src="lib/media/pasted-image-20230206171657.png">]]></description><link>the-guide/computer-science/parallel-computing/simt-architecture.html</link><guid isPermaLink="false">The Guide/Computer Science/Parallel Computing/SIMT Architecture.md</guid><pubDate>Fri, 20 Dec 2024 08:29:35 GMT</pubDate><enclosure url="lib/media/pasted-image-20230206163151.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src=&quot;lib/media/pasted-image-20230206163151.png&quot;&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[Threads vs. Processes]]></title><description><![CDATA[ 
 <br>In a Nutshell

<br>Processes are isolated and have their own memory, making them suitable for running separate applications.  
<br>Threads are lightweight and share resources, making them ideal for concurrent tasks within the same application.  

<br><br><br>Process
A process is an independent program in execution. It has its own memory space, resources, and a unique Process ID (PID). They are used to run independent applications (e.g., a browser and a text editor). Characteristics are ...

<br>Heavyweight: Requires significant resources to start and maintain.
<br>Isolated: Processes do not share memory by default.
<br>Can consist of multiple threads.
<br>Context switching between processes is relatively expensive.

<br>
<br>
Key Lingo:

<br>Program Code: The set of instructions to execute.
<br>Process Memory: Includes stack, heap, and data sections.
<br>Process Control Block (PCB): Stores process-specific information (e.g., PID, state, priority).


<br>
Lifecycle of a Process:

<br>New: The process is created.   
<br>Ready: Waiting for CPU allocation.    
<br>Running: Instructions are being executed.    
<br>Waiting: Blocked, waiting for resources or events.    
<br>Terminated: Process has completed execution.   


<br><br><br>Thread
A thread is the smallest unit of execution within a process. Multiple threads can exist within a single process, sharing the same memory space. They are used to perform concurrent tasks in a single application (e.g., multiple tabs in a web browser). Characteristics are ...

<br>Lightweight: Threads share memory and resources of their parent process.     
<br>Shared Resources: Threads within a process share code, data, and files.
<br>Fast Context Switching: Less overhead compared to processes.

<a data-href="Multithreading" href="the-guide/computer-science/parallel-computing/multithreading.html" class="internal-link" target="_self" rel="noopener nofollow">Multithreading</a> involves running multiple threads concurrently within a single process, see <a data-href="Multithreading" href="the-guide/computer-science/parallel-computing/multithreading.html" class="internal-link" target="_self" rel="noopener nofollow">Multithreading</a> note.
<br>
<br>
Key Lingo:    

<br>Thread ID
<br>Program Counter
<br>Registers
<br>Stack (separate for each thread)


<br>
Types of Threads:

<br>User Threads: Managed by user-level libraries.
<br>Kernel Threads: Managed directly by the operating system.
<br>Daemon Thread: runs in the background, does not block program from exiting. usually not critical for main programs execution flow


<br>
Lifecycle of a Thread: Similar to processes: New, Ready, Running, Waiting, Terminated.

<br><br><br>]]></description><link>the-guide/computer-science/parallel-computing/threads-vs.-processes.html</link><guid isPermaLink="false">The Guide/Computer Science/Parallel Computing/Threads vs. Processes.md</guid><pubDate>Sun, 26 Jan 2025 21:07:21 GMT</pubDate></item><item><title><![CDATA[B-Splines and De Boor's Algorithm]]></title><description><![CDATA[ 
 <br>In a Nutshell
Piecewise-defined polynomials that can represent complex shapes with a high degree of smoothness and flexibility. Generalization of <a data-tooltip-position="top" aria-label="Bernstein Polynomials, Bézier Curves and De Casteljau Algorithm" data-href="Bernstein Polynomials, Bézier Curves and De Casteljau Algorithm" href="the-guide/computer-science/bernstein-polynomials,-bézier-curves-and-de-casteljau-algorithm.html" class="internal-link" target="_self" rel="noopener nofollow">Bézier curves</a>.
<br><br>Definition
For a <a data-tooltip-position="top" aria-label="Set" data-href="Set" href="the-guide/mathematics/general-stuff/set.html" class="internal-link" target="_self" rel="noopener nofollow">set</a> of  control points  and a <a data-tooltip-position="top" aria-label="Mathematical Relations" data-href="Mathematical Relations" href="the-guide/mathematics/general-stuff/mathematical-relations.html" class="internal-link" target="_self" rel="noopener nofollow">non-decreasing</a> knot-vector , where The B-Spline <a data-tooltip-position="top" aria-label="Raumkurven" data-href="Raumkurven" href="the-guide/mathematics/differential-geometry/elementary-differential-geometry/raumkurven.html" class="internal-link" target="_self" rel="noopener nofollow">curve</a> of order  is given by where the B-Spline basis <a data-tooltip-position="top" aria-label="Mathematical Relations" data-href="Mathematical Relations" href="the-guide/mathematics/general-stuff/mathematical-relations.html" class="internal-link" target="_self" rel="noopener nofollow">functions</a> are recursively defined via

<br>For  
<br>For  

<br>Continuity
A B-Spline curve of degree  is is -times continuously differentiable
<br>Convex Hull of Control Points
Every B-Spline basis functions are  non-negative for all  Therefore, the curve is always inside the <a data-tooltip-position="top" aria-label="Convexity" data-href="Convexity" href="the-guide/mathematics/optimization/convex-optimization-lecture/convexity.html" class="internal-link" target="_self" rel="noopener nofollow">convex hull</a> of its control points.
<br>Local Control
Each control point  influences the curve only within the interval .
<br>
<br>Contrast to <a data-tooltip-position="top" aria-label="Bernstein Polynomials, Bézier Curves and De Casteljau Algorithm" data-href="Bernstein Polynomials, Bézier Curves and De Casteljau Algorithm" href="the-guide/computer-science/bernstein-polynomials,-bézier-curves-and-de-casteljau-algorithm.html" class="internal-link" target="_self" rel="noopener nofollow">Bezier</a>, where control is global
<br>Partition of Unity
The B-Spline basis polynomials of any degree  form a <a data-tooltip-position="top" aria-label="Partition of Unity" data-href="Partition of Unity" href="the-guide/mathematics/general-stuff/partition-of-unity.html" class="internal-link" target="_self" rel="noopener nofollow">partition of unity</a>
<br>Intuition
Looking at the examples, we can see that the curve is a linear interpolation of a linear interpolation of a linear interpolation ... (depending on degree). 
<br><br><br><br>]]></description><link>the-guide/computer-science/b-splines-and-de-boor&apos;s-algorithm.html</link><guid isPermaLink="false">The Guide/Computer Science/B-Splines and De Boor&apos;s Algorithm.md</guid><pubDate>Mon, 09 Sep 2024 15:36:08 GMT</pubDate></item><item><title><![CDATA[Bernstein Polynomials, Bézier Curves and De Casteljau Algorithm]]></title><description><![CDATA[ 
 <br>In a Nutshell
A Bézier curve is a <a data-tooltip-position="top" aria-label="Raumkurven" data-href="Raumkurven" href="the-guide/mathematics/differential-geometry/elementary-differential-geometry/raumkurven.html" class="internal-link" target="_self" rel="noopener nofollow">parametric curve</a> frequently used in computer graphics and related fields. The curve is defined by a set of control points that are interpolated using Bernstein polynomials.
<br><br><br>Definition
A Bernstein basis polynomial of degree  is defined as 
<br><img alt="center" src="lib/media/pasted-image-20240718124823.png" style="width: 300px; max-width: 100%;"><br>Convex Hull of Control Points
Every Bernstein polynomial is non-negative on unit intervalTherefore, the curve is always inside the <a data-tooltip-position="top" aria-label="Convexity" data-href="Convexity" href="the-guide/mathematics/optimization/convex-optimization-lecture/convexity.html" class="internal-link" target="_self" rel="noopener nofollow">convex hull</a> of its control points.
<br>Recursive Derivative Rule
The derivative of Bernstein Polynomials can be written as a combination of two lower degree Bernstein polynomials
<br>Partition of Unity
The Bernstein basis polynomials of any degree  form a <a data-tooltip-position="top" aria-label="Partition of Unity" data-href="Partition of Unity" href="the-guide/mathematics/general-stuff/partition-of-unity.html" class="internal-link" target="_self" rel="noopener nofollow">partition of unity</a>
<br><br><br>Definition
For a <a data-tooltip-position="top" aria-label="Set" data-href="Set" href="the-guide/mathematics/general-stuff/set.html" class="internal-link" target="_self" rel="noopener nofollow">set</a> of  control points , a Bézier curve is given by where  are the Bernstein polynomials defined above.
<br>For example, we can consider ...<br>
<br>Linear Bézier Curves
<br>Quadratic Bézier Curves
<br>Cubic Bézier Curves
<br>...
<br>Intuition
Looking at the examples, we can see that the curve is a linear interpolation of a linear interpolation of a linear interpolation ... (depending on degree). 
]]></description><link>the-guide/computer-science/bernstein-polynomials,-bézier-curves-and-de-casteljau-algorithm.html</link><guid isPermaLink="false">The Guide/Computer Science/Bernstein Polynomials, Bézier Curves and De Casteljau Algorithm.md</guid><pubDate>Mon, 09 Sep 2024 15:36:08 GMT</pubDate><enclosure url="lib/media/pasted-image-20240718124823.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src=&quot;lib/media/pasted-image-20240718124823.png&quot;&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[CPU - Central Processing Unit]]></title><description><![CDATA[ 
 <br>In a Nutshell
Primary processor in a computer.
<br><br><br>Control Unit (CU)

<br>Directs operations of the CPU.
<br>Decodes instructions and controls the flow of data between CPU, memory, and peripherals.

<br>Arithmetic Logic Unit (ALU)

<br>Performs mathematical operations (addition, subtraction, etc.).
<br>Executes logical operations (AND, OR, NOT).

<br>Registers

<br>Small, fast storage locations inside the CPU.
<br>Types:

<br>Program Counter (PC): Tracks the address of the next instruction.
<br>Instruction Register (IR): Holds the current instruction being executed.
<br>Accumulator (ACC): Stores intermediate results of calculations.



<br>Cache

<br>High-speed memory close to the CPU.
<br>Stores frequently accessed data to reduce latency.

<br><br><br>RISC (Reduced Instruction Set Computer)

<br>Simple instructions, executed in a single clock cycle.
<br>High efficiency and speed.  

<br>CISC (Complex Instruction Set Computer)

<br>Complex instructions that perform multiple operations.  
<br>Reduces the number of instructions but increases cycles per instruction. 

<br><br><br><br><br><br>
<br>Hyper-threading: Allows one core to handle two threads, improving multitasking.    
<br>Overclocking: Running CPU at higher than rated speeds, increasing performance but generating more heat.
]]></description><link>the-guide/computer-science/cpu-central-processing-unit.html</link><guid isPermaLink="false">The Guide/Computer Science/CPU - Central Processing Unit.md</guid><pubDate>Sun, 26 Jan 2025 21:05:44 GMT</pubDate></item><item><title><![CDATA[GIL - Global Interpreter Lock]]></title><description><![CDATA[ 
 <br>In a Nutshell
Mechanism used in computer-language&nbsp;<a data-tooltip-position="top" aria-label="https://en.wikipedia.org/wiki/Interpreter_(computing)" rel="noopener nofollow" class="external-link" title="Interpreter (computing)" href="https://en.wikipedia.org/wiki/Interpreter_(computing)" target="_blank">interpreters</a>&nbsp;to synchronize the execution of&nbsp;<a data-tooltip-position="top" aria-label="Threads vs. Processes" data-href="Threads vs. Processes" href="the-guide/computer-science/parallel-computing/threads-vs.-processes.html" class="internal-link" target="_self" rel="noopener nofollow">threads</a>&nbsp;so that only one native thread (per process) can execute basic operations (such as&nbsp;<a data-tooltip-position="top" aria-label="https://en.wikipedia.org/wiki/Memory_allocation" rel="noopener nofollow" class="external-link" title="Memory allocation" href="https://en.wikipedia.org/wiki/Memory_allocation" target="_blank">memory allocation</a>&nbsp;and&nbsp;<a data-tooltip-position="top" aria-label="https://en.wikipedia.org/wiki/Reference_counting" rel="noopener nofollow" class="external-link" title="Reference counting" href="https://en.wikipedia.org/wiki/Reference_counting" target="_blank">reference counting</a>) at a time.
<br>As a general rule, an interpreter that uses GIL will see only one thread to execute at a time, even if it runs on a&nbsp;<a data-tooltip-position="top" aria-label="https://en.wikipedia.org/wiki/Multi-core_processor" rel="noopener nofollow" class="external-link" title="Multi-core processor" href="https://en.wikipedia.org/wiki/Multi-core_processor" target="_blank">multi-core processor</a>, although some implementations provide for&nbsp;<a data-tooltip-position="top" aria-label="https://en.wikipedia.org/wiki/CPU-bound" rel="noopener nofollow" class="external-link" title="CPU-bound" href="https://en.wikipedia.org/wiki/CPU-bound" target="_blank">CPU intensive</a>&nbsp;code to release the GIL.<br><br><br>As <a data-tooltip-position="top" aria-label="- Python Programming Language -" data-href="- Python Programming Language -" href="life,-universe-and-everything/python/-python-programming-language-.html" class="internal-link" target="_self" rel="noopener nofollow">Python</a> is an interpreted language, the most common implementations such as CPython have a GIL (there are specialized versions without it). In particular, the Garbage Collector is not thread-safe.<br>
<br>Concurrency Limitation: The GIL limits the ability to take full advantage of multiple CPU cores in multi-threaded programs. Even if you have multiple threads, only one thread can execute Python bytecode at a time in a single process, creating a bottleneck. Even if you spawn multiple threads, only one thread can execute the code at a time, which means using multiple threads won't speed up CPU-bound computations.<br>

<br>I/O-bound tasks: For I/O-bound programs (such as network operations, file I/O, or database queries), the GIL is less of an issue. While one thread is blocked waiting for I/O operations, other threads can run. This is because the GIL is released during blocking I/O operations like reading or writing to a file or making a network request. As a result, multi-threading can still be beneficial for I/O-bound tasks.
<br><br>
<br><a data-href="Multiprocessing Module (Python Standard Library)" href="life,-universe-and-everything/python/multiprocessing-module-(python-standard-library).html" class="internal-link" target="_self" rel="noopener nofollow">Multiprocessing Module (Python Standard Library)</a> allows to run multiple processes, each with own interpreter and GIL. However, inter-process communication is expensive.
]]></description><link>the-guide/computer-science/gil-global-interpreter-lock.html</link><guid isPermaLink="false">The Guide/Computer Science/GIL - Global Interpreter Lock.md</guid><pubDate>Wed, 23 Apr 2025 22:23:02 GMT</pubDate></item><item><title><![CDATA[GPU - Graphics Processing Unit]]></title><description><![CDATA[ 
 <br>In a Nutshell
Originally processing unit specialized on graphics and imaging. Modern GPUs are highly parallel, highly <a data-tooltip-position="top" aria-label="Multithreading" data-href="Multithreading" href="the-guide/computer-science/parallel-computing/multithreading.html" class="internal-link" target="_self" rel="noopener nofollow">multithreaded</a> multiprocessors able to perform a vast amount of computations in parallel. 
<br><br><br>
<br>Components<br>
<img alt="center" src="lib/media/pasted-image-20230206155032.png"><img alt="center" src="lib/media/pasted-image-20230206155122.png">
<br>Scalable Architecture

<br>More SM with fixed SP
<br>More SP per SM


<br>SP very simple, more complex calculations are executed by SFUs
<br>Instruction-Cache

<br>Same instruction on all SPs


<br>MT Issue 

<br>Scheduling of threads for SPs

<br>Many inactive threads per SP




<br>C-Cache 

<br>Read-only memory for all SPs


<br>Shared Memory

<br>Cache for whole SM


<br>How are GPU integrated ?

<br><img alt="center" src="lib/media/pasted-image-20230206155441.png" style="width: 400px; max-width: 100%;">
<br><img alt="center" src="lib/media/pasted-image-20230206155505.png" style="width: 400px; max-width: 100%;">


<br><br><br><img alt="center" src="lib/media/pasted-image-20230206154755.png" style="width: 450px; max-width: 100%;"><br><br>
<br>CPU

<br>Complex control Logic

<br>Scheduling for instruction level parallelism
<br>Out-of-order execution


<br>Large amount of main emory per core


<br>GPU

<br>Most part of chip estate for processing, less for control and cache

<br>Memory management in hand of programmer, because cache system small
<br>High computational intensity 
<br>power efficient




]]></description><link>the-guide/computer-science/gpu-graphics-processing-unit.html</link><guid isPermaLink="false">The Guide/Computer Science/GPU - Graphics Processing Unit.md</guid><pubDate>Wed, 12 Feb 2025 22:21:38 GMT</pubDate><enclosure url="lib/media/pasted-image-20230206155032.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src=&quot;lib/media/pasted-image-20230206155032.png&quot;&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[Monte Carlo Tree Search]]></title><description><![CDATA[ 
 <br>In a Nutshell
Heuristic-driven search algorithm for decision-making problems with vast discrete state-<a data-tooltip-position="top" aria-label="Space" data-href="Space" href="the-guide/mathematics/general-stuff/space.html" class="internal-link" target="_self" rel="noopener nofollow">spaces</a>.
<br><br><img alt="Pasted image 20250212231452.png" src="lib/media/pasted-image-20250212231452.png"><br>Algorithm

<br>Selection

<br>Start at node representing current state, select next node according to <a data-tooltip-position="top" aria-label="Policy" data-href="Policy" href="the-guide/machine-learning/reinforcement-learning/policy.html" class="internal-link" target="_self" rel="noopener nofollow">policy</a> (below)


<br>Expansion

<br>When reaching a leaf, expand it by one or more child nodes (next states)


<br>Simulation

<br>Play out randomized or heuristic-driven rollout to termination
<br>Record reward and result (win/loss)


<br>Backpropagation

<br>Update statistics for all nodes on the path 



<br><br><br>To address the <a data-href="Exploration-Exploitation Trade-Off" href="the-guide/machine-learning/reinforcement-learning/exploration-exploitation-trade-off.html" class="internal-link" target="_self" rel="noopener nofollow">Exploration-Exploitation Trade-Off</a>, a common approach is to employ the <a data-tooltip-position="top" aria-label="Bayesian Optimization" data-href="Bayesian Optimization" href="the-guide/mathematics/optimization/bayesian-optimization.html" class="internal-link" target="_self" rel="noopener nofollow">UCB aquisition function from Bayesian Optimization</a>. Each <a data-tooltip-position="top" aria-label="Node or Vertex Set" data-href="Node or Vertex Set" href="the-guide/mathematics/graph-theory/node-or-vertex-set.html" class="internal-link" target="_self" rel="noopener nofollow">node</a> storesand at each decision step the policy is]]></description><link>the-guide/computer-science/monte-carlo-tree-search.html</link><guid isPermaLink="false">The Guide/Computer Science/Monte Carlo Tree Search.md</guid><pubDate>Sat, 29 Mar 2025 16:19:04 GMT</pubDate><enclosure url="lib/media/pasted-image-20250212231452.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src=&quot;lib/media/pasted-image-20250212231452.png&quot;&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[XLA]]></title><description><![CDATA[ 
 <br>In a Nutshell
XLA (Accelerated Linear Algebra) is an open-source compiler for <a data-tooltip-position="top" aria-label="- Machine Learning -" data-href="- Machine Learning -" href="the-guide/machine-learning/-machine-learning-.html" class="internal-link" target="_self" rel="noopener nofollow">machine learning</a>. The XLA compiler takes models from popular frameworks such as <a data-tooltip-position="top" aria-label="- PyTorch -" data-href="- PyTorch -" href="life,-universe-and-everything/pytorch/-pytorch-.html" class="internal-link" target="_self" rel="noopener nofollow">PyTorch</a>, TensorFlow, and <a data-tooltip-position="top" aria-label="- JAX -" data-href="- JAX -" href="life,-universe-and-everything/jax/-jax-.html" class="internal-link" target="_self" rel="noopener nofollow">JAX</a>, and optimizes the models for high-performance execution across different hardware platforms including <a data-tooltip-position="top" aria-label="GPU - Graphics Processing Unit" data-href="GPU - Graphics Processing Unit" href="the-guide/computer-science/gpu-graphics-processing-unit.html" class="internal-link" target="_self" rel="noopener nofollow">GPUs</a>, CPUs, and ML accelerators.
<br><br>...]]></description><link>the-guide/computer-science/xla.html</link><guid isPermaLink="false">The Guide/Computer Science/XLA.md</guid><pubDate>Thu, 27 Feb 2025 12:04:57 GMT</pubDate></item><item><title><![CDATA[Information Bottleneck Principle]]></title><description><![CDATA[ 
 <br>Find a maximally compressed mapping of the input <a data-tooltip-position="top" aria-label="Random Variable" data-href="Random Variable" href="the-guide/mathematics/probability-theory/random-variable.html" class="internal-link" target="_self" rel="noopener nofollow">random variable</a>  that preserves as much relevant information (here defined as the <a data-tooltip-position="top" aria-label="Mutual Information" data-href="Mutual Information" href="the-guide/information-theory/information-theory-1/information/mutual-information.html" class="internal-link" target="_self" rel="noopener nofollow">mutual information</a>) about the output  as possible.<br>
<br>Assume <a data-tooltip-position="top" aria-label="Statistical Independence" data-href="Statistical Independence" href="the-guide/mathematics/statistics/statistical-independence.html" class="internal-link" target="_self" rel="noopener nofollow">statistical dependence</a> between input and output
<br> implicitly determines the relevant and irrelevant features in  via their <a data-tooltip-position="top" aria-label="Probability Distribution" data-href="Probability Distribution" href="the-guide/mathematics/probability-theory/probability-distribution.html" class="internal-link" target="_self" rel="noopener nofollow">joint distribution</a>

<br>Compression by dismissing irrelevant parts 


<br>The relevant part of  is denoted  and is a <a data-tooltip-position="top" aria-label="Statistic" data-href="Statistic" href="the-guide/mathematics/statistics/statistic.html" class="internal-link" target="_self" rel="noopener nofollow">minimal sufficient statistic</a> of  with respect to , namely the simplest mapping that captures the<a data-tooltip-position="top" aria-label="Mutual Information" data-href="Mutual Information" href="the-guide/information-theory/information-theory-1/information/mutual-information.html" class="internal-link" target="_self" rel="noopener nofollow">mutual information</a> . Assuming the <a data-tooltip-position="top" aria-label="Markov Chain" data-href="Markov Chain" href="the-guide/computational-statistics/data-assimilation/markov-chain.html" class="internal-link" target="_self" rel="noopener nofollow">Markov chain</a> , we therefore have to minimize  for the minimal statistic while constraining with  for capturing information. Finding this optimal representation  is formulized via <br>
<br>Additional Markov chain constraint
<br>Parameter  is tradeoff parameter between complexity of representation and preserved relevant information
<br>...
]]></description><link>the-guide/information-theory/information-bottleneck-theory/information-bottleneck-principle.html</link><guid isPermaLink="false">The Guide/Information Theory/Information Bottleneck Theory/Information Bottleneck Principle.md</guid><pubDate>Tue, 12 Nov 2024 14:04:23 GMT</pubDate></item><item><title><![CDATA[Input Compression Bound]]></title><description><![CDATA[ 
 <br>
<br>Given all possible data, cluster in cells (pink) that are homogeneous with regard to the label, such that error of having wrong label in cell is smaller than an <img alt="center" src="lib/media/pasted-image-20230426162956.png" style="width: 200px; max-width: 100%;">
<br>Estimate cardinality of partition 

<br>

<br>Generalization error , probability of making error on new data
<br>Number of training samples 
<br>Confidence , probability of seeing bad sample


<br> bits of compression of  is like having factor  more training data


]]></description><link>the-guide/information-theory/information-bottleneck-theory/input-compression-bound.html</link><guid isPermaLink="false">The Guide/Information Theory/Information Bottleneck Theory/Input Compression Bound.md</guid><pubDate>Mon, 12 Aug 2024 17:07:21 GMT</pubDate><enclosure url="lib/media/pasted-image-20230426162956.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src=&quot;lib/media/pasted-image-20230426162956.png&quot;&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[Bregman Divergence]]></title><description><![CDATA[ 
 <br>Measure of difference between two points, defined in terms of a strictly <a data-tooltip-position="top" aria-label="Convexity" data-href="Convexity" href="the-guide/mathematics/optimization/convex-optimization-lecture/convexity.html" class="internal-link" target="_self" rel="noopener nofollow">convex function</a>, they form an important class of <a data-tooltip-position="top" aria-label="Divergence" data-href="Divergence" href="the-guide/information-theory/information-geometry/divergence.html" class="internal-link" target="_self" rel="noopener nofollow">divergences</a>. When the points are interpreted as <a data-tooltip-position="top" aria-label="Probability Distribution" data-href="Probability Distribution" href="the-guide/mathematics/probability-theory/probability-distribution.html" class="internal-link" target="_self" rel="noopener nofollow">probability distributions</a> – notably as either values of the parameter of a <a data-tooltip-position="top" aria-label="https://en.wikipedia.org/wiki/Parametric_model" rel="noopener nofollow" class="external-link" title="Parametric model" href="https://en.wikipedia.org/wiki/Parametric_model" target="_blank">parametric model</a> or as a data set of observed values – the resulting distance is a <a data-tooltip-position="top" aria-label="https://en.wikipedia.org/wiki/Statistical_distance" rel="noopener nofollow" class="external-link" title="Statistical distance" href="https://en.wikipedia.org/wiki/Statistical_distance" target="_blank">statistical distance</a>. The most basic Bregman divergence is the <a data-tooltip-position="top" aria-label="https://en.wikipedia.org/wiki/Squared_Euclidean_distance" rel="noopener nofollow" class="external-link" title="Squared Euclidean distance" href="https://en.wikipedia.org/wiki/Squared_Euclidean_distance" target="_blank">squared Euclidean distance</a>.<br>Bregman divergences are similar to <a data-tooltip-position="top" aria-label="https://en.wikipedia.org/wiki/Metric_(mathematics)" rel="noopener nofollow" class="external-link" title="Metric (mathematics)" href="https://en.wikipedia.org/wiki/Metric_(mathematics)" target="_blank">metrics</a>, but satisfy neither the <a data-tooltip-position="top" aria-label="https://en.wikipedia.org/wiki/Triangle_inequality" rel="noopener nofollow" class="external-link" title="Triangle inequality" href="https://en.wikipedia.org/wiki/Triangle_inequality" target="_blank">triangle inequality</a> (ever) nor symmetry (in general). However, they satisfy a generalization of the <a data-tooltip-position="top" aria-label="https://en.wikipedia.org/wiki/Pythagorean_theorem" rel="noopener nofollow" class="external-link" title="Pythagorean theorem" href="https://en.wikipedia.org/wiki/Pythagorean_theorem" target="_blank">Pythagorean theorem</a>.<br><br>Let  be a continuously-differentiable, strictly <a data-tooltip-position="top" aria-label="Convexity" data-href="Convexity" href="the-guide/mathematics/optimization/convex-optimization-lecture/convexity.html" class="internal-link" target="_self" rel="noopener nofollow">convex</a> function defined on a <a data-tooltip-position="top" aria-label="Convexity" data-href="Convexity" href="the-guide/mathematics/optimization/convex-optimization-lecture/convexity.html" class="internal-link" target="_self" rel="noopener nofollow">convex set</a> . The Bregman distance associated with the function  for points  is the difference between the value of  at point  and the value of the first-order <a data-tooltip-position="top" aria-label="https://en.wikipedia.org/wiki/Taylor_expansion" rel="noopener nofollow" class="external-link" href="https://en.wikipedia.org/wiki/Taylor_expansion" target="_blank">Taylor expansion</a> of  around point  evaluated at point : <br>
<br>Properties

<br>Non-negative
<br>Not commutative
<br><a data-tooltip-position="top" aria-label="Convexity" data-href="Convexity" href="the-guide/mathematics/optimization/convex-optimization-lecture/convexity.html" class="internal-link" target="_self" rel="noopener nofollow">Convex</a> in first argument, not necessarily in second
<br>Linearity


<br><br><br>
<br>For  the associated divergence is 
<br>For the negative <a data-tooltip-position="top" aria-label="Shannon Entropy" data-href="Shannon Entropy" href="the-guide/information-theory/information-theory-1/information/shannon-entropy.html" class="internal-link" target="_self" rel="noopener nofollow">entropy</a>-function , the associated divergence is the (generalized) <a data-tooltip-position="top" aria-label="Kullback-Leibler Divergence" data-href="Kullback-Leibler Divergence" href="the-guide/information-theory/information-theory-1/information/kullback-leibler-divergence.html" class="internal-link" target="_self" rel="noopener nofollow">KL-Divergence</a>
<br><br><br>Bregman projection of  onto set  under function  defined as <br>
<br>In contrast to euclidean geometry, bregman uses general curved geometry<img alt="center" src="lib/media/pasted-image-20230603121713.png" style="width: 300px; max-width: 100%;">
]]></description><link>the-guide/information-theory/information-geometry/bregman-divergence.html</link><guid isPermaLink="false">The Guide/Information Theory/Information Geometry/Bregman Divergence.md</guid><pubDate>Mon, 12 Aug 2024 17:07:21 GMT</pubDate><enclosure url="lib/media/pasted-image-20230603121713.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src=&quot;lib/media/pasted-image-20230603121713.png&quot;&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[Divergence]]></title><description><![CDATA[ 
 <br>In a Nutshell
Statistical distance in <a data-href="Information Geometry" href="the-guide/information-theory/information-geometry/information-geometry.html" class="internal-link" target="_self" rel="noopener nofollow">Information Geometry</a> that establishes the separation from one <a data-tooltip-position="top" aria-label="Probability Distribution" data-href="Probability Distribution" href="the-guide/mathematics/probability-theory/probability-distribution.html" class="internal-link" target="_self" rel="noopener nofollow">probability distribution</a> to another on a <a data-tooltip-position="top" aria-label="Statistical Manifolds" data-href="Statistical Manifolds" href="the-guide/information-theory/information-geometry/statistical-manifolds.html" class="internal-link" target="_self" rel="noopener nofollow">statistical manifolds</a>.
<br><br>Definition
Given a <a data-tooltip-position="top" aria-label="Mannigfaltigkeiten" data-href="Mannigfaltigkeiten" href="the-guide/mathematics/differential-geometry/riemannian-geometry/mannigfaltigkeiten-und-differenzierbare-abbildungen/mannigfaltigkeiten.html" class="internal-link" target="_self" rel="noopener nofollow">differentiable manifold</a>  of dimension , a divergence on  is a -function  satisfying:

<br>Non-negativity 
<br>Positivity 
<br>At every point ,  is a <a data-tooltip-position="top" aria-label="Matrices" data-href="Matrices" href="the-guide/mathematics/linear-algebra/matrices.html" class="internal-link" target="_self" rel="noopener nofollow">positive-definite</a> quadratic form for infinitesimal displacements  from .<br>
In applications to statistics, the manifold  is typically the space of parameters of a <a data-tooltip-position="top" aria-label="https://en.wikipedia.org/wiki/Parametric_family" rel="noopener nofollow" class="external-link" title="Parametric family" href="https://en.wikipedia.org/wiki/Parametric_family" target="_blank">parametric family of probability distributions</a>.

]]></description><link>the-guide/information-theory/information-geometry/divergence.html</link><guid isPermaLink="false">The Guide/Information Theory/Information Geometry/Divergence.md</guid><pubDate>Mon, 12 Aug 2024 17:07:21 GMT</pubDate></item><item><title><![CDATA[Information Geometry]]></title><description><![CDATA[ 
 <br>Information geometry is an interdisciplinary field that applies the techniques of differential geometry to study probability theory and statistics. It studies statistical manifolds ...<br><br><br>
<br><a rel="noopener nofollow" class="external-link" href="https://math.ucr.edu/home/baez/information/" target="_blank">https://math.ucr.edu/home/baez/information/</a>
<br>
]]></description><link>the-guide/information-theory/information-geometry/information-geometry.html</link><guid isPermaLink="false">The Guide/Information Theory/Information Geometry/Information Geometry.md</guid><pubDate>Mon, 12 Aug 2024 17:07:21 GMT</pubDate></item><item><title><![CDATA[Jensen-Shannon Divergence]]></title><description><![CDATA[ 
 <br>Definition
<a data-href="Divergence" href="the-guide/information-theory/information-geometry/divergence.html" class="internal-link" target="_self" rel="noopener nofollow">Divergence</a> based on the <a data-tooltip-position="top" aria-label="Kullback-Leibler Divergence" data-href="Kullback-Leibler Divergence" href="the-guide/information-theory/information-theory-1/information/kullback-leibler-divergence.html" class="internal-link" target="_self" rel="noopener nofollow">KL divergence</a> that measures similarity of <a data-tooltip-position="top" aria-label="Probability Distribution" data-href="Probability Distribution" href="the-guide/mathematics/probability-theory/probability-distribution.html" class="internal-link" target="_self" rel="noopener nofollow">probability distributions</a>. In contrast to the regular <a data-tooltip-position="top" aria-label="Kullback-Leibler Divergence" data-href="Kullback-Leibler Divergence" href="the-guide/information-theory/information-theory-1/information/kullback-leibler-divergence.html" class="internal-link" target="_self" rel="noopener nofollow">KL divergence</a>, it is symmetric and finite. For <a data-tooltip-position="top" aria-label="Probability Distribution" data-href="Probability Distribution" href="the-guide/mathematics/probability-theory/probability-distribution.html" class="internal-link" target="_self" rel="noopener nofollow">distributions</a>  and , it is defined via where  is a <a data-tooltip-position="top" aria-label="Mixture Distribution" data-href="Mixture Distribution" href="the-guide/mathematics/probability-theory/mixture-distribution.html" class="internal-link" target="_self" rel="noopener nofollow">mixture distribution</a> of  and . 
<br><br><br>The Jensen-Shannon Divergence is the <a data-tooltip-position="top" aria-label="Mutual Information" data-href="Mutual Information" href="the-guide/information-theory/information-theory-1/information/mutual-information.html" class="internal-link" target="_self" rel="noopener nofollow">mutual information</a> between a <a data-tooltip-position="top" aria-label="Random Variable" data-href="Random Variable" href="the-guide/mathematics/probability-theory/random-variable.html" class="internal-link" target="_self" rel="noopener nofollow">random variable</a> associated with  and a binary indicator <a data-tooltip-position="top" aria-label="Mathematical Relations" data-href="Mathematical Relations" href="the-guide/mathematics/general-stuff/mathematical-relations.html" class="internal-link" target="_self" rel="noopener nofollow">function</a> , which chooses uniformly between  and :<br>Intuition

<br>Value is chosen by first realizing  and then the distribution chosen by 
<br>If  and  are identically distributed, the realization of  does not yield any information and therefore does not reduce the uncertainty via the <a data-tooltip-position="top" aria-label="Shannon Entropy" data-href="Shannon Entropy" href="the-guide/information-theory/information-theory-1/information/shannon-entropy.html" class="internal-link" target="_self" rel="noopener nofollow">conditional entropy</a> ()
<br>If  and  have a different support (no problem with KL since defined via ), the indicator function yields disjoint paths in a binary decision and therefore  bit of information 

]]></description><link>the-guide/information-theory/information-geometry/jensen-shannon-divergence.html</link><guid isPermaLink="false">The Guide/Information Theory/Information Geometry/Jensen-Shannon Divergence.md</guid><pubDate>Sat, 08 Feb 2025 10:07:24 GMT</pubDate></item><item><title><![CDATA[Statistical Manifolds]]></title><description><![CDATA[ 
 ]]></description><link>the-guide/information-theory/information-geometry/statistical-manifolds.html</link><guid isPermaLink="false">The Guide/Information Theory/Information Geometry/Statistical Manifolds.md</guid><pubDate>Mon, 12 Aug 2024 17:07:21 GMT</pubDate></item><item><title><![CDATA[Channel Capacity]]></title><description><![CDATA[ 
 <br>The average amount of information exchanged over the <a data-tooltip-position="top" aria-label="Discrete Memoryless Channel" data-href="Discrete Memoryless Channel" href="the-guide/information-theory/information-theory-1/channel-coding/discrete-memoryless-channel.html" class="internal-link" target="_self" rel="noopener nofollow">channel</a>between two symbols  und  is given by the <a data-tooltip-position="top" aria-label="Mutual Information" data-href="Mutual Information" href="the-guide/information-theory/information-theory-1/information/mutual-information.html" class="internal-link" target="_self" rel="noopener nofollow">mutual information</a>. The <a data-tooltip-position="top" aria-label="Discrete Memoryless Channel" data-href="Discrete Memoryless Channel" href="the-guide/information-theory/information-theory-1/channel-coding/discrete-memoryless-channel.html" class="internal-link" target="_self" rel="noopener nofollow">channel</a> capacity is therefore defined as the maximum <a data-tooltip-position="top" aria-label="Mutual Information" data-href="Mutual Information" href="the-guide/information-theory/information-theory-1/information/mutual-information.html" class="internal-link" target="_self" rel="noopener nofollow">mutual information</a> in any single use of the channel  The unit is bits per channel use, the same as the <a data-tooltip-position="top" aria-label="Code Rate" data-href="Code Rate" href="the-guide/information-theory/information-theory-1/channel-coding/code-rate.html" class="internal-link" target="_self" rel="noopener nofollow">code rate</a>.<br>
<br>Properties

<br> since . 
<br> and 

<br> and 
<br>
<br> is always finite for finite alphabet <a data-tooltip-position="top" aria-label="Random Variable" data-href="Random Variable" href="the-guide/mathematics/probability-theory/random-variable.html" class="internal-link" target="_self" rel="noopener nofollow">sources</a>. 


<br>The mutual information is a <a data-tooltip-position="top" aria-label="Convexity" data-href="Convexity" href="the-guide/mathematics/optimization/convex-optimization-lecture/convexity.html" class="internal-link" target="_self" rel="noopener nofollow">concave</a> function and therefore has one global maximum.


<br>Remarks

<br>Highest <a data-tooltip-position="top" aria-label="Code Rate" data-href="Code Rate" href="the-guide/information-theory/information-theory-1/channel-coding/code-rate.html" class="internal-link" target="_self" rel="noopener nofollow">rate</a> at which information can be sent with arbitrarily low probability of error.
<br>While the <a data-tooltip-position="top" aria-label="Mutual Information" data-href="Mutual Information" href="the-guide/information-theory/information-theory-1/information/mutual-information.html" class="internal-link" target="_self" rel="noopener nofollow">mutual information</a> depends not only on the channel itself, but also on the way in which the channel is used (i.e., on the input source), the channel capacity only depends on the channel because of the maximization over the channel-independent input distribution. 


<br><br><br>
<br>Gaussian Channel

<br><a data-href="Discrete Memoryless Channel" href="the-guide/information-theory/information-theory-1/channel-coding/discrete-memoryless-channel.html" class="internal-link" target="_self" rel="noopener nofollow">Discrete Memoryless Channel</a> that simply adds gaussian noise with noise power  to the input <a data-tooltip-position="top" aria-label="Random Variable" data-href="Random Variable" href="the-guide/mathematics/probability-theory/random-variable.html" class="internal-link" target="_self" rel="noopener nofollow">variable</a> Without constraint, the capacity is infinite, but not practical (make  much larger than ). Using a transmit power constraint , the <a data-tooltip-position="top" aria-label="Channel Capacity" data-href="Channel Capacity" href="the-guide/information-theory/information-theory-1/channel-coding/channel-capacity.html" class="internal-link" target="_self" rel="noopener nofollow">channel capacity</a> yields  and is maximized when .
<br>Remarks

<br>In this case , as noise and input are <a data-tooltip-position="top" aria-label="Statistical Independence" data-href="Statistical Independence" href="the-guide/mathematics/statistics/statistical-independence.html" class="internal-link" target="_self" rel="noopener nofollow">i.i.d.</a>
<br>




<br>Bandlimited Channel

<br>The output of an continuous-time bandlimited channel with additive white Gaussian Noise can be described as  with  being the impulse response (of an ideal bandpass with cutoff frequency B). Using the <a data-tooltip-position="top" aria-label="Channel Capacity" data-href="Channel Capacity" href="the-guide/information-theory/information-theory-1/channel-coding/channel-capacity.html" class="internal-link" target="_self" rel="noopener nofollow">capacity</a> of the <a data-tooltip-position="top" aria-label="Channel Capacity" data-href="Channel Capacity" href="the-guide/information-theory/information-theory-1/channel-coding/channel-capacity.html" class="internal-link" target="_self" rel="noopener nofollow">gaussian channel</a> in a setting where  is bandlimited, the capacity yields 

<br>
<br> samples need to be taken every second


<br>Remarks

<br>Dependancy of C on B is approximately linear for small bandwidth, dependancy on SNR (log term) is logarithmic  more efficient to increase capacity by expanding bandwidth
<br> if , no information can be transmitted without any signal
<br> if , infinite transmission rate without noise




]]></description><link>the-guide/information-theory/information-theory-1/channel-coding/channel-capacity.html</link><guid isPermaLink="false">The Guide/Information Theory/Information Theory 1/Channel Coding/Channel Capacity.md</guid><pubDate>Wed, 23 Apr 2025 21:50:56 GMT</pubDate></item><item><title><![CDATA[Channel Coding Theorem]]></title><description><![CDATA[ 
 <br>"Which condition does the <a data-tooltip-position="top" aria-label="Code Rate" data-href="Code Rate" href="the-guide/information-theory/information-theory-1/channel-coding/code-rate.html" class="internal-link" target="_self" rel="noopener nofollow">data rate</a>  have to satisfy to guarantee (almost) error-free transmission ?"<br><img alt="Pasted image 20230212164756.png" src="lib/media/pasted-image-20230212164756.png"><br>Let information be transmitted through a <a data-tooltip-position="top" aria-label="Discrete Memoryless Channel" data-href="Discrete Memoryless Channel" href="the-guide/information-theory/information-theory-1/channel-coding/discrete-memoryless-channel.html" class="internal-link" target="_self" rel="noopener nofollow">discrete memoryless channel</a> of <a data-tooltip-position="top" aria-label="Channel Capacity" data-href="Channel Capacity" href="the-guide/information-theory/information-theory-1/channel-coding/channel-capacity.html" class="internal-link" target="_self" rel="noopener nofollow">capacity</a> . If the <a data-tooltip-position="top" aria-label="Code Rate" data-href="Code Rate" href="the-guide/information-theory/information-theory-1/channel-coding/code-rate.html" class="internal-link" target="_self" rel="noopener nofollow">transmission rate</a> (bpcu) then there exists a channel coding scheme (a sequence of  codes), for which the source output can be transmitted over the channel with an arbitrarily small probability of error (a probability of error  as ).<br><br>
<br>Conversely, if  then it is impossible to transmit information over the <a data-tooltip-position="top" aria-label="Discrete Memoryless Channel" data-href="Discrete Memoryless Channel" href="the-guide/information-theory/information-theory-1/channel-coding/discrete-memoryless-channel.html" class="internal-link" target="_self" rel="noopener nofollow">channel</a> with an arbitrarily small probability of error.<br>
<img alt="Screenshot from 2023-02-11 22-28-29 1.png" src="lib/media/screenshot-from-2023-02-11-22-28-29-1.png">
<br>Feedback can help with encoding and decoding, but has no influence on the channel capacity.
]]></description><link>the-guide/information-theory/information-theory-1/channel-coding/channel-coding-theorem.html</link><guid isPermaLink="false">The Guide/Information Theory/Information Theory 1/Channel Coding/Channel Coding Theorem.md</guid><pubDate>Wed, 23 Apr 2025 21:50:56 GMT</pubDate><enclosure url="lib/media/pasted-image-20230212164756.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src=&quot;lib/media/pasted-image-20230212164756.png&quot;&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[Code Rate]]></title><description><![CDATA[ 
 <br>
<br>Source Coding

<br>Assume a <a data-tooltip-position="top" aria-label="Set" data-href="Set" href="the-guide/mathematics/general-stuff/set.html" class="internal-link" target="_self" rel="noopener nofollow">set</a> of possible messages  and we want to represent each possible message by a string of  symbols from our code alphabet  (e.g.  and ).  The rate of an  code is   where the numerator is the number of symbols from the source alphabet needed to encode the message. The rate therefore is the average number of bits (in the original message) that each symbol of the code is responsible for representing.
<br>Lossless data compression possible, if 


<br>Channel Coding - Information Rate

<br>Proportion of data stream that is non-redundant
<br>Transmission rate over a <a data-tooltip-position="top" aria-label="Discrete Memoryless Channel" data-href="Discrete Memoryless Channel" href="the-guide/information-theory/information-theory-1/channel-coding/discrete-memoryless-channel.html" class="internal-link" target="_self" rel="noopener nofollow">channel</a> in  or bits per symbol.  
<br>Average <a data-tooltip-position="top" aria-label="Shannon Entropy" data-href="Shannon Entropy" href="the-guide/information-theory/information-theory-1/information/shannon-entropy.html" class="internal-link" target="_self" rel="noopener nofollow">entropy</a> per symbol
<br>The channel encoder maps one of  possible codewords uniquely to a codeword of length .


]]></description><link>the-guide/information-theory/information-theory-1/channel-coding/code-rate.html</link><guid isPermaLink="false">The Guide/Information Theory/Information Theory 1/Channel Coding/Code Rate.md</guid><pubDate>Wed, 23 Apr 2025 21:50:56 GMT</pubDate></item><item><title><![CDATA[Data Processing Inequality]]></title><description><![CDATA[ 
 <br>"There is no way to increase the amount of <a data-tooltip-position="top" aria-label="Mutual Information" data-href="Mutual Information" href="the-guide/information-theory/information-theory-1/information/mutual-information.html" class="internal-link" target="_self" rel="noopener nofollow">information</a> that  has about ."<br>If three <a data-tooltip-position="top" aria-label="Random Variable" data-href="Random Variable" href="the-guide/mathematics/probability-theory/random-variable.html" class="internal-link" target="_self" rel="noopener nofollow">random sources</a>  and  form a <a data-href="Markov Chain" href="the-guide/computational-statistics/data-assimilation/markov-chain.html" class="internal-link" target="_self" rel="noopener nofollow">Markov Chain</a> , then<br>
<br> can be thought of as some sort of processing 
]]></description><link>the-guide/information-theory/information-theory-1/channel-coding/data-processing-inequality.html</link><guid isPermaLink="false">The Guide/Information Theory/Information Theory 1/Channel Coding/Data Processing Inequality.md</guid><pubDate>Wed, 23 Apr 2025 21:50:56 GMT</pubDate></item><item><title><![CDATA[Discrete Memoryless Channel]]></title><description><![CDATA[ 
 <br>Counterpart of discrete memoryless <a data-tooltip-position="top" aria-label="Random Variable" data-href="Random Variable" href="the-guide/mathematics/probability-theory/random-variable.html" class="internal-link" target="_self" rel="noopener nofollow">source</a>:<br>
<br>Discrete = input and output source have finite alphabet size
<br>Memoryless = current output only depends on current input
<br>The Channelrelates the<br>
- input <a data-tooltip-position="top" aria-label="Random Variable" data-href="Random Variable" href="the-guide/mathematics/probability-theory/random-variable.html" class="internal-link" target="_self" rel="noopener nofollow">source</a>  with   and <br>
to the<br>
- output <a data-tooltip-position="top" aria-label="Random Variable" data-href="Random Variable" href="the-guide/mathematics/probability-theory/random-variable.html" class="internal-link" target="_self" rel="noopener nofollow">source</a>  with  and <br>
through <a data-tooltip-position="top" aria-label="Probability Distribution" data-href="Probability Distribution" href="the-guide/mathematics/probability-theory/probability-distribution.html" class="internal-link" target="_self" rel="noopener nofollow">transition probabilities</a> :<br>

<br><br><br>
<br><a data-tooltip-position="top" aria-label="Matrices" data-href="Matrices" href="the-guide/mathematics/linear-algebra/matrices.html" class="internal-link" target="_self" rel="noopener nofollow">Matrix</a> describing the mentioned <a data-tooltip-position="top" aria-label="Probability Distribution" data-href="Probability Distribution" href="the-guide/mathematics/probability-theory/probability-distribution.html" class="internal-link" target="_self" rel="noopener nofollow">transition probabilities</a> of a <a data-tooltip-position="top" aria-label="Discrete Memoryless Channel" data-href="Discrete Memoryless Channel" href="the-guide/information-theory/information-theory-1/channel-coding/discrete-memoryless-channel.html" class="internal-link" target="_self" rel="noopener nofollow">channel</a>

<br>
<br>row = input
<br>column = output


<br>This matrix is said to be symmetric if the rows and columns are permutations of each other.
<br><br><br>
<br>Binary Symmetric Channel

<br><img alt="Pasted image 20221119204822.png" src="lib/media/pasted-image-20221119204822.png" style="width: 400px; max-width: 100%;">
<br>The <a data-tooltip-position="top" aria-label="Channel Capacity" data-href="Channel Capacity" href="the-guide/information-theory/information-theory-1/channel-coding/channel-capacity.html" class="internal-link" target="_self" rel="noopener nofollow">channel capacity</a> of the binary symmetric channel is 
<br>Remarks

<br>Worst channel error if 
<br>Best channel error if  or 




<br>Ternary Symmetric Channel

<br><img alt="Pasted image 20221118135549.png" src="lib/media/pasted-image-20221118135549.png" style="width: 300px; max-width: 100%;">
<br>The <a data-tooltip-position="top" aria-label="Channel Capacity" data-href="Channel Capacity" href="the-guide/information-theory/information-theory-1/channel-coding/channel-capacity.html" class="internal-link" target="_self" rel="noopener nofollow">channel capacity</a> of this channel is 


<br>Noisy Typewriter Channel

<br>Channel with  chance to pick next symbol instead.

<br>




<br>Binary Erasure Channel

<br>


]]></description><link>the-guide/information-theory/information-theory-1/channel-coding/discrete-memoryless-channel.html</link><guid isPermaLink="false">The Guide/Information Theory/Information Theory 1/Channel Coding/Discrete Memoryless Channel.md</guid><pubDate>Wed, 23 Apr 2025 21:50:56 GMT</pubDate><enclosure url="lib/media/pasted-image-20221119204822.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src=&quot;lib/media/pasted-image-20221119204822.png&quot;&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[Fano's Inequality]]></title><description><![CDATA[ 
 <br>Suppose we observe a <a data-tooltip-position="top" aria-label="Random Variable" data-href="Random Variable" href="the-guide/mathematics/probability-theory/random-variable.html" class="internal-link" target="_self" rel="noopener nofollow">random variable</a> (source) Y and we want to use it to guess the value of another (correlated) hidden random variable X . This can be modeled as a <a data-href="Markov Chain" href="the-guide/computational-statistics/data-assimilation/markov-chain.html" class="internal-link" target="_self" rel="noopener nofollow">Markov Chain</a> .<br>With the guess , the probability of error  in an alphabet of size  is related to the <a data-tooltip-position="top" aria-label="Shannon Entropy" data-href="Shannon Entropy" href="the-guide/information-theory/information-theory-1/information/shannon-entropy.html" class="internal-link" target="_self" rel="noopener nofollow">conditional entropy</a> via <br><br>
<br> implies full correlation. 
<br>If the conditional entropy increases, the error probability grows.
<br>A stronger version yields 
]]></description><link>the-guide/information-theory/information-theory-1/channel-coding/fano&apos;s-inequality.html</link><guid isPermaLink="false">The Guide/Information Theory/Information Theory 1/Channel Coding/Fano&apos;s Inequality.md</guid><pubDate>Wed, 23 Apr 2025 21:50:56 GMT</pubDate></item><item><title><![CDATA[Joint Typicality]]></title><description><![CDATA[ 
 <br>Extension of <a data-tooltip-position="top" aria-label="Typical Sets" data-href="Typical Sets" href="the-guide/information-theory/information-theory-1/data-compression/typical-sets.html" class="internal-link" target="_self" rel="noopener nofollow">typicality</a> to a pair of sequences   with  (input) <a data-tooltip-position="top" aria-label="Statistical Independence" data-href="Statistical Independence" href="the-guide/mathematics/statistics/statistical-independence.html" class="internal-link" target="_self" rel="noopener nofollow">i.i.d.</a> and  (output) <a data-tooltip-position="top" aria-label="Statistical Independence" data-href="Statistical Independence" href="the-guide/mathematics/statistics/statistical-independence.html" class="internal-link" target="_self" rel="noopener nofollow">i.i.d.</a>.<br>"If I generate two sequences (input and output) independently, what is the probability that they appear as if the output was generated by the input that was delivered over the <a data-tooltip-position="top" aria-label="Discrete Memoryless Channel" data-href="Discrete Memoryless Channel" href="the-guide/information-theory/information-theory-1/channel-coding/discrete-memoryless-channel.html" class="internal-link" target="_self" rel="noopener nofollow">channel</a> ?" <br>
<br>This pair is called jointly typical with respect to a <a data-tooltip-position="top" aria-label="Probability Distribution" data-href="Probability Distribution" href="the-guide/mathematics/probability-theory/probability-distribution.html" class="internal-link" target="_self" rel="noopener nofollow">joint distribution</a>  if and if both sets are <a data-tooltip-position="top" aria-label="Typical Sets" data-href="Typical Sets" href="the-guide/information-theory/information-theory-1/data-compression/typical-sets.html" class="internal-link" target="_self" rel="noopener nofollow">typical</a> with respect to  and  respectively. 
<br>The set of all such sequences is called jointly typical sets
<br>The probability that a pair of independent typical sequences is jointly typical is
<br>Figure: There are  independent typical sequences, but only  pairs of jointly typical sequences <img alt="Pasted image 20221119212233.png" src="lib/media/pasted-image-20221119212233.png" style="width: 300px; max-width: 100%;">
]]></description><link>the-guide/information-theory/information-theory-1/channel-coding/joint-typicality.html</link><guid isPermaLink="false">The Guide/Information Theory/Information Theory 1/Channel Coding/Joint Typicality.md</guid><pubDate>Wed, 23 Apr 2025 21:50:56 GMT</pubDate><enclosure url="lib/media/pasted-image-20221119212233.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src=&quot;lib/media/pasted-image-20221119212233.png&quot;&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[Source-Channel Separation Theorem]]></title><description><![CDATA[ 
 <br><br>For any source  with a finite alphabet  that satisfies the <a data-tooltip-position="top" aria-label="Asymptotic Equipartition Property" data-href="Asymptotic Equipartition Property" href="the-guide/information-theory/information-theory-1/data-compression/asymptotic-equipartition-property.html" class="internal-link" target="_self" rel="noopener nofollow">AEP</a> and a <a data-tooltip-position="top" aria-label="Discrete Memoryless Channel" data-href="Discrete Memoryless Channel" href="the-guide/information-theory/information-theory-1/channel-coding/discrete-memoryless-channel.html" class="internal-link" target="_self" rel="noopener nofollow">channel</a> with <a data-tooltip-position="top" aria-label="Channel Capacity" data-href="Channel Capacity" href="the-guide/information-theory/information-theory-1/channel-coding/channel-capacity.html" class="internal-link" target="_self" rel="noopener nofollow">capacity</a> , there exists a source code together with a channel code yielding an arbitrarily low probability of error.<br><br>
<br>Follows from combination of <a data-href="Source Coding Theorem" href="the-guide/information-theory/information-theory-1/data-compression/source-coding-theorem.html" class="internal-link" target="_self" rel="noopener nofollow">Source Coding Theorem</a> and <a data-href="Channel Coding Theorem" href="the-guide/information-theory/information-theory-1/channel-coding/channel-coding-theorem.html" class="internal-link" target="_self" rel="noopener nofollow">Channel Coding Theorem</a>.
]]></description><link>the-guide/information-theory/information-theory-1/channel-coding/source-channel-separation-theorem.html</link><guid isPermaLink="false">The Guide/Information Theory/Information Theory 1/Channel Coding/Source-Channel Separation Theorem.md</guid><pubDate>Wed, 23 Apr 2025 21:50:56 GMT</pubDate></item><item><title><![CDATA[Convolutional Codes]]></title><description><![CDATA[ 
 <br>Class of <a data-tooltip-position="top" aria-label="Linear Block Code" data-href="Linear Block Code" href="the-guide/information-theory/information-theory-1/coding-theory/linear-block-code.html" class="internal-link" target="_self" rel="noopener nofollow">linear codes</a>, encoded block depends on current message block AND previous message blocks.<br><br><br>
<br>-convolutional encoder has 

<br>Memory order 

<br>Alternatively Constraint length 


<br>Number if input bits 
<br>Length of encoded sequence 
<br>Code Rate 
<br>An arbitrary block length


<br>Encoding via convolution with generator sequence  vialeads to encoding sequences .
<br><a data-tooltip-position="top" aria-label="Decoding Rules" data-href="Decoding Rules" href="the-guide/information-theory/information-theory-1/coding-theory/decoding-rules.html" class="internal-link" target="_self" rel="noopener nofollow">Codeword</a> is then generated by multiplexing the output sequences 

<br>Can be visualized via<img alt="center" src="lib/media/pasted-image-20230214161701.png" style="width: 500px; max-width: 100%;">
<br>Leads to State Diagram <img alt="center" src="lib/media/pasted-image-20230214161945.png" style="width: 400px; max-width: 100%;">


<br><br><br>
<br>Trellis-Decoding / Viterbi

<br>Use State Diagram above to draw trellis <img alt="center" src="lib/media/pasted-image-20230214163445.png" style="width: 500px; max-width: 100%;">
<br>Follow paths through the trellis (initial is always 00), noting <a data-tooltip-position="top" aria-label="Hamming Distance" data-href="Hamming Distance" href="the-guide/information-theory/information-theory-1/coding-theory/hamming-distance.html" class="internal-link" target="_self" rel="noopener nofollow">distance</a> between path and received vector.<img alt="center" src="lib/media/pasted-image-20230214170123.png" style="width: 550px; max-width: 100%;">

<br>Prune the path with the highest distance whenerv two paths meet<img alt="center" src="lib/media/pasted-image-20230214170312.png" style="width: 550px; max-width: 100%;">




<br>Corresponds to block-wise <a data-tooltip-position="top" aria-label="Decoding Rules" data-href="Decoding Rules" href="the-guide/information-theory/information-theory-1/coding-theory/decoding-rules.html" class="internal-link" target="_self" rel="noopener nofollow">MAP</a> decoding with hard decision.
<br><br><br>Smallest <a data-tooltip-position="top" aria-label="Hamming Weight" data-href="Hamming Weight" href="the-guide/information-theory/information-theory-1/coding-theory/hamming-weight.html" class="internal-link" target="_self" rel="noopener nofollow">Hamming weight</a> of a path emerging and cenverging again onto the all-zero path. <img alt="center" src="lib/media/pasted-image-20230214170811.png" style="width: 500px; max-width: 100%;">]]></description><link>the-guide/information-theory/information-theory-1/coding-theory/convolutional-codes.html</link><guid isPermaLink="false">The Guide/Information Theory/Information Theory 1/Coding Theory/Convolutional Codes.md</guid><pubDate>Wed, 23 Apr 2025 21:51:02 GMT</pubDate><enclosure url="lib/media/pasted-image-20230214161701.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src=&quot;lib/media/pasted-image-20230214161701.png&quot;&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[Decoding Rules]]></title><description><![CDATA[ 
 <br>Use an algorithm to determine sent codeword  from corrupted received codeword .<br>
<br>Model

<br><a data-href="Discrete Memoryless Channel" href="the-guide/information-theory/information-theory-1/channel-coding/discrete-memoryless-channel.html" class="internal-link" target="_self" rel="noopener nofollow">Discrete Memoryless Channel</a> 

<br>


<br>Transmission 

<br>


<br>Posterior probability that a codeword was sent given the received codeword (<a data-href="Bayes Theorem" href="the-guide/mathematics/statistics/bayes-theorem.html" class="internal-link" target="_self" rel="noopener nofollow">Bayes Theorem</a>) 

<br>




<br><br>
<br>Maximum-Likelihood (ML) Decoding

<br>Choose <a data-tooltip-position="top" aria-label="Decoding Rules" data-href="Decoding Rules" href="the-guide/information-theory/information-theory-1/coding-theory/decoding-rules.html" class="internal-link" target="_self" rel="noopener nofollow">codeword</a>   according to <a data-tooltip-position="top" aria-label="Maximum Likelihood Estimator" data-href="Maximum Likelihood Estimator" href="the-guide/mathematics/statistics/maximum-likelihood-estimator.html" class="internal-link" target="_self" rel="noopener nofollow">ML</a>, namely for which 
<br>Choose codeword that is most similiar assuming we don't know anything about the underlying probabilities of the codewords.
<br><a rel="noopener nofollow" class="external-link" href="https://www.youtube.com/watch?v=4dcxErj3_U0" target="_blank">https://www.youtube.com/watch?v=4dcxErj3_U0</a>


<br>Maximum-A-Posteriori (MAP) Decoding

<br>Choose <a data-tooltip-position="top" aria-label="Decoding Rules" data-href="Decoding Rules" href="the-guide/information-theory/information-theory-1/coding-theory/decoding-rules.html" class="internal-link" target="_self" rel="noopener nofollow">codeword</a>  according to <a data-tooltip-position="top" aria-label="Maximum A Posteriori Estimator" data-href="Maximum A Posteriori Estimator" href="the-guide/mathematics/statistics/maximum-a-posteriori-estimator.html" class="internal-link" target="_self" rel="noopener nofollow">MAP</a>, namely for which when compared to all other possible codewords.
<br>Refinement of ML, if distribution of  is kown. Choose most similiar codeword while respecting the probabilities of the codewords .
<br><a rel="noopener nofollow" class="external-link" href="https://www.youtube.com/watch?v=845xlSrrB38" target="_blank">https://www.youtube.com/watch?v=845xlSrrB38</a>


<br>Remarks

<br>Both coincide, if  is uniform (Apply <a data-href="Bayes Theorem" href="the-guide/mathematics/statistics/bayes-theorem.html" class="internal-link" target="_self" rel="noopener nofollow">Bayes Theorem</a> to MAP,  needs to be equivalent to  for ML to hold)
<br>ML more practical, as  is harder to get than .


]]></description><link>the-guide/information-theory/information-theory-1/coding-theory/decoding-rules.html</link><guid isPermaLink="false">The Guide/Information Theory/Information Theory 1/Coding Theory/Decoding Rules.md</guid><pubDate>Wed, 23 Apr 2025 21:51:02 GMT</pubDate></item><item><title><![CDATA[Gilbert-Varshamov Bound]]></title><description><![CDATA[ 
 <br>How close can we get to the <a data-tooltip-position="top" aria-label="Channel Capacity" data-href="Channel Capacity" href="the-guide/information-theory/information-theory-1/channel-coding/channel-capacity.html" class="internal-link" target="_self" rel="noopener nofollow">capacity</a> with <a data-href="Minimum-Distance Decoding" href="the-guide/information-theory/information-theory-1/coding-theory/minimum-distance-decoding.html" class="internal-link" target="_self" rel="noopener nofollow">Minimum-Distance Decoding</a> ?<br> Probability of a decoding error is not directly related to the minimum distance !<br>
<img alt="center" src="lib/media/pasted-image-20230213191524.png"><br><br>Average number of codewords that are at distance  from the all-zero codeword. Defining the relative distance as , one obtains The first root of  is the Gilbert-Varshamov Distance .<br>Conjecture
For large code lengths , it is not possible to create codes with a mininum distance significantly larger than .
]]></description><link>the-guide/information-theory/information-theory-1/coding-theory/gilbert-varshamov-bound.html</link><guid isPermaLink="false">The Guide/Information Theory/Information Theory 1/Coding Theory/Gilbert-Varshamov Bound.md</guid><pubDate>Wed, 23 Apr 2025 21:51:02 GMT</pubDate><enclosure url="lib/media/pasted-image-20230213191524.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src=&quot;lib/media/pasted-image-20230213191524.png&quot;&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[Hamming Ball]]></title><description><![CDATA[ 
 <br>Set  and a <a data-tooltip-position="top" aria-label="Hamming Distance" data-href="Hamming Distance" href="the-guide/information-theory/information-theory-1/coding-theory/hamming-distance.html" class="internal-link" target="_self" rel="noopener nofollow">Hamming Distance</a> define an -dimensional vector space. For a point , we define its t-neighborhood as the Hamming Ball<img alt="Pasted image 20221209103437.png" src="lib/media/pasted-image-20221209103437.png" style="width: 500px; max-width: 100%;"><br>
The number of points in a Hamming Ball is <br><br>
<br>Hamming balls do not overlap, if 
<br>In <a data-href="Minimum-Distance Decoding" href="the-guide/information-theory/information-theory-1/coding-theory/minimum-distance-decoding.html" class="internal-link" target="_self" rel="noopener nofollow">Minimum-Distance Decoding</a>, a the received <a data-tooltip-position="top" aria-label="Decoding Rules" data-href="Decoding Rules" href="the-guide/information-theory/information-theory-1/coding-theory/decoding-rules.html" class="internal-link" target="_self" rel="noopener nofollow">codeword</a> is decoded as the codeword in whose Hammin Ball it is located
<br>The resulting code can correct up to t bitflips
]]></description><link>the-guide/information-theory/information-theory-1/coding-theory/hamming-ball.html</link><guid isPermaLink="false">The Guide/Information Theory/Information Theory 1/Coding Theory/Hamming Ball.md</guid><pubDate>Wed, 23 Apr 2025 21:51:02 GMT</pubDate><enclosure url="lib/media/pasted-image-20221209103437.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src=&quot;lib/media/pasted-image-20221209103437.png&quot;&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[Hamming Distance]]></title><description><![CDATA[ 
 <br>Number of locations in which two <a data-tooltip-position="top" aria-label="Decoding Rules" data-href="Decoding Rules" href="the-guide/information-theory/information-theory-1/coding-theory/decoding-rules.html" class="internal-link" target="_self" rel="noopener nofollow">codewords</a> differ <br>
<br>Relative Distance defined as- Properties

<br>
<br>
<br>


<br><br>An  block code has the power to correct up to  errors if its minimum <a data-tooltip-position="top" aria-label="Hamming Distance" data-href="Hamming Distance" href="the-guide/information-theory/information-theory-1/coding-theory/hamming-distance.html" class="internal-link" target="_self" rel="noopener nofollow">distance</a> is <br>
<br>Example

<br>The  <a data-tooltip-position="top" aria-label="Repetition Code" data-href="Repetition Code" href="the-guide/information-theory/information-theory-1/coding-theory/repetition-code.html" class="internal-link" target="_self" rel="noopener nofollow">repetition code</a> can correct up to 2 errors, because the codewords  and  have .


]]></description><link>the-guide/information-theory/information-theory-1/coding-theory/hamming-distance.html</link><guid isPermaLink="false">The Guide/Information Theory/Information Theory 1/Coding Theory/Hamming Distance.md</guid><pubDate>Wed, 23 Apr 2025 21:51:02 GMT</pubDate></item><item><title><![CDATA[Hamming Weight]]></title><description><![CDATA[ 
 <br>Number of non-zero elements in the <a data-tooltip-position="top" aria-label="Decoding Rules" data-href="Decoding Rules" href="the-guide/information-theory/information-theory-1/coding-theory/decoding-rules.html" class="internal-link" target="_self" rel="noopener nofollow">codeword</a>.<br><br>
<br>Relation to <a data-href="Hamming Distance" href="the-guide/information-theory/information-theory-1/coding-theory/hamming-distance.html" class="internal-link" target="_self" rel="noopener nofollow">Hamming Distance</a> 
]]></description><link>the-guide/information-theory/information-theory-1/coding-theory/hamming-weight.html</link><guid isPermaLink="false">The Guide/Information Theory/Information Theory 1/Coding Theory/Hamming Weight.md</guid><pubDate>Wed, 23 Apr 2025 21:51:02 GMT</pubDate></item><item><title><![CDATA[LDPC Codes]]></title><description><![CDATA[ 
 <br>In a Nutshell
<a data-tooltip-position="top" aria-label="Linear Block Code" data-href="Linear Block Code" href="the-guide/information-theory/information-theory-1/coding-theory/linear-block-code.html" class="internal-link" target="_self" rel="noopener nofollow">Linear Block Codes</a> that are <a data-tooltip-position="top" aria-label="Channel Capacity" data-href="Channel Capacity" href="the-guide/information-theory/information-theory-1/channel-coding/channel-capacity.html" class="internal-link" target="_self" rel="noopener nofollow">capacity</a>-approaching and can be <a data-tooltip-position="top" aria-label="Decoding Rules" data-href="Decoding Rules" href="the-guide/information-theory/information-theory-1/coding-theory/decoding-rules.html" class="internal-link" target="_self" rel="noopener nofollow">decoded</a> in linear time to their <a data-tooltip-position="top" aria-label="Linear Block Code" data-href="Linear Block Code" href="the-guide/information-theory/information-theory-1/coding-theory/linear-block-code.html" class="internal-link" target="_self" rel="noopener nofollow">block length</a>.
<br><br><br>Search for (nearly) <a data-tooltip-position="top" aria-label="Perfect Codes" data-href="Perfect Codes" href="the-guide/information-theory/information-theory-1/coding-theory/perfect-codes.html" class="internal-link" target="_self" rel="noopener nofollow">perfect codes</a> limited in the parameters (<a data-href="Gilbert-Varshamov Bound" href="the-guide/information-theory/information-theory-1/coding-theory/gilbert-varshamov-bound.html" class="internal-link" target="_self" rel="noopener nofollow">Gilbert-Varshamov Bound</a>). Low-Density-Parity-Check Codes and <a data-href="Turbo Codes" href="the-guide/information-theory/information-theory-1/coding-theory/turbo-codes.html" class="internal-link" target="_self" rel="noopener nofollow">Turbo Codes</a> are the new standards.<br><br>
<br>-regular LDPC code is defined over null space of <a data-tooltip-position="top" aria-label="Linear Block Code" data-href="Linear Block Code" href="the-guide/information-theory/information-theory-1/coding-theory/linear-block-code.html" class="internal-link" target="_self" rel="noopener nofollow">parity-check matrix</a>  with

<br>Each column has <a data-href="Hamming Weight" href="the-guide/information-theory/information-theory-1/coding-theory/hamming-weight.html" class="internal-link" target="_self" rel="noopener nofollow">Hamming Weight</a> .
<br>Each row has <a data-href="Hamming Weight" href="the-guide/information-theory/information-theory-1/coding-theory/hamming-weight.html" class="internal-link" target="_self" rel="noopener nofollow">Hamming Weight</a> .
<br>Number of ones in common between any two columns is at max 1.
<br>With , the equality  holds.
<br>.
<br>


<br><br>
<br>Specified by fraction of columns  and fraction of rows  having edge weight .

<br>Degree distribution  and .
<br>With , the equality  holds.
<br>


<br><br>
<br> in general not sparse, rather encode by using back-substitution with sparse . 
<br>Step 1 - bring  into approximate upper triangular form

<br>Form upper triangular matrix ,  (gap) as small as possible                                                        <img alt="Pasted image 20230103132916.png" src="lib/media/pasted-image-20230103132916.png" style="width: 100px; max-width: 100%;">


<br>Step 2 - Clear <br>
- Gauss-Jordan leads to  where  and .
<br>Step 3 - Construct code <br>
- Parity equations are equal to 	- Leads to and afterwards In these computations the sparseness of  and back substitution for  is exploited in order to keep the complexity of order .
<br><br>
<br><a data-href="Message Passing Algorithm" href="the-guide/information-theory/information-theory-1/coding-theory/message-passing-algorithm.html" class="internal-link" target="_self" rel="noopener nofollow">Message Passing Algorithm</a>
<br><br>
<br>Methods to construct LDPC codes

<br>Random construction of <a data-tooltip-position="top" aria-label="Linear Block Code" data-href="Linear Block Code" href="the-guide/information-theory/information-theory-1/coding-theory/linear-block-code.html" class="internal-link" target="_self" rel="noopener nofollow">parity-check matrix</a>.
<br>Pseudo-random construction of <a data-tooltip-position="top" aria-label="Linear Block Code" data-href="Linear Block Code" href="the-guide/information-theory/information-theory-1/coding-theory/linear-block-code.html" class="internal-link" target="_self" rel="noopener nofollow">parity-check matrix</a>. 

<br>random first, then sort out undesirable configs


<br>Structured codes from combinatorial designs


]]></description><link>the-guide/information-theory/information-theory-1/coding-theory/ldpc-codes.html</link><guid isPermaLink="false">The Guide/Information Theory/Information Theory 1/Coding Theory/LDPC Codes.md</guid><pubDate>Wed, 23 Apr 2025 21:51:02 GMT</pubDate><enclosure url="lib/media/pasted-image-20230103132916.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src=&quot;lib/media/pasted-image-20230103132916.png&quot;&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[Linear Block Code]]></title><description><![CDATA[ 
 <br>"A code is linear if the modulo-2-sum of any two codewords gives another <a data-tooltip-position="top" aria-label="Decoding Rules" data-href="Decoding Rules" href="the-guide/information-theory/information-theory-1/coding-theory/decoding-rules.html" class="internal-link" target="_self" rel="noopener nofollow">codeword</a>."<br><br>
<br> linear block code

<br> is total  
<br> is number of  
<br> bits for parity check 


<br><br>
<br>An  linear block code can correct up to <a data-tooltip-position="top" aria-label="Hamming Ball" data-href="Hamming Ball" href="the-guide/information-theory/information-theory-1/coding-theory/hamming-ball.html" class="internal-link" target="_self" rel="noopener nofollow">t</a> errors, if its <a data-tooltip-position="top" aria-label="Hamming Distance" data-href="Hamming Distance" href="the-guide/information-theory/information-theory-1/coding-theory/hamming-distance.html" class="internal-link" target="_self" rel="noopener nofollow">minimum distance</a> is 
<br><br><br>
<br>Codeword as vector <br>
- Introduce vector 
<br>Coefficient Matrix 

<br>Generates parity-check bits via


<br>Generator Matrix 

<br>Concatenate identitiy <a data-tooltip-position="top" aria-label="Matrices" data-href="Matrices" href="the-guide/mathematics/linear-algebra/matrices.html" class="internal-link" target="_self" rel="noopener nofollow">matrix</a> with coefficient matrix to generate full codeword  with 

<br>Form presented above is called systematic form.
<br>Column permutation and row addition does not change effect.




<br>Parity-Check Matrix 

<br>Defined via which leads to .

<br> for non-corrupted codewords.

<br>Linear Block Code is defined as null space of parity check matrix


<br>Row corresponds to parity check equation.
<br>Column permutation and row addition does not change effect.
<br>Minimum <a data-tooltip-position="top" aria-label="Hamming Distance" data-href="Hamming Distance" href="the-guide/information-theory/information-theory-1/coding-theory/hamming-distance.html" class="internal-link" target="_self" rel="noopener nofollow">distance</a> is equal to smallest number of columns that are linearly dependant.




<br><br><br>Syndrome Vector to decode received message  with error vector  yields <br>
<br>The syndrome is zero if no error occurred.
<br>For each error theres a corresponding syndrome error.

<br>Position can be read off from parity-check matrix, if error is correctable.


]]></description><link>the-guide/information-theory/information-theory-1/coding-theory/linear-block-code.html</link><guid isPermaLink="false">The Guide/Information Theory/Information Theory 1/Coding Theory/Linear Block Code.md</guid><pubDate>Wed, 23 Apr 2025 21:51:02 GMT</pubDate></item><item><title><![CDATA[Message Passing Algorithm]]></title><description><![CDATA[ 
 <br><br>
<br>Can be used to combined <a data-href="Tanner Graph" href="the-guide/information-theory/information-theory-1/coding-theory/tanner-graph.html" class="internal-link" target="_self" rel="noopener nofollow">Tanner Graph</a> with <a data-tooltip-position="top" aria-label="Discrete Memoryless Channel" data-href="Discrete Memoryless Channel" href="the-guide/information-theory/information-theory-1/channel-coding/discrete-memoryless-channel.html" class="internal-link" target="_self" rel="noopener nofollow">Channel Matrix</a>
<br><img alt="center" src="lib/media/pasted-image-20230103142058.png" style="width: 300px; max-width: 100%;"><br><br>
<br>Sum-Product Message Passing

<br>
<br>minimizes bit-wise error


<br>Max-Product Message Passing

<br>
<br>minimizes block-wise error


<br><br>
<br>Draw factor graph, choose one arbitrary root node and unfold the tree.<img alt="center" src="lib/media/pasted-image-20230104095614.png" style="width: 250px; max-width: 100%;">
<br>Initialize leaves.<br>
<img alt="center" src="lib/media/pasted-image-20230104095715.png" style="width: 400px; max-width: 100%;">
<br>Message passing up and down

<br>For one incoming message, just copy.
<br>If funtion node is destination, dot product.
<br>If variable node is destination compute probabilities.


<br>Compute final belief 

<br>Dot product as above
<br>Pay attention to directions of messages 


]]></description><link>the-guide/information-theory/information-theory-1/coding-theory/message-passing-algorithm.html</link><guid isPermaLink="false">The Guide/Information Theory/Information Theory 1/Coding Theory/Message Passing Algorithm.md</guid><pubDate>Wed, 23 Apr 2025 21:51:02 GMT</pubDate><enclosure url="lib/media/pasted-image-20230103142058.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src=&quot;lib/media/pasted-image-20230103142058.png&quot;&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[Minimum-Distance Decoding]]></title><description><![CDATA[ 
 <br>In a Nutshell
Received <a data-tooltip-position="top" aria-label="Decoding Rules" data-href="Decoding Rules" href="the-guide/information-theory/information-theory-1/coding-theory/decoding-rules.html" class="internal-link" target="_self" rel="noopener nofollow">codeword</a> is decoded according to <a data-tooltip-position="top" aria-label="Decoding Rules" data-href="Decoding Rules" href="the-guide/information-theory/information-theory-1/coding-theory/decoding-rules.html" class="internal-link" target="_self" rel="noopener nofollow">ML</a> by choosing codeword from codebook with minimum <a data-tooltip-position="top" aria-label="Hamming Distance" data-href="Hamming Distance" href="the-guide/information-theory/information-theory-1/coding-theory/hamming-distance.html" class="internal-link" target="_self" rel="noopener nofollow">Hamming Distance</a>.
<br><br><br>
<br>Compute <a data-href="Hamming Distance" href="the-guide/information-theory/information-theory-1/coding-theory/hamming-distance.html" class="internal-link" target="_self" rel="noopener nofollow">Hamming Distance</a> between received codeword and all  possible codewords.
<br><br>
<br>Can be computationally inefficient, as it requires computing the distances between received codeword with every other possible codeword !
<br>Result can be ambiguous.
]]></description><link>the-guide/information-theory/information-theory-1/coding-theory/minimum-distance-decoding.html</link><guid isPermaLink="false">The Guide/Information Theory/Information Theory 1/Coding Theory/Minimum-Distance Decoding.md</guid><pubDate>Wed, 23 Apr 2025 21:51:02 GMT</pubDate></item><item><title><![CDATA[Perfect Codes]]></title><description><![CDATA[ 
 <br>Code for which the <a data-tooltip-position="top" aria-label="Hamming Ball" data-href="Hamming Ball" href="the-guide/information-theory/information-theory-1/coding-theory/hamming-ball.html" class="internal-link" target="_self" rel="noopener nofollow">Hamming Balls</a> fill the entire Hamming space and do not overlap. Only existing codes are  Hamming Codes and the  Golay Code.<br><br><a data-href="Linear Block Code" href="the-guide/information-theory/information-theory-1/coding-theory/linear-block-code.html" class="internal-link" target="_self" rel="noopener nofollow">Linear Block Code</a> with parity bits. Error correction trough parity-check bits (here: (7,4)):<img alt="Pasted image 20221209110801.png" src="lib/media/pasted-image-20221209110801.png"><br><br>Nearly perfect  block code with . The <a data-tooltip-position="top" aria-label="Linear Block Code" data-href="Linear Block Code" href="the-guide/information-theory/information-theory-1/coding-theory/linear-block-code.html" class="internal-link" target="_self" rel="noopener nofollow">coefficient matrix</a>  is the anti-adjacency matrix of an icosahedron.<br>
<img alt="Pasted image 20221215145030.png" src="lib/media/pasted-image-20221215145030.png" style="width: 200px; max-width: 100%;"><img alt="Pasted image 20221215145051.png" src="lib/media/pasted-image-20221215145051.png" style="width: 300px; max-width: 100%;">]]></description><link>the-guide/information-theory/information-theory-1/coding-theory/perfect-codes.html</link><guid isPermaLink="false">The Guide/Information Theory/Information Theory 1/Coding Theory/Perfect Codes.md</guid><pubDate>Wed, 23 Apr 2025 21:51:02 GMT</pubDate><enclosure url="lib/media/pasted-image-20221209110801.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src=&quot;lib/media/pasted-image-20221209110801.png&quot;&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[Reed-Muller Codes]]></title><description><![CDATA[ 
 <br><a data-tooltip-position="top" aria-label="Linear Block Code" data-href="Linear Block Code" href="the-guide/information-theory/information-theory-1/coding-theory/linear-block-code.html" class="internal-link" target="_self" rel="noopener nofollow">Linear</a>, multiple-error correcting code.  is an r-th order Reed-Muller code with<br>
<br>Code length 
<br><a data-tooltip-position="top" aria-label="Hamming Distance" data-href="Hamming Distance" href="the-guide/information-theory/information-theory-1/coding-theory/hamming-distance.html" class="internal-link" target="_self" rel="noopener nofollow">Minimum distance</a> 
<br>Dimension (number of vectors) .
<br><br>
<br>Construct  tuples of length  with alternating  blocks of zeros and ones.
<br>Define Boolean product as inner product between any to tuples.with AND-operation (1 only if ).
<br>With  compute set of independent vectors 
<br>These vectors ordered asform the rows of the <a data-tooltip-position="top" aria-label="Linear Block Code" data-href="Linear Block Code" href="the-guide/information-theory/information-theory-1/coding-theory/linear-block-code.html" class="internal-link" target="_self" rel="noopener nofollow">generator matrix</a> in non-systematic form. 
<br><br>
<br>Construction yields algebraic relations within basis vectos.
<br>Relations form multiple equations on every level.
<br>...
<br><br>
<br>Decoding algorithm suffers from error propagation.
]]></description><link>the-guide/information-theory/information-theory-1/coding-theory/reed-muller-codes.html</link><guid isPermaLink="false">The Guide/Information Theory/Information Theory 1/Coding Theory/Reed-Muller Codes.md</guid><pubDate>Wed, 23 Apr 2025 21:51:02 GMT</pubDate></item><item><title><![CDATA[Repetition Code]]></title><description><![CDATA[ 
 <br>Naive approach for error correction, simply repeat every symbol  times.<br>
<br>E.g.  
<br>Received symbol is picked by majority.
<br><br>What is the probability that repitition code used on a <a data-tooltip-position="top" aria-label="Discrete Memoryless Channel" data-href="Discrete Memoryless Channel" href="the-guide/information-theory/information-theory-1/channel-coding/discrete-memoryless-channel.html" class="internal-link" target="_self" rel="noopener nofollow">binary symmetric channel</a> produces  errors ?<br>
<br>
<br>Bit-flip probability 
<br> number of possible error allocations for  errors in sequences of length . 
]]></description><link>the-guide/information-theory/information-theory-1/coding-theory/repetition-code.html</link><guid isPermaLink="false">The Guide/Information Theory/Information Theory 1/Coding Theory/Repetition Code.md</guid><pubDate>Wed, 23 Apr 2025 21:51:02 GMT</pubDate></item><item><title><![CDATA[Standard Array]]></title><description><![CDATA[ 
 <br>Can be computed for a given  <a data-href="Linear Block Code" href="the-guide/information-theory/information-theory-1/coding-theory/linear-block-code.html" class="internal-link" target="_self" rel="noopener nofollow">Linear Block Code</a> <img alt="Pasted image 20221219124033.png" src="lib/media/pasted-image-20221219124033.png"><br>
<br>Results in  table
<br>Rows are calles cosets

<br>First row is all possible codewords

<br>all zero always included
<br>rows of <a data-tooltip-position="top" aria-label="Linear Block Code" data-href="Linear Block Code" href="the-guide/information-theory/information-theory-1/coding-theory/linear-block-code.html" class="internal-link" target="_self" rel="noopener nofollow">generator matrix</a> 
<br>all linear combinations of rows




<br>First element of each row is coset leader

<br>possible errors (single 1, zero else)


<br>If error pattern is a coset leader, them vector  is correctly decoded. The  cosets are called the correctable error patterns.
]]></description><link>the-guide/information-theory/information-theory-1/coding-theory/standard-array.html</link><guid isPermaLink="false">The Guide/Information Theory/Information Theory 1/Coding Theory/Standard Array.md</guid><pubDate>Wed, 23 Apr 2025 21:51:02 GMT</pubDate><enclosure url="lib/media/pasted-image-20221219124033.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src=&quot;lib/media/pasted-image-20221219124033.png&quot;&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[Tanner Graph]]></title><description><![CDATA[ 
 <br>In a Nutshell
Visualization for parity-check codes, e.g. <a data-href="LDPC Codes" href="the-guide/information-theory/information-theory-1/coding-theory/ldpc-codes.html" class="internal-link" target="_self" rel="noopener nofollow">LDPC Codes</a>.
<br><br>Define code membership function of an -<a data-tooltip-position="top" aria-label="Linear Block Code" data-href="Linear Block Code" href="the-guide/information-theory/information-theory-1/coding-theory/linear-block-code.html" class="internal-link" target="_self" rel="noopener nofollow">code</a> as <br>Tanner Graph
A Tanner <a data-href="Graph" href="the-guide/mathematics/graph-theory/graph.html" class="internal-link" target="_self" rel="noopener nofollow">Graph</a> is associated with a <a data-tooltip-position="top" aria-label="Linear Block Code" data-href="Linear Block Code" href="the-guide/information-theory/information-theory-1/coding-theory/linear-block-code.html" class="internal-link" target="_self" rel="noopener nofollow">parity-check matrix</a>  and consists of <a data-tooltip-position="top" aria-label="Linear Block Code" data-href="Linear Block Code" href="the-guide/information-theory/information-theory-1/coding-theory/linear-block-code.html" class="internal-link" target="_self" rel="noopener nofollow">n</a> variable nodes and <a data-tooltip-position="top" aria-label="Linear Block Code" data-href="Linear Block Code" href="the-guide/information-theory/information-theory-1/coding-theory/linear-block-code.html" class="internal-link" target="_self" rel="noopener nofollow">m</a> check nodes. An edge is created, if a variable participates in a parity check. 
<br><img alt="300|center" src="lib/media/pasted-image-20230103140037.png"><br>
<br>For a  <a data-tooltip-position="top" aria-label="LDPC Codes" data-href="LDPC Codes" href="the-guide/information-theory/information-theory-1/coding-theory/ldpc-codes.html" class="internal-link" target="_self" rel="noopener nofollow">regular code</a>, each variable node participates in  checks and each check consists of  participants.
<br><br>
<br>Cycle

<br>Sequence starting and ending at the same node, containing each other node only once.


<br>Girth

<br>Length of the smallest cycle.


]]></description><link>the-guide/information-theory/information-theory-1/coding-theory/tanner-graph.html</link><guid isPermaLink="false">The Guide/Information Theory/Information Theory 1/Coding Theory/Tanner Graph.md</guid><pubDate>Wed, 23 Apr 2025 21:51:02 GMT</pubDate><enclosure url="lib/media/pasted-image-20230103140037.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src=&quot;lib/media/pasted-image-20230103140037.png&quot;&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[Turbo Codes]]></title><description><![CDATA[ 
 <br><img alt="center" src="lib/media/pasted-image-20230215124925.png" style="width: 500px; max-width: 100%;"><br>
<br><br><br><br><br>
<br>"Recursive Systematic Convolutional" = <a data-tooltip-position="top" aria-label="Convolutional Codes" data-href="Convolutional Codes" href="the-guide/information-theory/information-theory-1/coding-theory/convolutional-codes.html" class="internal-link" target="_self" rel="noopener nofollow">Convolutional encoder</a> with rational transfer functioncorresponds to IIR filter in signal processing.

<br>Write out transfer functions to obtain equations
<br>Create table for all possible states (bit combinations of past inputs)

<br>Compute putput and next state for each state with input 0 and 1


<br>Draw state diagram
<br>Encode message, starting at 00 state

<br>After 00 as many steps as bits in the message




]]></description><link>the-guide/information-theory/information-theory-1/coding-theory/turbo-codes.html</link><guid isPermaLink="false">The Guide/Information Theory/Information Theory 1/Coding Theory/Turbo Codes.md</guid><pubDate>Wed, 23 Apr 2025 21:51:02 GMT</pubDate><enclosure url="lib/media/pasted-image-20230215124925.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src=&quot;lib/media/pasted-image-20230215124925.png&quot;&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[Arithmetic Coding]]></title><description><![CDATA[ 
 <br><a data-tooltip-position="top" aria-label="Classes of Codes" data-href="Classes of Codes" href="the-guide/information-theory/information-theory-1/data-compression/classes-of-codes.html" class="internal-link" target="_self" rel="noopener nofollow">Stream code</a> with high flexibility. Uses <a data-tooltip-position="top" aria-label="Cumulative Distribution Function" data-href="Cumulative Distribution Function" href="the-guide/mathematics/probability-theory/cumulative-distribution-function.html" class="internal-link" target="_self" rel="noopener nofollow">cdf</a> in order to encode message in accordance to variable length coding. Expansion of <a data-href="Shannon-Fano-Elias-Coding" href="the-guide/information-theory/information-theory-1/data-compression/shannon-fano-elias-coding.html" class="internal-link" target="_self" rel="noopener nofollow">Shannon-Fano-Elias-Coding</a> to sequences.  <br>G-Adic Expansion
With , the g-adic expansion is a sequence of numbers of the set  which we denote as  with 
<br>
<br>Example 2-adic expansion of 

<br>
<br>
<br>
<br>...


<br><br><br>
<br>Map sequence  to an interval using the <a data-tooltip-position="top" aria-label="Cumulative Distribution Function" data-href="Cumulative Distribution Function" href="the-guide/mathematics/probability-theory/cumulative-distribution-function.html" class="internal-link" target="_self" rel="noopener nofollow">cdf</a> of the sequence (contrast to joint cdf). We assign the interval  
<br>Map interval to a binary string by 2-adic expansion Locate interval corresponding to symbol string (example).  This corresponds to truncating the infinite binary string.  
<br><br>
<br>Encoding the  sequence CA yields 1101 <img alt="Pasted image 20221118131014.png" src="lib/media/pasted-image-20221118131014.png">
<br><br>
<br>The interval (right) with maximum length that lies within the interval (left) defines the code.
<br>The longer the sequence of source symbols, the shorter is the representing interval.
<br>The shorter the representing interval, the longer is the representing binary codeword.
<br>Requires single pass over data
]]></description><link>the-guide/information-theory/information-theory-1/data-compression/arithmetic-coding.html</link><guid isPermaLink="false">The Guide/Information Theory/Information Theory 1/Data Compression/Arithmetic Coding.md</guid><pubDate>Wed, 23 Apr 2025 21:50:50 GMT</pubDate><enclosure url="lib/media/pasted-image-20221118131014.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src=&quot;lib/media/pasted-image-20221118131014.png&quot;&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[Asymptotic Equipartition Property]]></title><description><![CDATA[ 
 <br>"For large , the uncertainty of most sequences is close to the <a data-tooltip-position="top" aria-label="Shannon Entropy" data-href="Shannon Entropy" href="the-guide/information-theory/information-theory-1/information/shannon-entropy.html" class="internal-link" target="_self" rel="noopener nofollow">entropy</a> of the source." <br><br>The <a data-href="Weak Law of Large Numbers" href="the-guide/mathematics/probability-theory/weak-law-of-large-numbers.html" class="internal-link" target="_self" rel="noopener nofollow">Weak Law of Large Numbers</a> also holds for any function of the <a data-tooltip-position="top" aria-label="Random Variable" data-href="Random Variable" href="the-guide/mathematics/probability-theory/random-variable.html" class="internal-link" target="_self" rel="noopener nofollow">random variable</a>. Choosing this function to be , this leads to  ]]></description><link>the-guide/information-theory/information-theory-1/data-compression/asymptotic-equipartition-property.html</link><guid isPermaLink="false">The Guide/Information Theory/Information Theory 1/Data Compression/Asymptotic Equipartition Property.md</guid><pubDate>Wed, 23 Apr 2025 21:50:50 GMT</pubDate></item><item><title><![CDATA[Average Codeword Length]]></title><description><![CDATA[ 
 <br>Let the <a data-tooltip-position="top" aria-label="Random Variable" data-href="Random Variable" href="the-guide/mathematics/probability-theory/random-variable.html" class="internal-link" target="_self" rel="noopener nofollow">source</a> alphabet contain  different symbols with probabilities , , and let the binary codeword assigned to  by the encoder have the length  bits.<br>
The average or expected codeword length is given by:<br><br>
<br> represents the average number of bits per source symbol required to encode the source.
<br>We want to design codes whose  is as small as possible.

<br>Concept of Variable Length Coding


]]></description><link>the-guide/information-theory/information-theory-1/data-compression/average-codeword-length.html</link><guid isPermaLink="false">The Guide/Information Theory/Information Theory 1/Data Compression/Average Codeword Length.md</guid><pubDate>Wed, 23 Apr 2025 21:50:50 GMT</pubDate></item><item><title><![CDATA[Classes of Codes]]></title><description><![CDATA[ 
 <br><br>A source code  for a <a data-tooltip-position="top" aria-label="Random Variable" data-href="Random Variable" href="the-guide/mathematics/probability-theory/random-variable.html" class="internal-link" target="_self" rel="noopener nofollow">random variable</a>  is a mapping from the range of this variable  to a set of finite-length strings of a chosen alphabet.<br>
<br>Example

<br>With alphabet  encode heads  and tails . 


<br><br>
<br>
Non-Singular

<br>Every source symbol maps to a different codeword.


<br>
Uniquely Decodable Codes

<br>Any encoded string has only one possible source string producing it
<br>Requires  knowledge of entire encoded string


<br>
Instantaneous Codes

<br>Any source symbol can be decoded as soon as the end of the codeword corresponding to it is reached.
<br>Equivalent to prefix-free codes, where no codeword is the prefix for any other codeword
<br>Optimal Instantaneous Codes

<br>For any <a data-tooltip-position="top" aria-label="Probability Distribution" data-href="Probability Distribution" href="the-guide/mathematics/probability-theory/probability-distribution.html" class="internal-link" target="_self" rel="noopener nofollow">distribution</a> there exists an optimal code (with minimal <a data-tooltip-position="top" aria-label="Average Codeword Length" data-href="Average Codeword Length" href="the-guide/information-theory/information-theory-1/data-compression/average-codeword-length.html" class="internal-link" target="_self" rel="noopener nofollow">average codeword length</a>) that satisfies

<br>Codeword lengths are ordered inversely with the probabilities
<br>The two longest codewords have the same lengths
<br>Two of the longest codewords differ only in the last bit position and correpsond to least likely symbol






<br><img alt="300|center" src="lib/media/pasted-image-20221116104721.png"><br><br>
<br>Symbol Codes

<br>Each symbol (symbol of the source or blocks of length ) is translated into a string of integer number of bits. In contrast, <a data-tooltip-position="top" aria-label="Classes of Codes" data-href="Classes of Codes" href="the-guide/information-theory/information-theory-1/data-compression/classes-of-codes.html" class="internal-link" target="_self" rel="noopener nofollow">stream codes</a> encode in a non-integer number of bits.
<br>Examples

<br><a data-href="Huffman Coding" href="the-guide/information-theory/information-theory-1/data-compression/huffman-coding.html" class="internal-link" target="_self" rel="noopener nofollow">Huffman Coding</a>
<br><a data-href="Shannon-Fano Code" href="the-guide/information-theory/information-theory-1/data-compression/shannon-fano-code.html" class="internal-link" target="_self" rel="noopener nofollow">Shannon-Fano Code</a>




<br>Stream Codes

<br>Encode a Symbol in a string of non-integer number of bits. Also called online code, encoding and decoding can be seperated from prior knowledge of the source. 
<br>Examples

<br><a data-tooltip-position="top" aria-label="Probability Distribution" data-href="Probability Distribution" href="the-guide/mathematics/probability-theory/probability-distribution.html" class="internal-link" target="_self" rel="noopener nofollow">Probability distribution</a> of the <a data-tooltip-position="top" aria-label="Random Variable" data-href="Random Variable" href="the-guide/mathematics/probability-theory/random-variable.html" class="internal-link" target="_self" rel="noopener nofollow">source</a> is known 

<br><a data-href="Arithmetic Coding" href="the-guide/information-theory/information-theory-1/data-compression/arithmetic-coding.html" class="internal-link" target="_self" rel="noopener nofollow">Arithmetic Coding</a> 


<br><a data-tooltip-position="top" aria-label="Probability Distribution" data-href="Probability Distribution" href="the-guide/mathematics/probability-theory/probability-distribution.html" class="internal-link" target="_self" rel="noopener nofollow">Probability distribution</a> is unknown 

<br><a data-href="Lempel-Ziv Code" href="the-guide/information-theory/information-theory-1/data-compression/lempel-ziv-code.html" class="internal-link" target="_self" rel="noopener nofollow">Lempel-Ziv Code</a>






]]></description><link>the-guide/information-theory/information-theory-1/data-compression/classes-of-codes.html</link><guid isPermaLink="false">The Guide/Information Theory/Information Theory 1/Data Compression/Classes of Codes.md</guid><pubDate>Wed, 23 Apr 2025 21:50:50 GMT</pubDate><enclosure url="lib/media/pasted-image-20221116104721.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src=&quot;lib/media/pasted-image-20221116104721.png&quot;&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[Codeword Length Variance]]></title><description><![CDATA[ 
 <br>Measure of the variability of the <a data-tooltip-position="top" aria-label="Average Codeword Length" data-href="Average Codeword Length" href="the-guide/information-theory/information-theory-1/data-compression/average-codeword-length.html" class="internal-link" target="_self" rel="noopener nofollow">codeword lengths</a> is the codeword length <a data-tooltip-position="top" aria-label="Covariance and Variance" data-href="Covariance and Variance" href="the-guide/mathematics/statistics/covariance-and-variance.html" class="internal-link" target="_self" rel="noopener nofollow">variance</a><br>
It can be shown that when a <a data-tooltip-position="top" aria-label="Huffman Coding" data-href="Huffman Coding" href="the-guide/information-theory/information-theory-1/data-compression/huffman-coding.html" class="internal-link" target="_self" rel="noopener nofollow">combined symbol</a> is moved as high as possible, then the variance is typically significantly smaller than when it is moved as low as possible.]]></description><link>the-guide/information-theory/information-theory-1/data-compression/codeword-length-variance.html</link><guid isPermaLink="false">The Guide/Information Theory/Information Theory 1/Data Compression/Codeword Length Variance.md</guid><pubDate>Wed, 23 Apr 2025 21:50:50 GMT</pubDate></item><item><title><![CDATA[Coding Efficiency]]></title><description><![CDATA[ 
 <br>The coding efficiency is defined as  <br>
<br> is the <a data-tooltip-position="top" aria-label="Average Codeword Length" data-href="Average Codeword Length" href="the-guide/information-theory/information-theory-1/data-compression/average-codeword-length.html" class="internal-link" target="_self" rel="noopener nofollow">average codeword length</a>. 
<br> denotes the minimum possible average codeword length.

<br>Can be computed using <a data-tooltip-position="top" aria-label="Shannon Entropy" data-href="Shannon Entropy" href="the-guide/information-theory/information-theory-1/information/shannon-entropy.html" class="internal-link" target="_self" rel="noopener nofollow">the entropy</a> via <a data-href="Source Coding Theorem" href="the-guide/information-theory/information-theory-1/data-compression/source-coding-theorem.html" class="internal-link" target="_self" rel="noopener nofollow">Source Coding Theorem</a>.


]]></description><link>the-guide/information-theory/information-theory-1/data-compression/coding-efficiency.html</link><guid isPermaLink="false">The Guide/Information Theory/Information Theory 1/Data Compression/Coding Efficiency.md</guid><pubDate>Wed, 23 Apr 2025 21:50:50 GMT</pubDate></item><item><title><![CDATA[Huffman Coding]]></title><description><![CDATA[ 
 <br>Question
"How can you construct an optimal (shortest <a data-tooltip-position="top" aria-label="Average Codeword Length" data-href="Average Codeword Length" href="the-guide/information-theory/information-theory-1/data-compression/average-codeword-length.html" class="internal-link" target="_self" rel="noopener nofollow">average codeword length</a>) <a data-tooltip-position="top" aria-label="Classes of Codes" data-href="Classes of Codes" href="the-guide/information-theory/information-theory-1/data-compression/classes-of-codes.html" class="internal-link" target="_self" rel="noopener nofollow">prefix-free</a> <a data-tooltip-position="top" aria-label="Classes of Codes" data-href="Classes of Codes" href="the-guide/information-theory/information-theory-1/data-compression/classes-of-codes.html" class="internal-link" target="_self" rel="noopener nofollow">symbol code</a> ?"
<br><br><br>Consider a <a data-tooltip-position="top" aria-label="Random Variable" data-href="Random Variable" href="the-guide/mathematics/probability-theory/random-variable.html" class="internal-link" target="_self" rel="noopener nofollow">discrete source</a>  taking values in the set  with known probabilities .<br>Algorithm

<br>Ordering: The source symbols are arranged in order of decreasing probability.
<br>Splitting: The two symbols of lowest probability are assigned 0 and 1.
<br>Combining: These two symbols are combined into a new source symbol with probability equal to the sum of two original probabilities. The probability of the new symbol is inserted into the list of probabilities according to its order 

<br>Usually the symbol is moved as high as possible for smaller <a data-tooltip-position="top" aria-label="Codeword Length Variance" data-href="Codeword Length Variance" href="the-guide/information-theory/information-theory-1/data-compression/codeword-length-variance.html" class="internal-link" target="_self" rel="noopener nofollow">codeword variance</a>.


<br>Repeating: The procedure is repeated until we are left with the final list of probabilities of only two for which 0 and 1 are assigned.
<br>Forming the code: The code for each original source symbol is found by tracing the sequence of 0s and 1s assigned to that symbol as well as its successors and writing this sequence in reversed order.

<br><img alt="center" src="lib/media/pasted-image-20221116110149.png" style="width: 300px; max-width: 100%;"><br><br>
<br>At each splitting stage, there is arbitrariness in the way 0 and 1 are assigned. However, the resulting differences are trivial.
<br>At each combining stage, if the probability of a combined symbol is equal to another probability in the list, then we may place this combined probability as high as possible (as we actually did in the above example) or, alternatively, as low as possible. This may alter the lengths of particular codewords, but the <a data-tooltip-position="top" aria-label="Average Codeword Length" data-href="Average Codeword Length" href="the-guide/information-theory/information-theory-1/data-compression/average-codeword-length.html" class="internal-link" target="_self" rel="noopener nofollow">average codeword length</a>  remains the same.
<br><br><br>Tackles problem of the extra bit in Shannon's <a data-href="Source Coding Theorem" href="the-guide/information-theory/information-theory-1/data-compression/source-coding-theorem.html" class="internal-link" target="_self" rel="noopener nofollow">Source Coding Theorem</a>.<br>Algorithm

<br>Divide the source symbol data sequence into blocks of length .
<br>Consider the blocks to be <a data-tooltip-position="top" aria-label="Statistical Independence" data-href="Statistical Independence" href="the-guide/mathematics/statistics/statistical-independence.html" class="internal-link" target="_self" rel="noopener nofollow">i.i.d.</a> samples of the extended source  .
<br>Then use <a data-href="Huffman Coding" href="the-guide/information-theory/information-theory-1/data-compression/huffman-coding.html" class="internal-link" target="_self" rel="noopener nofollow">Huffman Coding</a> for the blocks. This is a <a data-tooltip-position="top" aria-label="Classes of Codes" data-href="Classes of Codes" href="the-guide/information-theory/information-theory-1/data-compression/classes-of-codes.html" class="internal-link" target="_self" rel="noopener nofollow">symbol code</a> for blocks of length n.
<br>Removes problem of the extra bit, as now 
<br>Converges to the <a data-tooltip-position="top" aria-label="Shannon Entropy" data-href="Shannon Entropy" href="the-guide/information-theory/information-theory-1/information/shannon-entropy.html" class="internal-link" target="_self" rel="noopener nofollow">entropy</a>, but requires additional computations and sacrifices <a data-tooltip-position="top" aria-label="Classes of Codes" data-href="Classes of Codes" href="the-guide/information-theory/information-theory-1/data-compression/classes-of-codes.html" class="internal-link" target="_self" rel="noopener nofollow">instantaneous decodability</a> on the level of the <a data-tooltip-position="top" aria-label="Random Variable" data-href="Random Variable" href="the-guide/mathematics/probability-theory/random-variable.html" class="internal-link" target="_self" rel="noopener nofollow">source</a>  !

]]></description><link>the-guide/information-theory/information-theory-1/data-compression/huffman-coding.html</link><guid isPermaLink="false">The Guide/Information Theory/Information Theory 1/Data Compression/Huffman Coding.md</guid><pubDate>Wed, 23 Apr 2025 21:50:50 GMT</pubDate><enclosure url="lib/media/pasted-image-20221116110149.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src=&quot;lib/media/pasted-image-20221116110149.png&quot;&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[Kraft's Inequality]]></title><description><![CDATA[ 
 <br>Caution
We want a code with the minimal <a data-tooltip-position="top" aria-label="Average Codeword Length" data-href="Average Codeword Length" href="the-guide/information-theory/information-theory-1/data-compression/average-codeword-length.html" class="internal-link" target="_self" rel="noopener nofollow">average codeword length</a> according to Shannon's <a data-href="Source Coding Theorem" href="the-guide/information-theory/information-theory-1/data-compression/source-coding-theorem.html" class="internal-link" target="_self" rel="noopener nofollow">Source Coding Theorem</a>. However, we cannot make all codewords short and still be <a data-tooltip-position="top" aria-label="Classes of Codes" data-href="Classes of Codes" href="the-guide/information-theory/information-theory-1/data-compression/classes-of-codes.html" class="internal-link" target="_self" rel="noopener nofollow">uniquely decodable or prefix-free</a>. Hence, Kraft’s inequality gives a lower bound on the required codeword lengths.
<br>Kraft's Inequality
For any <a data-tooltip-position="top" aria-label="Classes of Codes" data-href="Classes of Codes" href="the-guide/information-theory/information-theory-1/data-compression/classes-of-codes.html" class="internal-link" target="_self" rel="noopener nofollow">instantaneous code</a>, the codeword lengths  with  and an alphabet of size two satisfy<br>

<br><br>
<br>Same properties can be proven for the more general non-instantaneous, uniquely decodable codes (Kraft McMillan). 
<br>For general alphabet size : 
]]></description><link>the-guide/information-theory/information-theory-1/data-compression/kraft&apos;s-inequality.html</link><guid isPermaLink="false">The Guide/Information Theory/Information Theory 1/Data Compression/Kraft&apos;s Inequality.md</guid><pubDate>Wed, 23 Apr 2025 21:50:50 GMT</pubDate></item><item><title><![CDATA[Lempel-Ziv Code]]></title><description><![CDATA[ 
 <br><a data-tooltip-position="top" aria-label="Classes of Codes" data-href="Classes of Codes" href="the-guide/information-theory/information-theory-1/data-compression/classes-of-codes.html" class="internal-link" target="_self" rel="noopener nofollow">Stream code</a>, does not rely on the source <a data-tooltip-position="top" aria-label="Probability Distribution" data-href="Probability Distribution" href="the-guide/mathematics/probability-theory/probability-distribution.html" class="internal-link" target="_self" rel="noopener nofollow">probability distribution</a>.<br>
<br>There is one single algorithm for any source.
<br>Applied in individual sequence compression and use adaptive dictionary algorithms.
<br>Initial expansion of the sequence will become a compression, since describing them by their prefix location becomes more and more efficient the longer the sequence.
<br><br>
<br>Parsing Sequence into String The source sequence is sequentially parsed into shortest strings that have not appeared so far.
<br>Determine length of Prefix

<br>Count number of strings  produced during the step above
<br>Prefix needs max  bits

<br>Can be shortened if later entries aren't used as prefixes (zeros cut off) or if prefixes arent used 




<br>Sequence encoding Each string is encoded by indicating the location of its prefix and the last bit.
<br><br>
<br>Used in compress, gzip, zip, deflate and in gif and png graphics format
<br>Algorithm passes twice over sequence, once for computing number of bits for the prefixes and once for calculating the pointers and producing the sequence
<br>Optimal in the limit of infinite sequences.
]]></description><link>the-guide/information-theory/information-theory-1/data-compression/lempel-ziv-code.html</link><guid isPermaLink="false">The Guide/Information Theory/Information Theory 1/Data Compression/Lempel-Ziv Code.md</guid><pubDate>Wed, 23 Apr 2025 21:50:50 GMT</pubDate></item><item><title><![CDATA[Nyquist-Shannon Sampling Theorem]]></title><description><![CDATA[ 
 <br>Theorem
If a <a data-tooltip-position="top" aria-label="Mathematical Relations" data-href="Mathematical Relations" href="the-guide/mathematics/general-stuff/mathematical-relations.html" class="internal-link" target="_self" rel="noopener nofollow">function</a>  contains no frequencies higher than  hertz, it can be completely determined from samples taken at time intervals less than .  
]]></description><link>the-guide/information-theory/information-theory-1/data-compression/nyquist-shannon-sampling-theorem.html</link><guid isPermaLink="false">The Guide/Information Theory/Information Theory 1/Data Compression/Nyquist-Shannon Sampling Theorem.md</guid><pubDate>Wed, 23 Apr 2025 21:50:50 GMT</pubDate></item><item><title><![CDATA[Rate Distortion Theory]]></title><description><![CDATA[ 
 <br>Description of arbitrary real number requires infinite precision, how well can we do with a finite representation ?<br>
Lossy () source coding (<a data-href="Source Coding Theorem" href="the-guide/information-theory/information-theory-1/data-compression/source-coding-theorem.html" class="internal-link" target="_self" rel="noopener nofollow">Source Coding Theorem</a> was focused on lossless compression).<br><img alt="Pasted image 20230127110517.png" src="lib/media/pasted-image-20230127110517.png" style="width: 600px; max-width: 100%;"><br><br>Define the "goodness" of a representation of a source.<br>
<br>Single-letter distortion measure is a mapping .

<br> denotes distortion when source symbol  is reproduced by .


<br>Distortion between sequences is computed via 
<br>Normal Measure

<br>Distortion measure is said to be normal if the distortion is zero when using the output symbol with minimum measure

<br>
<br>


<br>If  is minimizer of expected distortion over all inputs, then 

<br>Is the minimal distortion achievable by a constant estimate of .




<br>Distortion Function

<br>Hamming Distortion Measure 

<br>Expected single-letter Hamming distortion is equal to probability of error


<br>Squared-Error Distortion 


<br><br>
<br>An  rate-distortion code is defined by 

<br>Encoding function 
<br>Decoding function 

<br> are called reproduction sequences




<br>The <a data-tooltip-position="top" aria-label="Code Rate" data-href="Code Rate" href="the-guide/information-theory/information-theory-1/channel-coding/code-rate.html" class="internal-link" target="_self" rel="noopener nofollow">rate</a> of the code is 
<br>Distortion is computed via 
<br><br><br>A rate-distortion pair  is denoted achievable if for an , there exists for sufficiently large  an  rate-distortion code  () such thatThe rate-distortion region is the subset of  containing all achievable pairs .<br>
<img alt="center" src="lib/media/pasted-image-20230127114819.png" style="width: 350px; max-width: 100%;"><br>
<br> is convex and non-decreasing.
<br>
<br> if , as  uses minimum number of symbols (1)
<br>If  achievable, then

<br> with  also achievable (more symbols)
<br> with  also achievable (more distortion)


<br><br>For , the information rate-distortion function is defined as This can be formulated into an optimization problem of the form The minimum code rate for achieving distortion  is an information-theoretic quantity.]]></description><link>the-guide/information-theory/information-theory-1/data-compression/rate-distortion-theory.html</link><guid isPermaLink="false">The Guide/Information Theory/Information Theory 1/Data Compression/Rate Distortion Theory.md</guid><pubDate>Wed, 23 Apr 2025 21:50:50 GMT</pubDate><enclosure url="lib/media/pasted-image-20230127110517.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src=&quot;lib/media/pasted-image-20230127110517.png&quot;&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[Shannon-Fano Code]]></title><description><![CDATA[ 
 <br>Applies codewords of fixed lengths depending on probability.<br>Relaxing the integer constraint of the optimization problem for a <a data-tooltip-position="top" aria-label="Random Variable" data-href="Random Variable" href="the-guide/mathematics/probability-theory/random-variable.html" class="internal-link" target="_self" rel="noopener nofollow">source</a> <a data-tooltip-position="top" aria-label="Probability Distribution" data-href="Probability Distribution" href="the-guide/mathematics/probability-theory/probability-distribution.html" class="internal-link" target="_self" rel="noopener nofollow">distribution's</a>  optimal <a data-tooltip-position="top" aria-label="Classes of Codes" data-href="Classes of Codes" href="the-guide/information-theory/information-theory-1/data-compression/classes-of-codes.html" class="internal-link" target="_self" rel="noopener nofollow">instantaneous code</a>, the solution yields<br>
As with <a data-href="Huffman Coding" href="the-guide/information-theory/information-theory-1/data-compression/huffman-coding.html" class="internal-link" target="_self" rel="noopener nofollow">Huffman Coding</a>, this result is within one bit from the <a data-tooltip-position="top" aria-label="Source Coding Theorem" data-href="Source Coding Theorem" href="the-guide/information-theory/information-theory-1/data-compression/source-coding-theorem.html" class="internal-link" target="_self" rel="noopener nofollow">entropy bound</a>.<br><br>
<br>Can be non-optimal

<br>Two symbols with probabilites  and  are assigned codewords with length  and , respectively.


]]></description><link>the-guide/information-theory/information-theory-1/data-compression/shannon-fano-code.html</link><guid isPermaLink="false">The Guide/Information Theory/Information Theory 1/Data Compression/Shannon-Fano Code.md</guid><pubDate>Wed, 23 Apr 2025 21:50:50 GMT</pubDate></item><item><title><![CDATA[Shannon-Fano-Elias-Coding]]></title><description><![CDATA[ 
 <br><a data-href="Classes of Codes" href="the-guide/information-theory/information-theory-1/data-compression/classes-of-codes.html" class="internal-link" target="_self" rel="noopener nofollow">Classes of Codes</a> that uses <a data-tooltip-position="top" aria-label="Cumulative Distribution Function" data-href="Cumulative Distribution Function" href="the-guide/mathematics/probability-theory/cumulative-distribution-function.html" class="internal-link" target="_self" rel="noopener nofollow">cdf</a> in order to encode symbol . The  real number representing the value in the middle of the step corresponding to a symbol's probability is denoted<br><br>
<br>For each <br>
- Compute <a data-tooltip-position="top" aria-label="Arithmetic Coding" data-href="Arithmetic Coding" href="the-guide/information-theory/information-theory-1/data-compression/arithmetic-coding.html" class="internal-link" target="_self" rel="noopener nofollow">binary expansion</a> of  (in general infinite)<br>
- Truncate this binar representation so that it is in the intervall  using codewords of length <img alt="Pasted image 20221125103129.png" src="lib/media/pasted-image-20221125103129.png">
]]></description><link>the-guide/information-theory/information-theory-1/data-compression/shannon-fano-elias-coding.html</link><guid isPermaLink="false">The Guide/Information Theory/Information Theory 1/Data Compression/Shannon-Fano-Elias-Coding.md</guid><pubDate>Wed, 23 Apr 2025 21:50:50 GMT</pubDate><enclosure url="lib/media/pasted-image-20221125103129.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src=&quot;lib/media/pasted-image-20221125103129.png&quot;&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[Source Coding Theorem]]></title><description><![CDATA[ 
 <br>Given a discrete memoryless <a data-tooltip-position="top" aria-label="Random Variable" data-href="Random Variable" href="the-guide/mathematics/probability-theory/random-variable.html" class="internal-link" target="_self" rel="noopener nofollow">source</a> of <a data-tooltip-position="top" aria-label="Shannon Entropy" data-href="Shannon Entropy" href="the-guide/information-theory/information-theory-1/information/shannon-entropy.html" class="internal-link" target="_self" rel="noopener nofollow">entropy</a> , the average codeword length  for an <a data-tooltip-position="top" aria-label="Classes of Codes" data-href="Classes of Codes" href="the-guide/information-theory/information-theory-1/data-compression/classes-of-codes.html" class="internal-link" target="_self" rel="noopener nofollow">optimal</a> distortionless source encoding scheme is bounded as<br>
Thereby, the <a data-tooltip-position="top" aria-label="Average Codeword Length" data-href="Average Codeword Length" href="the-guide/information-theory/information-theory-1/data-compression/average-codeword-length.html" class="internal-link" target="_self" rel="noopener nofollow">average codeword length</a> is lower-bounded by the <a data-tooltip-position="top" aria-label="Shannon Entropy" data-href="Shannon Entropy" href="the-guide/information-theory/information-theory-1/information/shannon-entropy.html" class="internal-link" target="_self" rel="noopener nofollow">entropy</a> . Corresponds to the number of binary questions asked to determine the value of .<br><br><br>
<br>Divide  into the <a data-tooltip-position="top" aria-label="Typical Sets" data-href="Typical Sets" href="the-guide/information-theory/information-theory-1/data-compression/typical-sets.html" class="internal-link" target="_self" rel="noopener nofollow">typical set</a>  and its complement  
<br>As , the number  of bits required to describe a sequence of messages in  is bounded by 
<br>To each  we assign a codeword of length  which consists of the prefix 0 and a unique sequence out of  .
<br>To each  we assign a codeword of length , which consists of the prefix 0 and a unique sequence out of  .
<br><br><br>For  <a data-tooltip-position="top" aria-label="Statistical Independence" data-href="Statistical Independence" href="the-guide/mathematics/statistics/statistical-independence.html" class="internal-link" target="_self" rel="noopener nofollow">i.i.d.</a> ∼  and for every  there exists a code that maps sequences  of length  into a binary string (one-to-one mapping) and that satisfies  for .<br>
<br>Provides a lower bound for the <a data-tooltip-position="top" aria-label="Average Codeword Length" data-href="Average Codeword Length" href="the-guide/information-theory/information-theory-1/data-compression/average-codeword-length.html" class="internal-link" target="_self" rel="noopener nofollow">average codeword length</a>, source entropy is fundamental limit for number of bits per symbol to represent a source.
<br>Not constructive.
<br><br><br>
<br>For optimal symbol codes, the source coding theorem holds

<br>Average overhead of one bit


<br>For sources with an entropy close to 1, a lot of bits are redundant
]]></description><link>the-guide/information-theory/information-theory-1/data-compression/source-coding-theorem.html</link><guid isPermaLink="false">The Guide/Information Theory/Information Theory 1/Data Compression/Source Coding Theorem.md</guid><pubDate>Wed, 23 Apr 2025 21:50:50 GMT</pubDate></item><item><title><![CDATA[Typical Sets]]></title><description><![CDATA[ 
 <br>Info
From the <a data-tooltip-position="top" aria-label="Asymptotic Equipartition Property" data-href="Asymptotic Equipartition Property" href="the-guide/information-theory/information-theory-1/data-compression/asymptotic-equipartition-property.html" class="internal-link" target="_self" rel="noopener nofollow">AEP</a>, it directly follows that for large , most (with a high probability) sequences have the probabilityThe typical set  with respect to  is the set of sequences  for which holds that 
<br><br>
<br>Probability of drawing a typical sequence is  
<br>
<br>
<br><br><br>
<br>Information Theory

<br><a data-tooltip-position="top" aria-label="Statistical Independence" data-href="Statistical Independence" href="the-guide/mathematics/statistics/statistical-independence.html" class="internal-link" target="_self" rel="noopener nofollow">I.i.d.</a> sequence of length  with  and . The relative number of ones is plotted below.<img alt="center" src="lib/media/screenshot-from-2023-02-11-17-14-49.png">


<br>Betancourt

<br>Each Sequence is a point in -dimensional space with a probability assigned to it, typical set is <a data-tooltip-position="top" aria-label="Set" data-href="Set" href="the-guide/mathematics/general-stuff/set.html" class="internal-link" target="_self" rel="noopener nofollow">set</a> of points with largest influence on <a data-tooltip-position="top" aria-label="Expectations" data-href="Expectations" href="the-guide/mathematics/statistics/expectations.html" class="internal-link" target="_self" rel="noopener nofollow">expectation</a>. With higher dimensions, this set is more and more concentrated.
<br><a data-href="Expectations" href="the-guide/mathematics/statistics/expectations.html" class="internal-link" target="_self" rel="noopener nofollow">Expectations</a> is computed via integral  with distribution . Therefore, a point's relevance is determined by the product of <a data-tooltip-position="top" aria-label="Probability Density Function" data-href="Probability Density Function" href="the-guide/mathematics/probability-theory/probability-density-function.html" class="internal-link" target="_self" rel="noopener nofollow">density</a> and volume element. With higher dimensions, there is more and more volume outside any given region<img alt="center" src="lib/media/pasted-image-20231019111944.png" style="width: 500px; max-width: 100%;">
<br>The sweet-spot is the typical set, where the product contributes a lot to the integral. With higher dimensions, this leads to a concentration of this set because the relative volume becomes smaller and smaller.<img alt="center" src="lib/media/pasted-image-20231019114347.png" style="width: 300px; max-width: 100%;">
<br><a data-tooltip-position="top" aria-label="Estimator" data-href="Estimator" href="the-guide/mathematics/statistics/estimator.html" class="internal-link" target="_self" rel="noopener nofollow">Estimating</a> <a data-tooltip-position="top" aria-label="Expectations" data-href="Expectations" href="the-guide/mathematics/statistics/expectations.html" class="internal-link" target="_self" rel="noopener nofollow">expectation</a> only using typical set becomes very efficient for high dimensions !


]]></description><link>the-guide/information-theory/information-theory-1/data-compression/typical-sets.html</link><guid isPermaLink="false">The Guide/Information Theory/Information Theory 1/Data Compression/Typical Sets.md</guid><pubDate>Wed, 23 Apr 2025 21:50:50 GMT</pubDate><enclosure url="lib/media/screenshot-from-2023-02-11-17-14-49.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src=&quot;lib/media/screenshot-from-2023-02-11-17-14-49.png&quot;&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[Chain Rule for Entropy]]></title><description><![CDATA[ 
 <br>Success
The <a data-tooltip-position="top" aria-label="Shannon Entropy" data-href="Shannon Entropy" href="the-guide/information-theory/information-theory-1/information/shannon-entropy.html" class="internal-link" target="_self" rel="noopener nofollow">joint, conditional and marginal entropy</a> of a <a data-tooltip-position="top" aria-label="Probability Distribution" data-href="Probability Distribution" href="the-guide/mathematics/probability-theory/probability-distribution.html" class="internal-link" target="_self" rel="noopener nofollow">joint distribution</a> are related via the equalityFor three <a data-tooltip-position="top" aria-label="Random Variable" data-href="Random Variable" href="the-guide/mathematics/probability-theory/random-variable.html" class="internal-link" target="_self" rel="noopener nofollow">random variables</a> , one obtainsThe general form is
]]></description><link>the-guide/information-theory/information-theory-1/information/chain-rule-for-entropy.html</link><guid isPermaLink="false">The Guide/Information Theory/Information Theory 1/Information/Chain Rule for Entropy.md</guid><pubDate>Wed, 23 Apr 2025 21:50:45 GMT</pubDate></item><item><title><![CDATA[Conditioning Reduces Entropy]]></title><description><![CDATA[ 
 <br>Theorem
The <a data-tooltip-position="top" aria-label="Shannon Entropy" data-href="Shannon Entropy" href="the-guide/information-theory/information-theory-1/information/shannon-entropy.html" class="internal-link" target="_self" rel="noopener nofollow">conditional entropy</a> is always smaller or equal to the <a data-tooltip-position="top" aria-label="Shannon Entropy" data-href="Shannon Entropy" href="the-guide/information-theory/information-theory-1/information/shannon-entropy.html" class="internal-link" target="_self" rel="noopener nofollow">entropy</a> of the <a data-tooltip-position="top" aria-label="Random Variable" data-href="Random Variable" href="the-guide/mathematics/probability-theory/random-variable.html" class="internal-link" target="_self" rel="noopener nofollow">random variable</a> under condition alonewith equality only in the case of <a data-tooltip-position="top" aria-label="Statistical Independence" data-href="Statistical Independence" href="the-guide/mathematics/statistics/statistical-independence.html" class="internal-link" target="_self" rel="noopener nofollow">statistical independence</a>.
<br><br>
<br>Only viable for averages (<a data-href="Shannon Entropy" href="the-guide/information-theory/information-theory-1/information/shannon-entropy.html" class="internal-link" target="_self" rel="noopener nofollow">Shannon Entropy</a>) !

<br>Specific new evidence might increase uncertainty.


]]></description><link>the-guide/information-theory/information-theory-1/information/conditioning-reduces-entropy.html</link><guid isPermaLink="false">The Guide/Information Theory/Information Theory 1/Information/Conditioning Reduces Entropy.md</guid><pubDate>Wed, 23 Apr 2025 21:50:45 GMT</pubDate></item><item><title><![CDATA[Cross Entropy]]></title><description><![CDATA[ 
 <br>In a Nutshell
Information-theoretic quantity that measures the average number of bits (or other unit depending on log) of information needed to identify a realization of a true <a data-tooltip-position="top" aria-label="Probability Distribution" data-href="Probability Distribution" href="the-guide/mathematics/probability-theory/probability-distribution.html" class="internal-link" target="_self" rel="noopener nofollow">distribution</a>  based on a coding scheme that was optimized on another, related distribution  over the same support.
<br><br>Definition
Assuming two <a data-tooltip-position="top" aria-label="Random Variable" data-href="Random Variable" href="the-guide/mathematics/probability-theory/random-variable.html" class="internal-link" target="_self" rel="noopener nofollow">random sources</a>  with the same support, the cross entropy is given by for discrete variables and In this case, we have to assume that  and  are absolutely continuous with respect to some <a data-tooltip-position="top" aria-label="Measure" data-href="Measure" href="the-guide/mathematics/measure-theory/measure.html" class="internal-link" target="_self" rel="noopener nofollow">measure</a>, usually <a data-tooltip-position="top" aria-label="Lebesgue Measure" data-href="Lebesgue Measure" href="the-guide/mathematics/measure-theory/lebesgue-measure.html" class="internal-link" target="_self" rel="noopener nofollow">Lebesgue measure</a> on a Borel <a data-href="Sigma-Algebra" href="the-guide/mathematics/probability-theory/sigma-algebra.html" class="internal-link" target="_self" rel="noopener nofollow">Sigma-Algebra</a>. The notation is usually the same as for the <a data-tooltip-position="top" aria-label="Shannon Entropy" data-href="Shannon Entropy" href="the-guide/information-theory/information-theory-1/information/shannon-entropy.html" class="internal-link" target="_self" rel="noopener nofollow">joint entropy</a> and has to be clarified.
<br><br>Consider  values  from a training set that are <a data-tooltip-position="top" aria-label="Statistical Independence" data-href="Statistical Independence" href="the-guide/mathematics/statistics/statistical-independence.html" class="internal-link" target="_self" rel="noopener nofollow">i.i.d.</a>. Further assume a parametrized family of distributions  subject to maximization. We can express the <a data-tooltip-position="top" aria-label="Likelihood Function" data-href="Likelihood Function" href="the-guide/mathematics/statistics/likelihood-function.html" class="internal-link" target="_self" rel="noopener nofollow">likelihood</a> as usual via If we denote the count of occurrences of an observation as  (frequency therefore via ) and use the <a data-href="Perplexity" href="Perplexity" class="internal-link" target="_self" rel="noopener nofollow">Perplexity</a> . Using logarithm-rules, the perplexity can be written as , yieldingThe logarithm used to obtain the log likelihood does not affect extremization, which means that likelihood maximization amounts to cross-entropy minimization.<br><br>Given a test set of size  and a probability  of an <a data-tooltip-position="top" aria-label="Sigma-Algebra" data-href="Sigma-Algebra" href="the-guide/mathematics/probability-theory/sigma-algebra.html" class="internal-link" target="_self" rel="noopener nofollow">event</a> , estimated from a training set , we can obtain a Monte Carlo <a data-tooltip-position="top" aria-label="Estimator" data-href="Estimator" href="the-guide/mathematics/statistics/estimator.html" class="internal-link" target="_self" rel="noopener nofollow">estimator</a> of the cross entropy via ]]></description><link>the-guide/information-theory/information-theory-1/information/cross-entropy.html</link><guid isPermaLink="false">The Guide/Information Theory/Information Theory 1/Information/Cross Entropy.md</guid><pubDate>Wed, 23 Apr 2025 21:50:45 GMT</pubDate></item><item><title><![CDATA[Jensen's Inequality]]></title><description><![CDATA[ 
 <br>Quote
"For a <a data-tooltip-position="top" aria-label="Convexity" data-href="Convexity" href="the-guide/mathematics/optimization/convex-optimization-lecture/convexity.html" class="internal-link" target="_self" rel="noopener nofollow">convex</a> <a data-tooltip-position="top" aria-label="Mathematical Relations" data-href="Mathematical Relations" href="the-guide/mathematics/general-stuff/mathematical-relations.html" class="internal-link" target="_self" rel="noopener nofollow">function</a>, the average of the output is larger than the output of the average input."
<br><br>Jensen's Inequality
If  is a <a data-tooltip-position="top" aria-label="Convexity" data-href="Convexity" href="the-guide/mathematics/optimization/convex-optimization-lecture/convexity.html" class="internal-link" target="_self" rel="noopener nofollow">convex</a> function and  a <a data-tooltip-position="top" aria-label="Random Variable" data-href="Random Variable" href="the-guide/mathematics/probability-theory/random-variable.html" class="internal-link" target="_self" rel="noopener nofollow">random variable</a>, thenMore generally, if  is a function that is convex over some interval ,  and , it holds that The opposite is true for concave functions. Equality is attained for linear functions.
<br><br><br>Measure-Theoretic Form
Given a <a data-tooltip-position="top" aria-label="Probability Space" data-href="Probability Space" href="the-guide/mathematics/probability-theory/probability-space.html" class="internal-link" target="_self" rel="noopener nofollow">probability space</a> , a -<a data-tooltip-position="top" aria-label="Measurable Function" data-href="Measurable Function" href="the-guide/mathematics/measure-theory/measurable-function.html" class="internal-link" target="_self" rel="noopener nofollow">measurable function</a>  and a <a data-tooltip-position="top" aria-label="Convexity" data-href="Convexity" href="the-guide/mathematics/optimization/convex-optimization-lecture/convexity.html" class="internal-link" target="_self" rel="noopener nofollow">convex</a> <a data-tooltip-position="top" aria-label="Mathematical Relations" data-href="Mathematical Relations" href="the-guide/mathematics/general-stuff/mathematical-relations.html" class="internal-link" target="_self" rel="noopener nofollow">function</a> , it holds that 
<br>Generalized Version - Jensen Gap
Let  be a one-dimensional <a data-tooltip-position="top" aria-label="Random Variable" data-href="Random Variable" href="the-guide/mathematics/probability-theory/random-variable.html" class="internal-link" target="_self" rel="noopener nofollow">random variable</a> on  with <a data-tooltip-position="top" aria-label="Expectations" data-href="Expectations" href="the-guide/mathematics/statistics/expectations.html" class="internal-link" target="_self" rel="noopener nofollow">mean</a>  and <a data-tooltip-position="top" aria-label="Variance" data-href="Variance" href="Variance" class="internal-link" target="_self" rel="noopener nofollow">variance</a> . Let  and define Then is holds that If  is convex / concave, then  is monotonously increasing / decreasing
<br><br><br>
<br><a rel="noopener nofollow" class="external-link" href="https://www.youtube.com/watch?v=u0_X2hX6DWE" target="_blank">https://www.youtube.com/watch?v=u0_X2hX6DWE</a>
]]></description><link>the-guide/information-theory/information-theory-1/information/jensen&apos;s-inequality.html</link><guid isPermaLink="false">The Guide/Information Theory/Information Theory 1/Information/Jensen&apos;s Inequality.md</guid><pubDate>Wed, 23 Apr 2025 21:50:45 GMT</pubDate></item><item><title><![CDATA[Kullback-Leibler Divergence]]></title><description><![CDATA[ 
 <br>In a Nutshell
"The <a data-tooltip-position="top" aria-label="Expectations" data-href="Expectations" href="the-guide/mathematics/statistics/expectations.html" class="internal-link" target="_self" rel="noopener nofollow">expected</a> excess <a data-tooltip-position="top" aria-label="Shannon Entropy" data-href="Shannon Entropy" href="the-guide/information-theory/information-theory-1/information/shannon-entropy.html" class="internal-link" target="_self" rel="noopener nofollow">uncertainty</a> from using  as a model when the actual distribution is ."
<br><br>KL-Divergence
Type of <a data-tooltip-position="top" aria-label="Divergence" data-href="Divergence" href="the-guide/information-theory/information-geometry/divergence.html" class="internal-link" target="_self" rel="noopener nofollow">divergence</a>, specifically a <a data-href="Bregman Divergence" href="the-guide/information-theory/information-geometry/bregman-divergence.html" class="internal-link" target="_self" rel="noopener nofollow">Bregman Divergence</a> generated by the negative <a data-tooltip-position="top" aria-label="Shannon Entropy" data-href="Shannon Entropy" href="the-guide/information-theory/information-theory-1/information/shannon-entropy.html" class="internal-link" target="_self" rel="noopener nofollow">entropy</a> that measure how one <a data-tooltip-position="top" aria-label="Probability Distribution" data-href="Probability Distribution" href="the-guide/mathematics/probability-theory/probability-distribution.html" class="internal-link" target="_self" rel="noopener nofollow">probability distribution</a> P is different from a second, reference probability distribution Q.Sometimes also denoted relative entropy.
<br><br>Intuition

<br>In coding,  gives the expected number of extra bits required to code samples drawn from P using a code optimized for Q (P and Q can for example represent the estimated distribution and the true distribution).

<br>Codewords were distributed according to estimated distribution, leads to more average bits if real distribution is different.


<br>Can also be understood as the lost information when approximating  instead of .
<br>If for any  it holds that , while if  (different alphabets), the Kullback-Leibler Divergence becomes infinite.

<br><br><br>For <a data-tooltip-position="top" aria-label="Gaussian Distribution" data-href="Gaussian Distribution" href="the-guide/mathematics/probability-theory/gaussian-distribution.html" class="internal-link" target="_self" rel="noopener nofollow">Gaussian</a> <a data-tooltip-position="top" aria-label="Probability Distribution" data-href="Probability Distribution" href="the-guide/mathematics/probability-theory/probability-distribution.html" class="internal-link" target="_self" rel="noopener nofollow">distributions</a>, we can derive very simple closed.form expressions that find a lot if applications in loss functions. Given  and , it holds that , where the three terms can be categorized as <a data-tooltip-position="top" aria-label="Shannon Entropy" data-href="Shannon Entropy" href="the-guide/information-theory/information-theory-1/information/shannon-entropy.html" class="internal-link" target="_self" rel="noopener nofollow">entropy</a>,  and .<br>
<br>For single dimensions, we obtain 
<br><br><br>
<br>It is not symmetric () in the two distributions (in contrast to <a data-tooltip-position="top" aria-label="https://en.wikipedia.org/wiki/Variation_of_information" rel="noopener nofollow" class="external-link" title="Variation of information" href="https://en.wikipedia.org/wiki/Variation_of_information" target="_blank">variation of information</a>), and does not satisfy the <a data-tooltip-position="top" aria-label="https://en.wikipedia.org/wiki/Triangle_inequality" rel="noopener nofollow" class="external-link" title="Triangle inequality" href="https://en.wikipedia.org/wiki/Triangle_inequality" target="_blank">triangle inequality</a>. Instead, in terms of <a data-tooltip-position="top" aria-label="Information Geometry" data-href="Information Geometry" href="the-guide/information-theory/information-geometry/information-geometry.html" class="internal-link" target="_self" rel="noopener nofollow">information geometry</a>, it is a type of <a data-tooltip-position="top" aria-label="Divergence" data-href="Divergence" href="the-guide/information-theory/information-geometry/divergence.html" class="internal-link" target="_self" rel="noopener nofollow">divergence</a>, a generalization of <a data-tooltip-position="top" aria-label="https://en.wikipedia.org/wiki/Squared_Euclidean_distance" rel="noopener nofollow" class="external-link" title="Squared Euclidean distance" href="https://en.wikipedia.org/wiki/Squared_Euclidean_distance" target="_blank">squared distance</a>, and for certain classes of distributions (notably an <a data-tooltip-position="top" aria-label="https://en.wikipedia.org/wiki/Exponential_family" rel="noopener nofollow" class="external-link" title="Exponential family" href="https://en.wikipedia.org/wiki/Exponential_family" target="_blank">exponential family</a>), it satisfies a generalized <a data-tooltip-position="top" aria-label="https://en.wikipedia.org/wiki/Pythagorean_theorem" rel="noopener nofollow" class="external-link" title="Pythagorean theorem" href="https://en.wikipedia.org/wiki/Pythagorean_theorem" target="_blank">Pythagorean theorem</a> (which applies to squared distances).
]]></description><link>the-guide/information-theory/information-theory-1/information/kullback-leibler-divergence.html</link><guid isPermaLink="false">The Guide/Information Theory/Information Theory 1/Information/Kullback-Leibler Divergence.md</guid><pubDate>Wed, 23 Apr 2025 21:50:45 GMT</pubDate></item><item><title><![CDATA[Limiting Density of Discrete Points]]></title><description><![CDATA[ 
 <br>Adjustment to the differential entropy formula by Shannon, who never proved his version because his work focused on discrete case. The above has <a data-tooltip-position="top" aria-label="Shannon Entropy" data-href="Shannon Entropy" href="the-guide/information-theory/information-theory-1/information/shannon-entropy.html" class="internal-link" target="_self" rel="noopener nofollow">several limitations</a> that make this formulation flawed.<br>Definition
Instead, the LDDP is obtained by taking the limit of increasingly dense discrete <a data-tooltip-position="top" aria-label="Probability Distribution" data-href="Probability Distribution" href="the-guide/mathematics/probability-theory/probability-distribution.html" class="internal-link" target="_self" rel="noopener nofollow">distributions</a>, leading to where the technically infinite log-term is omitted. 
<br>The result is actually the negative <a data-tooltip-position="top" aria-label="Kullback-Leibler Divergence" data-href="Kullback-Leibler Divergence" href="the-guide/information-theory/information-theory-1/information/kullback-leibler-divergence.html" class="internal-link" target="_self" rel="noopener nofollow">KL-Divergece</a> from  (uniform) to .]]></description><link>the-guide/information-theory/information-theory-1/information/limiting-density-of-discrete-points.html</link><guid isPermaLink="false">The Guide/Information Theory/Information Theory 1/Information/Limiting Density of Discrete Points.md</guid><pubDate>Wed, 23 Apr 2025 21:50:45 GMT</pubDate></item><item><title><![CDATA[Maximum Entropy Distribution]]></title><description><![CDATA[ 
 <br><a class="internal-link" data-href="Covariance and Variance.md" href="the-guide/mathematics/statistics/covariance-and-variance.html" target="_self" rel="noopener nofollow"></a>&gt;[!success] Gaussian is maximum entropy Distribution given Expectation and Variance<br>
Let  be a real valued (continuous) <a data-tooltip-position="top" aria-label="Random Variable" data-href="Random Variable" href="the-guide/mathematics/probability-theory/random-variable.html" class="internal-link" target="_self" rel="noopener nofollow">random variable</a> with <a data-tooltip-position="top" aria-label="Expectations" data-href="Expectations" href="the-guide/mathematics/statistics/expectations.html" class="internal-link" target="_self" rel="noopener nofollow">expectation</a>  and  (<a data-href="Variance" href="Variance" class="internal-link" target="_self" rel="noopener nofollow">Variance</a>) . Thenwith equality if and only if . Hence, the <a data-tooltip-position="top" aria-label="Gaussian Distribution" data-href="Gaussian Distribution" href="the-guide/mathematics/probability-theory/gaussian-distribution.html" class="internal-link" target="_self" rel="noopener nofollow">Gaussian</a> <a data-tooltip-position="top" aria-label="Probability Distribution" data-href="Probability Distribution" href="the-guide/mathematics/probability-theory/probability-distribution.html" class="internal-link" target="_self" rel="noopener nofollow">distribution</a> is the maximum entropy distribution on  for given <a data-tooltip-position="top" aria-label="Expectations" data-href="Expectations" href="the-guide/mathematics/statistics/expectations.html" class="internal-link" target="_self" rel="noopener nofollow">first</a> and <a data-tooltip-position="top" aria-label="Variance" data-href="Variance" href="Variance" class="internal-link" target="_self" rel="noopener nofollow">second</a> order <a data-tooltip-position="top" aria-label="Probability Distribution" data-href="Probability Distribution" href="the-guide/mathematics/probability-theory/probability-distribution.html" class="internal-link" target="_self" rel="noopener nofollow">moment</a>.
<br><br>
<br>Maximum Entropy Distribution if only interval of alphabet is given is the [[Probability Distribution|uniform distribution<a class="internal-link" data-href="Covariance and Variance.md" href="the-guide/mathematics/statistics/covariance-and-variance.html" target="_self" rel="noopener nofollow"></a>m Entropy Distribution if only <a data-tooltip-position="top" aria-label="Expectations" data-href="Expectations" href="the-guide/mathematics/statistics/expectations.html" class="internal-link" target="_self" rel="noopener nofollow">expectation</a> is given is geometric distribution.

<br>Binomial, but with fixed number of successes instead of trials.


]]></description><link>the-guide/information-theory/information-theory-1/information/maximum-entropy-distribution.html</link><guid isPermaLink="false">The Guide/Information Theory/Information Theory 1/Information/Maximum Entropy Distribution.md</guid><pubDate>Wed, 23 Apr 2025 21:50:45 GMT</pubDate></item><item><title><![CDATA[Mutual Information]]></title><description><![CDATA[ 
 <br>Mutual Information of discrete Random Variables
let  be a pair of <a data-tooltip-position="top" aria-label="Random Variable" data-href="Random Variable" href="the-guide/mathematics/probability-theory/random-variable.html" class="internal-link" target="_self" rel="noopener nofollow">random variables</a> with values over the <a data-tooltip-position="top" aria-label="Measure" data-href="Measure" href="the-guide/mathematics/measure-theory/measure.html" class="internal-link" target="_self" rel="noopener nofollow">measurable space</a> . The mutual information between them is defined as the average information exchange between their symbols asUsing <a data-href="Bayes Theorem" href="the-guide/mathematics/statistics/bayes-theorem.html" class="internal-link" target="_self" rel="noopener nofollow">Bayes Theorem</a>, this can be written as 
<br><br>Intuition

<br>Quantifies how much ”guesswork” we can save by jointly guessing the pair  using the <a data-tooltip-position="top" aria-label="Probability Distribution" data-href="Probability Distribution" href="the-guide/mathematics/probability-theory/probability-distribution.html" class="internal-link" target="_self" rel="noopener nofollow">joint distribution</a> instead of guessing  and  separately using the marginals. This encompasses how much additional structure or information is encoded in the joint distribution when comparing it to the assumption that both of them are independent.

<br>Problems in Practice
Mutual information can be difficult to reliably <a data-tooltip-position="top" aria-label="Estimator" data-href="Estimator" href="the-guide/mathematics/statistics/estimator.html" class="internal-link" target="_self" rel="noopener nofollow">estimate</a> in practice, because we usually rely on density estimations. Even small errors in these can have large effects on the mutual information.
<br><br>Mutual Information of continuous Random Variables
The mutual information between two continuous random sources X and Y with the joint symbol <a data-tooltip-position="top" aria-label="Probability Density Function" data-href="Probability Density Function" href="the-guide/mathematics/probability-theory/probability-density-function.html" class="internal-link" target="_self" rel="noopener nofollow">pdf</a>  is given by
<br>
<br>Properties<br>
The properties are the same as for the discrete case.
]]></description><link>the-guide/information-theory/information-theory-1/information/mutual-information.html</link><guid isPermaLink="false">The Guide/Information Theory/Information Theory 1/Information/Mutual Information.md</guid><pubDate>Wed, 23 Apr 2025 21:50:45 GMT</pubDate></item><item><title><![CDATA[Shannon Entropy]]></title><description><![CDATA[ 
 <br>In a Nutshell
Core idea of Shannon's Information Theory that frames information as the resolution / reduction of uncertainty about an <a data-tooltip-position="top" aria-label="Sigma-Algebra" data-href="Sigma-Algebra" href="the-guide/mathematics/probability-theory/sigma-algebra.html" class="internal-link" target="_self" rel="noopener nofollow">event</a>.
<br><br><br>Entropy of a Random Variable
The Shannon Entropy of the discrete <a data-tooltip-position="top" aria-label="Random Variable" data-href="Random Variable" href="the-guide/mathematics/probability-theory/random-variable.html" class="internal-link" target="_self" rel="noopener nofollow">source</a>  with alphabet  is defined asand describes the average information per symbol / characterizes the source uncertainty.
<br>
<br>Properties

<br>
<br>Bounded as 

<br> is only achieved when one value has probability .
<br>Maximum entropy when all symbols have the same probability.




<br>Intuition

<br>Number of bits / binary questions needed per message in order to identify symbol from space of possible answers (alphabet). <a data-tooltip-position="top" aria-label="https://www.youtube.com/watch?v=v68zYyaEmEA" rel="noopener nofollow" class="external-link" href="https://www.youtube.com/watch?v=v68zYyaEmEA" target="_blank">Each bit cuts possible answers in half.</a> 
<br>Logarithm is needed, as Information about two observed <a data-tooltip-position="top" aria-label="Statistical Independence" data-href="Statistical Independence" href="the-guide/mathematics/statistics/statistical-independence.html" class="internal-link" target="_self" rel="noopener nofollow">independent</a> <a data-tooltip-position="top" aria-label="Random Variable" data-href="Random Variable" href="the-guide/mathematics/probability-theory/random-variable.html" class="internal-link" target="_self" rel="noopener nofollow">random variables</a> should add up (probabilities multiply).


<br>This can be extended in a natural way to <a data-tooltip-position="top" aria-label="Probability Distribution" data-href="Probability Distribution" href="the-guide/mathematics/probability-theory/probability-distribution.html" class="internal-link" target="_self" rel="noopener nofollow">joint</a> and <a data-tooltip-position="top" aria-label="Probability Distribution" data-href="Probability Distribution" href="the-guide/mathematics/probability-theory/probability-distribution.html" class="internal-link" target="_self" rel="noopener nofollow">conditional</a> <a data-tooltip-position="top" aria-label="Probability Distribution" data-href="Probability Distribution" href="the-guide/mathematics/probability-theory/probability-distribution.html" class="internal-link" target="_self" rel="noopener nofollow">distributions</a> via ...<br>Joint Entropy
The joint <a data-tooltip-position="top" aria-label="Shannon Entropy" data-href="Shannon Entropy" href="the-guide/information-theory/information-theory-1/information/shannon-entropy.html" class="internal-link" target="_self" rel="noopener nofollow">entropy</a> describes the average amount of information when drawing from a joint distribution of two <a data-tooltip-position="top" aria-label="Random Variable" data-href="Random Variable" href="the-guide/mathematics/probability-theory/random-variable.html" class="internal-link" target="_self" rel="noopener nofollow">random variables</a>
<br>
<br>Properties

<br> 
<br>
<br><a data-tooltip-position="top" aria-label="Statistical Independence" data-href="Statistical Independence" href="the-guide/mathematics/statistics/statistical-independence.html" class="internal-link" target="_self" rel="noopener nofollow">Independence</a> implies .

<br>Independence bound on entropy 




<br>Intuition

<br>Amount of information gathered from two events happening together at the same point in time.


<br>Conditional Entropy
The conditional <a data-tooltip-position="top" aria-label="Shannon Entropy" data-href="Shannon Entropy" href="the-guide/information-theory/information-theory-1/information/shannon-entropy.html" class="internal-link" target="_self" rel="noopener nofollow">entropy</a>  describes the average amount of remaining uncertainty about  after the <a data-tooltip-position="top" aria-label="Random Variable" data-href="Random Variable" href="the-guide/mathematics/probability-theory/random-variable.html" class="internal-link" target="_self" rel="noopener nofollow">random variable</a>  has been observed. It is defined via
<br>
<br>Properties 

<br>
<br>
<br><a data-tooltip-position="top" aria-label="Statistical Independence" data-href="Statistical Independence" href="the-guide/mathematics/statistics/statistical-independence.html" class="internal-link" target="_self" rel="noopener nofollow">Independence</a> of  and  leads to 
<br>If  depends deterministically on ,  holds.
<br>For <a data-tooltip-position="top" aria-label="Random Variable" data-href="Random Variable" href="the-guide/mathematics/probability-theory/random-variable.html" class="internal-link" target="_self" rel="noopener nofollow">continuous random variables</a> 


<br><br><br>For continuous <a data-tooltip-position="top" aria-label="Random Variable" data-href="Random Variable" href="the-guide/mathematics/probability-theory/random-variable.html" class="internal-link" target="_self" rel="noopener nofollow">random variables</a>, Shannon assumed a differential entropybut never actually derived it. It turns out that the above is flawed, because<br>
<br>It is not invariant under change of variables
<br>Can become negative
<br>Dimensions are wrong<br>
A corrected version was derived by Edwin Thompson Jaynes in form of <a data-tooltip-position="top" aria-label="Limiting Density of Discrete Points" data-href="Limiting Density of Discrete Points" href="the-guide/information-theory/information-theory-1/information/limiting-density-of-discrete-points.html" class="internal-link" target="_self" rel="noopener nofollow">LDDP</a>.
]]></description><link>the-guide/information-theory/information-theory-1/information/shannon-entropy.html</link><guid isPermaLink="false">The Guide/Information Theory/Information Theory 1/Information/Shannon Entropy.md</guid><pubDate>Wed, 23 Apr 2025 21:50:45 GMT</pubDate></item><item><title><![CDATA[Venn Diagram]]></title><description><![CDATA[ 
 <br>Visualizationof relation between <a data-tooltip-position="top" aria-label="Mutual Information" data-href="Mutual Information" href="the-guide/information-theory/information-theory-1/information/mutual-information.html" class="internal-link" target="_self" rel="noopener nofollow">mutual information</a>, <a data-tooltip-position="top" aria-label="Shannon Entropy" data-href="Shannon Entropy" href="the-guide/information-theory/information-theory-1/information/shannon-entropy.html" class="internal-link" target="_self" rel="noopener nofollow">joint entropy, conditional entropy and the marginal entropy</a> of two <a data-tooltip-position="top" aria-label="Random Variable" data-href="Random Variable" href="the-guide/mathematics/probability-theory/random-variable.html" class="internal-link" target="_self" rel="noopener nofollow">random variables</a>.<br>
<img alt="400|center" src="lib/media/pasted-image-20221110090640.png"><br><br>
<br>Blue circle is <a data-tooltip-position="top" aria-label="Shannon Entropy" data-href="Shannon Entropy" href="the-guide/information-theory/information-theory-1/information/shannon-entropy.html" class="internal-link" target="_self" rel="noopener nofollow">marginal entropy</a> !
<br>Example with three variables 

<br><img alt="Pasted image 20230210214529.png" src="lib/media/pasted-image-20230210214529.png" style="width: 250px; max-width: 100%;">

<br>Intersection of all circles may be negative p




]]></description><link>the-guide/information-theory/information-theory-1/information/venn-diagram.html</link><guid isPermaLink="false">The Guide/Information Theory/Information Theory 1/Information/Venn Diagram.md</guid><pubDate>Wed, 23 Apr 2025 21:50:45 GMT</pubDate><enclosure url="lib/media/pasted-image-20221110090640.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src=&quot;lib/media/pasted-image-20221110090640.png&quot;&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[Cheat Sheet Information Theory]]></title><description><![CDATA[ 
 <br><br>
<br>If Variables are <a data-tooltip-position="top" aria-label="Statistical Independence" data-href="Statistical Independence" href="the-guide/mathematics/statistics/statistical-independence.html" class="internal-link" target="_self" rel="noopener nofollow">independent</a> and identically distributed, they can be interchanged when computing <a data-tooltip-position="top" aria-label="Shannon Entropy" data-href="Shannon Entropy" href="the-guide/information-theory/information-theory-1/information/shannon-entropy.html" class="internal-link" target="_self" rel="noopener nofollow">entropy</a> or <a data-tooltip-position="top" aria-label="Mutual Information" data-href="Mutual Information" href="the-guide/information-theory/information-theory-1/information/mutual-information.html" class="internal-link" target="_self" rel="noopener nofollow">mutual information</a>.
<br>
<br><br>
<br>Since in the case on the left-hand side, we know what happens with : 
<br><a data-href="Chain Rule for Entropy" href="the-guide/information-theory/information-theory-1/information/chain-rule-for-entropy.html" class="internal-link" target="_self" rel="noopener nofollow">Chain Rule for Entropy</a>
<br>Can be used with <a data-href="Shannon Entropy" href="the-guide/information-theory/information-theory-1/information/shannon-entropy.html" class="internal-link" target="_self" rel="noopener nofollow">Shannon Entropy</a>

<br>If  <a data-tooltip-position="top" aria-label="Statistical Independence" data-href="Statistical Independence" href="the-guide/mathematics/statistics/statistical-independence.html" class="internal-link" target="_self" rel="noopener nofollow">i.i.d.</a>, then the above equals


<br>If  is a deterministic function of  there is no uncertainty about it: 

<br>If , then 
<br>Also (if  independent) since  is not a deterministic function of , it follows that 


<br>If  and  are independent, then 

<br>
<br>


<br>
<br><br>
<br>With condition 
<br>Chain Rule for Mutual Information 
<br>
]]></description><link>the-guide/information-theory/information-theory-1/cheat-sheet-information-theory.html</link><guid isPermaLink="false">The Guide/Information Theory/Information Theory 1/Cheat Sheet Information Theory.md</guid><pubDate>Mon, 12 Aug 2024 17:07:21 GMT</pubDate></item><item><title><![CDATA[Broadcast Channel]]></title><description><![CDATA[ 
 <br>In a Nutshell
Down-link multi-user communication scenario, where one base-station sends different information to different receivers.
<br><img alt="center" src="lib/media/pasted-image-20230621134240.png" style="width: 400px; max-width: 100%;"><br><br><br>In this first setting, we consider a 2-receiver <a data-tooltip-position="top" aria-label="Discrete Memoryless Channel" data-href="Discrete Memoryless Channel" href="the-guide/information-theory/information-theory-1/channel-coding/discrete-memoryless-channel.html" class="internal-link" target="_self" rel="noopener nofollow">discrete and memoryless</a> broadcast channel (DM-BC) , where the sender wants to send a common <a data-tooltip-position="top" aria-label="Linear Block Code" data-href="Linear Block Code" href="the-guide/information-theory/information-theory-1/coding-theory/linear-block-code.html" class="internal-link" target="_self" rel="noopener nofollow">message</a> containing independent private <a data-tooltip-position="top" aria-label="Linear Block Code" data-href="Linear Block Code" href="the-guide/information-theory/information-theory-1/coding-theory/linear-block-code.html" class="internal-link" target="_self" rel="noopener nofollow">messages</a> to each receiver.<br><img alt="center" src="lib/media/pasted-image-20230623124608.png" style="width: 400px; max-width: 100%;"><br><br>The maximum achievable rates are given via the <a data-tooltip-position="top" aria-label="Mutual Information" data-href="Mutual Information" href="the-guide/information-theory/information-theory-1/information/mutual-information.html" class="internal-link" target="_self" rel="noopener nofollow">mutual information</a>This results in two extreme points, where all the power is used to communicate with one user and the other is ignored, leading to the <a data-tooltip-position="top" aria-label="Code Rate" data-href="Code Rate" href="the-guide/information-theory/information-theory-1/channel-coding/code-rate.html" class="internal-link" target="_self" rel="noopener nofollow">rates</a> :<img alt="center" src="lib/media/pasted-image-20230623125240.png" style="width: 350px; max-width: 100%;">If the power isn't concentrated on one user, the interference of the other active users has to be added.<br>
<br>In symmetric case (same power) the sum rate is bound by the single user <a data-tooltip-position="top" aria-label="Channel Capacity" data-href="Channel Capacity" href="the-guide/information-theory/information-theory-1/channel-coding/channel-capacity.html" class="internal-link" target="_self" rel="noopener nofollow">capacity</a> 
<br>Rate pairs in capacity region can be achieved by sharing degrees of freedom between users. 
<br>Alternative approach to achieve rate pairs in the capacity region are:<br>
<br>Superposition Coding (Solid line)

<br>Consider superposed signals . This results in the power constraint A weaker user 1 decodes its own signal by treating the signal of user 2 as noise. User 2 does <a data-tooltip-position="top" aria-label="Successive Interference Cancellation" data-href="Successive Interference Cancellation" href="the-guide/information-theory/information-theory-2/successive-interference-cancellation.html" class="internal-link" target="_self" rel="noopener nofollow">SIC</a> on its own. Using this the rate pair can be achieved (user 2 now has less noise), which results in  in the symmetric case (compare to above, bound is now achieved). Leads to advantage over 


<br>Orthogonal Scheme (Dashed line)

<br>With power split  and , the rates are jointly achieved.  can be interpreted as e.g. the fraction of bandwidth or time.


<br>Comparison in non-symmetric case<img alt="center" src="lib/media/pasted-image-20230623172157.png" style="width: 300px; max-width: 100%;">

<br>In symmetric case, the rates are still the same.
<br>Different behavior than <a data-tooltip-position="top" aria-label="Multiple Access Channel" data-href="Multiple Access Channel" href="the-guide/information-theory/information-theory-2/multiple-access-channel.html" class="internal-link" target="_self" rel="noopener nofollow">MAC</a> <a data-tooltip-position="top" aria-label="Discrete Memoryless Channel" data-href="Discrete Memoryless Channel" href="the-guide/information-theory/information-theory-1/channel-coding/discrete-memoryless-channel.html" class="internal-link" target="_self" rel="noopener nofollow">channel</a>. 


<br><br><br>The boundary of the capacity region of the K-user BC is given by for all possible power splits . The optimal points are achieved by superposition coding and successive interference cancellation at the receivers, i.e. stronger users decoding all weaker users first, starting from the weakest. <br>
<br>Sum Capacity

<br>Achieved by allocating all power to the strongest user.


<br><br><br>In this scenario the channel is characterized by where  with  describes random, <a data-tooltip-position="top" aria-label="Statistical Independence" data-href="Statistical Independence" href="the-guide/mathematics/statistics/statistical-independence.html" class="internal-link" target="_self" rel="noopener nofollow">i.i.d.</a> fading of user . The transmit power is constrained to be .<br>
<br>Fast Fading

<br>If channel states are only available at receiver, the single-user bounds are which can be achieved by allocating all power to user , which therefore again represents the extreme points of the capacity region. This bound is also true for the sum rate and can be achieved by either transmitting only to a single user or by time-sharing. The rate pairs of the capacity region are achieved by orthogonal schemes.
<br>If channel state is available at transmitter and receiver, the sum capacity is achieved by only transmitting to the best user


<br>Proportional Fair Downlink Scheduling

<br>Track average throughput  over window  at any time step  for every user  by feeding back rates  to basestation and computing 
<br>Transmit to user  with largest 


]]></description><link>the-guide/information-theory/information-theory-2/broadcast-channel.html</link><guid isPermaLink="false">The Guide/Information Theory/Information Theory 2/Broadcast Channel.md</guid><pubDate>Mon, 09 Sep 2024 15:37:19 GMT</pubDate><enclosure url="lib/media/pasted-image-20230621134240.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src=&quot;lib/media/pasted-image-20230621134240.png&quot;&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[Dirty Paper Coding]]></title><description><![CDATA[ 
 <br>If transmitter knows interference at receiver, the <a data-tooltip-position="top" aria-label="Channel Capacity" data-href="Channel Capacity" href="the-guide/information-theory/information-theory-1/channel-coding/channel-capacity.html" class="internal-link" target="_self" rel="noopener nofollow">capacity</a> of the channel with interference is the same as without interference (Costa). This is achieved by employing precoding in order to cancel the interference. The capacity is thereby achieved without a power penalty and without requiring the receiver to know the interference signal.<br><br><br><img alt="Pasted image 20230715131013.png" src="lib/media/pasted-image-20230715131013.png">The received signal is denoted , using the definitions<br>
<br> known interference at receiver
<br> message
<br> signal in order to cancel interference
<br> noise at receiver
<br><br><br>The classical linear approach  fails, as large interference signal can cause large signal power<img alt="center" src="lib/media/pasted-image-20230715173241.png" style="width: 450px; max-width: 100%;">Instead send  with using a  and  is the extension of the constellation. Pick an  that minimizes . At the decoder solve the minimization problem <img alt="center" src="lib/media/pasted-image-20230715180323.png" style="width: 500px; max-width: 100%;"><br>
]]></description><link>the-guide/information-theory/information-theory-2/dirty-paper-coding.html</link><guid isPermaLink="false">The Guide/Information Theory/Information Theory 2/Dirty Paper Coding.md</guid><pubDate>Mon, 12 Aug 2024 17:07:21 GMT</pubDate><enclosure url="lib/media/pasted-image-20230715131013.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src=&quot;lib/media/pasted-image-20230715131013.png&quot;&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[Ergodic Capacity]]></title><description><![CDATA[ 
 <br>For random <a data-tooltip-position="top" aria-label="Gaussian Distribution" data-href="Gaussian Distribution" href="the-guide/mathematics/probability-theory/gaussian-distribution.html" class="internal-link" target="_self" rel="noopener nofollow">Gaussian</a> <a data-tooltip-position="top" aria-label="Channel Capacity" data-href="Channel Capacity" href="the-guide/information-theory/information-theory-1/channel-coding/channel-capacity.html" class="internal-link" target="_self" rel="noopener nofollow">channel</a>, we separate the effects of transmitted signal and <a data-tooltip-position="top" aria-label="Discrete Memoryless Channel" data-href="Discrete Memoryless Channel" href="the-guide/information-theory/information-theory-1/channel-coding/discrete-memoryless-channel.html" class="internal-link" target="_self" rel="noopener nofollow">channel</a> where  is the channel gain during the i-th channel use. Normally, this is treated deterministic, in slow/fast-fading channels we have to treat it random due to fluctuations of constructive/destructive components.<br><br>To investigate this effect, we consider the transmitted signal power  and the received signal power .<br>Definition
The <a data-tooltip-position="top" aria-label="Ergodicity" data-href="Ergodicity" href="the-guide/computational-statistics/data-assimilation/ergodicity.html" class="internal-link" target="_self" rel="noopener nofollow">ergodic</a> capacity is the instantaneous <a data-tooltip-position="top" aria-label="Channel Capacity" data-href="Channel Capacity" href="the-guide/information-theory/information-theory-1/channel-coding/channel-capacity.html" class="internal-link" target="_self" rel="noopener nofollow">capacity</a> averaged over the random channel gains : 
<br>
<br>For AWGN 
<br>Fast Fading

<br>If we know the <a data-tooltip-position="top" aria-label="Probability Density Function" data-href="Probability Density Function" href="the-guide/mathematics/probability-theory/probability-density-function.html" class="internal-link" target="_self" rel="noopener nofollow">pdf</a> of the channel gain , we can compute the ergodic capacity via 

<br>

<br>Gives average instead of instantaneous picture






<br>Slow Fading

<br>Outage

<br>For some particular channel realization, the chosen transmission rate is higher than the <a data-tooltip-position="top" aria-label="Channel Capacity" data-href="Channel Capacity" href="the-guide/information-theory/information-theory-1/channel-coding/channel-capacity.html" class="internal-link" target="_self" rel="noopener nofollow">instantaneous capacity</a> (capacity depends on gain, which in this case is random between realizations)


<br>Outage Capacity

<br><a data-tooltip-position="top" aria-label="Code Rate" data-href="Code Rate" href="the-guide/information-theory/information-theory-1/channel-coding/code-rate.html" class="internal-link" target="_self" rel="noopener nofollow">Transmission Rate</a> , which does not exceed the <a data-tooltip-position="top" aria-label="Channel Capacity" data-href="Channel Capacity" href="the-guide/information-theory/information-theory-1/channel-coding/channel-capacity.html" class="internal-link" target="_self" rel="noopener nofollow">instantaneous capacity</a> in  of channel realizations.




<br>Fading always hurts
Using <a data-href="Jensen's Inequality" href="the-guide/information-theory/information-theory-1/information/jensen's-inequality.html" class="internal-link" target="_self" rel="noopener nofollow">Jensen's Inequality</a>, we obtainThis means that in any case, fading can only lower the achievable capacity and therefore always hurts our communication system.
]]></description><link>the-guide/information-theory/information-theory-2/ergodic-capacity.html</link><guid isPermaLink="false">The Guide/Information Theory/Information Theory 2/Ergodic Capacity.md</guid><pubDate>Mon, 12 Aug 2024 17:07:21 GMT</pubDate></item><item><title><![CDATA[Ergodic MIMO Capacity]]></title><description><![CDATA[ 
 <br>Definition
<a data-tooltip-position="top" aria-label="Ergodic Capacity" data-href="Ergodic Capacity" href="the-guide/information-theory/information-theory-2/ergodic-capacity.html" class="internal-link" target="_self" rel="noopener nofollow">Ergodic capacity</a> of a <a data-tooltip-position="top" aria-label="Multi-Antenna Channels" data-href="Multi-Antenna Channels" href="the-guide/information-theory/information-theory-2/multi-antenna-channels.html" class="internal-link" target="_self" rel="noopener nofollow">MIMO channel</a>, meaning that the <a data-tooltip-position="top" aria-label="Multi-Antenna Channels" data-href="Multi-Antenna Channels" href="the-guide/information-theory/information-theory-2/multi-antenna-channels.html" class="internal-link" target="_self" rel="noopener nofollow">channel</a> <a data-tooltip-position="top" aria-label="Matrices" data-href="Matrices" href="the-guide/mathematics/linear-algebra/matrices.html" class="internal-link" target="_self" rel="noopener nofollow">matrix</a>  is no longer fixed and has to be treated as random. This yields an average capacity formula over  
<br>Theorem - Telatar 1999
The average <a data-tooltip-position="top" aria-label="Channel Capacity" data-href="Channel Capacity" href="the-guide/information-theory/information-theory-1/channel-coding/channel-capacity.html" class="internal-link" target="_self" rel="noopener nofollow">capacity</a> is maximized subject to the power constraint  when The maximum average capacity (ergodic capacity) is which can be understood as an average picture rather than an instantaneous one.
]]></description><link>the-guide/information-theory/information-theory-2/ergodic-mimo-capacity.html</link><guid isPermaLink="false">The Guide/Information Theory/Information Theory 2/Ergodic MIMO Capacity.md</guid><pubDate>Mon, 09 Sep 2024 15:37:19 GMT</pubDate></item><item><title><![CDATA[Graphical Multicast Network]]></title><description><![CDATA[ 
 <br>Unicast network modeled by <a data-tooltip-position="top" aria-label="Graph" data-href="Graph" href="the-guide/mathematics/graph-theory/graph.html" class="internal-link" target="_self" rel="noopener nofollow">directed graph</a>  with link <a data-tooltip-position="top" aria-label="Channel Capacity" data-href="Channel Capacity" href="the-guide/information-theory/information-theory-1/channel-coding/channel-capacity.html" class="internal-link" target="_self" rel="noopener nofollow">capacity</a> between nodes  and  denoted by .<img alt="center" src="lib/media/pasted-image-20230530152423.png"><br>
We want to send a message  from <a data-tooltip-position="top" aria-label="Node or Vertex Set" data-href="Node or Vertex Set" href="the-guide/mathematics/graph-theory/node-or-vertex-set.html" class="internal-link" target="_self" rel="noopener nofollow">node</a>  to node . What is the highest transmission <a data-tooltip-position="top" aria-label="Code Rate" data-href="Code Rate" href="the-guide/information-theory/information-theory-1/channel-coding/code-rate.html" class="internal-link" target="_self" rel="noopener nofollow">rate</a> achievable ?<br><br>Cutset and Cut Capacity
Partition  of the <a data-tooltip-position="top" aria-label="Set" data-href="Set" href="the-guide/mathematics/general-stuff/set.html" class="internal-link" target="_self" rel="noopener nofollow">set</a> of <a data-tooltip-position="top" aria-label="Node or Vertex Set" data-href="Node or Vertex Set" href="the-guide/mathematics/graph-theory/node-or-vertex-set.html" class="internal-link" target="_self" rel="noopener nofollow">nodes</a> such that source node  and target node  are not in the same set. The Cut Capacity is given by
<br><img alt="center" src="lib/media/pasted-image-20230530154716.png"><br><br>Max-Flow Min-Cut Theorem
Network <a data-tooltip-position="top" aria-label="Channel Capacity" data-href="Channel Capacity" href="the-guide/information-theory/information-theory-1/channel-coding/channel-capacity.html" class="internal-link" target="_self" rel="noopener nofollow">capacity</a> from source to target set is given by An error-free transmission is achievable via routing and linear operations at the relay nodes (linear network coding)
<br><br><br>
<br>For destination  define cutset as above, network capactiy cannot exceed samellest cut capacity of every possible cut for every possible node
<br>If all nodes apart from sending node are destinations, above bound can be achieved by routing

<br>If destination is subset this is not valid anymore


]]></description><link>the-guide/information-theory/information-theory-2/graphical-multicast-network.html</link><guid isPermaLink="false">The Guide/Information Theory/Information Theory 2/Graphical Multicast Network.md</guid><pubDate>Wed, 23 Apr 2025 13:17:12 GMT</pubDate><enclosure url="lib/media/pasted-image-20230530152423.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src=&quot;lib/media/pasted-image-20230530152423.png&quot;&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[MIMO Capacity]]></title><description><![CDATA[ 
 <br>In a Nutshell
Various results regarding the <a data-tooltip-position="top" aria-label="Channel Capacity" data-href="Channel Capacity" href="the-guide/information-theory/information-theory-1/channel-coding/channel-capacity.html" class="internal-link" target="_self" rel="noopener nofollow">channel capacity</a> for communication over parallel channels, i.e. via multiple antennas per user.
<br><br><br>
<br>Case 1 - Parallel channels with uncorrelated noise

<br>Solution - Distribute power using <a data-tooltip-position="top" aria-label="Water-Filling" data-href="Water-Filling" href="the-guide/information-theory/information-theory-2/water-filling.html" class="internal-link" target="_self" rel="noopener nofollow">water-filling</a>, if channels have gain use levels  


<br>Case 2 - <a data-tooltip-position="top" aria-label="Multi-Antenna Channels" data-href="Multi-Antenna Channels" href="the-guide/information-theory/information-theory-2/multi-antenna-channels.html" class="internal-link" target="_self" rel="noopener nofollow">MIMO channel</a> with white noise

<br>Transmitter knows channel 

<br>Pre + post-processing to transform to parallel channels


<br>Transmitter doesn't know channel 

<br>




<br>Case 3 - Colored noise parallel channels with <a data-tooltip-position="top" aria-label="Covariance and Variance" data-href="Covariance and Variance" href="the-guide/mathematics/statistics/covariance-and-variance.html" class="internal-link" target="_self" rel="noopener nofollow">covariance</a> matrix (always possible, since per construction <a data-tooltip-position="top" aria-label="Matrices" data-href="Matrices" href="the-guide/mathematics/linear-algebra/matrices.html" class="internal-link" target="_self" rel="noopener nofollow">hermitian</a>)

<br>Pre-processing message  with  to get signal .
<br>Post-processing  with 
<br>After post-processing  is diagonal, but  reconstructed


<br>Case 4 - <a data-tooltip-position="top" aria-label="Multi-Antenna Channels" data-href="Multi-Antenna Channels" href="the-guide/information-theory/information-theory-2/multi-antenna-channels.html" class="internal-link" target="_self" rel="noopener nofollow">MIMO channel</a> with colored noise

<br>Uninformed Case - Case 2b) if covariance of noise is invertible
<br>Informed Case - Optimization problem, yielding noise absorbing channel matrix 


<br>If  has to be treated as random consider <a data-href="Ergodic MIMO Capacity" href="the-guide/information-theory/information-theory-2/ergodic-mimo-capacity.html" class="internal-link" target="_self" rel="noopener nofollow">Ergodic MIMO Capacity</a>.
<br><br>Theorem - Telatar (1995)
Given a signal ,  noise  and fixed  the MIMO <a data-tooltip-position="top" aria-label="Channel Capacity" data-href="Channel Capacity" href="the-guide/information-theory/information-theory-1/channel-coding/channel-capacity.html" class="internal-link" target="_self" rel="noopener nofollow">capacity</a> is 
<br>To investigate the effect of adding more antennas, we first consider adding only to receiver / transmitter.<br>
<br>SIMO  (column)     

<br>Addition of receiving antennas yields only logarithmic increase of capacity


<br>MISO  (row)    <br>
- Addition of transmitters yields only logarithmic increase of capacity<br>
Therefore, we can only gain from MIMO if both receiver and transmitter numbers are increased, capacity then grows nearly proportionally to !
<br>Proof
For all random vectors with a <a data-tooltip-position="top" aria-label="Covariance and Variance" data-href="Covariance and Variance" href="the-guide/mathematics/statistics/covariance-and-variance.html" class="internal-link" target="_self" rel="noopener nofollow">covariance</a> matrix , the <a data-tooltip-position="top" aria-label="Maximum Entropy Distribution" data-href="Maximum Entropy Distribution" href="the-guide/information-theory/information-theory-1/information/maximum-entropy-distribution.html" class="internal-link" target="_self" rel="noopener nofollow">maximum entropy distribution</a> is the zero-<a data-tooltip-position="top" aria-label="Expectations" data-href="Expectations" href="the-guide/mathematics/statistics/expectations.html" class="internal-link" target="_self" rel="noopener nofollow">mean</a> <a data-tooltip-position="top" aria-label="Gaussian Distribution" data-href="Gaussian Distribution" href="the-guide/mathematics/probability-theory/gaussian-distribution.html" class="internal-link" target="_self" rel="noopener nofollow">circular-symmetric complex Gaussian</a> whose entropy is Therefore, with the usual argument we reach the formula above.
<br>Colored Noise Capacity
For the colored noise case, where the random vector  has the <a data-tooltip-position="top" aria-label="Covariance and Variance" data-href="Covariance and Variance" href="the-guide/mathematics/statistics/covariance-and-variance.html" class="internal-link" target="_self" rel="noopener nofollow">covariance</a> <a data-tooltip-position="top" aria-label="Matrices" data-href="Matrices" href="the-guide/mathematics/linear-algebra/matrices.html" class="internal-link" target="_self" rel="noopener nofollow">matrix</a> . Therefore, with the usual argument one obtains the expression 
<br><br><br>Assume  and known [[Discrete Memoryless Channel<a class="internal-link" data-href="Expectations.md" href="the-guide/mathematics/statistics/expectations.html" target="_self" rel="noopener nofollow"></a>] at transmitter, i.e. transmitter knows how different signals from its various antennas interfere and are superpositioned with each other before they arrive at receiver. The channel model is given byThe capacity can be computed by <br>
<br>Decompose known <a data-tooltip-position="top" aria-label="Matrices" data-href="Matrices" href="the-guide/mathematics/linear-algebra/matrices.html" class="internal-link" target="_self" rel="noopener nofollow">matrix</a>  sing <a data-tooltip-position="top" aria-label="Singular Value Decomposition" data-href="Singular Value Decomposition" href="the-guide/mathematics/linear-algebra/singular-value-decomposition.html" class="internal-link" target="_self" rel="noopener nofollow">SVD</a> , yielding the model equation 
<br>Applying  from the left yields system of virtual parallel <a data-tooltip-position="top" aria-label="Discrete Memoryless Channel" data-href="Discrete Memoryless Channel" href="the-guide/information-theory/information-theory-1/channel-coding/discrete-memoryless-channel.html" class="internal-link" target="_self" rel="noopener nofollow">Gaussian channels</a>

<br>
<br>

<br>Original vector to be transmitted has to be  such that  is sent over the channel
<br>Power is preserved  


<br> with same <a data-tooltip-position="top" aria-label="Probability Distribution" data-href="Probability Distribution" href="the-guide/mathematics/probability-theory/probability-distribution.html" class="internal-link" target="_self" rel="noopener nofollow">distribution</a> as 

<br>
<br>


<br>Power is preserved

<br> 




<br>General Case 

<br>In general, the <a data-tooltip-position="top" aria-label="Channel Capacity" data-href="Channel Capacity" href="the-guide/information-theory/information-theory-1/channel-coding/channel-capacity.html" class="internal-link" target="_self" rel="noopener nofollow">capacity</a> for the informed MIMO channel yields with are the <a data-tooltip-position="top" aria-label="Water-Filling" data-href="Water-Filling" href="the-guide/information-theory/information-theory-2/water-filling.html" class="internal-link" target="_self" rel="noopener nofollow">water-filling</a> allocations with eigenchannels .  corresponds to the -th singular value. <img alt="center" src="lib/media/pasted-image-20230416173754.png" style="width: 450px; max-width: 100%;">
<br>Intuition: Channel with largest singular value is most important for received signal, should be assigned most resources
<br>If transmit covariance matrix  is given, the capacity can be computed via 


<br>High SNR Case

<br>Water level much larger than largest noise power, differences between channels neglectable. Assign equal power to all channels.
<br>The coefficient  describes the rank of the matrix , for direct line-of-sight transmission , while for high scattering  increases.
<br>
<br>Well conditioned channel matrices are preferable


<br>Low SNR Case

<br>Water level much lower that largest noise power level
<br>Allocate power to channel with strongest eigenmode
<br>Ill-conditioned (rank-one) channel matrices preferable.


<br><br><br>Receiver knows <a data-tooltip-position="top" aria-label="Multi-Antenna Channels" data-href="Multi-Antenna Channels" href="the-guide/information-theory/information-theory-2/multi-antenna-channels.html" class="internal-link" target="_self" rel="noopener nofollow">MIMO channel matrix</a> , transmitter doesn't. The output <a data-tooltip-position="top" aria-label="Covariance and Variance" data-href="Covariance and Variance" href="the-guide/mathematics/statistics/covariance-and-variance.html" class="internal-link" target="_self" rel="noopener nofollow">covariance matrix</a> is assumed to be  with . <br>
<br>Transmitter has only statistical channel state information, select signal covariance to optimize in terms of average <a data-tooltip-position="top" aria-label="Mutual Information" data-href="Mutual Information" href="the-guide/information-theory/information-theory-1/information/mutual-information.html" class="internal-link" target="_self" rel="noopener nofollow">mutual information</a> yielding to an optimization over .
<br>Transmitter has no channel state information, select signal covariance under worst-case statistics, i.e. . This yields the capacity which for a large number of transmit antennas  approaches  times the SISO capacity , as then .
]]></description><link>the-guide/information-theory/information-theory-2/mimo-capacity.html</link><guid isPermaLink="false">The Guide/Information Theory/Information Theory 2/MIMO Capacity.md</guid><pubDate>Fri, 23 Aug 2024 12:24:29 GMT</pubDate><enclosure url="lib/media/pasted-image-20230416173754.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src=&quot;lib/media/pasted-image-20230416173754.png&quot;&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[Multi-Antenna Channels]]></title><description><![CDATA[ 
 <br>Communication setting with multiple antennas. The signal (considering frequency flat fading) at the -th receive antenna is where <br>
<br> is gain between -th receive and -th transmit antenna
<br> is signal sent from -th antenna
<br> is noise in the -th antenna
<br>Channel Matrix
Allows to write system input-output relation in <a data-tooltip-position="top" aria-label="Matrices" data-href="Matrices" href="the-guide/mathematics/linear-algebra/matrices.html" class="internal-link" target="_self" rel="noopener nofollow">matrix</a> form via with transmit signal , receive signal  and noise vector .  is a  <a data-tooltip-position="top" aria-label="Discrete Memoryless Channel" data-href="Discrete Memoryless Channel" href="the-guide/information-theory/information-theory-1/channel-coding/discrete-memoryless-channel.html" class="internal-link" target="_self" rel="noopener nofollow">channel matrix</a> of the formwhere  is coefficient denoting the gain for receiver  from transmitter .
<br><br>Special Cases<br>
<br>SIMO Channel<img alt="center" src="lib/media/pasted-image-20230416145627.png" style="width: 300px; max-width: 100%;">

<br> vector 
<br>

<br>Scalar signal, as only one antenna sends a symbol at each timestep




<br>MISO Channel<img alt="center" src="lib/media/pasted-image-20230416150023.png" style="width: 300px; max-width: 100%;">

<br> vector 
<br>

<br>Inner Product, superposition of all signals received




<br><a data-tooltip-position="top" aria-label="MIMO Capacity" data-href="MIMO Capacity" href="the-guide/information-theory/information-theory-2/mimo-capacity.html" class="internal-link" target="_self" rel="noopener nofollow">MIMO</a> Channel

<br>Combination of the above 


]]></description><link>the-guide/information-theory/information-theory-2/multi-antenna-channels.html</link><guid isPermaLink="false">The Guide/Information Theory/Information Theory 2/Multi-Antenna Channels.md</guid><pubDate>Fri, 23 Aug 2024 12:24:21 GMT</pubDate><enclosure url="lib/media/pasted-image-20230416145627.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src=&quot;lib/media/pasted-image-20230416145627.png&quot;&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[Multiple Access Channel]]></title><description><![CDATA[ 
 <br>Uplink scenario, where different users send different information to a base-station.<br>
<img alt="center" src="lib/media/pasted-image-20230621134215.png" style="width: 350px; max-width: 100%;"><br><br><br>A 2-sender <a data-tooltip-position="top" aria-label="Discrete Memoryless Channel" data-href="Discrete Memoryless Channel" href="the-guide/information-theory/information-theory-1/channel-coding/discrete-memoryless-channel.html" class="internal-link" target="_self" rel="noopener nofollow">discrete memoryless channel</a> (DM-MAC) , where the senders want to send independent messages to receiver respectively.<br>
<img alt="center" src="lib/media/pasted-image-20230621135228.png" style="width: 400px; max-width: 100%;">A  <a data-tooltip-position="top" aria-label="Linear Block Code" data-href="Linear Block Code" href="the-guide/information-theory/information-theory-1/coding-theory/linear-block-code.html" class="internal-link" target="_self" rel="noopener nofollow">code</a> consists of<br>
<br>Two message sets  and  with different <a data-tooltip-position="top" aria-label="Code Rate" data-href="Code Rate" href="the-guide/information-theory/information-theory-1/channel-coding/code-rate.html" class="internal-link" target="_self" rel="noopener nofollow">code rates</a>
<br>Two encoders, each assigning a codeword  to each message 
<br>A <a data-tooltip-position="top" aria-label="Decoding Rules" data-href="Decoding Rules" href="the-guide/information-theory/information-theory-1/coding-theory/decoding-rules.html" class="internal-link" target="_self" rel="noopener nofollow">decoder</a> assigning an estimate  or an error  to each received sequence 

<br>We assume <a data-tooltip-position="top" aria-label="Statistical Independence" data-href="Statistical Independence" href="the-guide/mathematics/statistics/statistical-independence.html" class="internal-link" target="_self" rel="noopener nofollow">independence</a> of the two messages 


<br><br>The average probability of error is  and a rate pair  is said to be achievable, if there exists a sequence of  codes with arbitrarily low  as . The closure of all achievable rate pairs is denoted the capacity region .  The maximum achievable rates are given via the mutual information <br>
<br>
<br>
<br><br>
which leads to the region<img alt="center" src="lib/media/pasted-image-20230621150754.png" style="width: 200px; max-width: 100%;">
<br>Performance Measures

<br>Symmetric Capacitymax rate at which both users can reliably communicate simultaneously.
<br>Sum Capacitymaximum total throughput.


<br>Bounds using Single-user Detection

<br> are bounded by individual <a data-tooltip-position="top" aria-label="Channel Capacity" data-href="Channel Capacity" href="the-guide/information-theory/information-theory-1/channel-coding/channel-capacity.html" class="internal-link" target="_self" rel="noopener nofollow">capacity</a> considering other user as noise
<br>Sum of individual capacities is non-achievable limit if using single-user detection, as this would require knowing both symbols beforehand
<br>Square rate region


<br>Bounds using SIC (Multi-user Receiver)

<br>Two-user Gaussian Channel 

<br> are bounded by individual <a data-tooltip-position="top" aria-label="Channel Capacity" data-href="Channel Capacity" href="the-guide/information-theory/information-theory-1/channel-coding/channel-capacity.html" class="internal-link" target="_self" rel="noopener nofollow">capacity</a> considering the other user is absent, minimal interference (A, D)
<br> is bounded by single user with combined power  (<a data-tooltip-position="top" aria-label="MIMO Capacity" data-href="MIMO Capacity" href="the-guide/information-theory/information-theory-2/mimo-capacity.html" class="internal-link" target="_self" rel="noopener nofollow">MIMO uninformed</a>)
<br><a data-tooltip-position="top" aria-label="Successive Interference Cancellation" data-href="Successive Interference Cancellation" href="the-guide/information-theory/information-theory-2/successive-interference-cancellation.html" class="internal-link" target="_self" rel="noopener nofollow">Successive Interference Cancellation</a> (SIC) - <a data-tooltip-position="top" aria-label="Decoding Rules" data-href="Decoding Rules" href="the-guide/information-theory/information-theory-1/coding-theory/decoding-rules.html" class="internal-link" target="_self" rel="noopener nofollow">Decode</a> one (stronger) user first, treating the other as noise (leading to lower <a data-tooltip-position="top" aria-label="Code Rate" data-href="Code Rate" href="the-guide/information-theory/information-theory-1/channel-coding/code-rate.html" class="internal-link" target="_self" rel="noopener nofollow">rate</a>), subtract decoded message from received signal, leading to signal and decoding without interference for the other user (B, C). Points in between (diagonal) are achieved by frequency or time splitting  <img alt="center" src="lib/media/pasted-image-20230621180528.png" style="width: 300px; max-width: 100%;">
<br>B to C are Pareto-Optimal Points

<br>If you want to increase rate of one user, you can only do this by lowering the others






<br>Examples - Whether inner or outer bound coincide depends on the <a data-tooltip-position="top" aria-label="Discrete Memoryless Channel" data-href="Discrete Memoryless Channel" href="the-guide/information-theory/information-theory-1/channel-coding/discrete-memoryless-channel.html" class="internal-link" target="_self" rel="noopener nofollow">channel</a>, e.g.

<br>Binary Multiplier DM-MAC  (binary) <img alt="center" src="lib/media/pasted-image-20230621160331.png" style="width: 150px; max-width: 100%;">

<br>: , since inputs are <a data-tooltip-position="top" aria-label="Statistical Independence" data-href="Statistical Independence" href="the-guide/mathematics/statistics/statistical-independence.html" class="internal-link" target="_self" rel="noopener nofollow">i.i.d.</a> 
<br>: , since inputs are <a data-tooltip-position="top" aria-label="Statistical Independence" data-href="Statistical Independence" href="the-guide/mathematics/statistics/statistical-independence.html" class="internal-link" target="_self" rel="noopener nofollow">i.i.d.</a> 
<br>: , since it is binary in any case


<br>Binary Erasure DM-MAC  (ternary, 2 if both inputs are 1, error if mixed input)<img alt="center" src="lib/media/pasted-image-20230621163520.png" style="width: 150px; max-width: 100%;">

<br> as above
<br>: (Maybe ask) If both inputs are uniform in half of the cases we get , because we can decode both input symbols if  is  or . For the other half of the cases, we know that we received the error because of a mixed input, equivalent to . We can therefore achieve a rate of . Since both single channels are bounded, the best strategy is  or .




<br><br><br>
<br>K-User Gaussian Channel 

<br> single user bounds given by single <a data-tooltip-position="top" aria-label="Channel Capacity" data-href="Channel Capacity" href="the-guide/information-theory/information-theory-1/channel-coding/channel-capacity.html" class="internal-link" target="_self" rel="noopener nofollow">capacity</a>
<br>Capacity Region by  bounds, leads to  corner points


<br>Sum Capacity

<br>Sum rate of  users is always bounded by single user with combined power, capacity achievable without considering fairness <a data-tooltip-position="top" aria-label="MIMO Capacity" data-href="MIMO Capacity" href="the-guide/information-theory/information-theory-2/mimo-capacity.html" class="internal-link" target="_self" rel="noopener nofollow">MIMO-like case</a>:In equal power case 

<br>without SIC (single-user detector), this results in  which approaches  as . Growing Interference is the limiting factor !
<br>with SIC, the sum capacity is unbounded 




<br>Symmetric Capacity

<br>Capacity achievable if all users should have the same rate (capacity considering maximum fairness):In the equal power case this results in which is achieved by orthogonal multiplexing, i.e. allocating a fraction  of the resources (bandwidth,...) to each user. Note that in this scenario .


<br>In practice, one has to consider the generalization to  users with individual power budget and individual channel gains <img alt="center" src="lib/media/pasted-image-20230623123159.png" style="width: 300px; max-width: 100%;">The results above can be extended to this scenario by replacing the powers  by the amplified powers .
<br><br><br>In this scenario the channel is characterized by where  with  is random fading of user . Two subcases are treated:<br>
<br>Slow Fading

<br>Channel characteristics change between symbols, but not during a single one. Supposed all users transmitt at rate , the <a data-tooltip-position="top" aria-label="Ergodic Capacity" data-href="Ergodic Capacity" href="the-guide/information-theory/information-theory-2/ergodic-capacity.html" class="internal-link" target="_self" rel="noopener nofollow">outage probability</a> is for some <a data-tooltip-position="top" aria-label="Set" data-href="Set" href="the-guide/mathematics/general-stuff/set.html" class="internal-link" target="_self" rel="noopener nofollow">set</a> of users .


<br>Fast Fading

<br>Channel characteristics changes during single symbols. In this case, one can show using <a data-tooltip-position="top" aria-label="Jensen's Inequality" data-href="Jensen's Inequality" href="the-guide/information-theory/information-theory-1/information/jensen's-inequality.html" class="internal-link" target="_self" rel="noopener nofollow">Jensen's inequality</a>, that the sum capacity of the AWGN MAC channel is an upper bound for the capacity with fading. This effect is reduced when the number of channels is increased, as then the effect of fading is averaged out. 
<br>If all channel characteristics are known, the best strategy is to always only use the channel with best channel characteristic at each time step


<br>Fading can only hurt
]]></description><link>the-guide/information-theory/information-theory-2/multiple-access-channel.html</link><guid isPermaLink="false">The Guide/Information Theory/Information Theory 2/Multiple Access Channel.md</guid><pubDate>Mon, 12 Aug 2024 17:07:21 GMT</pubDate><enclosure url="lib/media/pasted-image-20230621134215.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src=&quot;lib/media/pasted-image-20230621134215.png&quot;&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[Multiuser Channels]]></title><description><![CDATA[ 
 <br>In a Nutshell
Instead of having point-to-point communication as with <a data-tooltip-position="top" aria-label="Channel Coding Theorem" data-href="Channel Coding Theorem" href="the-guide/information-theory/information-theory-1/channel-coding/channel-coding-theorem.html" class="internal-link" target="_self" rel="noopener nofollow">single channels</a> or special <a data-tooltip-position="top" aria-label="Multi-Antenna Channels" data-href="Multi-Antenna Channels" href="the-guide/information-theory/information-theory-2/multi-antenna-channels.html" class="internal-link" target="_self" rel="noopener nofollow">multi-antenna channels</a>, multiple users use the channel, leading to interference.
<br><br><br>
<br><a data-href="Multiple Access Channel" href="the-guide/information-theory/information-theory-2/multiple-access-channel.html" class="internal-link" target="_self" rel="noopener nofollow">Multiple Access Channel</a><img alt="center" src="lib/media/pasted-image-20230621134215.png" style="width: 250px; max-width: 100%;">
<br><a data-href="Broadcast Channel" href="the-guide/information-theory/information-theory-2/broadcast-channel.html" class="internal-link" target="_self" rel="noopener nofollow">Broadcast Channel</a><img alt="center" src="lib/media/pasted-image-20230621134240.png" style="width: 300px; max-width: 100%;">
<br>Relay Channel<img alt="center" src="lib/media/pasted-image-20230621134307.png" style="width: 350px; max-width: 100%;">

<br>Can be considered as a combination of broadcast (source to destination and relay) and multiple access (relay and source to destination)


]]></description><link>the-guide/information-theory/information-theory-2/multiuser-channels.html</link><guid isPermaLink="false">The Guide/Information Theory/Information Theory 2/Multiuser Channels.md</guid><pubDate>Mon, 09 Sep 2024 15:37:19 GMT</pubDate><enclosure url="lib/media/pasted-image-20230621134215.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src=&quot;lib/media/pasted-image-20230621134215.png&quot;&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[Multiuser Diversity]]></title><description><![CDATA[ 
 <br>In the full channel state information case the optimal strategy for the sum <a data-tooltip-position="top" aria-label="Channel Capacity" data-href="Channel Capacity" href="the-guide/information-theory/information-theory-1/channel-coding/channel-capacity.html" class="internal-link" target="_self" rel="noopener nofollow">capacity</a> both in the <a data-tooltip-position="top" aria-label="Multiple Access Channel" data-href="Multiple Access Channel" href="the-guide/information-theory/information-theory-2/multiple-access-channel.html" class="internal-link" target="_self" rel="noopener nofollow">uplink</a> and <a data-tooltip-position="top" aria-label="Broadcast Channel" data-href="Broadcast Channel" href="the-guide/information-theory/information-theory-2/broadcast-channel.html" class="internal-link" target="_self" rel="noopener nofollow">downlink</a> scenario is to allocate all power to single users, resulting in the point-to-point case with fading of magnitude . However, this results in higher rates because<br>
<br>(uplink) The power of multiple users is allocated to a single one.
<br>With more and more users the probability that one of them has a strong channel is high. By allocating all available power to that user the resources are used in the most efficient manner.<br>
The effect is better the more users are in the system, but the quality highly depends on the <a data-tooltip-position="top" aria-label="Probability Distribution" data-href="Probability Distribution" href="the-guide/mathematics/probability-theory/probability-distribution.html" class="internal-link" target="_self" rel="noopener nofollow">distribution</a> of the random gains (heavier tails are better).
<br><br><br>In order to reap the above benefits, the base station has to have access to the channel quality of each user:<br>
<br>In <a data-tooltip-position="top" aria-label="Multiple Access Channel" data-href="Multiple Access Channel" href="the-guide/information-theory/information-theory-2/multiple-access-channel.html" class="internal-link" target="_self" rel="noopener nofollow">uplink</a>, the base station has to track the quality
<br>In <a data-tooltip-position="top" aria-label="Broadcast Channel" data-href="Broadcast Channel" href="the-guide/information-theory/information-theory-2/broadcast-channel.html" class="internal-link" target="_self" rel="noopener nofollow">downlink</a>, each user has to track its channel quality (SNR) and feed it back to the base station.<br>
The base station then has to schedule the transmission accordingly  opportunistic scheduling.
<br><br><br>
<br>In reality the fading of different users is often non-symmetric, because of distance to the base-station or movement.
<br>In practice there are also latency requirements.
<br>For slow and limited fading the effect is limited.
<br>Quality of channel estimation and the feedback in the downlink case are bottlenecks.
<br>Multiuser Diversity can be combined with <a data-tooltip-position="top" aria-label="Broadcast Channel" data-href="Broadcast Channel" href="the-guide/information-theory/information-theory-2/broadcast-channel.html" class="internal-link" target="_self" rel="noopener nofollow">superposition coding</a> by clustering the users in classes. For each class, the strongest user is picked and all the strongest users are used for superposition coding.
]]></description><link>the-guide/information-theory/information-theory-2/multiuser-diversity.html</link><guid isPermaLink="false">The Guide/Information Theory/Information Theory 2/Multiuser Diversity.md</guid><pubDate>Mon, 12 Aug 2024 17:07:21 GMT</pubDate></item><item><title><![CDATA[Network Coding Theorem]]></title><description><![CDATA[ 
 <br>Theorem
The <a data-tooltip-position="top" aria-label="Channel Capacity" data-href="Channel Capacity" href="the-guide/information-theory/information-theory-1/channel-coding/channel-capacity.html" class="internal-link" target="_self" rel="noopener nofollow">capacity</a> of a noiseless <a data-tooltip-position="top" aria-label="Graphical Multicast Network" data-href="Graphical Multicast Network" href="the-guide/information-theory/information-theory-2/graphical-multicast-network.html" class="internal-link" target="_self" rel="noopener nofollow">multicast network</a> with destination <a data-tooltip-position="top" aria-label="Set" data-href="Set" href="the-guide/mathematics/general-stuff/set.html" class="internal-link" target="_self" rel="noopener nofollow">set</a>  is 
<br>
<br>Coincides with <a data-tooltip-position="top" aria-label="Graphical Multicast Network" data-href="Graphical Multicast Network" href="the-guide/information-theory/information-theory-2/graphical-multicast-network.html" class="internal-link" target="_self" rel="noopener nofollow">cutset upper bound</a>
<br>Can be achieved with zero error by linear operations<a data-href="Network Coding Theorem" href="the-guide/information-theory/information-theory-2/network-coding-theorem.html" class="internal-link" target="_self" rel="noopener nofollow">Network Coding Theorem</a>
]]></description><link>the-guide/information-theory/information-theory-2/network-coding-theorem.html</link><guid isPermaLink="false">The Guide/Information Theory/Information Theory 2/Network Coding Theorem.md</guid><pubDate>Mon, 12 Aug 2024 17:07:21 GMT</pubDate></item><item><title><![CDATA[Successive Interference Cancellation]]></title><description><![CDATA[ 
 <br>Technique used to decode when a code contains multiple packages. Decode the stronger signal first and subtract it from the total signal to step-wise reduce noise for the weaker signals.]]></description><link>the-guide/information-theory/information-theory-2/successive-interference-cancellation.html</link><guid isPermaLink="false">The Guide/Information Theory/Information Theory 2/Successive Interference Cancellation.md</guid><pubDate>Mon, 12 Aug 2024 17:07:21 GMT</pubDate></item><item><title><![CDATA[Water-Filling]]></title><description><![CDATA[ 
 <br>Set of parallel additive white Gaussian noise <a data-tooltip-position="top" aria-label="Discrete Memoryless Channel" data-href="Discrete Memoryless Channel" href="the-guide/information-theory/information-theory-1/channel-coding/discrete-memoryless-channel.html" class="internal-link" target="_self" rel="noopener nofollow">channels</a> with common power constraint and the input-output relationship with .<br>How to distribute the power among channels to maximize the total <a data-tooltip-position="top" aria-label="Channel Capacity" data-href="Channel Capacity" href="the-guide/information-theory/information-theory-1/channel-coding/channel-capacity.html" class="internal-link" target="_self" rel="noopener nofollow">capacity</a> ?<br>
<br>Set water level  such that 

<br> ignores negative values
<br>Intuition - available power is area under water level


<br>Allocate channels with the lowest noise powers<br>
<img alt="center" src="lib/media/pasted-image-20230415192035.png" style="width: 350px; max-width: 100%;">
]]></description><link>the-guide/information-theory/information-theory-2/water-filling.html</link><guid isPermaLink="false">The Guide/Information Theory/Information Theory 2/Water-Filling.md</guid><pubDate>Mon, 12 Aug 2024 17:07:21 GMT</pubDate><enclosure url="lib/media/pasted-image-20230415192035.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src=&quot;lib/media/pasted-image-20230415192035.png&quot;&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[Density Operator]]></title><description><![CDATA[ 
 <br>Operator denoted by a <a data-tooltip-position="top" aria-label="Matrices" data-href="Matrices" href="the-guide/mathematics/linear-algebra/matrices.html" class="internal-link" target="_self" rel="noopener nofollow">matrix</a> to describe the <a data-tooltip-position="top" aria-label="Quantum State" data-href="Quantum State" href="the-guide/information-theory/quantum-information-theory/quantum-state.html" class="internal-link" target="_self" rel="noopener nofollow">quantum state</a> of a physical system that allows computation of outcome probabilities of measurements performed on that system. It is an alternative, but equivalent way to describing these states using state vectors / <a data-tooltip-position="top" aria-label="Dirac Notation" data-href="Dirac Notation" href="the-guide/information-theory/quantum-information-theory/dirac-notation.html" class="internal-link" target="_self" rel="noopener nofollow">kets</a> .<br><br><br>Definition
<a data-tooltip-position="top" aria-label="Matrices" data-href="Matrices" href="the-guide/mathematics/linear-algebra/matrices.html" class="internal-link" target="_self" rel="noopener nofollow">Positive semi-definite and hermitian</a> operator of trace one acting on the <a data-href="Inner Product Space and Hilbert Space" href="the-guide/mathematics/functional-analysis-and-calculus-of-variations/inner-product-space-and-hilbert-space.html" class="internal-link" target="_self" rel="noopener nofollow">Inner Product Space and Hilbert Space</a> of the system. It is defined via For a pure state, which can be written with regard to an orthonormal basis  as , the entries of the resulting density matrix are 
<br><br>
<br><a data-tooltip-position="top" aria-label="Matrices" data-href="Matrices" href="the-guide/mathematics/linear-algebra/matrices.html" class="internal-link" target="_self" rel="noopener nofollow">Hermitian</a> 
<br>Idempotent for pure states 
<br><br><br>In contrast to the state vector , the density operator can also represent incoherent superpositions (vector only coherent). For a <a data-tooltip-position="top" aria-label="Set" data-href="Set" href="the-guide/mathematics/general-stuff/set.html" class="internal-link" target="_self" rel="noopener nofollow">set</a> of states , the density operator can be written as where  represents the probability of the system being in that state.]]></description><link>the-guide/information-theory/quantum-information-theory/density-operator.html</link><guid isPermaLink="false">The Guide/Information Theory/Quantum Information Theory/Density Operator.md</guid><pubDate>Mon, 24 Feb 2025 23:20:55 GMT</pubDate></item><item><title><![CDATA[Dirac Notation]]></title><description><![CDATA[ 
 <br>In a Nutshell
Also called Bra-Ket notation, alternative formalism for linear algebra and linear operators on complex <a data-tooltip-position="top" aria-label="Vector Space" data-href="Vector Space" href="the-guide/mathematics/general-stuff/vector-space.html" class="internal-link" target="_self" rel="noopener nofollow">vector spaces</a> that is extensively used in quantum mechanics.
<br><br>A ket  denotes a vector  in an abstract (complex) <a data-tooltip-position="top" aria-label="Vector Space" data-href="Vector Space" href="the-guide/mathematics/general-stuff/vector-space.html" class="internal-link" target="_self" rel="noopener nofollow">vector space</a>  and represents the state of a system.<br>A bra  denotes a linear form  that maps each vector in  to a number.<br>A linear functional acting on a vector is denoted .]]></description><link>the-guide/information-theory/quantum-information-theory/dirac-notation.html</link><guid isPermaLink="false">The Guide/Information Theory/Quantum Information Theory/Dirac Notation.md</guid><pubDate>Thu, 17 Apr 2025 08:01:15 GMT</pubDate></item><item><title><![CDATA[Holevo's Theorem]]></title><description><![CDATA[ 
 <br>Limitative theorem in Quantum Information theory, establishes an upper bound of information that can be known about a quantum state.<br>Holevo's Theorem
Given a <a data-tooltip-position="top" aria-label="Set" data-href="Set" href="the-guide/mathematics/general-stuff/set.html" class="internal-link" target="_self" rel="noopener nofollow">set</a> of <a data-tooltip-position="top" aria-label="Quantum State" data-href="Quantum State" href="the-guide/information-theory/quantum-information-theory/quantum-state.html" class="internal-link" target="_self" rel="noopener nofollow">mixed states</a>  and a <a data-tooltip-position="top" aria-label="Probability Distribution" data-href="Probability Distribution" href="the-guide/mathematics/probability-theory/probability-distribution.html" class="internal-link" target="_self" rel="noopener nofollow">distribution</a> over them as , we assume  to be drawn according to this distribution. then for any measurement performed on , the amount of <a data-tooltip-position="top" aria-label="Mutual Information" data-href="Mutual Information" href="the-guide/information-theory/information-theory-1/information/mutual-information.html" class="internal-link" target="_self" rel="noopener nofollow">accessible information</a> about  by knowing the outcome  is bounded as where  and <a data-href="Von Neumann Entropy" href="the-guide/information-theory/quantum-information-theory/von-neumann-entropy.html" class="internal-link" target="_self" rel="noopener nofollow">Von Neumann Entropy</a> . The upper bound above is called the Holevo information. 
]]></description><link>the-guide/information-theory/quantum-information-theory/holevo&apos;s-theorem.html</link><guid isPermaLink="false">The Guide/Information Theory/Quantum Information Theory/Holevo&apos;s Theorem.md</guid><pubDate>Mon, 12 Aug 2024 17:07:21 GMT</pubDate></item><item><title><![CDATA[Postulates of Quantum Mechanics]]></title><description><![CDATA[ 
 <br><br>Postulate 1
The state of an isolated physical system at time  is represented by an -dimensional <a data-tooltip-position="top" aria-label="Quantum State" data-href="Quantum State" href="the-guide/information-theory/quantum-information-theory/quantum-state.html" class="internal-link" target="_self" rel="noopener nofollow">state vector</a>  belonging to a <a data-tooltip-position="top" aria-label="Inner Product Space and Hilbert Space" data-href="Inner Product Space and Hilbert Space" href="the-guide/mathematics/functional-analysis-and-calculus-of-variations/inner-product-space-and-hilbert-space.html" class="internal-link" target="_self" rel="noopener nofollow">Hilbert space</a> . 
<br><br><br>Postulate 3
A <a data-tooltip-position="top" aria-label="Measure" data-href="Measure" href="the-guide/mathematics/measure-theory/measure.html" class="internal-link" target="_self" rel="noopener nofollow">measurable</a> physical quantity  can be described by a <a data-tooltip-position="top" aria-label="Matrices" data-href="Matrices" href="the-guide/mathematics/linear-algebra/matrices.html" class="internal-link" target="_self" rel="noopener nofollow">hermitian operator</a>  acting in the state space . The operator is an observable, which means that its <a data-tooltip-position="top" aria-label="Eigenvalue Problem" data-href="Eigenvalue Problem" href="the-guide/mathematics/linear-algebra/eigenvalue-problem.html" class="internal-link" target="_self" rel="noopener nofollow">eigenvectors</a> form a basis of  and the result of performing a measurement must be one of the eigenvalues of  
<br>Postulate 4
The measurement of  in a system in normalized state  gives the <a data-tooltip-position="top" aria-label="Eigenvalue Problem" data-href="Eigenvalue Problem" href="the-guide/mathematics/linear-algebra/eigenvalue-problem.html" class="internal-link" target="_self" rel="noopener nofollow">eigenvalue</a>  of an eigenvector  with probability with  as the projections of the state onto the eigenvectors of the observable .
<br>
<br>In contrast to classical mechanics, where we can predict a systems state, we can only predict probability in the limit of infinite measurements.
]]></description><link>the-guide/information-theory/quantum-information-theory/postulates-of-quantum-mechanics.html</link><guid isPermaLink="false">The Guide/Information Theory/Quantum Information Theory/Postulates of Quantum Mechanics.md</guid><pubDate>Mon, 24 Feb 2025 23:20:55 GMT</pubDate></item><item><title><![CDATA[Quantum Jensen-Shannon Divergence]]></title><description><![CDATA[ 
 <br>Extension of the <a data-href="Jensen-Shannon Divergence" href="the-guide/information-theory/information-geometry/jensen-shannon-divergence.html" class="internal-link" target="_self" rel="noopener nofollow">Jensen-Shannon Divergence</a> to measure the similarity between <a data-tooltip-position="top" aria-label="Quantum State" data-href="Quantum State" href="the-guide/information-theory/quantum-information-theory/quantum-state.html" class="internal-link" target="_self" rel="noopener nofollow">mixed quantum states</a>.<br>Definition
Given two <a data-tooltip-position="top" aria-label="Quantum State" data-href="Quantum State" href="the-guide/information-theory/quantum-information-theory/quantum-state.html" class="internal-link" target="_self" rel="noopener nofollow">quantum states</a>, defined via their <a data-tooltip-position="top" aria-label="Density Operator" data-href="Density Operator" href="the-guide/information-theory/quantum-information-theory/density-operator.html" class="internal-link" target="_self" rel="noopener nofollow">density operators</a>  and , the quantum Jensen-Shannon <a data-tooltip-position="top" aria-label="Divergence" data-href="Divergence" href="the-guide/information-theory/information-geometry/divergence.html" class="internal-link" target="_self" rel="noopener nofollow">divergence</a> is defined as 
<br>
<br>Squareroot is <a data-tooltip-position="top" aria-label="Metric Space and Completeness" data-href="Metric Space and Completeness" href="the-guide/mathematics/general-stuff/metric-space-and-completeness.html" class="internal-link" target="_self" rel="noopener nofollow">metric</a> on the <a data-tooltip-position="top" aria-label="Space" data-href="Space" href="the-guide/mathematics/general-stuff/space.html" class="internal-link" target="_self" rel="noopener nofollow">space</a> of <a data-tooltip-position="top" aria-label="Density Operator" data-href="Density Operator" href="the-guide/information-theory/quantum-information-theory/density-operator.html" class="internal-link" target="_self" rel="noopener nofollow">density</a> <a data-tooltip-position="top" aria-label="Matrices" data-href="Matrices" href="the-guide/mathematics/linear-algebra/matrices.html" class="internal-link" target="_self" rel="noopener nofollow">matrices</a>
<br>Can be used to measure <a data-tooltip-position="top" aria-label="Graph" data-href="Graph" href="the-guide/mathematics/graph-theory/graph.html" class="internal-link" target="_self" rel="noopener nofollow">graph</a> dissimiliarity given a shared <a data-tooltip-position="top" aria-label="Node or Vertex Set" data-href="Node or Vertex Set" href="the-guide/mathematics/graph-theory/node-or-vertex-set.html" class="internal-link" target="_self" rel="noopener nofollow">node set</a> by considering  based on the <a data-tooltip-position="top" aria-label="Graph Laplacians" data-href="Graph Laplacians" href="the-guide/mathematics/graph-theory/graph-laplacians.html" class="internal-link" target="_self" rel="noopener nofollow">graph Laplacian</a> as a density matrix. In this case, the QJSD is related to the <a data-tooltip-position="top" aria-label="Von Neumann Entropy" data-href="Von Neumann Entropy" href="the-guide/information-theory/quantum-information-theory/von-neumann-entropy.html" class="internal-link" target="_self" rel="noopener nofollow">Von Neumann graph entropy</a>  via where  is a graph with <a data-tooltip-position="top" aria-label="Adjacency Matrix" data-href="Adjacency Matrix" href="the-guide/mathematics/graph-theory/adjacency-matrix.html" class="internal-link" target="_self" rel="noopener nofollow">adjacency matrix</a> 
]]></description><link>the-guide/information-theory/quantum-information-theory/quantum-jensen-shannon-divergence.html</link><guid isPermaLink="false">The Guide/Information Theory/Quantum Information Theory/Quantum Jensen-Shannon Divergence.md</guid><pubDate>Sat, 29 Mar 2025 16:19:05 GMT</pubDate></item><item><title><![CDATA[Quantum State]]></title><description><![CDATA[ 
 <br>In classical physics, a <a data-tooltip-position="top" aria-label="Static and Dynamic Systems" data-href="Static and Dynamic Systems" href="the-guide/robotics,-dynamics-and-control/dynamics/static-and-dynamic-systems.html" class="internal-link" target="_self" rel="noopener nofollow">dynamical</a> state is described using a vector of variables, e.g. position and velocity of an object that evolve in time. These evolution are governed by equations of motions and thus remain strictly determined.<br>
A quantum state in general is a vector of complex numbers describing a state that also evolve under equations of motion. However, these values are quantized and limited by uncertainty relations, which is why they will only provide a <a data-tooltip-position="top" aria-label="Probability Distribution" data-href="Probability Distribution" href="the-guide/mathematics/probability-theory/probability-distribution.html" class="internal-link" target="_self" rel="noopener nofollow">probability distribution</a> over outcomes.<br><br>Perfectly known state <br>
<br>Denoted via the <a data-tooltip-position="top" aria-label="Dirac Notation" data-href="Dirac Notation" href="the-guide/information-theory/quantum-information-theory/dirac-notation.html" class="internal-link" target="_self" rel="noopener nofollow">ket</a> ,  
<br><br>Incomplete information about a state, we only know probabilities for certain pure states.]]></description><link>the-guide/information-theory/quantum-information-theory/quantum-state.html</link><guid isPermaLink="false">The Guide/Information Theory/Quantum Information Theory/Quantum State.md</guid><pubDate>Mon, 03 Mar 2025 15:30:25 GMT</pubDate></item><item><title><![CDATA[Von Neumann Entropy]]></title><description><![CDATA[ 
 <br>Extension of the <a data-href="Gibbs Entropy" href="Gibbs Entropy" class="internal-link" target="_self" rel="noopener nofollow">Gibbs Entropy</a> and <a data-tooltip-position="top" aria-label="Shannon Entropy" data-href="Shannon Entropy" href="the-guide/information-theory/information-theory-1/information/shannon-entropy.html" class="internal-link" target="_self" rel="noopener nofollow">Shannon entropy</a> from classical systems to quantum systems and <a data-tooltip-position="top" aria-label="Quantum State" data-href="Quantum State" href="the-guide/information-theory/quantum-information-theory/quantum-state.html" class="internal-link" target="_self" rel="noopener nofollow">quantum states</a>.<br><br>Definition
Given the <a data-tooltip-position="top" aria-label="Matrices" data-href="Matrices" href="the-guide/mathematics/linear-algebra/matrices.html" class="internal-link" target="_self" rel="noopener nofollow">matrix</a>  representing the <a data-tooltip-position="top" aria-label="Density Operator" data-href="Density Operator" href="the-guide/information-theory/quantum-information-theory/density-operator.html" class="internal-link" target="_self" rel="noopener nofollow">density operator</a>, the Von Neumann Entropy (VNE) is defined via If the density matrix is given as an <a data-tooltip-position="top" aria-label="Eigenvalue Problem" data-href="Eigenvalue Problem" href="the-guide/mathematics/linear-algebra/eigenvalue-problem.html" class="internal-link" target="_self" rel="noopener nofollow">eigendecomposition</a> , the formula simplifies to which is the <a data-tooltip-position="top" aria-label="Shannon Entropy" data-href="Shannon Entropy" href="the-guide/information-theory/information-theory-1/information/shannon-entropy.html" class="internal-link" target="_self" rel="noopener nofollow">Shannon entropy</a> of the eigenvalues.
<br><br>
<br>VNE is zero iff  represents a <a data-tooltip-position="top" aria-label="Quantum State" data-href="Quantum State" href="the-guide/information-theory/quantum-information-theory/quantum-state.html" class="internal-link" target="_self" rel="noopener nofollow">pure state</a>
<br>VNE is maximal ( for -dimensional <a data-tooltip-position="top" aria-label="Inner Product Space and Hilbert Space" data-href="Inner Product Space and Hilbert Space" href="the-guide/mathematics/functional-analysis-and-calculus-of-variations/inner-product-space-and-hilbert-space.html" class="internal-link" target="_self" rel="noopener nofollow">Hilbert space</a>) for a maximally mixed state
<br>VNE satisfies the bound 
<br>Subadditivity - given <a data-tooltip-position="top" aria-label="Density Operator" data-href="Density Operator" href="the-guide/information-theory/quantum-information-theory/density-operator.html" class="internal-link" target="_self" rel="noopener nofollow">reduced density matrices</a>  of a general <a data-tooltip-position="top" aria-label="Quantum State" data-href="Quantum State" href="the-guide/information-theory/quantum-information-theory/quantum-state.html" class="internal-link" target="_self" rel="noopener nofollow">state</a> , it holds that  
<br><br><br>Given an <a data-tooltip-position="top" aria-label="Graph" data-href="Graph" href="the-guide/mathematics/graph-theory/graph.html" class="internal-link" target="_self" rel="noopener nofollow">undirected graph</a> , the Von Neumann graph entropy is defined via the <a data-tooltip-position="top" aria-label="Eigenvalue Problem" data-href="Eigenvalue Problem" href="the-guide/mathematics/linear-algebra/eigenvalue-problem.html" class="internal-link" target="_self" rel="noopener nofollow">eigenvalues</a>  of the <a data-tooltip-position="top" aria-label="Graph Laplacians" data-href="Graph Laplacians" href="the-guide/mathematics/graph-theory/graph-laplacians.html" class="internal-link" target="_self" rel="noopener nofollow">graph Laplacian</a> <a data-tooltip-position="top" aria-label="Matrices" data-href="Matrices" href="the-guide/mathematics/linear-algebra/matrices.html" class="internal-link" target="_self" rel="noopener nofollow">matrix</a>  as where .]]></description><link>the-guide/information-theory/quantum-information-theory/von-neumann-entropy.html</link><guid isPermaLink="false">The Guide/Information Theory/Quantum Information Theory/Von Neumann Entropy.md</guid><pubDate>Mon, 24 Feb 2025 23:20:55 GMT</pubDate></item><item><title><![CDATA[Butterworth Filter]]></title><description><![CDATA[ 
 <br>In a Nutshell
Filter with frequency response that is maximally flat (least amount of ripples) in the passband while still offering smooth transition into the stopband.
<br><br>Transfer Function and Magnitude Response
The transfer function of an -th order Butterworth filter iswhere  is the cutoff frequency. The resulting magnitude response is
<br>Effect of Parameters

<br>Filter Order - increasing the order results in sharper transitions between passband and stopband, with  having a gentle  rollof,  a steeper  and so on. However, higher filter order increases the number of poles, increasing computational complexity and phase distortion.
<br>Cutoff Frequency - frequency at which the energy is halved, important to adjust based on wanted and unwanted frequencies.
<br>Gain - scales the output, usually 

<br>The key idea of the Butterworth filter is to ensure an equi-spaced distribution of its poles in the left-half--plane. <br>Poles
The poles are given bywhere only the ones with negative real parts are used.
<br><br>from scipy.signal import butter, filtfilt

# Example: Low-pass Butterworth filter
order = 4            # Filter order (affects roll-off)
cutoff = 2.0         # Cutoff frequency (Hz)
fs = 100              # Sampling frequency (Hz)

# Design filter
b, a = butter(order, cutoff / (0.5 * fs), btype='low')

# Apply filter to data
filtered_signal = filtfilt(b, a, raw_signal)
]]></description><link>the-guide/integral-transforms-and-signals/butterworth-filter.html</link><guid isPermaLink="false">The Guide/Integral Transforms and Signals/Butterworth Filter.md</guid><pubDate>Thu, 17 Apr 2025 07:07:06 GMT</pubDate></item><item><title><![CDATA[FFT - Fast Fourier Transform]]></title><description><![CDATA[ 
 <br>In a Nutshell
Algorithm that reduces computational complexity of the <a data-tooltip-position="top" aria-label="Fourier Series and Transform" data-href="Fourier Series and Transform" href="the-guide/integral-transforms-and-signals/fourier-series-and-transform.html" class="internal-link" target="_self" rel="noopener nofollow">discrete Fourier transform</a> and its inverse from  to  by exploiting symmetry and periodicity.
<br><br>Recursive Splitting<br>
Split DFT into even and odd partsand rewriteusing  for even and  for odd parts. This results in two  sized FFTs. Recursively applying this results in  stages.<br>Butterfly Structure<br>
Combine intermediate results ...<br>
]]></description><link>the-guide/integral-transforms-and-signals/fft-fast-fourier-transform.html</link><guid isPermaLink="false">The Guide/Integral Transforms and Signals/FFT - Fast Fourier Transform.md</guid><pubDate>Thu, 17 Apr 2025 07:07:06 GMT</pubDate></item><item><title><![CDATA[Fourier Series and Transform]]></title><description><![CDATA[ 
 <br>In a Nutshell
The Fourier transform is an analysis process that decomposes complex-valued functions into its constituent frequencies and amplitudes. In application, it is mostly used to transform a time-domain signal into its frequency-domain representation. This transformation allows us to analyze the frequencies present in a signal.
<br><br>Definition Fourier Transform
Given a continuous <a data-tooltip-position="top" aria-label="Mathematical Relations" data-href="Mathematical Relations" href="the-guide/mathematics/general-stuff/mathematical-relations.html" class="internal-link" target="_self" rel="noopener nofollow">function</a> , its Fourier Transform  iswhere  is the angular frequency in radians per second. The inverse transformation is 
<br><br><br>Definition - DFT
For a discrete sequence  of  samples, the Discrete Fourier Transform (DFT)  is given by where  is the index of the frequency component. The IDFT is computed via 
<br><br>
<br>Linearity
<br>Time Shifting
<br>Frequency Shift
<br>Convolution
]]></description><link>the-guide/integral-transforms-and-signals/fourier-series-and-transform.html</link><guid isPermaLink="false">The Guide/Integral Transforms and Signals/Fourier Series and Transform.md</guid><pubDate>Thu, 17 Apr 2025 07:07:06 GMT</pubDate></item><item><title><![CDATA[Graph Signal]]></title><description><![CDATA[<a class="tag" href="?query=tag:GSP-Course" style="background-color: rgb(4, 108, 116); color: white; font-weight: 700; border: none; border-radius: 1em; padding: 0.2em 0.5em;">#GSP-Course</a> 
 <br>A <a data-tooltip-position="top" aria-label="Graph" data-href="Graph" href="the-guide/mathematics/graph-theory/graph.html" class="internal-link" target="_self" rel="noopener nofollow">graph</a> signal  is a real vector,  , where the entry , is the real scalar associated with <a data-tooltip-position="top" aria-label="Node or Vertex Set" data-href="Node or Vertex Set" href="the-guide/mathematics/graph-theory/node-or-vertex-set.html" class="internal-link" target="_self" rel="noopener nofollow">node</a>  (source of information).<img alt="center" src="lib/media/pasted-image-20221215082447.png" style="width: 300px; max-width: 100%;"><br><br>
<br>Smoothness<br>
- Smooth or low-frequency, if <a data-href="Graph Laplacians" href="the-guide/mathematics/graph-theory/graph-laplacians.html" class="internal-link" target="_self" rel="noopener nofollow">Graph Laplacians</a> yields vector close to zero	- Non-Smooth or high-frequency, if product is larger.<br>
<a href=".?query=tag:GSP-Course" class="tag" target="_blank" rel="noopener nofollow">#GSP-Course</a>
]]></description><link>the-guide/integral-transforms-and-signals/graph-signal.html</link><guid isPermaLink="false">The Guide/Integral Transforms and Signals/Graph Signal.md</guid><pubDate>Thu, 17 Apr 2025 07:07:06 GMT</pubDate><enclosure url="lib/media/pasted-image-20221215082447.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src=&quot;lib/media/pasted-image-20221215082447.png&quot;&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[Heisenberg-Gabor Limit]]></title><description><![CDATA[ 
 <br>In a Nutshell
Principle in signal analysis based on the <a data-href="Heisenberg Uncertainty Principle" href="Heisenberg Uncertainty Principle" class="internal-link" target="_self" rel="noopener nofollow">Heisenberg Uncertainty Principle</a> in quantum mechanics. States that a signal cannot be simultaneously localized in time and frequency.
<br><br>Heisenberg Gabor Theorem
Given a <a data-tooltip-position="top" aria-label="Mathematical Relations" data-href="Mathematical Relations" href="the-guide/mathematics/general-stuff/mathematical-relations.html" class="internal-link" target="_self" rel="noopener nofollow">function</a> / signal  with <a data-tooltip-position="top" aria-label="Fourier Series and Transform" data-href="Fourier Series and Transform" href="the-guide/integral-transforms-and-signals/fourier-series-and-transform.html" class="internal-link" target="_self" rel="noopener nofollow">Fourier transform</a> , the productof
<br>Intuition

<br>If a signal becomes more and more localized in frequency it becomes essentially a sine-wave, which is infinitely spread in time.
<br>If a function does not go in forever, it has to have a cutoff somewhere. This infinite slope need infinitely high frequencies to be modeled.

]]></description><link>the-guide/integral-transforms-and-signals/heisenberg-gabor-limit.html</link><guid isPermaLink="false">The Guide/Integral Transforms and Signals/Heisenberg-Gabor Limit.md</guid><pubDate>Thu, 17 Apr 2025 07:07:06 GMT</pubDate></item><item><title><![CDATA[Savitzky-Golay Filter]]></title><description><![CDATA[ 
 <br>In a Nutshell
Family of digital filters used to smooth noisy data and estimate <a data-tooltip-position="top" aria-label="Derivative, Gradient, Jacobian and Hessian" data-href="Derivative, Gradient, Jacobian and Hessian" href="the-guide/mathematics/analysis-and-calculus/derivative,-gradient,-jacobian-and-hessian.html" class="internal-link" target="_self" rel="noopener nofollow">derivatives</a> by fitting it with low-degree polynomials.
<br>Also known as LOESS.<br><br>Given a discrete signal  sampled at , SGF fits a polynomial of degree  based on a window of  points <br>Local Optimization Problem
Without loss of generality, we shift this window to  to fit a parameterized polynomial of degree  by solving the resulting least squares problem over the window
<br>Effect of Degree and Window Size

<br>Larger  leads to more smoothed out shapes, but may remove sharp features
<br>Higher order polynomials are able to fit more complex trends, but may pick up noise information.

<br><br>To solve the above optimization problem, we can revert to its <a data-tooltip-position="top" aria-label="Matrices" data-href="Matrices" href="the-guide/mathematics/linear-algebra/matrices.html" class="internal-link" target="_self" rel="noopener nofollow">matrix</a> representationwhere  holds the polynomial coefficients. The design matrix  is assembled viafor uniformly spaced samples or via for general data. <br>Smoothing
Based on the above, we follow the usual <a data-tooltip-position="top" aria-label="Linear Least Squares and Ridge Regression" data-href="Linear Least Squares and Ridge Regression" href="the-guide/mathematics/optimization/linear-least-squares-and-ridge-regression.html" class="internal-link" target="_self" rel="noopener nofollow">LLS</a> algorithm to computeThe smoothed value  at the center of the window can be computed using the first row of  with 
<br>Estimating Derivatives
The -th derivative at the center point of the interval can be estimated with 
<br>Practical Considerations
For uniformly spaced data, we only have to compute and invert  once, after which we only need matrix-vector products to compute the smoothed value, yieldingFor the general case, we have to recompute and invert the matrix at every sample, yielding
<br><br><br>
<br>Adaptive Window Sizes 

<br>Enforce a fixed timeframe by adapting the number of samples considered at each point. This will automatically yield the more expensive general case


<br><a data-tooltip-position="top" aria-label="Linear Least Squares and Ridge Regression" data-href="Linear Least Squares and Ridge Regression" href="the-guide/mathematics/optimization/linear-least-squares-and-ridge-regression.html" class="internal-link" target="_self" rel="noopener nofollow">Weighted Least Squares</a>

<br>Weight points in the minimization problem, e.g. using a Gaussian kernel


]]></description><link>the-guide/integral-transforms-and-signals/savitzky-golay-filter.html</link><guid isPermaLink="false">The Guide/Integral Transforms and Signals/Savitzky-Golay Filter.md</guid><pubDate>Thu, 17 Apr 2025 07:07:07 GMT</pubDate></item><item><title><![CDATA[Total Variation]]></title><description><![CDATA[ 
 <br>In a Nutshell
Quantification of the amount of change / oscillations of a <a data-tooltip-position="top" aria-label="Mathematical Relations" data-href="Mathematical Relations" href="the-guide/mathematics/general-stuff/mathematical-relations.html" class="internal-link" target="_self" rel="noopener nofollow">function</a>. 
<br>Can be used as a regularizer to compute a smoothed signal that preserves edges, see <a data-href="Total Variation Regularization" href="the-guide/integral-transforms-and-signals/total-variation-regularization.html" class="internal-link" target="_self" rel="noopener nofollow">Total Variation Regularization</a>.<br><br>General Formulation
Given a function  defined on a domain , TV is defined via
<br><br><br>Time Signal Variation
Given samples , the total variation is defined as

<br>Regular case (uniform timesteps)

<br>Denominator is normalization


<br>Irregular case 

<br>Two dimensional, e.g. <a data-tooltip-position="top" aria-label="Matrices" data-href="Matrices" href="the-guide/mathematics/linear-algebra/matrices.html" class="internal-link" target="_self" rel="noopener nofollow">matrix</a> representing an image <br><br><br>Graph Signal Variation
Expanding the above concept to <a data-tooltip-position="top" aria-label="Graph Signal" data-href="Graph Signal" href="the-guide/integral-transforms-and-signals/graph-signal.html" class="internal-link" target="_self" rel="noopener nofollow">graph signals</a> on <a data-tooltip-position="top" aria-label="Graph" data-href="Graph" href="the-guide/mathematics/graph-theory/graph.html" class="internal-link" target="_self" rel="noopener nofollow">graphs</a>, this leads to where

<br>At each <a data-tooltip-position="top" aria-label="Node or Vertex Set" data-href="Node or Vertex Set" href="the-guide/mathematics/graph-theory/node-or-vertex-set.html" class="internal-link" target="_self" rel="noopener nofollow">node</a>, we measure the variation to the neighbouring nodes given by the weights and accumulate this over all nodes.

]]></description><link>the-guide/integral-transforms-and-signals/total-variation.html</link><guid isPermaLink="false">The Guide/Integral Transforms and Signals/Total Variation.md</guid><pubDate>Thu, 17 Apr 2025 07:07:07 GMT</pubDate></item><item><title><![CDATA[Total Variation Regularization]]></title><description><![CDATA[ 
 <br>In a Nutshell
Optimization-based de-noising or smoothing technique derived from <a data-href="Total Variation" href="the-guide/integral-transforms-and-signals/total-variation.html" class="internal-link" target="_self" rel="noopener nofollow">Total Variation</a> used in signal and image processing. 
<br><br><br>For the general formulation, we can define the Total Variation regularization of a function based on the <a data-tooltip-position="top" aria-label="Convexity" data-href="Convexity" href="the-guide/mathematics/optimization/convex-optimization-lecture/convexity.html" class="internal-link" target="_self" rel="noopener nofollow">convex</a> <a data-tooltip-position="top" aria-label="Optimization Problem" data-href="Optimization Problem" href="the-guide/mathematics/optimization/convex-optimization-lecture/optimization-problem.html" class="internal-link" target="_self" rel="noopener nofollow">optimization problem</a>where  is a fidelty term, e.g. the -norm.<br><br><br>To obtain a denoises 1D signal  based on a noisy observation , we solve the <a data-tooltip-position="top" aria-label="Optimization Problem" data-href="Optimization Problem" href="the-guide/mathematics/optimization/convex-optimization-lecture/optimization-problem.html" class="internal-link" target="_self" rel="noopener nofollow">optimization problem</a>where<br>
<br>the  ensures staying close to the original data
<br>the  encourages smoothness, removing noise
<br><br>Advantages
In contrast to e.g. Gaussian smoothing, TV regularization performs well in preserving edges / discontinuities in a signal, making it ideal for piecewise constant signals.
<br>Problems
TV regularization requires solving a global optimization problem over the whole signal, making it unsuitable for e.g. filtering. Additionally, the absolute value requires numerical methods to solve, which may scale poorly for high dimensions.
]]></description><link>the-guide/integral-transforms-and-signals/total-variation-regularization.html</link><guid isPermaLink="false">The Guide/Integral Transforms and Signals/Total Variation Regularization.md</guid><pubDate>Thu, 17 Apr 2025 07:07:07 GMT</pubDate></item><item><title><![CDATA[Activation Functions]]></title><description><![CDATA[ 
 <br>In a Nutshell
Overview of activation functions used in <a data-tooltip-position="top" aria-label="Artificial Neural Network" data-href="Artificial Neural Network" href="the-guide/machine-learning/deep-learning/artificial-neural-network.html" class="internal-link" target="_self" rel="noopener nofollow">neural networks</a>. Only best practices available, not fully understood!
<br><br><br>Sigmoid
The Sigmoid <a data-tooltip-position="top" aria-label="Mathematical Relations" data-href="Mathematical Relations" href="the-guide/mathematics/general-stuff/mathematical-relations.html" class="internal-link" target="_self" rel="noopener nofollow">function</a> is defined asIt can be interpreted as a smoothed step function.
<br>−4−3−2−101234−1-0.8-0.6-0.4-0.200.20.40.60.81<br>Gradient Issues
The <a data-tooltip-position="top" aria-label="Derivative, Gradient, Jacobian and Hessian" data-href="Derivative, Gradient, Jacobian and Hessian" href="the-guide/mathematics/analysis-and-calculus/derivative,-gradient,-jacobian-and-hessian.html" class="internal-link" target="_self" rel="noopener nofollow">gradient</a> is zero almost everywhere.
<br><br><br>Tanh
The hyperbolic tangent function is a smooth, saturating functionapproximating a linear function with saturation towards . This function can mitigate the gradient issues experienced with sigmoid.
<br>−2-1.5−1-0.500.511.52-1.2−1-0.8-0.6-0.4-0.200.20.40.60.811.2<br><br><br>Softmax
The softmax function maps a <a data-tooltip-position="top" aria-label="Vector Space" data-href="Vector Space" href="the-guide/mathematics/general-stuff/vector-space.html" class="internal-link" target="_self" rel="noopener nofollow">vector</a> of so-called logits  to a <a data-tooltip-position="top" aria-label="Probability Distribution" data-href="Probability Distribution" href="the-guide/mathematics/probability-theory/probability-distribution.html" class="internal-link" target="_self" rel="noopener nofollow">probability distribution</a> over  discrete classes viawhere  is the temperature scaling. High negative values are mapped to low probabilities, while high positive values are mapped to high probabilities.
<br>Temperature Effect
Temperature scaling controls the effect of multiple high logit values. 

<br>A high temperature  increased sharp peaks (decreases <a data-tooltip-position="top" aria-label="Shannon Entropy" data-href="Shannon Entropy" href="the-guide/information-theory/information-theory-1/information/shannon-entropy.html" class="internal-link" target="_self" rel="noopener nofollow">entropy</a>)

<br>Avoiding Overflows
The formula is prone to overflows. A well-known workaround is to computeinstead.
<br><br><br>ReLu
The Rectified Linear Unit or ReLU is defined via the positive part of its argument
<br>−2-1.5−1-0.500.511.52−2-1.5−1-0.500.511.52<br>Dying ReLu Problem
Using ReLu can lead to inactive or "dead" neurons throughout the training process. The reasons for this are not well understood, but using another initialization or switching to leaky ReLu can work against that issue.
<br><br><br>Leaky ReLu
The leaky Rectified Linear Unit or leaky ReLU is an extension of the above and defined via the positive part of its argumentThis allows for small negative values, which can work against the dying ReLu problem, because the neurons cannot completely "die".
<br>−2-1.5−1-0.500.511.52−2-1.5−1-0.500.511.52]]></description><link>the-guide/machine-learning/deep-learning/activation-functions.html</link><guid isPermaLink="false">The Guide/Machine Learning/Deep Learning/Activation Functions.md</guid><pubDate>Thu, 17 Apr 2025 07:11:23 GMT</pubDate></item><item><title><![CDATA[Artificial Neural Network]]></title><description><![CDATA[ 
 <br><br><br>
<br>Single neuron model can be grouped together in layers of m inputs and n outputs with

<br>Input 
<br>Parametes or weigths 
<br>Bias 

<br>often included in the weights  by using an extended input with an additional 1 


<br>Pre-activation vector 
<br>Activation function 
<br>Output vector 


<br>How to learn Parameters ?

<br><a data-href="Forward and Backward Propagation" href="the-guide/machine-learning/deep-learning/forward-and-backward-propagation.html" class="internal-link" target="_self" rel="noopener nofollow">Forward and Backward Propagation</a>


<br><br><br><img alt="center" src="lib/media/pasted-image-20221219165934.png" style="width: 350px; max-width: 100%;"><br>
<br>Network Building Blocks

<br>Model Type

<br>Topology (Fully connected, sparsely connected, ...)

<br><a data-href="Graph" href="the-guide/mathematics/graph-theory/graph.html" class="internal-link" target="_self" rel="noopener nofollow">Graph</a>-Theory


<br>Neural Elements

<br><a data-href="Activation Functions" href="the-guide/machine-learning/deep-learning/activation-functions.html" class="internal-link" target="_self" rel="noopener nofollow">Activation Functions</a>
<br>Output Neurons

<br>Regression -&gt; Linear function
<br>Classification -&gt; Sigmoid function
<br>Multiclass-classification -&gt; Softmax function


<br>Loss Function






<br><br>
<br>Overfitting

<br>NN have a lot of parameters  prone to Overfitting  fight off with a prior

<br>Regularization

<br>Regularize input data


<br>Early Stopping

<br>Stop when validation error starts rising again


<br>Input Noise Augmentation

<br>Add artificial noise 


<br>Dropout

<br>Prune less relevant neurons and rescale activations for others






<br><br>
<br>Do

<br>A lot of data available

<br>Easier to learn features than building them
<br>Does not scale with number of data points




<br>Don't

<br>You have a small dataset
<br>You need temporal structure
<br>You need to quantify uncertainty


]]></description><link>the-guide/machine-learning/deep-learning/artificial-neural-network.html</link><guid isPermaLink="false">The Guide/Machine Learning/Deep Learning/Artificial Neural Network.md</guid><pubDate>Sun, 30 Mar 2025 16:17:46 GMT</pubDate><enclosure url="lib/media/pasted-image-20221219165934.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src=&quot;lib/media/pasted-image-20221219165934.png&quot;&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[Batch Ensemble]]></title><description><![CDATA[ 
 <br>In a Nutshell
Ensemble method that aims at lowering computational memory cost of regular <a data-href="Deep Ensembles" href="the-guide/machine-learning/deep-learning/deep-ensembles.html" class="internal-link" target="_self" rel="noopener nofollow">Deep Ensembles</a>.
<br><br>When using <a data-tooltip-position="top" aria-label="Deep Ensembles" data-href="Deep Ensembles" href="the-guide/machine-learning/deep-learning/deep-ensembles.html" class="internal-link" target="_self" rel="noopener nofollow">deep ensembles</a> to predict uncertainty, we can use parallel computation to keep the training time in check provided that we have enough resources. However, each network requires its own weights and biases stored separately, which can yield very high storage demands.  <br>The Batch Ensemble method splits the weights into two types: <br>
<br>Slow Weight <a data-tooltip-position="top" aria-label="Matrices" data-href="Matrices" href="the-guide/mathematics/linear-algebra/matrices.html" class="internal-link" target="_self" rel="noopener nofollow">Matrix</a>  that is shared by each ensemble members
<br>Fast Weight Vectors  and  that are unique for each member<br>
Each member then uses its fast weights to transform the shared weights via <img alt="center" src="lib/media/pasted-image-20240509093352.png" style="width: 400px; max-width: 100%;">
<br>Advantages
Significantly lowers memory demand, because each member has to store two vectors instead of a matrix.
<br><img alt="center" src="lib/media/pasted-image-20240509093458.png" style="width: 300px; max-width: 100%;">]]></description><link>the-guide/machine-learning/deep-learning/batch-ensemble.html</link><guid isPermaLink="false">The Guide/Machine Learning/Deep Learning/Batch Ensemble.md</guid><pubDate>Sat, 05 Oct 2024 19:01:09 GMT</pubDate><enclosure url="lib/media/pasted-image-20240509093352.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src=&quot;lib/media/pasted-image-20240509093352.png&quot;&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[Batch Normalization]]></title><description><![CDATA[ 
 <br>In a Nutshell
Technique used in deep learning applications to stabilize training. Based mainly on empirical evidence, mathematical backgrounds are not well understood.
<br><br>Consider a mini-batch of dimension , where  is the dimension of the data, e.g.  for images with <a data-tooltip-position="top" aria-label="Convolutional Neural Network" data-href="Convolutional Neural Network" href="the-guide/machine-learning/deep-learning/convolutional-neural-network.html" class="internal-link" target="_self" rel="noopener nofollow">CNN's</a>,  is the size of the mini-batch and  denotes the number of channels.<br>
Batch Normalization now computes the empirical <a data-tooltip-position="top" aria-label="Expectations" data-href="Expectations" href="the-guide/mathematics/statistics/expectations.html" class="internal-link" target="_self" rel="noopener nofollow">mean</a>  and <a data-tooltip-position="top" aria-label="Covariance and Variance" data-href="Covariance and Variance" href="the-guide/mathematics/statistics/covariance-and-variance.html" class="internal-link" target="_self" rel="noopener nofollow">variance</a>  of each of the inputs dimensions in order to shift each input via reaching activations with zero mean and unit variance for each dimension (technically  changes this, but it is only a small constant to prevent overflows).<br>
To increase representation power of the network, we can use this in a transformation step with learnable parameters  and  via  ]]></description><link>the-guide/machine-learning/deep-learning/batch-normalization.html</link><guid isPermaLink="false">The Guide/Machine Learning/Deep Learning/Batch Normalization.md</guid><pubDate>Mon, 12 Aug 2024 17:07:11 GMT</pubDate></item><item><title><![CDATA[Convolutional Neural Network]]></title><description><![CDATA[ 
 <br>Type of feed-forward <a data-tooltip-position="top" aria-label="Artificial Neural Network" data-href="Artificial Neural Network" href="the-guide/machine-learning/deep-learning/artificial-neural-network.html" class="internal-link" target="_self" rel="noopener nofollow">neural network</a> that learns feature engineering via filters / kernel optimization. <br><br><br>
<br>Convolution Layers
<br>Pooling Layers - Downsampling operation, filter is shifted by filter size and a single value is selected for each window
<br>Fully Connected - regular layer on flattened input to solve downstream tasks after extracting features via kernels<br>
<img alt="center" src="lib/media/pasted-image-20240422124041.png">
<br><br><br>
<br>Filter Size  - a filter of size  applied to an input with  channels performs convolutions on an input of size  and produces an output feature map of size <img alt="center" src="lib/media/pasted-image-20240422125913.png">
<br>Stride  - number of pixels by which the kernel window moves after each operation
<br>Zero-Padding - adding  zeros to each side of the boundaries of the input, ...

<br>No padding, last convolution is dropped if dimensions do not match
<br>Half padding, append zeros only at the ends of each dimension
<br>Full padding, end convolutions are applied everywhere and filter sees the whole input


<br>Receptive Field  - pixel field of size  of the input that a pixel at layer  can see, depends on the architecture. Using the convention , we can deduct the formula 
<br><br><br>
<br><a rel="noopener nofollow" class="external-link" href="https://stanford.edu/~shervine/teaching/cs-230/cheatsheet-convolutional-neural-networks" target="_blank">https://stanford.edu/~shervine/teaching/cs-230/cheatsheet-convolutional-neural-networks</a>
]]></description><link>the-guide/machine-learning/deep-learning/convolutional-neural-network.html</link><guid isPermaLink="false">The Guide/Machine Learning/Deep Learning/Convolutional Neural Network.md</guid><pubDate>Mon, 12 Aug 2024 17:07:11 GMT</pubDate><enclosure url="lib/media/pasted-image-20240422124041.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src=&quot;lib/media/pasted-image-20240422124041.png&quot;&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[Deep Ensembles]]></title><description><![CDATA[ 
 <br>Simple trick to incorporate <a data-tooltip-position="top" aria-label="Uncertainty Estimation" data-href="Uncertainty Estimation" href="the-guide/machine-learning/generative-models/uncertainty-estimation.html" class="internal-link" target="_self" rel="noopener nofollow">uncertainty</a> and improve accuracy of learning algorithms. Randomly initializes  copies of the model with different seeds and then performs regular <a data-tooltip-position="top" aria-label="Gradient Descent" data-href="Gradient Descent" href="the-guide/mathematics/optimization/convex-optimization-lecture/algorithms/gradient-descent.html" class="internal-link" target="_self" rel="noopener nofollow">stochastic gradient descent</a> on each of them. In forward pass, predictions of all models are combined.<br><img alt="center" src="lib/media/pasted-image-20240509094058.png" style="width: 400px; max-width: 100%;"><br>While <a data-tooltip-position="top" aria-label="Variational Inference" data-href="Variational Inference" href="the-guide/computational-statistics/variational-inference.html" class="internal-link" target="_self" rel="noopener nofollow">variational</a> <a data-tooltip-position="top" aria-label="- Bayesian Statistics -" data-href="- Bayesian Statistics -" href="the-guide/mathematics/statistics/-bayesian-statistics-.html" class="internal-link" target="_self" rel="noopener nofollow">Bayesian</a> methods do capture uncertainty of a single mode quite well, they fail to explore diversity of multiple modes.<br><br><br>
<br>Test-Time Augmentation - Add artificial translation/rotation/blurring etc. to each ensemble member, but a different type to each
<br>Hyperparameter Ensembles - in addition to different seeds, use different hyperparameters for each ensemble members to further boost the effects of deep ensembles.<img alt="center" src="lib/media/pasted-image-20240509102748.png" style="width: 250px; max-width: 100%;">
<br><a data-href="Batch Ensemble" href="the-guide/machine-learning/deep-learning/batch-ensemble.html" class="internal-link" target="_self" rel="noopener nofollow">Batch Ensemble</a>
<br>Adaptive Diversity Promoting - explicitly regularizes members to be more different by constraining the loss where 
<br>DICE - force conditional distribution to be unpredictable from each other
]]></description><link>the-guide/machine-learning/deep-learning/deep-ensembles.html</link><guid isPermaLink="false">The Guide/Machine Learning/Deep Learning/Deep Ensembles.md</guid><pubDate>Thu, 17 Apr 2025 16:23:35 GMT</pubDate><enclosure url="lib/media/pasted-image-20240509094058.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src=&quot;lib/media/pasted-image-20240509094058.png&quot;&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[Forward and Backward Propagation]]></title><description><![CDATA[ 
 <br><img alt="center" src="lib/media/pasted-image-20221219180019.png" style="width: 400px; max-width: 100%;"><br>
<br>Algorithm

<br>Forward Propagation

<br>Compute 

<br>Pre-activations and activations at each hidden layer
<br>
<br>
<br>


<br>Output 

<br>


<br>Loss function

<br>




<br>Backpropagation

<br>Compute the contribution of each parameter to the loss<img alt="center" src="lib/media/pasted-image-20221219181017.png" style="width: 350px; max-width: 100%;"> 

<br>Update parameters to minimize loss function using <a data-href="Gradient Descent" href="the-guide/mathematics/optimization/convex-optimization-lecture/algorithms/gradient-descent.html" class="internal-link" target="_self" rel="noopener nofollow">Gradient Descent</a>






]]></description><link>the-guide/machine-learning/deep-learning/forward-and-backward-propagation.html</link><guid isPermaLink="false">The Guide/Machine Learning/Deep Learning/Forward and Backward Propagation.md</guid><pubDate>Mon, 12 Aug 2024 17:07:11 GMT</pubDate><enclosure url="lib/media/pasted-image-20221219180019.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src=&quot;lib/media/pasted-image-20221219180019.png&quot;&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[LSTM - Long Short-Term Memory Networks]]></title><description><![CDATA[ 
 <br>Type of <a data-href="Recurrent Neural Network" href="the-guide/machine-learning/deep-learning/recurrent-neural-network.html" class="internal-link" target="_self" rel="noopener nofollow">Recurrent Neural Network</a><br><a rel="noopener nofollow" class="external-link" href="https://en.wikipedia.org/wiki/Long_short-term_memory#:~:text=Long%20short%2Dterm%20memory%20(LSTM,and%20other%20sequence%20learning%20methods" target="_blank">https://en.wikipedia.org/wiki/Long_short-term_memory#:~:text=Long%20short%2Dterm%20memory%20(LSTM,and%20other%20sequence%20learning%20methods</a>.]]></description><link>the-guide/machine-learning/deep-learning/lstm-long-short-term-memory-networks.html</link><guid isPermaLink="false">The Guide/Machine Learning/Deep Learning/LSTM - Long Short-Term Memory Networks.md</guid><pubDate>Wed, 05 Mar 2025 16:58:24 GMT</pubDate></item><item><title><![CDATA[Recurrent Neural Network]]></title><description><![CDATA[ 
 <br>In a Nutshell
<a data-tooltip-position="top" aria-label="Artificial Neural Network" data-href="Artificial Neural Network" href="the-guide/machine-learning/deep-learning/artificial-neural-network.html" class="internal-link" target="_self" rel="noopener nofollow">Artificial neural network</a> with b-directional information flow, allows the output of some nodes to affect the same node again. It can thereby store states in an internal memory, allowing it to process arbitrary sequences.
<br><img alt="center" src="lib/media/pasted-image-20240422162400.png" style="width: 400px; max-width: 100%;">For each timestep , the activation  and the output  are computed viaThe building block above can be connected in many different ways to create different network architectures.<br>Gradient Issues
RNN's are very susceptible to the vanishing and / or exploding gradient phenomena. 
<br><br><br><br>...<br><br>...<br><br><br>
<br><a rel="noopener nofollow" class="external-link" href="https://stanford.edu/~shervine/teaching/cs-230/cheatsheet-recurrent-neural-networks" target="_blank">https://stanford.edu/~shervine/teaching/cs-230/cheatsheet-recurrent-neural-networks</a>
]]></description><link>the-guide/machine-learning/deep-learning/recurrent-neural-network.html</link><guid isPermaLink="false">The Guide/Machine Learning/Deep Learning/Recurrent Neural Network.md</guid><pubDate>Sat, 08 Feb 2025 19:31:07 GMT</pubDate><enclosure url="lib/media/pasted-image-20240422162400.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src=&quot;lib/media/pasted-image-20240422162400.png&quot;&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[Training Sequence Models]]></title><description><![CDATA[ 
 <br>In a Nutshell
Some considerations, tips and tricks when training sequence models like <a data-tooltip-position="top" aria-label="Recurrent Neural Network" data-href="Recurrent Neural Network" href="the-guide/machine-learning/deep-learning/recurrent-neural-network.html" class="internal-link" target="_self" rel="noopener nofollow">RNNs</a> or <a data-tooltip-position="top" aria-label="Transformer" data-href="Transformer" href="the-guide/machine-learning/large-language-models/transformer.html" class="internal-link" target="_self" rel="noopener nofollow">Transformers</a>.
<br><br>...<br>Teacher Forcing 
Training strategy: Instead of having the model use its own predictions as input for the next time step, you feed the actual, ground-truth token from the training data.
This means that when the model is predicting the <a data-tooltip-position="top" aria-label="Tokenization" data-href="Tokenization" href="the-guide/machine-learning/large-language-models/tokenization.html" class="internal-link" target="_self" rel="noopener nofollow">token</a> at position , it receives the correct token from position  as input, rather than the token it predicted in the previous step. 
<br>Advantages

<br>
Faster Convergence: Because the model is always given the correct context, it learns the sequence patterns more quickly without the risk of compounding errors.

<br>
Stability: It prevents error propagation during training, mistakes made at one step don’t adversely affect subsequent predictions.


<br>Problems

<br>During inference, the model doesn’t have access to the ground-truth tokens and must rely on its own previous predictions, which can lead to a mismatch between training and inference—a phenomenon known as exposure bias.

]]></description><link>the-guide/machine-learning/deep-learning/training-sequence-models.html</link><guid isPermaLink="false">The Guide/Machine Learning/Deep Learning/Training Sequence Models.md</guid><pubDate>Sat, 29 Mar 2025 18:58:45 GMT</pubDate></item><item><title><![CDATA[(Variational) Auto Encoder]]></title><description><![CDATA[ 
 <br>In a Nutshell
An Auto Decoder (AE) is a <a data-tooltip-position="top" aria-label="Artificial Neural Network" data-href="Artificial Neural Network" href="the-guide/machine-learning/deep-learning/artificial-neural-network.html" class="internal-link" target="_self" rel="noopener nofollow">neural network</a> that follows an encoder-decoder structure to learn an identity <a data-tooltip-position="top" aria-label="Mathematical Relations" data-href="Mathematical Relations" href="the-guide/mathematics/general-stuff/mathematical-relations.html" class="internal-link" target="_self" rel="noopener nofollow">map</a> in an unsupervised way. During this process, a compressed representation of the data is learned in a latent space bottleneck. While a vanilla AE outputs a vector in latent space, a Variational Auto Encoder (VAE) outputs the parameters of a parameterized <a data-tooltip-position="top" aria-label="Probability Distribution" data-href="Probability Distribution" href="the-guide/mathematics/probability-theory/probability-distribution.html" class="internal-link" target="_self" rel="noopener nofollow">distribution</a> in latent space for every input.
<br><br><br>In the vanilla case, we map an input  into a lower dimensional latent space  via . The resulting vector is then forwarded through a decoder , yielding the output . We optimize both networks using e.g. <a data-tooltip-position="top" aria-label="Forward and Backward Propagation" data-href="Forward and Backward Propagation" href="the-guide/machine-learning/deep-learning/forward-and-backward-propagation.html" class="internal-link" target="_self" rel="noopener nofollow">backpropagation</a> and a loss on the input-output error, e.g. This yields a latent compression of the data that can be compared to a (possibly non-linear) <a data-tooltip-position="top" aria-label="Principal Component Analysis (PCA)" data-href="Principal Component Analysis (PCA)" href="Principal Component Analysis (PCA)" class="internal-link" target="_self" rel="noopener nofollow">principal component analysis</a> / dimensionality reduction.<img alt="center" src="lib/media/pasted-image-20240521134214.png" style="width: 450px; max-width: 100%;">The non-linearity arises in deep architectures, where the data is then not projected onto a <a data-tooltip-position="top" aria-label="Space" data-href="Space" href="the-guide/mathematics/general-stuff/space.html" class="internal-link" target="_self" rel="noopener nofollow">subspace</a>, but on a non-linear <a data-tooltip-position="top" aria-label="Mannigfaltigkeiten" data-href="Mannigfaltigkeiten" href="the-guide/mathematics/differential-geometry/riemannian-geometry/mannigfaltigkeiten-und-differenzierbare-abbildungen/mannigfaltigkeiten.html" class="internal-link" target="_self" rel="noopener nofollow">manifold</a>.<br><br>
<br>Denoising AE

<br>Add dropout between input and first layer, randomly deactivating some input values. This is done to essentially corrupt the input in order to improve robustness.


<br>Sparse AE

<br>Adds regularization term to loss that penalizes activation of multiple nodes, yielding sparse activation patterns for single samples.<img alt="center" src="lib/media/pasted-image-20240521135359.png" style="width: 200px; max-width: 100%;">


<br>Stacked AE

<br>Stacks multiple latent space layers, enables transformations in latent space<img alt="center" src="lib/media/pasted-image-20240521135157.png" style="width: 200px; max-width: 100%;"> 


<br><br><br>The variational auto encoder extends the above to the probabilistic setting, enabling the auto encoder to be used as a generative model. <br>Given data points , we want to train an encoder-decoder architecture such that we can later use the decoder part to output a likelihood distribution  for samples in . The parameters  in this case are the combined parameters of the latent distribution  and of the function mapping .<br>Mathematically, we maximize the probability of generating the data via However, to evaluate the probabilities for each sample, we'd have to marginalize over all possible latent space realizationswhich is infeasible.<br><br>To overcome this, we introduce a distribution  that feeds us likely latent variables given an input , i.e. an encoder network for training.<img alt="center" src="lib/media/pasted-image-20240521141421.png" style="width: 400px; max-width: 100%;"><br>
In this extended setting, we now want to maximize the probability of generating data via the <a data-tooltip-position="top" aria-label="Likelihood Function" data-href="Likelihood Function" href="the-guide/mathematics/statistics/likelihood-function.html" class="internal-link" target="_self" rel="noopener nofollow">log-likelihood</a>  while minimizing the <a data-tooltip-position="top" aria-label="Kullback-Leibler Divergence" data-href="Kullback-Leibler Divergence" href="the-guide/information-theory/information-theory-1/information/kullback-leibler-divergence.html" class="internal-link" target="_self" rel="noopener nofollow">KL-divergence</a> between our  and the unknown, true . <br>Using the <a data-tooltip-position="top" aria-label="Evidence Lower Bound" data-href="Evidence Lower Bound" href="the-guide/mathematics/probability-theory/evidence-lower-bound.html" class="internal-link" target="_self" rel="noopener nofollow">ELBO</a>, we can extend the term  and derive the identity where the left side is exactly the objective above. <br>Variational Auto Encoder Objective
We can now instead optimize the right side of the equality, where all the components are given / feasible to compute. The final loss function is with the objective The big advantage of this is that we now compute the <a data-tooltip-position="top" aria-label="Kullback-Leibler Divergence" data-href="Kullback-Leibler Divergence" href="the-guide/information-theory/information-theory-1/information/kullback-leibler-divergence.html" class="internal-link" target="_self" rel="noopener nofollow">KL-divergence</a> between  and , which we set in the beginning, making the objective feasible to compute.
<br>We need to minimize the loss function above w.r.t both  and , requiring two gradients. The gradient for  is straight-forward to obtain by using logarithm rules and pulling the gradient inside the <a data-tooltip-position="top" aria-label="Expectations" data-href="Expectations" href="the-guide/mathematics/statistics/expectations.html" class="internal-link" target="_self" rel="noopener nofollow">expectation</a> <br><br>However, for , we cannot compute a notion of a gradient, because of the sampling process  being dependent on the parameters we are trying to optimize. To overcome this, we use a <a data-tooltip-position="top" aria-label="Gaussian Distribution" data-href="Gaussian Distribution" href="the-guide/mathematics/probability-theory/gaussian-distribution.html" class="internal-link" target="_self" rel="noopener nofollow">multivariate Gaussian</a> with diagonal <a data-tooltip-position="top" aria-label="Covariance and Variance" data-href="Covariance and Variance" href="the-guide/mathematics/statistics/covariance-and-variance.html" class="internal-link" target="_self" rel="noopener nofollow">covariance</a> <a data-tooltip-position="top" aria-label="Matrices" data-href="Matrices" href="the-guide/mathematics/linear-algebra/matrices.html" class="internal-link" target="_self" rel="noopener nofollow">matrix</a> to encode  and use the re-parameterization trick, i.e.where we now learn the parameters  and  while pushing the stochasticity onto .<br><img alt="center" src="lib/media/pasted-image-20240521143122.png" style="width: 400px; max-width: 100%;"><br>
This enables us to now express the expectation over  and obtain The final architecture of the variational auto encoder can be summarized in the following diagram.<br>
<img alt="center" src="lib/media/pasted-image-20240521143251.png" style="width: 450px; max-width: 100%;"><br><br>By introducing the reparameterization, we can now write the entire loss function in terms of sampling over . Formally, we can express  as where we use the <a data-tooltip-position="top" aria-label="Derivative, Gradient, Jacobian and Hessian" data-href="Derivative, Gradient, Jacobian and Hessian" href="the-guide/mathematics/analysis-and-calculus/derivative,-gradient,-jacobian-and-hessian.html" class="internal-link" target="_self" rel="noopener nofollow">Jacobian</a> of the transformation we applied. Inserting this into the loss, we obtain <br><br>
<br>-VAE

<br>Reweights the loss terms to enable <a data-tooltip-position="top" aria-label="Feature Disentanglement" data-href="Feature Disentanglement" href="the-guide/machine-learning/generative-models/feature-disentanglement.html" class="internal-link" target="_self" rel="noopener nofollow">feature disentanglement</a>


<br>...
<br><br><br>If we follow through with the definitions above, we get<br>
<br>For , we can instead write which follows from the <a data-tooltip-position="top" aria-label="Probability Density Function" data-href="Probability Density Function" href="the-guide/mathematics/probability-theory/probability-density-function.html" class="internal-link" target="_self" rel="noopener nofollow">pdf</a> of a <a data-tooltip-position="top" aria-label="Gaussian Distribution" data-href="Gaussian Distribution" href="the-guide/mathematics/probability-theory/gaussian-distribution.html" class="internal-link" target="_self" rel="noopener nofollow">Gaussian</a> with unit <a data-tooltip-position="top" aria-label="Covariance and Variance" data-href="Covariance and Variance" href="the-guide/mathematics/statistics/covariance-and-variance.html" class="internal-link" target="_self" rel="noopener nofollow">variance</a>.
<br>For , we can use the same argument and even use that we fix the distribution to be a standard Gaussian, resulting in very simple formulas for the KL
<br>Maximization of the <a data-tooltip-position="top" aria-label="Likelihood Function" data-href="Likelihood Function" href="the-guide/mathematics/statistics/likelihood-function.html" class="internal-link" target="_self" rel="noopener nofollow">likelihood</a> can be done via minimization of the <a data-href="Cross Entropy" href="the-guide/information-theory/information-theory-1/information/cross-entropy.html" class="internal-link" target="_self" rel="noopener nofollow">Cross Entropy</a>, for which pre-implemented functions exist. We use those with the input  and the reconstructed output .
]]></description><link>the-guide/machine-learning/generative-models/(variational)-auto-encoder.html</link><guid isPermaLink="false">The Guide/Machine Learning/Generative Models/(Variational) Auto Encoder.md</guid><pubDate>Tue, 25 Feb 2025 11:18:08 GMT</pubDate><enclosure url="lib/media/pasted-image-20240521134214.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src=&quot;lib/media/pasted-image-20240521134214.png&quot;&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[Bayesian Neural Networks]]></title><description><![CDATA[ 
 <br>In a Nutshell
Deep Learning method that combines <a data-tooltip-position="top" aria-label="- Bayesian Statistics -" data-href="- Bayesian Statistics -" href="the-guide/mathematics/statistics/-bayesian-statistics-.html" class="internal-link" target="_self" rel="noopener nofollow">Bayesian inference</a> with neural networks by taking a probabilistic interpretation of model parameters.
<br><br>
<br>instead of fixed weight , use parameterized distribution, in most cases <a data-tooltip-position="top" aria-label="Gaussian Distribution" data-href="Gaussian Distribution" href="the-guide/mathematics/probability-theory/gaussian-distribution.html" class="internal-link" target="_self" rel="noopener nofollow">Gaussians</a>
<br>pick a <a data-tooltip-position="top" aria-label="Bayes Theorem" data-href="Bayes Theorem" href="the-guide/mathematics/statistics/bayes-theorem.html" class="internal-link" target="_self" rel="noopener nofollow">prior</a>, mostly zero mean
<br>perform training using <a data-href="Variational Inference" href="the-guide/computational-statistics/variational-inference.html" class="internal-link" target="_self" rel="noopener nofollow">Variational Inference</a> or <a data-href="Markov-Chain Monte Carlo Methods" href="the-guide/computational-statistics/data-assimilation/smoothing-algorithms/markov-chain-monte-carlo-methods.html" class="internal-link" target="_self" rel="noopener nofollow">Markov-Chain Monte Carlo Methods</a>
<br>Problems 
Statistical

<br>Does not incorporate information about network structured
<br>Prior based on heuristic, how to use for e.g. <a data-tooltip-position="top" aria-label="Exploration-Exploitation Trade-Off" data-href="Exploration-Exploitation Trade-Off" href="the-guide/machine-learning/reinforcement-learning/exploration-exploitation-trade-off.html" class="internal-link" target="_self" rel="noopener nofollow">exploration</a> ?
<br>Tend to underfit for larger problems

<a data-tooltip-position="top" aria-label="Optimization Problem" data-href="Optimization Problem" href="the-guide/mathematics/optimization/convex-optimization-lecture/optimization-problem.html" class="internal-link" target="_self" rel="noopener nofollow">Optimization</a>

<br>Sensitive to parametrization, prior can be dominant
<br>Cold Posterior Effect - empirically, versions with downscaled prior perform better
<br>Can be parameter inefficient

<br><br><br>Rank- Bayesian Neural Networks<br>
Combine <a data-tooltip-position="top" aria-label="Batch Ensemble" data-href="Batch Ensemble" href="the-guide/machine-learning/deep-learning/batch-ensemble.html" class="internal-link" target="_self" rel="noopener nofollow">batch ensemble</a> with BNN by ...]]></description><link>the-guide/machine-learning/generative-models/bayesian-neural-networks.html</link><guid isPermaLink="false">The Guide/Machine Learning/Generative Models/Bayesian Neural Networks.md</guid><pubDate>Thu, 17 Apr 2025 16:23:36 GMT</pubDate></item><item><title><![CDATA[Diffusion Models]]></title><description><![CDATA[ 
 <br>In a Nutshell
Type of <a data-tooltip-position="top" aria-label="Generative Models" data-href="Generative Models" href="the-guide/machine-learning/generative-models/generative-models.html" class="internal-link" target="_self" rel="noopener nofollow">generative model</a> that defines a <a data-tooltip-position="top" aria-label="Markov Chain" data-href="Markov Chain" href="the-guide/computational-statistics/data-assimilation/markov-chain.html" class="internal-link" target="_self" rel="noopener nofollow">Markov chain</a> to slowly add random noise to data and then learns to reverse that process to generate the data from noise. The <a data-tooltip-position="top" aria-label="Latent Variable Models" data-href="Latent Variable Models" href="the-guide/machine-learning/generative-models/latent-variable-models.html" class="internal-link" target="_self" rel="noopener nofollow">latent space</a> therefore has the same dimension as the data.
<br><img alt="center" src="lib/media/pasted-image-20240801103838.png" style="width: 500px; max-width: 100%;"><br><img alt="center" src="lib/media/pasted-image-20240813190416.png"><br><br><br>We assume data that is distributed with some "real" data distribution . We define a process to increasingly corrupt the data artificially and in a controlled manner.<br>Forward Process 
Defines a <a data-href="Markov Chain" href="the-guide/computational-statistics/data-assimilation/markov-chain.html" class="internal-link" target="_self" rel="noopener nofollow">Markov Chain</a>, i.e. the <a data-tooltip-position="top" aria-label="Probability Density Function" data-href="Probability Density Function" href="the-guide/mathematics/probability-theory/probability-density-function.html" class="internal-link" target="_self" rel="noopener nofollow">pdf</a> of a sample at timestep  given the sample at the last timestep . In applications, we add small amount of <a data-tooltip-position="top" aria-label="Gaussian Distribution" data-href="Gaussian Distribution" href="the-guide/mathematics/probability-theory/gaussian-distribution.html" class="internal-link" target="_self" rel="noopener nofollow">Gaussian</a> noise gradually where  is the diffusion rate. The sample thereby converges to .
Any trajectory probability can be computed via However, the fact that we use purely Gaussians enables the <a data-href="The Reparametrization Trick" href="the-guide/mathematics/probability-theory/the-reparametrization-trick.html" class="internal-link" target="_self" rel="noopener nofollow">The Reparametrization Trick</a>, which makes it possible to sample  at any time step  in closed form. For simpler formulas, we employ the notationWith this, we can recursively insert the reparametrized formula to obtainAltogether, we obtain the closed form, single-shot sampling procedure defined byUsually, we choose larger variances the noisier the data gets , which results in 
<br><img alt="center" src="lib/media/pasted-image-20240813183908.png"><br>Reverse Diffusion Process 
If we can learn how to invert the above process, we can generate samples from the latent space by de-noising them. For that, we use a <a data-tooltip-position="top" aria-label="Artificial Neural Network" data-href="Artificial Neural Network" href="the-guide/machine-learning/deep-learning/artificial-neural-network.html" class="internal-link" target="_self" rel="noopener nofollow">neural network</a> to encode giving the trajectory probability 
<br><br><br>As usual, the objective is formalized as a <a data-tooltip-position="top" aria-label="Maximum Likelihood Estimator" data-href="Maximum Likelihood Estimator" href="the-guide/mathematics/statistics/maximum-likelihood-estimator.html" class="internal-link" target="_self" rel="noopener nofollow">maximum</a> <a data-tooltip-position="top" aria-label="Likelihood Function" data-href="Likelihood Function" href="the-guide/mathematics/statistics/likelihood-function.html" class="internal-link" target="_self" rel="noopener nofollow">likelihood</a> approach, where we want the model to produce data with a high probability. Therefore, we minimize the negative log-likelihood As in other <a data-tooltip-position="top" aria-label="Generative Models" data-href="Generative Models" href="the-guide/machine-learning/generative-models/generative-models.html" class="internal-link" target="_self" rel="noopener nofollow">generative</a> frameworks like the <a data-tooltip-position="top" aria-label="(Variational) Auto Encoder" data-href="(Variational) Auto Encoder" href="the-guide/machine-learning/generative-models/(variational)-auto-encoder.html" class="internal-link" target="_self" rel="noopener nofollow">VAE</a> this is intractable, so we use the <a data-tooltip-position="top" aria-label="Evidence Lower Bound" data-href="Evidence Lower Bound" href="the-guide/mathematics/probability-theory/evidence-lower-bound.html" class="internal-link" target="_self" rel="noopener nofollow">ELBO</a> <br>Intuition
The probability of a specific  is high, if even for unlikely "noising paths" we are able to generate a de-noising path with a high probability. Mathematically, this yields very small ratios in the expectation, of which the negative logarithm is a high number.
Inversely, if even for highly likely noising-paths (e.g. only very small noise at every step), we are not able to create a high-probability path, the output of  is unlikely.
<br>This objective can be rewritten using the trajectory probabilities to obtain In , we simply pulled apart the product, while in  we extended the condition artificially (<a data-tooltip-position="top" aria-label="Markov Chain" data-href="Markov Chain" href="the-guide/computational-statistics/data-assimilation/markov-chain.html" class="internal-link" target="_self" rel="noopener nofollow">Makrov property</a>) and Bayes Theorem showedFor , we pulled the second sum inside the logarithm, such that all but one log-fraction cancelled out. Enumerator and denominator can then be split using log-rules and redistributed to reach the final form. The term in red does not depend on any model parameters (only the conditional distribution is parameterized, see trajectory probability for ).<br>This means we can expressin closed form based on <br>In this objective, we only need the reverse conditional probability, which turns out to be tractable using <a data-href="Bayes Theorem" href="the-guide/mathematics/statistics/bayes-theorem.html" class="internal-link" target="_self" rel="noopener nofollow">Bayes Theorem</a>In , we absorbed all constants and then use the ideas from equating coefficients (see <a data-tooltip-position="top" aria-label="Gaussian Distribution > Closed-Form Solutions" data-href="Gaussian Distribution#Closed-Form Solutions" href="the-guide/mathematics/probability-theory/gaussian-distribution.html#Closed-Form_Solutions" class="internal-link" target="_self" rel="noopener nofollow">derivation</a>) to absorb everything we don't need into the constant in . With this, we arrive at the closed-form expressionsThese can already be used to train a model by using the posterior to obtain a mean based on samples and then train on the mean squared error of the means. However, in applications we reparameterize again to instead predict the noise.<br><br>As in the beginning, we can represent . At training time,  is available by the noising process, which is why we can also parameterize the noise. This yields the alternative formulationsRewriting the mean squared error with these formulations yields <br>Loss Function - Denoising Diffusion Probabilistic Model (DDPM)
Empirically, it was shown that leaving out the weighting term can actually improve training, yielding the objective
<br><br><br>Some general approaches to extend the vanilla DDPM algorithm are ...<br>
<br>
Sophisticated Noise Schedulers - several strategies exists, all share near-linear dropoff in the middle of the training and subtle changes towards the end and beginning

<br>
<a data-tooltip-position="top" aria-label="Covariance and Variance" data-href="Covariance and Variance" href="the-guide/mathematics/statistics/covariance-and-variance.html" class="internal-link" target="_self" rel="noopener nofollow">Variance</a> Parametrization - in the above derivations of DDPM, the reverse process variance  was fixed after choosing a variance schedule. Several works explore the idea of learning a variance. This approach has to be used with care, as even only diagonal variances have shown to de-stabilize training. A rather simple, but effective approach extends the loss by where  only guides the learing of an averaged where  is the learnable parameter. Even in this context, training can be unstable. 

<br>
Strided Sampling - methods that use a subset of timesteps for de-noising in order to speed up inference.

<br>
Combine with GAN

<br>
Consistency Models

<br>
Advanced Architectures, such as <a data-href="U-Net" href="the-guide/machine-learning/generative-models/u-net.html" class="internal-link" target="_self" rel="noopener nofollow">U-Net</a> or <a data-href="Transformer" href="the-guide/machine-learning/large-language-models/transformer.html" class="internal-link" target="_self" rel="noopener nofollow">Transformer</a>

<br>
Latent Diffusion Models - Perform noising and de-noising in a compressed latent space, e.g. using encoder, decoder architectures.

<br>
Conditional Generation 

<br>Classifier Guided Diffusion - gradient of a pre-trained (on diffused images) classifier  
<br>Classifier-Free Guidance - single neural network for conditional and unconditional model


<br><br>
<br>Some connection to <a data-href="Stochastic Gradient Langevin Dynamics" href="the-guide/computational-statistics/stochastic-gradient-langevin-dynamics.html" class="internal-link" target="_self" rel="noopener nofollow">Stochastic Gradient Langevin Dynamics</a>
]]></description><link>the-guide/machine-learning/generative-models/diffusion-models.html</link><guid isPermaLink="false">The Guide/Machine Learning/Generative Models/Diffusion Models.md</guid><pubDate>Tue, 25 Feb 2025 11:18:08 GMT</pubDate><enclosure url="lib/media/pasted-image-20240801103838.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src=&quot;lib/media/pasted-image-20240801103838.png&quot;&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[Distributed Generative Models]]></title><description><![CDATA[ 
 <br>
<br>
Motivation

<br>Data Availability
<br>(International) Cooperation
<br>Combine heterogeneous datasets from multiple sources without having to collect it


<br>
Problems<br>
- Data from different clients concerning the same setting may not be identically distributed, violates i.i.d. assumption that underlies most derivations<br>
- Leads to Client drift - clients or the whole model can perfrom significantly worse<br>
<img alt="center" src="lib/media/pasted-image-20240814183317.png" style="width: 300px; max-width: 100%;">

<br><br><br>All approaches we consider can be categorized based on how clients communicate with each other and which data is shared. For data-sharing, general strategies are <br>
<br>Shared Parameters- each client goes through full training and the locally optimized parameters are then shared. There are even architectures, in which only a subset of layers is shared, e.g. only the first layers of the decoder.
<br>Shared Gradients - each client computes a loss locally, but the backwards pass is done based on gradient information from all clients combined
<br>Shared Features - each client computes the forward pass locally, then the loss is computed based on outputs of all clients combined
<br><img alt="center" src="lib/media/pasted-image-20240814183551.png"><br>
<br>Local Learning
<br>Central Learning - All client send their data to a central data center, where it is combined and one global model is trained, which can afterwards be accessed by all clients.
<br>
Federated Learning
Concept in which different clients with distinct and non-shareable datasets update parameters locally and then send the model back to a data-center / other clients. 

<br>Centralized Federated Learning - generally, a communication round consists of  


<br>Server selects a client,
<br>Server sends the global model to selected client 
<br>The model is trained locally using the local data only 
<br>Client sends back the trained model to the server
<br>Server combines with the return of other clients (e.g. averaged  in Federate Average) or simply send to the next client, resulting in a cycle


<br>Decentralized Federated Learning - Alternatively, the model can be send directly to other clients. The way in which the model is send around can be realized through various topologies. 

<br>Ring Topologies - each client sends the updates to its neighbor, which can be done in rounds or simultaneously for each disjoint pair.
<br>Segmented Gossip Learning - ??





<br>Swarm Learning - 
<br><br>
<br>FedGAN - <a data-tooltip-position="top" aria-label="Generative Adversarial Network (GAN)" data-href="Generative Adversarial Network (GAN)" href="the-guide/machine-learning/generative-models/generative-adversarial-network-(gan).html" class="internal-link" target="_self" rel="noopener nofollow">GANs</a> are trained locally on client level, parameters are shared with a central server - Descriminator and Generator or only one ?
<br>Multi-Device GAN - category of methods using parallel instances, e.g.

<br>Multiple Descriminators - a central server creates multiple images based on combined feedback of several descriminators, who then receive all images and are trained seperately
<br>Both Generators and Descriminators are trained locally, but resulting parameters are regularly combined at server


<br>Federated Learning VAE - Encoder and Decoder are sent, trained and sent back with various clients by a server
<br>...
<br>Mix-Generator Module - central server has only part of the layers, e.g. of a decoder. At some point in the architecture, activations are sent to different clients, where each has local parameters for the remaining layers.
<br>Bottle GAN - ??
<br><br><br>Threats to distributed learning systems can occur at the server or client level. <br><img alt="center" src="lib/media/pasted-image-20240814193017.png"><br>
<br>Model Poisoning 

<br>Targeted - let model learn backdoor task that can be exploited later without corrupting the performance of the model on its main tasks. This can be done by e.g. bringing in a trigger label that is not in the original dataset
<br>Untargeted - prevent model from convergence / performing good on its downstream tasks


<br>Model Replacement 

<br>Parameters of the model are replaced or corrupted directly


<br>GANs for Poisoning 

<br>Use GAN to purposefully generate samples in order to corrupt global data distribution


<br>On the other hand, <a data-href="Generative Models" href="the-guide/machine-learning/generative-models/generative-models.html" class="internal-link" target="_self" rel="noopener nofollow">Generative Models</a> can also be employed to defend systems against such attacks.<br>
<br>VAE Detection of Malicious Clients 

<br>Use latent space of a VAE to exclude suspicious data, e.g. that drives distribution in a different direction than all other clients


<br>...
<br>Another matter is privacy - ]]></description><link>the-guide/machine-learning/generative-models/distributed-generative-models.html</link><guid isPermaLink="false">The Guide/Machine Learning/Generative Models/Distributed Generative Models.md</guid><pubDate>Sat, 05 Oct 2024 19:01:37 GMT</pubDate><enclosure url="lib/media/pasted-image-20240814183317.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src=&quot;lib/media/pasted-image-20240814183317.png&quot;&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[Expectation Maximization Algorithm]]></title><description><![CDATA[ 
 <br>In a Nutshell
Iterative algorithm that maximizes the <a data-tooltip-position="top" aria-label="Likelihood Function" data-href="Likelihood Function" href="the-guide/mathematics/statistics/likelihood-function.html" class="internal-link" target="_self" rel="noopener nofollow">log-likelihood</a> of data / observations   over the parameters of a chosen <a data-tooltip-position="top" aria-label="Latent Variable Models" data-href="Latent Variable Models" href="the-guide/machine-learning/generative-models/latent-variable-models.html" class="internal-link" target="_self" rel="noopener nofollow">statistical model with latent variables</a>. Based on the fact that the logarithm is a <a data-tooltip-position="top" aria-label="Convexity" data-href="Convexity" href="the-guide/mathematics/optimization/convex-optimization-lecture/convexity.html" class="internal-link" target="_self" rel="noopener nofollow">strictly convex</a> <a data-tooltip-position="top" aria-label="Mathematical Relations" data-href="Mathematical Relations" href="the-guide/mathematics/general-stuff/mathematical-relations.html" class="internal-link" target="_self" rel="noopener nofollow">function</a> and exploits <a data-href="Jensen's Inequality" href="the-guide/information-theory/information-theory-1/information/jensen's-inequality.html" class="internal-link" target="_self" rel="noopener nofollow">Jensen's Inequality</a>.
<br><br><br>We want to fit a parameterized model  to <a data-tooltip-position="top" aria-label="Statistical Independence" data-href="Statistical Independence" href="the-guide/mathematics/statistics/statistical-independence.html" class="internal-link" target="_self" rel="noopener nofollow">i.i.d.</a> observations , i.e. maximize the likelihood This objective is based on the following theorem.<br>Theorem
Minimizing the <a data-tooltip-position="top" aria-label="Kullback-Leibler Divergence" data-href="Kullback-Leibler Divergence" href="the-guide/information-theory/information-theory-1/information/kullback-leibler-divergence.html" class="internal-link" target="_self" rel="noopener nofollow">KL-divergence</a> between two distributions, here a parameterized  and a "true"  is equivalent to maximizing the <a data-tooltip-position="top" aria-label="Likelihood Function" data-href="Likelihood Function" href="the-guide/mathematics/statistics/likelihood-function.html" class="internal-link" target="_self" rel="noopener nofollow">log-likelihood</a>, because
<br>This means that we can maximize the likelihood on the left in order to find a good approximation . This approximation can be computed explicitly using calculus of variations and a parameterized family of distributions, but these are too simplistic in many cases. <br><br><br>To overcome this, we employ implicitly parameterized distributions, which samples a <a data-tooltip-position="top" aria-label="Latent Variable Models" data-href="Latent Variable Models" href="the-guide/machine-learning/generative-models/latent-variable-models.html" class="internal-link" target="_self" rel="noopener nofollow">latent variable</a>  from a pre-defined simple distribution  and then converts the sample using complicated functions, e.g. <a data-tooltip-position="top" aria-label="Artificial Neural Network" data-href="Artificial Neural Network" href="the-guide/machine-learning/deep-learning/artificial-neural-network.html" class="internal-link" target="_self" rel="noopener nofollow">neural networks</a>, into parameters of other simple distributions . <br>This results in a family of <a data-tooltip-position="top" aria-label="Probability Distribution" data-href="Probability Distribution" href="the-guide/mathematics/probability-theory/probability-distribution.html" class="internal-link" target="_self" rel="noopener nofollow">joint distributions</a>  over , from which we can easily sample by first sampling , put it into the function and them sample from the resulting distribution. The parameters  in this case are the combined parameters of the latent distribution  and of the function mapping .<br>To perform the initial maximization with this new model, we'd have to compute the term marginalizing over the latent space for every sample, which is usually impossible in closed-form and infeasible in practice. With the usual <a data-tooltip-position="top" aria-label="Likelihood Function" data-href="Likelihood Function" href="the-guide/mathematics/statistics/likelihood-function.html" class="internal-link" target="_self" rel="noopener nofollow">log-trick</a>, the objective at this point reads <br><br><br>Maximizing this log-likelihood is impossible, since we do not have access to the latent variables. However, the EM algorithm can deal with this problem by first finding an <a data-tooltip-position="top" aria-label="Estimator" data-href="Estimator" href="the-guide/mathematics/statistics/estimator.html" class="internal-link" target="_self" rel="noopener nofollow">estimate</a> for the likelihood function and then maximizing the whole term. <br>Mathematically, we first realize that the maximization problem for a single sample can also be stated as maximizing the differencewhere  denotes the old estimate for the parameters.<br>Using <a data-href="Jensen's Inequality" href="the-guide/information-theory/information-theory-1/information/jensen's-inequality.html" class="internal-link" target="_self" rel="noopener nofollow">Jensen's Inequality</a> and logarithm rules, we can reformulate the above difference term toWith this, we define for which the inequality holds. Since  is upper bounded by the log-likelihood and both <a data-tooltip-position="top" aria-label="Mathematical Relations" data-href="Mathematical Relations" href="the-guide/mathematics/general-stuff/mathematical-relations.html" class="internal-link" target="_self" rel="noopener nofollow">functions</a> coincide at the current iterate (), the new iterated that increase  will also increase . <img alt="center" src="lib/media/pasted-image-20240610184911.png" style="width: 350px; max-width: 100%;"><br>
We can reformulate the objective further by considering the maximization. First of all, we can drop terms that are constant w.r.t. , i.e.  and ignoring the denominator. Additionally, we reformulate<br>Objective
The EM algorithm calls for maximizing  over , or formallywhich yields a two-step algorithm.
<br>Algorithm

<br>E-Step - <a data-tooltip-position="top" aria-label="Estimator" data-href="Estimator" href="the-guide/mathematics/statistics/estimator.html" class="internal-link" target="_self" rel="noopener nofollow">estimate</a> conditional <a data-tooltip-position="top" aria-label="Expectations" data-href="Expectations" href="the-guide/mathematics/statistics/expectations.html" class="internal-link" target="_self" rel="noopener nofollow">expectation</a> 

<br>We have  data  and current parameters , so we can compute the expectation 


<br>M-Step - maximize this expression as above to obtain updated parameters 

<br>Additional Information - Relationship with K-Means
Can be seen as a generalization of K-means clustering. In the former, datapoints are assigned "hard", each can only belong to one, while EM assigns probabilities of beloning to each cluster, which are the hidden parameters (?).
<br><br><br>
<br><a rel="noopener nofollow" class="external-link" href="https://leimao.github.io/blog/Expectation-Maximization-Algorithm/#Expectation-Maximization-As-Maximization-Maximization-Procedure" target="_blank">https://leimao.github.io/blog/Expectation-Maximization-Algorithm/#Expectation-Maximization-As-Maximization-Maximization-Procedure</a>
<br><a rel="noopener nofollow" class="external-link" href="https://people.duke.edu/~ccc14/sta-663/EMAlgorithm.html" target="_blank">https://people.duke.edu/~ccc14/sta-663/EMAlgorithm.html</a>
]]></description><link>the-guide/machine-learning/generative-models/expectation-maximization-algorithm.html</link><guid isPermaLink="false">The Guide/Machine Learning/Generative Models/Expectation Maximization Algorithm.md</guid><pubDate>Sat, 05 Oct 2024 19:01:09 GMT</pubDate><enclosure url="lib/media/pasted-image-20240610184911.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src=&quot;lib/media/pasted-image-20240610184911.png&quot;&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[Feature Disentanglement]]></title><description><![CDATA[<a class="tag" href="?query=tag:Generative-Models-Course" style="background-color: rgb(4, 108, 116); color: white; font-weight: 700; border: none; border-radius: 1em; padding: 0.2em 0.5em;">#Generative-Models-Course</a> 
 <br><a data-tooltip-position="top" aria-label="- Machine Learning -" data-href="- Machine Learning -" href="the-guide/machine-learning/-machine-learning-.html" class="internal-link" target="_self" rel="noopener nofollow">Learning-based models</a> that compress the data into an abstract latent space representation, such as the <a data-href="(Variational) Auto Encoder" href="the-guide/machine-learning/generative-models/(variational)-auto-encoder.html" class="internal-link" target="_self" rel="noopener nofollow">(Variational) Auto Encoder</a> do so in an unorganized way that is in general not traceable for the user. However, in content generation or knowledge transfer (e.g. MR to CT segmentation), interpretable latent features are desirable.<br><img alt="center" src="lib/media/pasted-image-20240905155149.png" style="width: 350px; max-width: 100%;"><br>There is no commonly accepted definition for disentanglement, but ...<br>
<br>?
<br>?
<br>?<br>
In general, we distinguish between ...
<br>Multi-Dimensional Disentanglement

<br>We know that a single latent variable has effect on a group of values and we want to disentangle this into one latent variable each


<br>Supervised Disentanglement

<br>Provide disentangled features, mostly infeasible due do missing data


<br>Unsupervised Disentanglement

<br>Try to create as many factors of variation as possible without loosing information
<br>Often difficult to get interpretable features while also getting good results


<br><br><br>There exist methods to enforce disentanglement based on the <a data-tooltip-position="top" aria-label="Generative Models" data-href="Generative Models" href="the-guide/machine-learning/generative-models/generative-models.html" class="internal-link" target="_self" rel="noopener nofollow">generative models</a> covered in the <a href=".?query=tag:Generative-Models-Course" class="tag" target="_blank" rel="noopener nofollow">#Generative-Models-Course</a> lecture.<br>
<br>Disentanglement based on <a data-href="(Variational) Auto Encoder" href="the-guide/machine-learning/generative-models/(variational)-auto-encoder.html" class="internal-link" target="_self" rel="noopener nofollow">(Variational) Auto Encoder</a>

<br>The bottleneck structure means there is some sort of disentanglement built-in, as this is usually of much lesser dimension than the input. However, the data in that latent space is rarely organized in an interpretable way. A simple approach is the -VAE which adds a weighting parameter to the loss via With this, the training focuses more on creating an isotropic multivariate <a data-tooltip-position="top" aria-label="Gaussian Distribution" data-href="Gaussian Distribution" href="the-guide/mathematics/probability-theory/gaussian-distribution.html" class="internal-link" target="_self" rel="noopener nofollow">Gaussian</a> in the latent space, which lowers correlation of features.


<br>Disentanglement based on <a data-href="Generative Adversarial Network (GAN)" href="the-guide/machine-learning/generative-models/generative-adversarial-network-(gan).html" class="internal-link" target="_self" rel="noopener nofollow">Generative Adversarial Network (GAN)</a>

<br>InfoGAN - Adds a regularizer to the conditional objective that allows the generator to lower the cost by increasing the <a data-tooltip-position="top" aria-label="Mutual Information" data-href="Mutual Information" href="the-guide/information-theory/information-theory-1/information/mutual-information.html" class="internal-link" target="_self" rel="noopener nofollow">mutual information</a> between ???
<br>Dr-GAN - Tries to enable pose-invariant face classification by removing the pose from the latent representation


<br><br><br>-VAE Metric<br>
<img alt="center" src="lib/media/pasted-image-20240905173106.png" style="width: 400px; max-width: 100%;"><br>
<br>Choose a latent dimension  from 
<br>Sample two <a data-tooltip-position="top" aria-label="Set" data-href="Set" href="the-guide/mathematics/general-stuff/set.html" class="internal-link" target="_self" rel="noopener nofollow">sets</a> of latent representations  based on , where only one component  of the latent space is fixed
<br>Generate images  based on  and  with the decoder and encode them again to obtain .
<br>Calculate the difference of the new latent representation 
<br>Use the average of these differences to train a classifier to predict  and use it as a disentanglement score
]]></description><link>the-guide/machine-learning/generative-models/feature-disentanglement.html</link><guid isPermaLink="false">The Guide/Machine Learning/Generative Models/Feature Disentanglement.md</guid><pubDate>Thu, 27 Feb 2025 12:04:59 GMT</pubDate><enclosure url="lib/media/pasted-image-20240905155149.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src=&quot;lib/media/pasted-image-20240905155149.png&quot;&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[Flow Matching]]></title><description><![CDATA[ 
 <br>In a Nutshell
More efficient approach for training <a data-tooltip-position="top" aria-label="Normalizing Flows" data-href="Normalizing Flows" href="the-guide/machine-learning/generative-models/normalizing-flows.html" class="internal-link" target="_self" rel="noopener nofollow">Continuous Normalizing Flows</a> by directly regressing a vector field instead of having to simulate it to obtain the likelihood and then train by <a data-tooltip-position="top" aria-label="Maximum Likelihood Estimator" data-href="Maximum Likelihood Estimator" href="the-guide/mathematics/statistics/maximum-likelihood-estimator.html" class="internal-link" target="_self" rel="noopener nofollow">maximum likelihood</a>.
<br><br><br>In the following, we assume to have data samples  from a distribution  with unknown <a data-tooltip-position="top" aria-label="Probability Density Function" data-href="Probability Density Function" href="the-guide/mathematics/probability-theory/probability-density-function.html" class="internal-link" target="_self" rel="noopener nofollow">density</a>.<br>Problems of CNF's
<a data-tooltip-position="top" aria-label="Normalizing Flows" data-href="Normalizing Flows" href="the-guide/machine-learning/generative-models/normalizing-flows.html" class="internal-link" target="_self" rel="noopener nofollow">CNF's</a> can be trained by minimizing the likelihoodwith unknown parametric distribution  that should be close to . This is done by integrating the backwards time-evolution of samples  and log-likelihood, both of which are functions of an underlying vector field . This yields very expressive models, but also leads to slow training due to expensive ODE simulation and bad scaling with high dimensions.
<br>Training with Velocities instead of Likelihood
In Flow Matching, we want to make the training more efficient by directly regressing the vector field  in the loss instead of backwards-integrating the resulting ODE. Mathematically, we want to reach the objective withwhich is obviously infeasible because we do not know the true vector field  and can only sample from .
<br><br><br>We first observe that the vector field  defines a probability path / bridge  interpolating  and . We know from the Continuity Equation that constructing  gives us the same information as constructing , as they are coupled by this differential equation.<br>However, we can only sample from  via the dataset , so in order to enable supervision, we have to find a marginal path that fulfills the boundary conditions  and .<br>The conditional path  needs to make sure that all initial mass is concentrated at , which can be achieved by simply pointing all vectors at that point while adjusting the scale to the time scale . This can be attained by simply using This conditional path also has to satisfy the Continuity Equation, this time with a vector field that is also conditioned on  :<br>Theorem
The conditional vector field can express the marginal vector field via 
<br>Additional Information
Intuition - We can imagine a probability cloud that is e.g. donut- or halfmoon-shaped around each target  to encode the probability of a point in space to be on a path to this target at time . We can construct a marginal vector field that e.g. points to the target from every point in space and then weight it according to the ratio 
<br>Proof
We start by inserting the marginal path into the left side of the initial Continuity Equation, yielding We can pull the time derivative inside the integral by the <a data-tooltip-position="top" aria-label="Integration Tricks and Theorems" data-href="Integration Tricks and Theorems" href="the-guide/mathematics/analysis-and-calculus/integration-tricks-and-theorems.html" class="internal-link" target="_self" rel="noopener nofollow">Leibniz rule</a> and insert the Continuity Equation for the conditional path to obtain We also used that  is just a factor and can be pulled inside the divergence. Applying the <a data-tooltip-position="top" aria-label="Integration Tricks and Theorems" data-href="Integration Tricks and Theorems" href="the-guide/mathematics/analysis-and-calculus/integration-tricks-and-theorems.html" class="internal-link" target="_self" rel="noopener nofollow">Leibniz rule</a> again, as the divergence only contains space derivatives, we can can pull out out of the integral and extend by : Since  does not depend on , we can pull the term out of the integral and reach the exact representation of  of the Theorem
<br>With this, we can replace the initial intractable loss with the new loss to obtain the new Conditional Flow Matching loss. <br>Theorem 
Under the assumption that  for all  and , the losses  and  are equal up to a constant independent of . Hence, their gradients coincide.
<br>However, we now put a preference on the vector field that can be learned via the choice of .<br>Additional Information
For example, we can construct a conditional flow  using two functions that only depend on time, e.g.where  and . Therefore, if  is distributed according to , we have that at any intermediate step Now  is the vector field underlying this evolution. We can incorporate this into the loss by instead of sampling over , we sample from  and push the sample forward via  to obtain  (essentially <a data-tooltip-position="top" aria-label="Law of the unconscious Statistician" data-href="Law of the unconscious Statistician" href="the-guide/mathematics/statistics/law-of-the-unconscious-statistician.html" class="internal-link" target="_self" rel="noopener nofollow">LOTUS</a>.) The loss can be adapted to <img alt="center" src="lib/media/pasted-image-20240514185549.png" style="width: 350px; max-width: 100%;">
<br>( ... )<br>Problems
We still need a tractable , e.g. by choosing a <a data-tooltip-position="top" aria-label="Gaussian Distribution" data-href="Gaussian Distribution" href="the-guide/mathematics/probability-theory/gaussian-distribution.html" class="internal-link" target="_self" rel="noopener nofollow">Gaussian</a> path with linearly interpolated <a data-tooltip-position="top" aria-label="Expectations" data-href="Expectations" href="the-guide/mathematics/statistics/expectations.html" class="internal-link" target="_self" rel="noopener nofollow">mean</a>  and <a data-tooltip-position="top" aria-label="Covariance and Variance" data-href="Covariance and Variance" href="the-guide/mathematics/statistics/covariance-and-variance.html" class="internal-link" target="_self" rel="noopener nofollow">standard deviation</a>  with boundary conditions 
Additionally, two key issues arise from conditional paths crossing each other, namely

<br>Marginal paths that are hard to integrate at inference
<br>Slow training convergence due to high loss <a class="internal-link" data-href="Covariance and Variance.md" href="the-guide/mathematics/statistics/covariance-and-variance.html" target="_self" rel="noopener nofollow"></a>sible  for noisy )

Marginal paths do not cross (uniqueness of <a class="internal-link" data-href="Covariance and Variance.md" href="the-guide/mathematics/statistics/covariance-and-variance.html" target="_self" rel="noopener nofollow"></a> trying to align  with the vector fields that are not possible under the marginal field !
<br><br><br>The above is refered to as one-sided conditioning, because we construct the probability path by marginalizing over . More general, we can consider conditioning over <a data-tooltip-position="top" aria-label="Latent Variable Models" data-href="Latent Variable Models" href="the-guide/machine-learning/generative-models/latent-variable-models.html" class="internal-link" target="_self" rel="noopener nofollow">latent</a> variables , yielding the loss Several authors suggested using , resulting in two-sided conditioning and the marginal path The objective adapts accordingly to <br>The easiest approach is to consider independent samples  and a linear interpolation, which allows for non-Gaussian reference distributions , if we can impose the boundary conditions  and .  <br>We can, however, also consider correlated pairs with a <a data-tooltip-position="top" aria-label="Probability Distribution" data-href="Probability Distribution" href="the-guide/mathematics/probability-theory/probability-distribution.html" class="internal-link" target="_self" rel="noopener nofollow">joint density</a> . One proposed way to do this is to use an <a data-tooltip-position="top" aria-label="Optimal Transport" data-href="Optimal Transport" href="the-guide/mathematics/optimal-transport/optimal-transport.html" class="internal-link" target="_self" rel="noopener nofollow">optimal transport</a> coupling  between the samples of a batch of data. The plan  can be computed via the <a data-tooltip-position="top" aria-label="Sinkhorn-Knopp Algorithm" data-href="Sinkhorn-Knopp Algorithm" href="the-guide/mathematics/optimal-transport/sinkhorn-knopp-algorithm.html" class="internal-link" target="_self" rel="noopener nofollow">Sinkhorn-Knopp algorithm</a> and used to generate joint training data, either via expectations or subsequent sampling. This significantly reduces the problem of crossing paths and enables additional priors via the <a data-tooltip-position="top" aria-label="Metric Space and Completeness" data-href="Metric Space and Completeness" href="the-guide/mathematics/general-stuff/metric-space-and-completeness.html" class="internal-link" target="_self" rel="noopener nofollow">metric</a> of the <a data-tooltip-position="top" aria-label="Optimal Transport" data-href="Optimal Transport" href="the-guide/mathematics/optimal-transport/optimal-transport.html" class="internal-link" target="_self" rel="noopener nofollow">OT</a> formulation.<br><br><br>
<br>LOTUS ?
<br>
<br><br>
<br><a rel="noopener nofollow" class="external-link" href="https://mlg.eng.cam.ac.uk/blog/2024/01/20/flow-matching.html" target="_blank">https://mlg.eng.cam.ac.uk/blog/2024/01/20/flow-matching.html</a>
]]></description><link>the-guide/machine-learning/generative-models/flow-matching.html</link><guid isPermaLink="false">The Guide/Machine Learning/Generative Models/Flow Matching.md</guid><pubDate>Tue, 08 Apr 2025 15:02:00 GMT</pubDate><enclosure url="lib/media/pasted-image-20240514185549.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src=&quot;lib/media/pasted-image-20240514185549.png&quot;&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[Generative Adversarial Network (GAN)]]></title><description><![CDATA[ 
 <br>In a Nutshell
Class of machine learning frameworks for enabling unsupervised learning, prominent for generative AI. Two <a data-tooltip-position="top" aria-label="Artificial Neural Network" data-href="Artificial Neural Network" href="the-guide/machine-learning/deep-learning/artificial-neural-network.html" class="internal-link" target="_self" rel="noopener nofollow">neural networks</a> contest each other, where one agents gain is another agents loss.
<br><br><br>There are 2 players: generator and discriminator. <br><img alt="center" src="lib/media/pasted-image-20240515104914.png" style="width: 400px; max-width: 100%;"><br>Mathematical Framework
A GAN game is deefined on a <a data-tooltip-position="top" aria-label="Probability Space" data-href="Probability Space" href="the-guide/mathematics/probability-theory/probability-space.html" class="internal-link" target="_self" rel="noopener nofollow">probability space</a> .

<br>The generators strategy <a data-tooltip-position="top" aria-label="Set" data-href="Set" href="the-guide/mathematics/general-stuff/set.html" class="internal-link" target="_self" rel="noopener nofollow">set</a> is the set  of all <a data-tooltip-position="top" aria-label="Measure" data-href="Measure" href="the-guide/mathematics/measure-theory/measure.html" class="internal-link" target="_self" rel="noopener nofollow">probability measures</a>  on 
<br>The discriminators strategy set is the set of all <a data-tooltip-position="top" aria-label="Markov Kernel" data-href="Markov Kernel" href="the-guide/computational-statistics/data-assimilation/markov-kernel.html" class="internal-link" target="_self" rel="noopener nofollow">Markov kernels</a> <br>
The objective is a zero-sum game, the generator aims to minimize while the discriminator tries to maximize.

<br>In practice, the discriminator is a neural network, realizing a deterministic <a data-tooltip-position="top" aria-label="Mathematical Relations" data-href="Mathematical Relations" href="the-guide/mathematics/general-stuff/mathematical-relations.html" class="internal-link" target="_self" rel="noopener nofollow">function</a> . The generator is usually implemented as a <a data-tooltip-position="top" aria-label="Push-Forward Measure" data-href="Push-Forward Measure" href="the-guide/mathematics/measure-theory/push-forward-measure.html" class="internal-link" target="_self" rel="noopener nofollow">pushforward</a> , where we start with an easy <a data-tooltip-position="top" aria-label="Latent Variable Models" data-href="Latent Variable Models" href="the-guide/machine-learning/generative-models/latent-variable-models.html" class="internal-link" target="_self" rel="noopener nofollow">latent</a> <a data-tooltip-position="top" aria-label="Random Variable" data-href="Random Variable" href="the-guide/mathematics/probability-theory/random-variable.html" class="internal-link" target="_self" rel="noopener nofollow">random variable</a> , e.g. distributed with a <a data-tooltip-position="top" aria-label="Gaussian Distribution" data-href="Gaussian Distribution" href="the-guide/mathematics/probability-theory/gaussian-distribution.html" class="internal-link" target="_self" rel="noopener nofollow">Gaussian</a> and then define a function . The distribution  is the distribution of . The objective changes to <br>
<br>Training

<br>Known dataset as initial training data for Discriminator D

<br>Train until acceptable output accuracy


<br>Let generator G generate images, discriminator D decides real or fake
<br>Both are trained using <a data-tooltip-position="top" aria-label="Forward and Backward Propagation" data-href="Forward and Backward Propagation" href="the-guide/machine-learning/deep-learning/forward-and-backward-propagation.html" class="internal-link" target="_self" rel="noopener nofollow">backpropagation</a>


<br>Warning
GAN's generally suffer from 

<br>Numerical Instability
<br>Sensitivity to evaluation metric

<br><br><br>Introduces a condition  to enable supervision of the generation process. The condition is fed into the generator and the discriminator during both training episodes. The objective changes to This way, the generator can learn multiple classes of images etc. in the same unsupervised manner.<br><img alt="center" src="lib/media/pasted-image-20240515104934.png" style="width: 400px; max-width: 100%;"><br><br><br>Extension to enable translation between to domains  and  by introducing a second pair of players, resulting in two teams of two. Each generator learns to generate e.g. an image on the domain  given a realization of  v.v., which leads to a cycle of translation during which all participants are trained based on discriminator feedback. The new loss function is with the cycle loss <br><img alt="center" src="lib/media/pasted-image-20240515105009.png" style="width: 420px; max-width: 100%;">]]></description><link>the-guide/machine-learning/generative-models/generative-adversarial-network-(gan).html</link><guid isPermaLink="false">The Guide/Machine Learning/Generative Models/Generative Adversarial Network (GAN).md</guid><pubDate>Sat, 05 Oct 2024 19:01:09 GMT</pubDate><enclosure url="lib/media/pasted-image-20240515104914.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src=&quot;lib/media/pasted-image-20240515104914.png&quot;&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[Generative Models]]></title><description><![CDATA[ 
 <br>In a Nutshell
Class of statistical models that can generate new data instances. 
<br><br>Based on the idea that we can find an underlying <a data-tooltip-position="top" aria-label="Probability Distribution" data-href="Probability Distribution" href="the-guide/mathematics/probability-theory/probability-distribution.html" class="internal-link" target="_self" rel="noopener nofollow">probability distribution</a>Types of generative models are ...<br>
<br><a data-href="Bayesian Neural Networks" href="the-guide/machine-learning/generative-models/bayesian-neural-networks.html" class="internal-link" target="_self" rel="noopener nofollow">Bayesian Neural Networks</a>

<br>Tend to underfit, sensitive to initialization


<br><a data-tooltip-position="top" aria-label="(Variational) Auto Encoder" data-href="(Variational) Auto Encoder" href="the-guide/machine-learning/generative-models/(variational)-auto-encoder.html" class="internal-link" target="_self" rel="noopener nofollow">Variational Auto Encoders</a>

<br>Capture distribution reasonably well, but struggle to generate detailed samples
<br>Trained by maximizing <a data-tooltip-position="top" aria-label="Evidence Lower Bound" data-href="Evidence Lower Bound" href="the-guide/mathematics/probability-theory/evidence-lower-bound.html" class="internal-link" target="_self" rel="noopener nofollow">ELBO</a>


<br><a data-tooltip-position="top" aria-label="Generative Adversarial Network (GAN)" data-href="Generative Adversarial Network (GAN)" href="the-guide/machine-learning/generative-models/generative-adversarial-network-(gan).html" class="internal-link" target="_self" rel="noopener nofollow">GANs</a>

<br>Good sample quality, but struggle to capture full distribution and are costly
<br>Trained by minimax of the classification error loss


<br><a data-tooltip-position="top" aria-label="Normalizing Flows" data-href="Normalizing Flows" href="the-guide/machine-learning/generative-models/normalizing-flows.html" class="internal-link" target="_self" rel="noopener nofollow">Flow Based Generative Models</a>

<br>Deterministic mapping of complicated distribution to simple one
<br>Trained using <a data-tooltip-position="top" aria-label="Likelihood Function" data-href="Likelihood Function" href="the-guide/mathematics/statistics/likelihood-function.html" class="internal-link" target="_self" rel="noopener nofollow">negative log-likelihood</a>


<br>Energy Based Models

<br>...


<br><a data-href="Diffusion Models" href="the-guide/machine-learning/generative-models/diffusion-models.html" class="internal-link" target="_self" rel="noopener nofollow">Diffusion Models</a>

<br>Define a <a data-href="Markov Chain" href="the-guide/computational-statistics/data-assimilation/markov-chain.html" class="internal-link" target="_self" rel="noopener nofollow">Markov Chain</a> to slowly add noise to samples until they converge towards an easy distribution (mostly Gaussian). During training, we learn to invert this process
<br>Trained based on <a data-tooltip-position="top" aria-label="Evidence Lower Bound" data-href="Evidence Lower Bound" href="the-guide/mathematics/probability-theory/evidence-lower-bound.html" class="internal-link" target="_self" rel="noopener nofollow">ELBO</a> formulation for mean or noise vector


<br><img alt="center" src="lib/media/pasted-image-20240801103818.png" style="width: 500px; max-width: 100%;"><br><br>Challenges

<br>Causality - instances that were not present in training data can be unpredictable, e.g. generating images with the prompt "A horse riding an astronaut on the moon".
<br>Additional Control Input - conditions are often imprinted using <a data-tooltip-position="top" aria-label="Large Language Models" data-href="Large Language Models" href="the-guide/machine-learning/large-language-models/large-language-models.html" class="internal-link" target="_self" rel="noopener nofollow">LLM's</a>, wich themselves are not fully understood, making fine-grained control with guarantees impossible
<br>Privacy - a famous work by Google showed that it is possible to extract training data from e.g. <a data-href="Diffusion Models" href="the-guide/machine-learning/generative-models/diffusion-models.html" class="internal-link" target="_self" rel="noopener nofollow">Diffusion Models</a>, raising new questions about pricavy.
<br>Data Corruption - the data itself is always flawed, having implications on every aspect of downstream-tasks
<br>Hardware Requirements

<br><img alt="center" src="lib/media/pasted-image-20240906121425.png" style="width: 350px; max-width: 100%;">]]></description><link>the-guide/machine-learning/generative-models/generative-models.html</link><guid isPermaLink="false">The Guide/Machine Learning/Generative Models/Generative Models.md</guid><pubDate>Sat, 05 Oct 2024 19:01:09 GMT</pubDate><enclosure url="lib/media/pasted-image-20240801103818.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src=&quot;lib/media/pasted-image-20240801103818.png&quot;&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[Image Segmentation]]></title><description><![CDATA[ 
 <br>In a Nutshell
Application in digital image processing and computer vision that tries to partition a digital image into multiple segments / regions to simplify and/or change the representation, enabling more meaningful / easier analysis.
<br><br>...]]></description><link>the-guide/machine-learning/generative-models/image-segmentation.html</link><guid isPermaLink="false">The Guide/Machine Learning/Generative Models/Image Segmentation.md</guid><pubDate>Sat, 05 Oct 2024 19:01:09 GMT</pubDate></item><item><title><![CDATA[Latent Variable Models]]></title><description><![CDATA[ 
 <br>In a Nutshell
Framework in <a data-tooltip-position="top" aria-label="Statistics Overview" data-href="Statistics Overview" href="the-guide/mathematics/statistics/statistics-overview.html" class="internal-link" target="_self" rel="noopener nofollow">statistics</a> for understanding complex data <a data-tooltip-position="top" aria-label="Probability Distribution" data-href="Probability Distribution" href="the-guide/mathematics/probability-theory/probability-distribution.html" class="internal-link" target="_self" rel="noopener nofollow">distributions</a> by introducing unobserved / latent <a data-tooltip-position="top" aria-label="Random Variable" data-href="Random Variable" href="the-guide/mathematics/probability-theory/random-variable.html" class="internal-link" target="_self" rel="noopener nofollow">variables</a> to capture structure that cannot be observed directly.
<br><br>The well-understood <a data-tooltip-position="top" aria-label="Set" data-href="Set" href="the-guide/mathematics/general-stuff/set.html" class="internal-link" target="_self" rel="noopener nofollow">set</a> of fundamental <a data-tooltip-position="top" aria-label="Probability Distribution" data-href="Probability Distribution" href="the-guide/mathematics/probability-theory/probability-distribution.html" class="internal-link" target="_self" rel="noopener nofollow">probability distributions</a>, most notably <a data-tooltip-position="top" aria-label="Gaussian Distribution" data-href="Gaussian Distribution" href="the-guide/mathematics/probability-theory/gaussian-distribution.html" class="internal-link" target="_self" rel="noopener nofollow">Gaussians</a> is often not sufficient to capture complex data distributions in real world applications, particularly for processes with outliers or higher order structure. In addition to that, a <a data-tooltip-position="top" aria-label="Gaussian Distribution" data-href="Gaussian Distribution" href="the-guide/mathematics/probability-theory/gaussian-distribution.html" class="internal-link" target="_self" rel="noopener nofollow">multivariate Gaussian</a> of dimension  requires  to specify its <a data-tooltip-position="top" aria-label="Covariance and Variance" data-href="Covariance and Variance" href="the-guide/mathematics/statistics/covariance-and-variance.html" class="internal-link" target="_self" rel="noopener nofollow">covariance</a>.<br>Latent variables are <a data-tooltip-position="top" aria-label="Random Variable" data-href="Random Variable" href="the-guide/mathematics/probability-theory/random-variable.html" class="internal-link" target="_self" rel="noopener nofollow">random variables</a> that cannot be directly observed (in contrast to observable random variables), but are inferred from observed data. The goal is to capture abstract and fundamental features of the data. Mathematically, the resulting probabilistic model is where  is a combined parameter vector. <br>
<br> is the prior distribution in the latent space
<br> is the <a data-tooltip-position="top" aria-label="Bayes Theorem" data-href="Bayes Theorem" href="the-guide/mathematics/statistics/bayes-theorem.html" class="internal-link" target="_self" rel="noopener nofollow">likelihood</a> of a datapoint given a realization in the latent space<br>
To draw a sample from this distribution, we first draw a  from the parameterized prior and then draw  based on the likelihood (which can itself be a parameterized distribution conditioned on  or deterministic). The whole system can be visualized as a hierarchy.<br>
<img alt="center" src="lib/media/pasted-image-20240723123040.png" style="width: 300px; max-width: 100%;">
<br><br><br>In order to train a latent variable model or to understand the abstract latent distributions, we need to perform inference on the latent variables by determining This information about the latent variable can be used to ..<br>
<br><a data-tooltip-position="top" aria-label="Estimator" data-href="Estimator" href="the-guide/mathematics/statistics/estimator.html" class="internal-link" target="_self" rel="noopener nofollow">Estimating</a> the most probable  given some data, e.g. for training a model
<br><a data-tooltip-position="top" aria-label="Uncertainty Estimation" data-href="Uncertainty Estimation" href="the-guide/machine-learning/generative-models/uncertainty-estimation.html" class="internal-link" target="_self" rel="noopener nofollow">Uncertainty</a> quantification for the latent variable by characterizing a full distribution
<br>Infeasible Marginalization
In practice, we often pay the price for the more expressive latent variable models by having to deal with intractable distributions. In order to compute the <a data-tooltip-position="top" aria-label="Bayes Theorem" data-href="Bayes Theorem" href="the-guide/mathematics/statistics/bayes-theorem.html" class="internal-link" target="_self" rel="noopener nofollow">posterior</a> above, we need to integrate over the latent space
<br>To overcome this, several approximate methods have been developed ...<br>
<br><a data-href="Expectation Maximization Algorithm" href="the-guide/machine-learning/generative-models/expectation-maximization-algorithm.html" class="internal-link" target="_self" rel="noopener nofollow">Expectation Maximization Algorithm</a> is mainly used for models Gaussian mixture models and is a probabilistiy version of <a data-href="K-Means" href="K-Means" class="internal-link" target="_self" rel="noopener nofollow">K-Means</a>
<br><a data-href="Variational Inference" href="the-guide/computational-statistics/variational-inference.html" class="internal-link" target="_self" rel="noopener nofollow">Variational Inference</a> approximates the true posterior with a simpler <a data-tooltip-position="top" aria-label="Probability Distribution" data-href="Probability Distribution" href="the-guide/mathematics/probability-theory/probability-distribution.html" class="internal-link" target="_self" rel="noopener nofollow">distirbution</a>  and minimizes its distance / divergence to the true . This results in an <a data-tooltip-position="top" aria-label="Optimization Problem" data-href="Optimization Problem" href="the-guide/mathematics/optimization/convex-optimization-lecture/optimization-problem.html" class="internal-link" target="_self" rel="noopener nofollow">optimization problem</a> that is typically easier to solve.
<br><a data-href="Markov-Chain Monte Carlo Methods" href="the-guide/computational-statistics/data-assimilation/smoothing-algorithms/markov-chain-monte-carlo-methods.html" class="internal-link" target="_self" rel="noopener nofollow">Markov-Chain Monte Carlo Methods</a> focus on generating samples from the desired posterior that are then used to estimate it.
]]></description><link>the-guide/machine-learning/generative-models/latent-variable-models.html</link><guid isPermaLink="false">The Guide/Machine Learning/Generative Models/Latent Variable Models.md</guid><pubDate>Sat, 05 Oct 2024 19:01:09 GMT</pubDate><enclosure url="lib/media/pasted-image-20240723123040.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src=&quot;lib/media/pasted-image-20240723123040.png&quot;&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[Neural Cellular Automata]]></title><description><![CDATA[ 
 <br>In a Nutshell
Computational model that combines concepts from <a data-tooltip-position="top" aria-label="Artificial Neural Network" data-href="Artificial Neural Network" href="the-guide/machine-learning/deep-learning/artificial-neural-network.html" class="internal-link" target="_self" rel="noopener nofollow">neural networks</a> with so-called Cellular Automata to create a self-organizing system.
<br><br><br>Based on a grid world with cells, we apply a local rule based on the neighborhood of each cell. With more and more iterations, this spreads information over the grid.<img alt="center" src="lib/media/pasted-image-20240905174250.png" style="width: 250px; max-width: 100%;">The most famous Cellular Automata  is "Conways Game of Life" or "Life", which uses the update Rules<br>
<br>Living Cell with less than two living neighbors becomes dead cell
<br>Living Cell with two or three living neighbors stays living cell
<br>Living Cell with more than three living neighbors becomes dead cell
<br>Dead Cell with exactly three living neighbors becomes living cell
<br><img alt="center" src="lib/media/pasted-image-20240905174657.png" style="width: 500px; max-width: 100%;"><br><br><br>Neural Cellular Automata try to learn the rules of a Cellular Automata instead of designing them by hand.<br>
<img alt="center" src="lib/media/pasted-image-20240905175238.png"><br>Neural networks are used in this process by applying convolutional kernels and processing the channel-wise results.<img alt="center" src="lib/media/pasted-image-20240905183032.png"><br>
The most important concepts in this are ...<br>
<br>Fire Rate

<br>NCAs not activated in each step, only fraction of cells changes in each step
<br>Low fire rate in combination with low step number can lead to cases where a pixel is never activated


<br>Channel Size

<br>As in <a data-tooltip-position="top" aria-label="Convolutional Neural Network" data-href="Convolutional Neural Network" href="the-guide/machine-learning/deep-learning/convolutional-neural-network.html" class="internal-link" target="_self" rel="noopener nofollow">CNNs</a>, more channels can encode more information about a cell


<br>Perceptive Range

<br>Depends on number of iterations, since at each step a cell can only see its neighbors


<br><br><br>During training, we have to backpropagate through time as the generation process is to be learned<br><img alt="center" src="lib/media/pasted-image-20240905183450.png"><br>Advantages

<br>Due to the way NCAs produce images by growing them from initial seeds by applying the same rules over and over again, we can use very small and simple architecture that have minimal hardware requirements after training.

<br>Challenges

<br>High RAM Requirements

<br><a data-tooltip-position="top" aria-label="Forward and Backward Propagation" data-href="Forward and Backward Propagation" href="the-guide/machine-learning/deep-learning/forward-and-backward-propagation.html" class="internal-link" target="_self" rel="noopener nofollow">Backpropagation</a> scales with number of parameters x width x height x steps, making it infeasible for larger problems


<br>Training Difficulties

<br>Because of the slow information spread, it can be difficult to find stable rules


<br>Performance often poor

<br>Not well understood yet



For these reasons, NCAs are restricted to small models and often require constraining the neighbor-communication by pre-designed <a data-tooltip-position="top" aria-label="Convolutional Neural Network" data-href="Convolutional Neural Network" href="the-guide/machine-learning/deep-learning/convolutional-neural-network.html" class="internal-link" target="_self" rel="noopener nofollow">convolutional layers</a>.
<br>Because of this, strategies to improve training and / or performance are<br>
<br>What persists, exists - amount of steps can blow up hardware requirements, instead save random intermediate output states in a pool and use them as initial states again<img alt="center" src="lib/media/pasted-image-20240905182542.png" style="width: 200px; max-width: 100%;">
<br><br><br>
<br>Regenerating NCA can learn to recover damaged images by supplying them with destroyed images via the pool
<br>Rotated Outputs - after learning update rules, it is possible to apply transformations to these to transformed outputs
<br>
]]></description><link>the-guide/machine-learning/generative-models/neural-cellular-automata.html</link><guid isPermaLink="false">The Guide/Machine Learning/Generative Models/Neural Cellular Automata.md</guid><pubDate>Sat, 05 Oct 2024 19:01:09 GMT</pubDate><enclosure url="lib/media/pasted-image-20240905174250.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src=&quot;lib/media/pasted-image-20240905174250.png&quot;&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[Normalizing Flows]]></title><description><![CDATA[ 
 <br>In a Nutshell
Approach to <a data-tooltip-position="top" aria-label="Generative Models" data-href="Generative Models" href="the-guide/machine-learning/generative-models/generative-models.html" class="internal-link" target="_self" rel="noopener nofollow">generative models</a> that exploits the change-of-variables formula to map a simple distribution, in nearly every case a Gaussian to a complex data distribution via  invertible transformations. In contrast to other popular models, the <a data-tooltip-position="top" aria-label="Likelihood Function" data-href="Likelihood Function" href="the-guide/mathematics/statistics/likelihood-function.html" class="internal-link" target="_self" rel="noopener nofollow">log-likelihood</a> is tractable.
<br><br><br>In generative modeling, we want  to approximately map a simple distribution, e.g.  to the complex distribution  of the data. We want to use a <a data-tooltip-position="top" aria-label="Artificial Neural Network" data-href="Artificial Neural Network" href="the-guide/machine-learning/deep-learning/artificial-neural-network.html" class="internal-link" target="_self" rel="noopener nofollow">neural network</a> as a flexible <a data-tooltip-position="top" aria-label="Mathematical Relations" data-href="Mathematical Relations" href="the-guide/mathematics/general-stuff/mathematical-relations.html" class="internal-link" target="_self" rel="noopener nofollow">transformation</a>  to obtain the <a data-tooltip-position="top" aria-label="Push-Forward Measure" data-href="Push-Forward Measure" href="the-guide/mathematics/measure-theory/push-forward-measure.html" class="internal-link" target="_self" rel="noopener nofollow">pushforward</a> This yields the natural objective of maximizing the probability of the data  distributed with  under the model (<a data-tooltip-position="top" aria-label="Likelihood Function" data-href="Likelihood Function" href="the-guide/mathematics/statistics/likelihood-function.html" class="internal-link" target="_self" rel="noopener nofollow">log-likelihood</a>) via To do this, we have to ...<br>
<br>Estimate , because we do not know  before training  !
<br>Init a neural network for  that fulfills the constraints ...<br>
- Invertibility for <br>
- Efficient computation of the <a data-tooltip-position="top" aria-label="Derivative, Gradient, Jacobian and Hessian" data-href="Derivative, Gradient, Jacobian and Hessian" href="the-guide/mathematics/analysis-and-calculus/derivative,-gradient,-jacobian-and-hessian.html" class="internal-link" target="_self" rel="noopener nofollow">Jacobian</a><br>
In general, we use  transformations to gain flexibility 
<br><br><br>To meet these constraints we have to carefully design , for which several methods exist. A particular interesting one is the full-rank residual.<br>Full-rank Residual
Middle-ground between expressivenss and efficient determinant estimation. They take the form which can be composed to get new, more complex flows. The log-likelihood can then be computed by summing over each flows contribution where  and .
<br>
<br>Requires parameterized  to be  Lipschitz to guarantee invertibility
<br>We sample  and push it forward to .
<br>Other approaches for designing efficient invertible flows are<br>
<br>NICE - partition intermediate variables into disjoint <a data-tooltip-position="top" aria-label="Set" data-href="Set" href="the-guide/mathematics/general-stuff/set.html" class="internal-link" target="_self" rel="noopener nofollow">subsets</a>  and 

<br>Forward, we map via identity up until  and then use where  is a <a data-tooltip-position="top" aria-label="Artificial Neural Network" data-href="Artificial Neural Network" href="the-guide/machine-learning/deep-learning/artificial-neural-network.html" class="internal-link" target="_self" rel="noopener nofollow">neural network</a> with  inputs and  outputs.

<br>Invertibility simply via subtraction
<br>Jacobian is lower triangular with all- diagonal, determinant is  by construction




<br>Real-NVP - same as above but with different mapping

<br>Forward, we map via identity up until  and then usewhere  and  are <a data-tooltip-position="top" aria-label="Artificial Neural Network" data-href="Artificial Neural Network" href="the-guide/machine-learning/deep-learning/artificial-neural-network.html" class="internal-link" target="_self" rel="noopener nofollow">neural networks</a> wit  inputs and  outputs.

<br>Invertibility ...
<br>Jacobian ...




<br>Problem
Flow-based models, especially discrete one are known to perform notoriously bad on clustered data distributions.
<br><img alt="center" src="lib/media/pasted-image-20240814195654.png" style="width: 300px; max-width: 100%;"><br><br><br>The setting above requires us to handpick the transformations , i.e. their total number  and a fitting . However, we can observe ... <br><br>Taking the residual flows above, we can consider and take the limit  of . Under certain conditions, compositions of residual flows  are given by <a data-tooltip-position="top" aria-label="ODEs - Ordinary Differential Equations" data-href="ODEs - Ordinary Differential Equations" href="the-guide/mathematics/differential-equations/odes-ordinary-differential-equations.html" class="internal-link" target="_self" rel="noopener nofollow">ODE's</a> of the form where we can now define the desired map  / the flow to be the solution of the ODE at time  with initial condition : <br><br>In the discrete setting, we had to compute to compute the log-abs determinant of the Jacobian to inform our flow about changes in the infinitesimal volumes when transforming.<br>
In the continuous case, we can instead impose physics constraints via the Transport / Continuity equation where the time evolution  is now denoted probability path induced by . The position  of a sample along its path is also time-dependent, yielding the total derivative in log space<br>Additional Information
The full derivation includes the <a data-tooltip-position="top" aria-label="https://en.wikipedia.org/wiki/Divergence" rel="noopener nofollow" class="external-link" href="https://en.wikipedia.org/wiki/Divergence" target="_blank">product rule for the divergence operator</a> with a scalar valued function, in our case , yieldingBy the chain rule for the total derivative with respect to time (parameters  and  of ), we obtain 
<br>The log-density is thereby computed by integrating this equation over time, i.e. where we inserted the parameterized vector field neural network . This means that at time , we get from which we can compute the last term, because we can obtain  and  by solving backwards in time and we know .<br>CNF's
All together, we need to solve a joint vector field to know how particles and the density evolves. We can do this using an ODE solve for every sample and then estimate the log-likelihood 
<br>Why use continuous setting ?

<br>One less hyperparameter: we do not have to choose , this is implicitly done by the solver for a given error threshold
<br>Lipschitz constraint relaxed, as we do not have specific  

<br><br>
<br>Draw a random sample  (at time ) from the dataset 
<br>Solve the ODE backwards in time (since flow constructed reversible) with initial conditionsto reach position  and .
<br>Adjust  to maximize  via <a data-tooltip-position="top" aria-label="Forward and Backward Propagation" data-href="Forward and Backward Propagation" href="the-guide/machine-learning/deep-learning/forward-and-backward-propagation.html" class="internal-link" target="_self" rel="noopener nofollow">backpropagation</a> + <a data-tooltip-position="top" aria-label="Gradient Descent" data-href="Gradient Descent" href="the-guide/mathematics/optimization/convex-optimization-lecture/algorithms/gradient-descent.html" class="internal-link" target="_self" rel="noopener nofollow">SGD</a>
<br><br><br>
<br><a rel="noopener nofollow" class="external-link" href="https://www.youtube.com/watch?v=i7LjDvsLWCg" target="_blank">https://www.youtube.com/watch?v=i7LjDvsLWCg</a>
<br><a rel="noopener nofollow" class="external-link" href="https://mlg.eng.cam.ac.uk/blog/2024/01/20/flow-matching.html" target="_blank">https://mlg.eng.cam.ac.uk/blog/2024/01/20/flow-matching.html</a>
<br><br><br>
<br>Example Linear  - Assume a <a data-tooltip-position="top" aria-label="Mathematical Relations" data-href="Mathematical Relations" href="the-guide/mathematics/general-stuff/mathematical-relations.html" class="internal-link" target="_self" rel="noopener nofollow">bijective function mapping</a>  and consider the <a data-tooltip-position="top" aria-label="Probability Density Function" data-href="Probability Density Function" href="the-guide/mathematics/probability-theory/probability-density-function.html" class="internal-link" target="_self" rel="noopener nofollow">density</a> induced by the procedure of sampling from another distribution  and transforming the sample. Using change-of-variables, the density  of y can be stated via with <a data-tooltip-position="top" aria-label="Derivative, Gradient, Jacobian and Hessian" data-href="Derivative, Gradient, Jacobian and Hessian" href="the-guide/mathematics/analysis-and-calculus/derivative,-gradient,-jacobian-and-hessian.html" class="internal-link" target="_self" rel="noopener nofollow">Jacobian</a> . 
]]></description><link>the-guide/machine-learning/generative-models/normalizing-flows.html</link><guid isPermaLink="false">The Guide/Machine Learning/Generative Models/Normalizing Flows.md</guid><pubDate>Sat, 25 Jan 2025 17:52:28 GMT</pubDate><enclosure url="lib/media/pasted-image-20240814195654.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src=&quot;lib/media/pasted-image-20240814195654.png&quot;&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[Riemannian Flow Matching]]></title><description><![CDATA[ 
 <br>In a Nutshell
Generalization of the <a data-href="Flow Matching" href="the-guide/machine-learning/generative-models/flow-matching.html" class="internal-link" target="_self" rel="noopener nofollow">Flow Matching</a> algorithm to non-euclidean geometries, i.e. <a data-tooltip-position="top" aria-label="Mannigfaltigkeiten" data-href="Mannigfaltigkeiten" href="the-guide/mathematics/differential-geometry/riemannian-geometry/mannigfaltigkeiten-und-differenzierbare-abbildungen/mannigfaltigkeiten.html" class="internal-link" target="_self" rel="noopener nofollow">Riemannian manifolds</a>.
<br><br>
<br>
Adapt <a data-tooltip-position="top" aria-label="Metric Space and Completeness" data-href="Metric Space and Completeness" href="the-guide/mathematics/general-stuff/metric-space-and-completeness.html" class="internal-link" target="_self" rel="noopener nofollow">metric</a> in losswhere  is again obtained by pushing forwards  via 

<br>
Adapt conditional flow to manifold

<br>Closed form manifolds (transformation <a data-tooltip-position="top" aria-label="Group" data-href="Group" href="the-guide/mathematics/group-theory/group.html" class="internal-link" target="_self" rel="noopener nofollow">groups</a>, <a data-tooltip-position="top" aria-label="Hyperbolischer Raum" data-href="Hyperbolischer Raum" href="the-guide/mathematics/differential-geometry/riemannian-geometry/hyperbolischer-raum.html" class="internal-link" target="_self" rel="noopener nofollow">hyperbolic space</a>, ... )

<br>Use <a data-tooltip-position="top" aria-label="Exponentialabbildung" data-href="Exponentialabbildung" href="the-guide/mathematics/differential-geometry/riemannian-geometry/metriken-und-zusammenhänge/exponentialabbildung.html" class="internal-link" target="_self" rel="noopener nofollow">exponential map</a> instead of linear interpolation


<br>General manifolds


]]></description><link>the-guide/machine-learning/generative-models/riemannian-flow-matching.html</link><guid isPermaLink="false">The Guide/Machine Learning/Generative Models/Riemannian Flow Matching.md</guid><pubDate>Mon, 24 Feb 2025 23:49:44 GMT</pubDate></item><item><title><![CDATA[U-Net]]></title><description><![CDATA[ 
 <br>In a Nutshell
Deep <a data-tooltip-position="top" aria-label="Artificial Neural Network" data-href="Artificial Neural Network" href="the-guide/machine-learning/deep-learning/artificial-neural-network.html" class="internal-link" target="_self" rel="noopener nofollow">neural network</a> architecture that is mostly used in <a data-tooltip-position="top" aria-label="Image Segmentation" data-href="Image Segmentation" href="the-guide/machine-learning/generative-models/image-segmentation.html" class="internal-link" target="_self" rel="noopener nofollow">image segmentation</a>. Employs a <a data-tooltip-position="top" aria-label="(Variational) Auto Encoder" data-href="(Variational) Auto Encoder" href="the-guide/machine-learning/generative-models/(variational)-auto-encoder.html" class="internal-link" target="_self" rel="noopener nofollow">auto-encoder</a>-like principle of down- and upsampling in its <a data-tooltip-position="top" aria-label="Convolutional Neural Network" data-href="Convolutional Neural Network" href="the-guide/machine-learning/deep-learning/convolutional-neural-network.html" class="internal-link" target="_self" rel="noopener nofollow">convolutional layers</a>, while also feeding back information of every encoding step to the relative decoding layer.
<br><img alt="center" src="lib/media/pasted-image-20240729121410.png" style="width: 500px; max-width: 100%;"><br>The architecture is able to capture global features in its down-sampling approach while preserving local features by feeding them back into the up-sampling steps.<br><br><br><br><br>Adds a <a data-tooltip-position="top" aria-label="Bayes Theorem" data-href="Bayes Theorem" href="the-guide/mathematics/statistics/bayes-theorem.html" class="internal-link" target="_self" rel="noopener nofollow">prior and posterior</a> <a data-tooltip-position="top" aria-label="Artificial Neural Network" data-href="Artificial Neural Network" href="the-guide/machine-learning/deep-learning/artificial-neural-network.html" class="internal-link" target="_self" rel="noopener nofollow">neural network</a> to the architecture and combines the result with the output of the U-Net by appending the channels.<br>
<br>The posterior net takes in the original image and the ground truth to put out a mean and variance in a latent space.
<br>The prior does the same, but only uses the input.
<br>The latent random variable is used to sample the additional channels
<br>The posterior network is only used during training to guide the prior network by minimizing the <a data-tooltip-position="top" aria-label="Kullback-Leibler Divergence" data-href="Kullback-Leibler Divergence" href="the-guide/information-theory/information-theory-1/information/kullback-leibler-divergence.html" class="internal-link" target="_self" rel="noopener nofollow">KL-divergence</a> between both distributions.<img alt="center" src="lib/media/pasted-image-20240907093750.png">
<br>During inference we can repeatedly sample from the latent space to obtain different predictions, which can capture uncertainty <img alt="center" src="lib/media/pasted-image-20240907093805.png">
<br><br>Incorporates prior on multiple scales of the U-net. At higher resolutions, priors generate local changes, while they produce global variations on lower resolutions.<br><br>Adds probabilistic latent random variables as in <a data-href="Bayesian Neural Networks" href="the-guide/machine-learning/generative-models/bayesian-neural-networks.html" class="internal-link" target="_self" rel="noopener nofollow">Bayesian Neural Networks</a>, but only for the last layers on the highest resolution (lower right) to capture multiple local minima.<img alt="center" src="lib/media/pasted-image-20240907095126.png"><br>
<img alt="center" src="lib/media/pasted-image-20240907095151.png" style="width: 300px; max-width: 100%;"><br><br>
<br><a rel="noopener nofollow" class="external-link" href="https://www.youtube.com/watch?v=NhdzGfB1q74" target="_blank">https://www.youtube.com/watch?v=NhdzGfB1q74</a>
]]></description><link>the-guide/machine-learning/generative-models/u-net.html</link><guid isPermaLink="false">The Guide/Machine Learning/Generative Models/U-Net.md</guid><pubDate>Sat, 05 Oct 2024 19:01:09 GMT</pubDate><enclosure url="lib/media/pasted-image-20240729121410.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src=&quot;lib/media/pasted-image-20240729121410.png&quot;&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[Uncertainty Estimation]]></title><description><![CDATA[ 
 <br>In a Nutshell
Require a <a data-tooltip-position="top" aria-label="- Machine Learning -" data-href="- Machine Learning -" href="the-guide/machine-learning/-machine-learning-.html" class="internal-link" target="_self" rel="noopener nofollow">machine learning</a> model to return a <a data-tooltip-position="top" aria-label="Probability Distribution" data-href="Probability Distribution" href="the-guide/mathematics/probability-theory/probability-distribution.html" class="internal-link" target="_self" rel="noopener nofollow">probability distribution</a> over predictions rather than a single value in order to quantify if we can trust the model.
<br><br><br>Deep Learning models fail silently, e.g. for unseen objects in image segmentation, a most likely label can be assigned even when all probabilities are very bad. Even when the model performs well after training, over time shifts can lead to detoriation of performance ...<br>Types of Shifts

<br>Concept Drift - e.g. new diseases (classes) we want to incorporate that were not in the data initially
<br>Population Shift - our data is taken from another continent, country, year etc
<br>Prevalence Shift - our data is old, does not capture new trends 
<br>Malicious Attack - data can be corrupted in a systematic way not seen during training
<br>Aquisition shift - data is taken using old measurement device or stored in old format / data type that aren't up to date 

<br><img alt="center" src="lib/media/pasted-image-20240729090757.png"><br><img alt="center" src="lib/media/pasted-image-20240729090810.png"><br><br><br>
<br>
Predictive Uncertainty = predicting the whole distribution  instead of single points

<br>Classification adds a confidence score
<br>Regression puts out mean and variance


<br>
Inter-observer Variability

<br>Different graders assign different labels to the same datapoint (image etc.)


<br>
Intra-observer Variability

<br>The same grader assigns different labels for the same image when seeing it multiple times


<br>Aleatoric Uncertainty - Data
Arises die to inherent stochasticity in the data itself, e.g. due to

<br>Noise in the labeling (inter-observer/  intra-observer variability) or in the measurements
<br>Partial observability - additional features (e.g. extended measurements) can reduce 

It is thereby irreducible for given features, persists even in the limit of infinite data.
<br>Epistemic Uncertainty - Model
Arises from the fact that data is never complete, multiple parameters could describe it equally well, but yield different behavior for unseen data. Vanishes in the limit of infinite data.
<br>
<br>Out-of-Distribution Uncertainty

<br>Evaluate model on datapoints that do not belong to any existing classes


<br><br><br>The question remains on how to actually measure the quality of uncertainty estimates. Some methods are...<br>
<br>Entropy
<br>Expected Calibration Error (ECE)

<br>How good does the predicted confidence of our model align with the accuracy of the prediction (how certain was the model vs. how good was the model)


<br>Negative <a data-tooltip-position="top" aria-label="Likelihood Function" data-href="Likelihood Function" href="the-guide/mathematics/statistics/likelihood-function.html" class="internal-link" target="_self" rel="noopener nofollow">Log-Likelihood</a>

<br>Only gets small for 
<br>Can overemphasize tails


<br>Brier Score

<br>Bounded , quadratic penalty given by If a correct label was predicted with high confidence, the summand is very small (). 
<br>Inadequate for very rare or frequent events, since this can distort the score.


<br><br><br><a data-href="Variational Inference" href="the-guide/computational-statistics/variational-inference.html" class="internal-link" target="_self" rel="noopener nofollow">Variational Inference</a><br>
...<br><a data-href="Bayesian Neural Networks" href="the-guide/machine-learning/generative-models/bayesian-neural-networks.html" class="internal-link" target="_self" rel="noopener nofollow">Bayesian Neural Networks</a>
Train mean and variance for each weight, realization via sampling. Captures epistemic and aleatroric uncertainty (<a data-tooltip-position="top" aria-label="Bayesian Neural Networks" data-href="Bayesian Neural Networks" href="the-guide/machine-learning/generative-models/bayesian-neural-networks.html" class="internal-link" target="_self" rel="noopener nofollow">Details</a>).
<br>Monte Carlo Dropout
Technique for neural networks to <a data-tooltip-position="top" aria-label="Uncertainty Estimation" data-href="Uncertainty Estimation" href="the-guide/machine-learning/generative-models/uncertainty-estimation.html" class="internal-link" target="_self" rel="noopener nofollow">estimate uncertainty</a>. Leverages dropout regularization technique and also applies it during inference. Intuitively, we create an artificial ensemble of models by setting random weights to zero at each evaluation. To predict uncertainty, we have to run multiple forwards runs and aggregate them, using the <a data-tooltip-position="top" aria-label="Expectations" data-href="Expectations" href="the-guide/mathematics/statistics/expectations.html" class="internal-link" target="_self" rel="noopener nofollow">mean</a> for prediction and the <a data-tooltip-position="top" aria-label="Covariance and Variance" data-href="Covariance and Variance" href="the-guide/mathematics/statistics/covariance-and-variance.html" class="internal-link" target="_self" rel="noopener nofollow">variance</a> for uncertainty estimation. It thereby captures epistemic uncertainty.
<br>Temperature Scaling of Softmax Layers
Parameterize a softmax output layer with additional temperature parameter  via optimize using e.g. ECE, NLL on a separate calibration dataset.
<br>
<br>For , the predictions become sharper and more confident, for  they get less confident
<br>Can be integrated after initial training
<br><a data-href="Deep Ensembles" href="the-guide/machine-learning/deep-learning/deep-ensembles.html" class="internal-link" target="_self" rel="noopener nofollow">Deep Ensembles</a> 
Family of methods that use parallel training of different random seeds and then use max or average of all models for prediction (<a data-tooltip-position="top" aria-label="Deep Ensembles" data-href="Deep Ensembles" href="the-guide/machine-learning/deep-learning/deep-ensembles.html" class="internal-link" target="_self" rel="noopener nofollow">Details</a>).
]]></description><link>the-guide/machine-learning/generative-models/uncertainty-estimation.html</link><guid isPermaLink="false">The Guide/Machine Learning/Generative Models/Uncertainty Estimation.md</guid><pubDate>Thu, 27 Feb 2025 12:05:01 GMT</pubDate><enclosure url="lib/media/pasted-image-20240729090757.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src=&quot;lib/media/pasted-image-20240729090757.png&quot;&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[Uncertainty Estimation in Image Segmentation]]></title><description><![CDATA[<a class="tag" href="?query=tag:Generative-Models-Course" style="background-color: rgb(4, 108, 116); color: white; font-weight: 700; border: none; border-radius: 1em; padding: 0.2em 0.5em;">#Generative-Models-Course</a> 
 <br>Approaches for <a data-tooltip-position="top" aria-label="Uncertainty Estimation" data-href="Uncertainty Estimation" href="the-guide/machine-learning/generative-models/uncertainty-estimation.html" class="internal-link" target="_self" rel="noopener nofollow">uncertainty estimation</a> in <a data-tooltip-position="top" aria-label="Image Segmentation" data-href="Image Segmentation" href="the-guide/machine-learning/generative-models/image-segmentation.html" class="internal-link" target="_self" rel="noopener nofollow">image segmentation</a> applications covered in the <a href=".?query=tag:Generative-Models-Course" class="tag" target="_blank" rel="noopener nofollow">#Generative-Models-Course</a>  course. In general, the output of such methods is an uncertainty map over the segmented image.<br><br><br>Generalized Energy Distance
Captures how dissimilar samples are from ground truth while accounting for how diverse the samples from the ground truth and the learned distribution are.
<br><br><br>
<br>Monte-Carlo Dropout

<br>Easy to implement, but can hurt performance and can produce outcomes, where random pixels have a high uncertainty


<br><a data-href="Deep Ensembles" href="the-guide/machine-learning/deep-learning/deep-ensembles.html" class="internal-link" target="_self" rel="noopener nofollow">Deep Ensembles</a> based on <a data-href="U-Net" href="the-guide/machine-learning/generative-models/u-net.html" class="internal-link" target="_self" rel="noopener nofollow">U-Net</a> architecture

<br>Easy to implement, but require a lot of resources and can be inefficient due to many ensemble members producing similar outputs


<br><a data-href="U-Net" href="the-guide/machine-learning/generative-models/u-net.html" class="internal-link" target="_self" rel="noopener nofollow">U-Net</a> Extensions 

<br>(Hierarchical-) Probabilistic U-Net


<br><br><br>Developed at the same time as the <a data-tooltip-position="top" aria-label="U-Net" data-href="U-Net" href="the-guide/machine-learning/generative-models/u-net.html" class="internal-link" target="_self" rel="noopener nofollow">HPU-Net</a> to incorporate global variations in the data. Use a prior and posterior net to learn a latent representation (posterior gets ground truth additionally to input) and minimize the <a data-tooltip-position="top" aria-label="Kullback-Leibler Divergence" data-href="Kullback-Leibler Divergence" href="the-guide/information-theory/information-theory-1/information/kullback-leibler-divergence.html" class="internal-link" target="_self" rel="noopener nofollow">KL-divergence</a> on each level. <img alt="center" src="lib/media/pasted-image-20240907094734.png"><br><br><br>Capture <a data-tooltip-position="top" aria-label="Uncertainty Estimation" data-href="Uncertainty Estimation" href="the-guide/machine-learning/generative-models/uncertainty-estimation.html" class="internal-link" target="_self" rel="noopener nofollow">aleatoric uncertainty</a> by assuming<br>
<br>Logits are a deterministic function and <a data-tooltip-position="top" aria-label="Statistical Independence" data-href="Statistical Independence" href="the-guide/mathematics/statistics/statistical-independence.html" class="internal-link" target="_self" rel="noopener nofollow">independent</a> from each other<img alt="center" src="lib/media/pasted-image-20240907103216.png" style="width: 200px; max-width: 100%;">
<br>Every output is independent from its neighbors<img alt="center" src="lib/media/pasted-image-20240907103439.png" style="width: 200px; max-width: 100%;"><br>
This enables us to nicely factorize the <a data-tooltip-position="top" aria-label="Likelihood Function" data-href="Likelihood Function" href="the-guide/mathematics/statistics/likelihood-function.html" class="internal-link" target="_self" rel="noopener nofollow">likelihood</a> via However, in many cases the second assumption can be too restrictive. We can model logit-dependancy through the <a data-tooltip-position="top" aria-label="Covariance and Variance" data-href="Covariance and Variance" href="the-guide/mathematics/statistics/covariance-and-variance.html" class="internal-link" target="_self" rel="noopener nofollow">covariance</a> of a <a data-tooltip-position="top" aria-label="Gaussian Distribution" data-href="Gaussian Distribution" href="the-guide/mathematics/probability-theory/gaussian-distribution.html" class="internal-link" target="_self" rel="noopener nofollow">Gaussian</a>and we can estimate the term above by using  samples and <a data-tooltip-position="top" aria-label="Monte Carlo Integration" data-href="Monte Carlo Integration" href="the-guide/mathematics/probability-theory/monte-carlo-integration.html" class="internal-link" target="_self" rel="noopener nofollow">MC-integration</a>For large dimensions, the covariance becomes intractable and may require e.g. low-rank approximation.
<br><img alt="center" src="lib/media/pasted-image-20240830114914.png" style="width: 500px; max-width: 100%;"><br>Comparison of graphical models for a) <a data-tooltip-position="top" aria-label="Artificial Neural Network" data-href="Artificial Neural Network" href="the-guide/machine-learning/deep-learning/artificial-neural-network.html" class="internal-link" target="_self" rel="noopener nofollow">neural networks</a>, b) conditional random fields and c) SNN.<br><br><br>Additionally capture epistemic uncertainty ]]></description><link>the-guide/machine-learning/generative-models/uncertainty-estimation-in-image-segmentation.html</link><guid isPermaLink="false">The Guide/Machine Learning/Generative Models/Uncertainty Estimation in Image Segmentation.md</guid><pubDate>Sat, 05 Oct 2024 19:01:09 GMT</pubDate><enclosure url="lib/media/pasted-image-20240907094734.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src=&quot;lib/media/pasted-image-20240907094734.png&quot;&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[Variational Neural Cellular Automata]]></title><description><![CDATA[ 
 <br><br><br>In nature, the process of cellular growth and differentiation has lead to an amazing<br>
diversity of organisms — algae, starfish, giant sequoia, tardigrades, and orcas are<br>
all created by the same generative process. Inspired by the incredible diversity of<br>
this biological generative process, we propose a generative model, the Variational<br>
Neural Cellular Automata (VNCA), which is loosely inspired by the biological<br>
processes of cellular growth and differentiation. Unlike previous related works,<br>
the VNCA is a proper probabilistic generative model, and we evaluate it accord-<br>
ing to best practices. We find that the VNCA learns to reconstruct samples well<br>
and that despite its relatively few parameters and simple local-only communica-<br>
tion, the VNCA can learn to generate a large variety of output from information<br>
encoded in a common vector format. While there is a significant gap to the cur-<br>
rent state-of-the-art in terms of generative modeling performance, we show that<br>
the VNCA can learn a purely self-organizing generative process of data. Addi-<br>
tionally, we show that the VNCA can learn a distribution of stable attractors that<br>
can recover from significant damage.<br><br><br>Combines <a data-href="(Variational) Auto Encoder" href="the-guide/machine-learning/generative-models/(variational)-auto-encoder.html" class="internal-link" target="_self" rel="noopener nofollow">(Variational) Auto Encoder</a> with <a data-href="Neural Cellular Automata" href="the-guide/machine-learning/generative-models/neural-cellular-automata.html" class="internal-link" target="_self" rel="noopener nofollow">Neural Cellular Automata</a> as a decoder architecture.<br><img alt="center" src="lib/media/pasted-image-20240906192342.png"><br>The objective is the same as for the VAE, but the decoder works based on the NCA with slightly altered update behavior:<br>
<br>A sample from the <a data-tooltip-position="top" aria-label="Latent Variable Models" data-href="Latent Variable Models" href="the-guide/machine-learning/generative-models/latent-variable-models.html" class="internal-link" target="_self" rel="noopener nofollow">latent space</a> is a single cell that is repeated in a  grid
<br>Updates alter between<br>
- Multiple standard NCA steps in a fixed subset of pixels (grows overtime due to step 2)<br>
- A doubling step, where each cell is copied to its right, down and down-right neighbor while pushing other cells outwards.<br>
The paper also introduces a non-doubling variant based entirely on the usual NCA in the decoder
<br>Seeds entire image with single-cell sample from latent space
<br>Uses pool of corrupted intermediate results to train for reconstruction, which breaks the ELBO theory (only works empirically !)
<br>For damage recovery, but worse generative model
<br><br><br>
<br>Straight forward contribution, but lacks application apart from minimal MNIST
]]></description><link>the-guide/machine-learning/generative-models/variational-neural-cellular-automata.html</link><guid isPermaLink="false">The Guide/Machine Learning/Generative Models/Variational Neural Cellular Automata.md</guid><pubDate>Sat, 05 Oct 2024 19:01:09 GMT</pubDate><enclosure url="lib/media/pasted-image-20240906192342.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src=&quot;lib/media/pasted-image-20240906192342.png&quot;&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[VOS Learning What You Dont Know by Virtual Outlier Synthesis]]></title><description><![CDATA[ 
 <br><br><br>Out-of-distribution (OOD) detection has received much attention lately due to its importance in the safe deployment of neural networks. One of the key challenges is that models lack supervision signals from unknown data, and as a result, can produce overconfident predictions on OOD data. Previous approaches rely on real outlier datasets for model regularization, which can be costly and sometimes infeasible to obtain in practice. In this paper, we present VOS, a novel framework for OOD detection by adaptively synthesizing virtual outliers that can meaningfully regularize the model's decision boundary during training. Specifically, VOS samples virtual outliers from the low-likelihood region of the class-conditional distribution estimated in the feature space. Alongside, we introduce a novel unknown-aware training objective, which contrastively shapes the uncertainty space between the ID data and synthesized outlier data. VOS achieves competitive performance on both object detection and image classification models, reducing the FPR95 by up to 9.36% compared to the previous best method on object detectors. <br>
<br>Code is available at <a rel="noopener nofollow" class="external-link" href="https://github.com/deeplearning-wisc/vos" target="_blank">https://github.com/deeplearning-wisc/vos</a>.
<br><br><br>Tries to tackle the problem of OOD data in classification. In the picture below, we can see that the classifier in the middle predicts samples far away from the clusters wrong because it uses the minimal distance as a reference. In reality, we don't know anything about the regions near the boundary and would like them to be identified as outliers.<br>
<img alt="center" src="lib/media/pasted-image-20240606103822.png" style="width: 500px; max-width: 100%;"><br>The setting of the paper is a classification task with input space  and label space . Additionally, they extend this setting to additional detection of bounding boxes, encoded by their vertices  in an image. They use <a data-tooltip-position="top" aria-label="Artificial Neural Network" data-href="Artificial Neural Network" href="the-guide/machine-learning/deep-learning/artificial-neural-network.html" class="internal-link" target="_self" rel="noopener nofollow">neural networks</a> with parameters  for bounding box regression  and classification . The whole concept is depicted in the following scheme:<br><img alt="center" src="lib/media/pasted-image-20240606104218.png" style="width: 550px; max-width: 100%;"><br><br>Given training samples  and a <a data-tooltip-position="top" aria-label="Latent Variable Models" data-href="Latent Variable Models" href="the-guide/machine-learning/generative-models/latent-variable-models.html" class="internal-link" target="_self" rel="noopener nofollow">latent</a> representation  of an object (here input, bounding box), compute empirical <a data-tooltip-position="top" aria-label="Expectations" data-href="Expectations" href="the-guide/mathematics/statistics/expectations.html" class="internal-link" target="_self" rel="noopener nofollow">mean</a>  and <a data-tooltip-position="top" aria-label="Covariance and Variance" data-href="Covariance and Variance" href="the-guide/mathematics/statistics/covariance-and-variance.html" class="internal-link" target="_self" rel="noopener nofollow">covariance</a>  via  is the number of data points in class . The covariance is the mean of all covariances of the possible classes . This is based on empirical results, in which a separate covariance did not yield a significant advantage. For efficiency, the above is computed based on a class-conditional queue with  instances each.<br>
To sample virtual outliers, sample from the above distributions by choosing an  to obtain the class-conditioned <a data-tooltip-position="top" aria-label="Probability Distribution" data-href="Probability Distribution" href="the-guide/mathematics/probability-theory/probability-distribution.html" class="internal-link" target="_self" rel="noopener nofollow">distribution</a> The output of the classification branch for an outlier is then computed via the linear transformation of the last layer, e.g. with weight <a data-tooltip-position="top" aria-label="Matrices" data-href="Matrices" href="the-guide/mathematics/linear-algebra/matrices.html" class="internal-link" target="_self" rel="noopener nofollow">matrix</a> . This output is denoted logit branch and has values in  ( is , negative lower confidence, positive higher) that are afterwards transformed to a distribution via a last soft-max layer.<br><br>The authors employ the energy score of the logit output as an uncertainty measurementwhere a learnable weight  enables more flexibility. Obtaining only low confidence values, the energy quickly explodes to infinity, while a high confidence score will yield a small negative number. This enables an uncertainty regularizer of the form The indicator functions are intractable, which is why a smooth approximation via the binary Sigmoid loss is used. This results in where a <a data-tooltip-position="top" aria-label="Artificial Neural Network" data-href="Artificial Neural Network" href="the-guide/machine-learning/deep-learning/artificial-neural-network.html" class="internal-link" target="_self" rel="noopener nofollow">MLP</a>  enables more complex energy surfaces for complicated data. Overall, the training objective becomes <br><br>Based on the above training, the authors add an uncertainty classification branch for OOD detection by feeding the energy values to a logistics regression given a predicted object  via For OOD detection, we pick a threshold of the output probability  and assign the outlier label if . If it is smaller, we assign the predicted label from the classification branch.<br><br><br>
<br>Simple, easy to use, add to existing pipelines
<br>Relies in a lot of heuristics, empirical results ... especially the covariances are weird
<br>Very abstract reasoning about basically everything, no real translation to results in applications 
]]></description><link>the-guide/machine-learning/generative-models/vos-learning-what-you-dont-know-by-virtual-outlier-synthesis.html</link><guid isPermaLink="false">The Guide/Machine Learning/Generative Models/VOS Learning What You Dont Know by Virtual Outlier Synthesis.md</guid><pubDate>Sat, 05 Oct 2024 19:01:09 GMT</pubDate><enclosure url="lib/media/pasted-image-20240606103822.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src=&quot;lib/media/pasted-image-20240606103822.png&quot;&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[Graph Based Machine Learning]]></title><description><![CDATA[ 
 <br>Attributes of a Graph

<br>Nodes encode features 
<br>Edges ecode relationships between nodes
<br>Global context is any representation or set of features that summarizes or depends on the entire graph, rather than just a node’s local neighborhood. It can be implemented through special pooling/readout layers or by re-injecting a graph-level embedding into each node’s update step.

<br>
<br><a data-href="Graph Neural Networks" href="the-guide/machine-learning/graph-based-learning/graph-neural-networks.html" class="internal-link" target="_self" rel="noopener nofollow">Graph Neural Networks</a>
<br><a data-href="Graph Convolutional Neural Networks" href="the-guide/machine-learning/graph-based-learning/graph-convolutional-neural-networks.html" class="internal-link" target="_self" rel="noopener nofollow">Graph Convolutional Neural Networks</a>
<br><a data-href="GSP in Machine Learning" href="the-guide/machine-learning/graph-based-learning/gsp-in-machine-learning.html" class="internal-link" target="_self" rel="noopener nofollow">GSP in Machine Learning</a>
<br><br><br>Graph-level Tasks
Predict <a data-tooltip-position="top" aria-label="Graph" data-href="Graph" href="the-guide/mathematics/graph-theory/graph.html" class="internal-link" target="_self" rel="noopener nofollow">properties of an entire graph</a>, mapping a graph input to any form of output.
<br>Node-level Tasks
Predicting an output for each node based on its features and connectedness in the graph.
<br>Edge-level Tasks
Given nodes and node features, predict edge information to build graphs structure between them.
]]></description><link>the-guide/machine-learning/graph-based-learning/graph-based-machine-learning.html</link><guid isPermaLink="false">The Guide/Machine Learning/Graph-Based Learning/Graph Based Machine Learning.md</guid><pubDate>Thu, 10 Apr 2025 21:17:48 GMT</pubDate></item><item><title><![CDATA[Graph Convolutional Neural Networks]]></title><description><![CDATA[ 
 <br><br>
<br>Given

<br>Training set  with data pairs 
<br>Loss Function 
<br>Function class 


<br>Goal

<br>Find 


<br>Info
Choose <a data-tooltip-position="top" aria-label="Graph Filters" data-href="Graph Filters" href="the-guide/mathematics/graph-theory/graph-filters.html" class="internal-link" target="_self" rel="noopener nofollow">Graph filter</a> as function class  where the Graph shift operator  is assumed as given.

<br>

<br><img alt="center" src="lib/media/pasted-image-20230209160057.png"><br>Limited Expressiveness
Optimization over filter coefficients, can only learn linear maps  Limited expressive power
<br><br>Graph Perceptrons
Choose graph perceptrons as function class  and optimizeThis allows learning of nonlinear maps.
<br><img alt="center" src="lib/media/pasted-image-20230209160114.png"><br><br>Training<br>
<br>Layers of Graph Perceptrons that feed into each other
<br> Mathematically, we solve for optimal parameterswhere  corresponds to encoded prior information <br> <img alt="400|center" src="lib/media/pasted-image-20230209160249.png"><br>
<br>Parametrized on filter tensor 
<br>

<br>Matrix  corresponds to encoded prior information 


<br><br>Higher Dimensional Features
The features can be extended to <a data-tooltip-position="top" aria-label="Vector Space" data-href="Vector Space" href="the-guide/mathematics/general-stuff/vector-space.html" class="internal-link" target="_self" rel="noopener nofollow">vector-valued</a> quantities. In the resulting feature <a data-tooltip-position="top" aria-label="Matrices" data-href="Matrices" href="the-guide/mathematics/linear-algebra/matrices.html" class="internal-link" target="_self" rel="noopener nofollow">matrix</a>, each row represents a single <a data-tooltip-position="top" aria-label="Node or Vertex Set" data-href="Node or Vertex Set" href="the-guide/mathematics/graph-theory/node-or-vertex-set.html" class="internal-link" target="_self" rel="noopener nofollow">node</a>, while each column represents a feature. Mathematically, this extends the convolution to where  is a shift operator for each graph signal feature and  is linear combination of features at each node (increases parameter count). Feature-wise, this corresponds to applying  filters to each of the  features .
<br><img alt="center" src="lib/media/pasted-image-20230209162031.png"><br>
<br>Pooling

<br>Local summary of a feature in order to decrease computational cost


<br><br>Comparison GNN to arbitrary NN

<br>GNN is a particular case of a fully connected NN 

<br>NN attains smaller cost and performs better on the training set
<br>Generally more parameters to tweak


<br>GNN generalizes better to new data

<br>Exploits internal symmetries of graph 
<br>Permutation Equivariance

<br>Signal processing with GNNs is independent of labeling





]]></description><link>the-guide/machine-learning/graph-based-learning/graph-convolutional-neural-networks.html</link><guid isPermaLink="false">The Guide/Machine Learning/Graph-Based Learning/Graph Convolutional Neural Networks.md</guid><pubDate>Sun, 06 Apr 2025 18:40:46 GMT</pubDate><enclosure url="lib/media/pasted-image-20230209160057.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src=&quot;lib/media/pasted-image-20230209160057.png&quot;&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[Graph Neural Networks]]></title><description><![CDATA[ 
 <br>In a Nutshell
<a data-tooltip-position="top" aria-label="Optimization Problem" data-href="Optimization Problem" href="the-guide/mathematics/optimization/convex-optimization-lecture/optimization-problem.html" class="internal-link" target="_self" rel="noopener nofollow">Optimizable</a> transformation of all attributes of a <a data-tooltip-position="top" aria-label="Graph" data-href="Graph" href="the-guide/mathematics/graph-theory/graph.html" class="internal-link" target="_self" rel="noopener nofollow">graph</a> (<a data-tooltip-position="top" aria-label="Node or Vertex Set" data-href="Node or Vertex Set" href="the-guide/mathematics/graph-theory/node-or-vertex-set.html" class="internal-link" target="_self" rel="noopener nofollow">nodes</a>, edges, <a data-tooltip-position="top" aria-label="Graph Based Machine Learning" data-href="Graph Based Machine Learning" href="the-guide/machine-learning/graph-based-learning/graph-based-machine-learning.html" class="internal-link" target="_self" rel="noopener nofollow">global context</a>) that preserve graph symmetry (permutations).
<br><br><br>If we solely rely on things we now from other deep learning approaches, we could use an independent mapping for all the feature <a data-tooltip-position="top" aria-label="Vector Space" data-href="Vector Space" href="the-guide/mathematics/general-stuff/vector-space.html" class="internal-link" target="_self" rel="noopener nofollow">vectors</a> of the <a data-tooltip-position="top" aria-label="Node or Vertex Set" data-href="Node or Vertex Set" href="the-guide/mathematics/graph-theory/node-or-vertex-set.html" class="internal-link" target="_self" rel="noopener nofollow">node set</a>, the edges and the global contexts.<br><img alt="center" src="lib/media/pasted-image-20250406221346.png" style="width: 450px; max-width: 100%;"><br>However, this does not incorporate connectivity information into the learning, the connectivity does not change.]]></description><link>the-guide/machine-learning/graph-based-learning/graph-neural-networks.html</link><guid isPermaLink="false">The Guide/Machine Learning/Graph-Based Learning/Graph Neural Networks.md</guid><pubDate>Sun, 06 Apr 2025 20:17:08 GMT</pubDate><enclosure url="lib/media/pasted-image-20250406221346.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src=&quot;lib/media/pasted-image-20250406221346.png&quot;&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[GSP in Machine Learning]]></title><description><![CDATA[<a class="tag" href="?query=tag:GSP-Course" style="background-color: rgb(4, 108, 116); color: white; font-weight: 700; border: none; border-radius: 1em; padding: 0.2em 0.5em;">#GSP-Course</a> 
 <br>In a Nutshell
Graph Signal Processing perspective of applications for <a data-tooltip-position="top" aria-label="Graph" data-href="Graph" href="the-guide/mathematics/graph-theory/graph.html" class="internal-link" target="_self" rel="noopener nofollow">graphs</a> in machine learning applications introduced in the <a href=".?query=tag:GSP-Course" class="tag" target="_blank" rel="noopener nofollow">#GSP-Course</a> .
<br><br>When is GSP useful in machine learning ?
If one can construct a similiarity graph with edge weights according to distance between inputs  and  to capture information about relative data position in -dimensional feature space.<br>
Alternatively, if data points can be defined on graph, e.g. point classification.
<br><br>Measure Label Signal Smoothness
Given a similarity <a data-tooltip-position="top" aria-label="Graph" data-href="Graph" href="the-guide/mathematics/graph-theory/graph.html" class="internal-link" target="_self" rel="noopener nofollow">graph</a> with <a data-tooltip-position="top" aria-label="Graph Laplacians" data-href="Graph Laplacians" href="the-guide/mathematics/graph-theory/graph-laplacians.html" class="internal-link" target="_self" rel="noopener nofollow">Laplacian</a>  and a label signal , we can define the smoothness as
If considering the <a data-tooltip-position="top" aria-label="Graph Fourier Transform" data-href="Graph Fourier Transform" href="the-guide/mathematics/graph-theory/graph-fourier-transform.html" class="internal-link" target="_self" rel="noopener nofollow">GFT</a>, the smoothness can be computed using the frequency representation  via 
<br>Intuition for Laplacian-based Smoothness

<br>Summand only non-zero if one point belongs to a class  and its neighbours do not.
<br>The larger , the closer points of different classes are (difficult classification)

<br>If  then there are no connections between points in class  and points of other classes



<br>Intuition for GFT Computation

<br><a data-tooltip-position="top" aria-label="Eigenvalue Problem" data-href="Eigenvalue Problem" href="the-guide/mathematics/linear-algebra/eigenvalue-problem.html" class="internal-link" target="_self" rel="noopener nofollow">Eigenvalues</a> are non-negative and increase with 
<br>For smoother signals, most energy is in the low frequencies

<br><br><br>In a <a data-tooltip-position="top" aria-label="- Machine Learning -" data-href="- Machine Learning -" href="the-guide/machine-learning/-machine-learning-.html" class="internal-link" target="_self" rel="noopener nofollow">semi-supervised learning</a> setting, only part of the given data is labeled.<br>GFT in Lable Propagation<br>
<br>Construct similarity graph using all available data points, e.g. using active learning to identify and label a subset  of the data. This results in 

<br>Data points  labeled
<br>Data points  unlabeled
<br>All data points collected in signal 


<br>Use penalty term based on <a data-tooltip-position="top" aria-label="Graph Laplacians" data-href="Graph Laplacians" href="the-guide/mathematics/graph-theory/graph-laplacians.html" class="internal-link" target="_self" rel="noopener nofollow">Laplacian</a>, as  will be high if energy in the high frequencies

<br>Can be extended by using ideal high pass filters, which only penalize if energy above certain frequency


<br>Low Pass Interpolation Techniques

<br>Desired signal is expected to only have energy in the first K frequencies of the <a data-tooltip-position="top" aria-label="Graph Fourier Transform" data-href="Graph Fourier Transform" href="the-guide/mathematics/graph-theory/graph-fourier-transform.html" class="internal-link" target="_self" rel="noopener nofollow">GFT</a>
<br>Use  as penalty term, where  consists of the last  columns of  (high frequencys)
<br>Use low frequency reconstruction techniques


<br><br><br>In an <a data-tooltip-position="top" aria-label="- Machine Learning -" data-href="- Machine Learning -" href="the-guide/machine-learning/-machine-learning-.html" class="internal-link" target="_self" rel="noopener nofollow">unsupervised learning</a> setting, the given data is missing labels.<br>
<br>Similiarity graph can be used to identify data clusters
<br>k-Means clustering

<br>We want to group a set  of vectors  into clusters  with  for 
<br>Algorithm

<br>Assign each vector to its closest cluster centroid
<br>Compute new centroid for each  via 




]]></description><link>the-guide/machine-learning/graph-based-learning/gsp-in-machine-learning.html</link><guid isPermaLink="false">The Guide/Machine Learning/Graph-Based Learning/GSP in Machine Learning.md</guid><pubDate>Sun, 06 Apr 2025 19:02:18 GMT</pubDate></item><item><title><![CDATA[- Imitation Learning -]]></title><description><![CDATA[ 
 <br>In a Nutshell
Supervised learning approach in which an agent learns to replicate the observed actions of an expert <a data-tooltip-position="top" aria-label="Policy" data-href="Policy" href="the-guide/machine-learning/reinforcement-learning/policy.html" class="internal-link" target="_self" rel="noopener nofollow">policy</a>, given the state.
<br><img alt="center" src="lib/media/pasted-image-20230223110118.png" style="width: 300px; max-width: 100%;"><br><br><br>Imitation learning can be categorized into the following approaches:<br>
<br><a data-tooltip-position="top" aria-label="Behavioral Cloning" data-href="Behavioral Cloning" href="the-guide/machine-learning/imitation-learning/behavioral-cloning.html" class="internal-link" target="_self" rel="noopener nofollow">Behavioral Cloning</a> (BC):

<br>A supervised learning approach where the agent learns to mimic the expert's actions directly from state-action pairs.


<br><a data-tooltip-position="top" aria-label="Inverse Reinforcement Learning" data-href="Inverse Reinforcement Learning" href="the-guide/machine-learning/imitation-learning/inverse-reinforcement-learning.html" class="internal-link" target="_self" rel="noopener nofollow">Inverse Reinforcement Learning</a> (IRL):

<br>Focuses on inferring the underlying reward function from expert demonstrations.


<br><a data-tooltip-position="top" aria-label="Adversarial Imitation Learning" data-href="Adversarial Imitation Learning" href="the-guide/machine-learning/imitation-learning/adversarial-imitation-learning.html" class="internal-link" target="_self" rel="noopener nofollow">Adversarial Imitation Learning</a> (AIL)

<br>Combines concepts from generative adversarial networks (GANs) and imitation learning.        
<br>The agent's policy is trained to generate trajectories indistinguishable from those of the expert, using a discriminator to evaluate similarity.


]]></description><link>the-guide/machine-learning/imitation-learning/-imitation-learning-.html</link><guid isPermaLink="false">The Guide/Machine Learning/Imitation Learning/- Imitation Learning -.md</guid><pubDate>Sat, 18 Jan 2025 21:04:12 GMT</pubDate><enclosure url="lib/media/pasted-image-20230223110118.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src=&quot;lib/media/pasted-image-20230223110118.png&quot;&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[Adversarial Imitation Learning]]></title><description><![CDATA[ 
 <br>
<br>Distribution Matching 

<br>Adversarial Imitation Learning

<br>Minimize the occupancy measure

<br>
<br>
<br>Try to match probability of reaching a state when employing a policy






]]></description><link>the-guide/machine-learning/imitation-learning/adversarial-imitation-learning.html</link><guid isPermaLink="false">The Guide/Machine Learning/Imitation Learning/Adversarial Imitation Learning.md</guid><pubDate>Fri, 17 Jan 2025 13:40:44 GMT</pubDate></item><item><title><![CDATA[Behavioral Cloning]]></title><description><![CDATA[ 
 <br>In a Nutshell
Simplest approach to <a data-tooltip-position="top" aria-label="- Imitation Learning -" data-href="- Imitation Learning -" href="the-guide/machine-learning/imitation-learning/-imitation-learning-.html" class="internal-link" target="_self" rel="noopener nofollow">imitation learning</a> based on <a data-tooltip-position="top" aria-label="- Machine Learning -" data-href="- Machine Learning -" href="the-guide/machine-learning/-machine-learning-.html" class="internal-link" target="_self" rel="noopener nofollow">supervised learning</a>. An agent learns to replicate the observed actions of an expert policy, given the state.
<br><br>Generally, a dataset is collected from an expert, yielding demonstrations . It is assumed that the expert follows a desirable expert policy . The goal is to train any machine learning model (mostly <a data-tooltip-position="top" aria-label="Artificial Neural Network" data-href="Artificial Neural Network" href="the-guide/machine-learning/deep-learning/artificial-neural-network.html" class="internal-link" target="_self" rel="noopener nofollow">neural networks</a> and more advanced architectures) to get as close as possible to the experts behavior, yielding<br>Advantages

<br>Simplicity: BC is straightforward to implement as it relies purely on supervised learning techniques.
<br>No Assumptions on the Environment: BC does not require explicit knowledge of the environment dynamics, making it model-agnostic.

<br>Disadvantages / Problems

<br>High Data Requirements: BC often requires a large number of demonstrations with high variability to learn effectively.
<br>Generalization Issues: Policies learned via BC may struggle to generalize to states not present in the training data.
<br>Covariate Shift:

<br>During training, the agent observes states sampled from the expert's trajectory. However, during execution, the agent's own policy determines the trajectory. As a result, the agent may encounter states outside the training distribution.
<br>This issue can cause compounding errors since the agent is not trained to recover from unseen or erroneous states.



<br><br><br>DAGGER (Dataset Aggregation)
Key Idea: Allow the agent to ask for new demonstrations in states it encounters during its own rollouts.
<br>
<br>Procedure:

<br>Train an initial policy  on the expert demonstrations.
<br>During the agent's rollouts, query the expert for the correct action in visited states.
<br>Aggregate the newly collected state-action pairs with the existing dataset.
<br>Re-train the policy on the updated dataset.


<br>DART (Disturbances for Robust Training)
Key Idea: Inject noise into the expert demonstrations to mimic potential errors or variability in execution.
<br>
<br>Procedure:

<br>Add stochastic perturbations to the expert’s actions in the demonstration data.
<br>Train the policy on the noisy demonstrations.


]]></description><link>the-guide/machine-learning/imitation-learning/behavioral-cloning.html</link><guid isPermaLink="false">The Guide/Machine Learning/Imitation Learning/Behavioral Cloning.md</guid><pubDate>Thu, 17 Apr 2025 07:39:47 GMT</pubDate></item><item><title><![CDATA[Inverse Reinforcement Learning]]></title><description><![CDATA[ 
 <br>
<br>Inverse <a data-href="- Reinforcement Learning -" href="the-guide/machine-learning/reinforcement-learning/-reinforcement-learning-.html" class="internal-link" target="_self" rel="noopener nofollow">- Reinforcement Learning -</a>

<br>Recover reward function  that explains observed execution traces / teachers behaviour

<br>


<br>Advantages

<br>Easy to transfer to new tasks
<br>Compact description


<br>Problems

<br>Requires solving a <a data-tooltip-position="top" aria-label="Markov Decision Process" data-href="Markov Decision Process" href="the-guide/machine-learning/reinforcement-learning/markov-decision-process.html" class="internal-link" target="_self" rel="noopener nofollow">MDP</a>

<br>Hard for high-DOF settings, many constraints
<br>Ill-posed, zero reward is a solution


<br>Assumes optimal expert
<br>Assumes we can enumerate all policies
<br>Limited Data, only traces of expert are available, not entire policy


<br>Feature Based Reward Function

<br>Assume reward function is linear in features 

<br>Find  s.t. 


<br>Use sample trajectories to estimate feature expectations, resolves access to entire policy problem

<br>Demonstrations scales with number of features


<br><img alt="center" src="lib/media/pasted-image-20230223121649.png" style="width: 300px; max-width: 100%;">
<br>Max-Margin IRL

<br>Structured Max-Margin

<br>Add reward margin to problem to deal with ill-posedness
<br>

<br>s.t. 

<br>Margin larger for policies that are very different






<br>Expert Suboptimality

<br>

<br>s.t. 
<br>Minimize total violation, every constraint can be violated a bit




<br>Constraint Generation

<br>Only constrain on policies 
<br>Iteratively pick out most violating policy and adapt policy






<br>Maximum Entropy Approach  statistical modeling with least commitment

<br>Maximum Entropy IRL

<br>Maximize entropy of trajectories

<br>
<br>s.t. 


<br>Does not take system dynamics into account, trajectory with large return can be highly unlikely


<br>Maximum Causal Entropy IRL

<br>Additional constraints to be consistent with system dynamics






]]></description><link>the-guide/machine-learning/imitation-learning/inverse-reinforcement-learning.html</link><guid isPermaLink="false">The Guide/Machine Learning/Imitation Learning/Inverse Reinforcement Learning.md</guid><pubDate>Thu, 20 Feb 2025 16:36:30 GMT</pubDate><enclosure url="lib/media/pasted-image-20230223121649.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src=&quot;lib/media/pasted-image-20230223121649.png&quot;&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[Attention Masking in Transformers]]></title><description><![CDATA[ 
 <br>In a Nutshell
Methods used to hide parts of the input an LLM can see during processing. 
<br><br>For example, during training of an autoregressive <a data-tooltip-position="top" aria-label="Transformer" data-href="Transformer" href="the-guide/machine-learning/large-language-models/transformer.html" class="internal-link" target="_self" rel="noopener nofollow">transformer</a>, we can feed in a whole sequence and hide future tokens from the model using masking. This makes training more efficient, see <a data-tooltip-position="top" aria-label="Training Sequence Models" data-href="Training Sequence Models" href="the-guide/machine-learning/deep-learning/training-sequence-models.html" class="internal-link" target="_self" rel="noopener nofollow">Teacher Forcing</a>. To present the autoregressive nature, we need to hide future tokens during the processing. This can be achieved via masking:<br>Attention Computation with Masking
The <a data-tooltip-position="top" aria-label="Attention Mechanisms" data-href="Attention Mechanisms" href="the-guide/machine-learning/large-language-models/attention-mechanisms.html" class="internal-link" target="_self" rel="noopener nofollow">attention output</a> is computed as:where  is the dimensionality of the key <a data-tooltip-position="top" aria-label="Vector Space" data-href="Vector Space" href="the-guide/mathematics/general-stuff/vector-space.html" class="internal-link" target="_self" rel="noopener nofollow">vectors</a> and  is the mask <a data-tooltip-position="top" aria-label="Matrices" data-href="Matrices" href="the-guide/mathematics/linear-algebra/matrices.html" class="internal-link" target="_self" rel="noopener nofollow">matrix</a> used to control the attention via ...

<br>Padding Mask: For positions corresponding to padding tokens, set  so that their contributions are effectively ignored.
<br>Causal Masking (Autoregressive Models): To prevent a token at position  from attending to future tokens (), define:This ensures that each token's prediction is based solely on previous tokens, preserving the autoregressive property.

]]></description><link>the-guide/machine-learning/large-language-models/attention-masking-in-transformers.html</link><guid isPermaLink="false">The Guide/Machine Learning/Large Language Models/Attention Masking in Transformers.md</guid><pubDate>Sat, 29 Mar 2025 19:12:55 GMT</pubDate></item><item><title><![CDATA[Attention Mechanisms]]></title><description><![CDATA[ 
 <br>In a Nutshell
Mechanisms to weight inputs to a model, usually <a data-tooltip-position="top" aria-label="Large Language Models" data-href="Large Language Models" href="the-guide/machine-learning/large-language-models/large-language-models.html" class="internal-link" target="_self" rel="noopener nofollow">LLMs</a> by relevance.
<br><br><br>Most common form of self-attention. Given an input sequence , use learnable linear projections into queries, keys and values to compute the attention score The term  computes pairwise similarities, allowing the model to attend do important parts of the input. The scaling factor is used to prevent the similarities from growing too large in magnitude, which would yield vanishing gradients for the <a data-tooltip-position="top" aria-label="Activation Functions" data-href="Activation Functions" href="the-guide/machine-learning/deep-learning/activation-functions.html" class="internal-link" target="_self" rel="noopener nofollow">softmax</a>.<br>
The <a data-tooltip-position="top" aria-label="Activation Functions" data-href="Activation Functions" href="the-guide/machine-learning/deep-learning/activation-functions.html" class="internal-link" target="_self" rel="noopener nofollow">softmax</a> <a data-tooltip-position="top" aria-label="Mathematical Relations" data-href="Mathematical Relations" href="the-guide/mathematics/general-stuff/mathematical-relations.html" class="internal-link" target="_self" rel="noopener nofollow">function</a> is used to generate a <a data-tooltip-position="top" aria-label="Probability Distribution" data-href="Probability Distribution" href="the-guide/mathematics/probability-theory/probability-distribution.html" class="internal-link" target="_self" rel="noopener nofollow">probability distribution</a> from logits. <br>Additional Information
During training, the model is provided full sequences (see <a data-tooltip-position="top" aria-label="Training Sequence Models" data-href="Training Sequence Models" href="the-guide/machine-learning/deep-learning/training-sequence-models.html" class="internal-link" target="_self" rel="noopener nofollow">Teacher Forcing</a>). Since later tokens in a sentence should not influence earlier ones, <a data-tooltip-position="top" aria-label="Attention Masking in Transformers" data-href="Attention Masking in Transformers" href="the-guide/machine-learning/large-language-models/attention-masking-in-transformers.html" class="internal-link" target="_self" rel="noopener nofollow">attention masking</a> is used to force all values under the diagonal to zero by setting those values to  before applying the <a data-tooltip-position="top" aria-label="Activation Functions" data-href="Activation Functions" href="the-guide/machine-learning/deep-learning/activation-functions.html" class="internal-link" target="_self" rel="noopener nofollow">softmax</a>.
<br><br><br>Instead of using a single attention mechanism like above, we can run several heads in parallel. For each input , project into lower dimensionalto compute the attention score for each head  in parallel. <br>The outputs are then concatenated and projected back into a common <a data-tooltip-position="top" aria-label="Latent Variable Models" data-href="Latent Variable Models" href="the-guide/machine-learning/generative-models/latent-variable-models.html" class="internal-link" target="_self" rel="noopener nofollow">latent</a> <a data-tooltip-position="top" aria-label="Space" data-href="Space" href="the-guide/mathematics/general-stuff/space.html" class="internal-link" target="_self" rel="noopener nofollow">space</a>where  is again learned. <br>Intuition
Gives the model the ability to shift context based on the surrounding input it is given with by updating the embedding of each token.
<br><br><br>Extends the above methods to also learn local coherence / attention based on attention history, i.e. in speech or handwriting recognition. <br>When computing the attention weights, we get a vector  which encodes how much each position contributes to the query. We can use a history of these weights and apply a learnable filterto capture local patterns and shifts. To compute the activation at the current step, we combine the current decoder state, the encoder output and the location features  in an energy score, e.g. ]]></description><link>the-guide/machine-learning/large-language-models/attention-mechanisms.html</link><guid isPermaLink="false">The Guide/Machine Learning/Large Language Models/Attention Mechanisms.md</guid><pubDate>Sun, 30 Mar 2025 15:34:19 GMT</pubDate></item><item><title><![CDATA[GPTs - Generative Pretrained Transformers]]></title><description><![CDATA[ 
 <br>In a Nutshell
Type of <a data-tooltip-position="top" aria-label="Large Language Models" data-href="Large Language Models" href="the-guide/machine-learning/large-language-models/large-language-models.html" class="internal-link" target="_self" rel="noopener nofollow">LLM</a> models based on the <a data-tooltip-position="top" aria-label="Transformer" data-href="Transformer" href="the-guide/machine-learning/large-language-models/transformer.html" class="internal-link" target="_self" rel="noopener nofollow">transformer</a> architecture. Usually pre-trained on large data sets of unlabeled text.
<br><br>GPTs are a specific realization of a large language model based on the more genereal <a data-href="Transformer" href="the-guide/machine-learning/large-language-models/transformer.html" class="internal-link" target="_self" rel="noopener nofollow">Transformer</a> architecture of <a data-tooltip-position="top" aria-label="Artificial Neural Network" data-href="Artificial Neural Network" href="the-guide/machine-learning/deep-learning/artificial-neural-network.html" class="internal-link" target="_self" rel="noopener nofollow">neural networks</a>.<br><br><br>
<br>Pretraining: They are first trained on massive amounts of unlabeled text data using an autoregressive <a data-tooltip-position="top" aria-label="Generative Models" data-href="Generative Models" href="the-guide/machine-learning/generative-models/generative-models.html" class="internal-link" target="_self" rel="noopener nofollow">generative</a> objective. During this phase, they learn language patterns, grammar, facts about the world, and even some reasoning abilities.
<br>Fine-tuning: After pretraining, GPTs can be fine-tuned on smaller, task-specific datasets to improve performance on particular applications like translation, summarization, or question-answering.
]]></description><link>the-guide/machine-learning/large-language-models/gpts-generative-pretrained-transformers.html</link><guid isPermaLink="false">The Guide/Machine Learning/Large Language Models/GPTs - Generative Pretrained Transformers.md</guid><pubDate>Sat, 05 Apr 2025 13:22:00 GMT</pubDate></item><item><title><![CDATA[Large Language Models]]></title><description><![CDATA[<a class="tag" href="?query=tag:Deep-Learning" style="background-color: rgb(4, 108, 116); color: white; font-weight: 700; border: none; border-radius: 1em; padding: 0.2em 0.5em;">#Deep-Learning</a> 
 <br>In a Nutshell
Computational model designed for processing human language, commonly built using <a href=".?query=tag:Deep-Learning" class="tag" target="_blank" rel="noopener nofollow">#Deep-Learning</a>  techniques, particularly <a data-tooltip-position="top" aria-label="Artificial Neural Network" data-href="Artificial Neural Network" href="the-guide/machine-learning/deep-learning/artificial-neural-network.html" class="internal-link" target="_self" rel="noopener nofollow">neural networks</a>, and trained on vast amounts of textual data.
<br>For great visuals, see <a rel="noopener nofollow" class="external-link" href="https://bbycroft.net/llm" target="_blank">https://bbycroft.net/llm</a><br><br><br><a data-href="GPTs - Generative Pretrained Transformers" href="the-guide/machine-learning/large-language-models/gpts-generative-pretrained-transformers.html" class="internal-link" target="_self" rel="noopener nofollow">GPTs - Generative Pretrained Transformers</a><br>
<br><a data-href="Transformer" href="the-guide/machine-learning/large-language-models/transformer.html" class="internal-link" target="_self" rel="noopener nofollow">Transformer</a> based with encoder-decoder 
<br>BERT - Bidirectional Encoder Representations from Transformers<br>
<br>...
]]></description><link>the-guide/machine-learning/large-language-models/large-language-models.html</link><guid isPermaLink="false">The Guide/Machine Learning/Large Language Models/Large Language Models.md</guid><pubDate>Mon, 07 Apr 2025 10:00:08 GMT</pubDate></item><item><title><![CDATA[Positional Encoding]]></title><description><![CDATA[ 
 <br>In a Nutshell
Way to imprint information about word order into <a data-href="Large Language Models" href="the-guide/machine-learning/large-language-models/large-language-models.html" class="internal-link" target="_self" rel="noopener nofollow">Large Language Models</a>.
<br><br>Since <a data-tooltip-position="top" aria-label="Large Language Models" data-href="Large Language Models" href="the-guide/machine-learning/large-language-models/large-language-models.html" class="internal-link" target="_self" rel="noopener nofollow">LLMs</a> are based on vector-valued <a data-tooltip-position="top" aria-label="Word Embedding" data-href="Word Embedding" href="the-guide/machine-learning/large-language-models/word-embedding.html" class="internal-link" target="_self" rel="noopener nofollow">word embeddings</a>, information about word order is initially lost. Therefore, it is common practice to imprint these information via a positional encoding model.<br>
<img alt="center" src="lib/media/pasted-image-20250316180827.png" style="width: 400px; max-width: 100%;"><br>Naturally, such a model should have the following properties:<br>
<br>Unique encoding for each position regardless of sequence length. A token at position 5 should have the same encoding in a sequence of length  and 
<br>Linearity
<br>Generalization to any sequence length
<br>Deterministic
<br>Extensible to higher dimensions - should also work for higher dimensional inputs, e.g. for multimodal models
<br><br><br>The first intuitive idea would be to simply add the position as an integer. However, this creates problems with signal-to-noise, because the values of the positional embedding can be of different magnitude than those of the <a data-tooltip-position="top" aria-label="Word Embedding" data-href="Word Embedding" href="the-guide/machine-learning/large-language-models/word-embedding.html" class="internal-link" target="_self" rel="noopener nofollow">embedding</a>. <br>Normalizing the positional encoding resolves this, but now we are no longer independent of sequence length as demanded by property 3.<br>Using a binary number system instead of a decimal gives the desired poperties, but as it is naturally not <a data-tooltip-position="top" aria-label="Continuity" data-href="Continuity" href="the-guide/mathematics/analysis-and-calculus/continuity.html" class="internal-link" target="_self" rel="noopener nofollow">continuous</a>, this has negative effects on the optimization process.<br><img alt="BinaryPositionalEncodingPlot-ezgif.com-optimize.gif" src="lib/media/binarypositionalencodingplot-ezgif.com-optimize.gif"><br><br><br>As a natural continuous version of the binary encoding, we can use sine and cosine functions instead. The famous Attention is all you need paper introduced encodings based on the index via where  is the position of the token,  is the index in the positional encoding vector and  is the model dimension.<br><img alt="SteppedPositionalEncodingPlot-ezgif.com-optimize.gif" src="lib/media/steppedpositionalencodingplot-ezgif.com-optimize.gif"><br><br><br>While the above already works well in practice, rotary encodings provide a way to encode relative instead of global position for each token efficiently while also scaling better in higher dimensions. This is achieved by rotating the <a data-tooltip-position="top" aria-label="Attention Mechanisms" data-href="Attention Mechanisms" href="the-guide/machine-learning/large-language-models/attention-mechanisms.html" class="internal-link" target="_self" rel="noopener nofollow">query and key vectors in the attention mechanism</a>, affecting the <a data-tooltip-position="top" aria-label="Inner Product Space and Hilbert Space" data-href="Inner Product Space and Hilbert Space" href="the-guide/mathematics/functional-analysis-and-calculus-of-variations/inner-product-space-and-hilbert-space.html" class="internal-link" target="_self" rel="noopener nofollow">dot product</a>For a token at position  with an <a data-tooltip-position="top" aria-label="Word Embedding" data-href="Word Embedding" href="the-guide/machine-learning/large-language-models/word-embedding.html" class="internal-link" target="_self" rel="noopener nofollow">embedding vector</a> , we first split the vector into  pairs:<br><br>For each pair , define an angle:<br><br>Then, the rotation matrix for the -th pair is:<br><br>The rotated components for the -th pair are computed as:<br><br>This rotation is applied element-wise to both the query and key vectors in the attention mechanism. When computing the dot product between a rotated query  and key , the phase differences encode the relative positional information between tokens.]]></description><link>the-guide/machine-learning/large-language-models/positional-encoding.html</link><guid isPermaLink="false">The Guide/Machine Learning/Large Language Models/Positional Encoding.md</guid><pubDate>Sat, 29 Mar 2025 17:11:39 GMT</pubDate><enclosure url="lib/media/pasted-image-20250316180827.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src=&quot;lib/media/pasted-image-20250316180827.png&quot;&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[Tokenization]]></title><description><![CDATA[ 
 ]]></description><link>the-guide/machine-learning/large-language-models/tokenization.html</link><guid isPermaLink="false">The Guide/Machine Learning/Large Language Models/Tokenization.md</guid><pubDate>Sat, 29 Mar 2025 17:11:28 GMT</pubDate></item><item><title><![CDATA[Transformer]]></title><description><![CDATA[<a class="tag" href="?query=tag:Machine-Learning" style="background-color: rgb(4, 108, 116); color: white; font-weight: 700; border: none; border-radius: 1em; padding: 0.2em 0.5em;">#Machine-Learning</a> 
 <br>In a Nutshell
Transformer models, introduced in Vaswani et al.'s 2017 paper "Attention Is All You Need," revolutionized NLP and other <a href=".?query=tag:Machine-Learning" class="tag" target="_blank" rel="noopener nofollow">#Machine-Learning</a> areas by enabling parallelized processing and capturing long-range dependencies effectively.
<br><br><br>
<br><a data-tooltip-position="top" aria-label="Tokenization" data-href="Tokenization" href="the-guide/machine-learning/large-language-models/tokenization.html" class="internal-link" target="_self" rel="noopener nofollow">Tokenizers</a>, which convert text into tokens.
<br>Embedding layer, which converts tokens (see <a data-href="Word Embedding" href="the-guide/machine-learning/large-language-models/word-embedding.html" class="internal-link" target="_self" rel="noopener nofollow">Word Embedding</a>) and positions of the tokens (see <a data-href="Positional Encoding" href="the-guide/machine-learning/large-language-models/positional-encoding.html" class="internal-link" target="_self" rel="noopener nofollow">Positional Encoding</a>) into vector representations.
<br>Transformer layers, which carry out repeated transformations on the vector representations, extracting more and more linguistic information. These consist of alternating attention and feedforward layers. There are two major types of transformer layers: encoder layers and decoder layers, with further variants.
<br>Un-embedding layer, which converts the final vector representations back to a <a data-href="Probability Distribution" href="the-guide/mathematics/probability-theory/probability-distribution.html" class="internal-link" target="_self" rel="noopener nofollow">Probability Distribution</a> over the tokens.
<br><img alt="center" src="lib/media/pasted-image-20250330180455.png" style="width: 400px; max-width: 100%;"><br><br>Why was this so revolutionary ?
Transformer replace sequential recurrence of <a data-tooltip-position="top" aria-label="Recurrent Neural Network" data-href="Recurrent Neural Network" href="the-guide/machine-learning/deep-learning/recurrent-neural-network.html" class="internal-link" target="_self" rel="noopener nofollow">RNNs</a> with parallelizable attention, enabling efficient training on long sequences  (more efficient and avoids vanishing <a data-tooltip-position="top" aria-label="Derivative, Gradient, Jacobian and Hessian" data-href="Derivative, Gradient, Jacobian and Hessian" href="the-guide/mathematics/analysis-and-calculus/derivative,-gradient,-jacobian-and-hessian.html" class="internal-link" target="_self" rel="noopener nofollow">gradient</a>).
Additionally, each token can dynamically attend to relevant context at multiple abstraction levels (via multi-head attention).
<br><br><br>Encoder and decoder differ in their input, the first takes the prompt while the decoder uses previously generated tokens (or targets during training). Additionally, the decoder uses the output of the encoder as context and has to apply masking.<br><br>Input Embeddings and Positional Encoding
The initial input is a <a data-tooltip-position="top" aria-label="Set" data-href="Set" href="the-guide/mathematics/general-stuff/set.html" class="internal-link" target="_self" rel="noopener nofollow">set</a> of  tokens , each of which is converted to a -dimensional <a data-tooltip-position="top" aria-label="Vector Space" data-href="Vector Space" href="the-guide/mathematics/general-stuff/vector-space.html" class="internal-link" target="_self" rel="noopener nofollow">vector</a> embeddingsee <a data-href="Word Embedding" href="the-guide/machine-learning/large-language-models/word-embedding.html" class="internal-link" target="_self" rel="noopener nofollow">Word Embedding</a> and <a data-href="Positional Encoding" href="the-guide/machine-learning/large-language-models/positional-encoding.html" class="internal-link" target="_self" rel="noopener nofollow">Positional Encoding</a> for more information. The input to the model is then
<br>Multi-Headed Self-Attention
For each input , we compute the attention score for each head  in parallel. For detailed information, see <a data-href="Attention Mechanisms" href="the-guide/machine-learning/large-language-models/attention-mechanisms.html" class="internal-link" target="_self" rel="noopener nofollow">Attention Mechanisms</a>.<br>
The outputs are then concatenated and projected back into a common <a data-tooltip-position="top" aria-label="Latent Variable Models" data-href="Latent Variable Models" href="the-guide/machine-learning/generative-models/latent-variable-models.html" class="internal-link" target="_self" rel="noopener nofollow">latent</a> <a data-tooltip-position="top" aria-label="Space" data-href="Space" href="the-guide/mathematics/general-stuff/space.html" class="internal-link" target="_self" rel="noopener nofollow">space</a>
<br>Feed-Forward Layers
Each output vector of the multi-headed attention layer undergoes a standard feed-forward transformationwith , .
<br>Residual Connections and Layer Norms
Each layer, consisting of one attention process followed by a couple feed-forward layers has a residual connection and layer normalizationwherewith <a data-tooltip-position="top" aria-label="Expectations" data-href="Expectations" href="the-guide/mathematics/statistics/expectations.html" class="internal-link" target="_self" rel="noopener nofollow">mean</a> , <a data-tooltip-position="top" aria-label="Covariance and Variance" data-href="Covariance and Variance" href="the-guide/mathematics/statistics/covariance-and-variance.html" class="internal-link" target="_self" rel="noopener nofollow">variance</a>  and tunable parameters .
<br><br>Input Embeddings and Positional Encoding
The initial input is a <a data-tooltip-position="top" aria-label="Set" data-href="Set" href="the-guide/mathematics/general-stuff/set.html" class="internal-link" target="_self" rel="noopener nofollow">set</a> of  previously generated or target tokens , each of which is converted to a -dimensional <a data-tooltip-position="top" aria-label="Vector Space" data-href="Vector Space" href="the-guide/mathematics/general-stuff/vector-space.html" class="internal-link" target="_self" rel="noopener nofollow">vector</a> embeddingsee <a data-href="Word Embedding" href="the-guide/machine-learning/large-language-models/word-embedding.html" class="internal-link" target="_self" rel="noopener nofollow">Word Embedding</a> and <a data-href="Positional Encoding" href="the-guide/machine-learning/large-language-models/positional-encoding.html" class="internal-link" target="_self" rel="noopener nofollow">Positional Encoding</a> for more information. The input to the model is then
<br>ToDo --- Finalize the following<br>
<br>
Multi-Headed Self-Attention is the same
Multi-Headed Cross-Attention
For each input , we compute the attention score for each head  in parallel. For detailed information, see <a data-href="Attention Mechanisms" href="the-guide/machine-learning/large-language-models/attention-mechanisms.html" class="internal-link" target="_self" rel="noopener nofollow">Attention Mechanisms</a>.<br>
The outputs are then concatenated and projected back into a common <a data-tooltip-position="top" aria-label="Latent Variable Models" data-href="Latent Variable Models" href="the-guide/machine-learning/generative-models/latent-variable-models.html" class="internal-link" target="_self" rel="noopener nofollow">latent</a> <a data-tooltip-position="top" aria-label="Space" data-href="Space" href="the-guide/mathematics/general-stuff/space.html" class="internal-link" target="_self" rel="noopener nofollow">space</a>


<br>
Multi-Headed cross-attention 

<br>Use Keys and values from encoder, queries from decoder
<br>Use Masking during training


<br>
Linear, Resicual and Norm are the same

<br>ToDo --- Finalize above<br><br>Output Layer
The decoder output has to be projected back into the vocabulary space of dimension  viawhere .  The next <a data-tooltip-position="top" aria-label="Tokenization" data-href="Tokenization" href="the-guide/machine-learning/large-language-models/tokenization.html" class="internal-link" target="_self" rel="noopener nofollow">token</a> can then be chosen via <a data-tooltip-position="top" aria-label="Maximum Likelihood Estimator" data-href="Maximum Likelihood Estimator" href="the-guide/mathematics/statistics/maximum-likelihood-estimator.html" class="internal-link" target="_self" rel="noopener nofollow">maximum likelihood</a>, beam search or sampling.
<br><br><br>During training, the encoder processes the full input once and the decoder uses <a data-tooltip-position="top" aria-label="Training Sequence Models" data-href="Training Sequence Models" href="the-guide/machine-learning/deep-learning/training-sequence-models.html" class="internal-link" target="_self" rel="noopener nofollow">teacher forcing</a>—receiving the actual target sequence (shifted right) to generate its outputs.<br>
During inference, the encoder remains the same, but the decoder generates tokens one by one, feeding its own previous predictions back as input for the next step.<br>Objective<br>
To train the above model, we can compare the predicted probabilities  with true target tokens  via the <a data-tooltip-position="top" aria-label="Cross Entropy" data-href="Cross Entropy" href="the-guide/information-theory/information-theory-1/information/cross-entropy.html" class="internal-link" target="_self" rel="noopener nofollow">cross-entropy</a>]]></description><link>the-guide/machine-learning/large-language-models/transformer.html</link><guid isPermaLink="false">The Guide/Machine Learning/Large Language Models/Transformer.md</guid><pubDate>Wed, 23 Apr 2025 21:55:34 GMT</pubDate><enclosure url="lib/media/pasted-image-20250330180455.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src=&quot;lib/media/pasted-image-20250330180455.png&quot;&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[Word Embedding]]></title><description><![CDATA[ 
 <br>In a Nutshell
Different approaches to obtain abstract representations of words as <a data-tooltip-position="top" aria-label="Field" data-href="Field" href="the-guide/mathematics/general-stuff/field.html" class="internal-link" target="_self" rel="noopener nofollow">real-valued</a> <a data-tooltip-position="top" aria-label="Vector Space" data-href="Vector Space" href="the-guide/mathematics/general-stuff/vector-space.html" class="internal-link" target="_self" rel="noopener nofollow">vectors</a>.
<br><br><br>One-Hot Encoding
For a vocabulary of size , represent each word as a vector , wherewhere  is the fixed index of each word. 
<br>Problems

<br>Requires high dimensional space
<br>No semantic (similarity) information, all vectors orthogonal

<br>Bag of Words
A full document is represented as a vector, where each component represents the frequency of that word in the document
<br>Problems

<br>High dimensions
<br>Ignores order and semantics

<br>Term Frequency-Inverse Document Frequency
Reweights BoW to downweight common words.
<br><br><br>
<br>Word2Vec
<br>Continuous Bag of Words
<br><br><br>
<br>GloVe
<br>BERT
]]></description><link>the-guide/machine-learning/large-language-models/word-embedding.html</link><guid isPermaLink="false">The Guide/Machine Learning/Large Language Models/Word Embedding.md</guid><pubDate>Sat, 29 Mar 2025 17:44:04 GMT</pubDate></item><item><title><![CDATA[(Deep) Q-Learning]]></title><description><![CDATA[ 
 <br>In a Nutshell
Class of <a data-tooltip-position="top" aria-label="Model-Free Reinforcement Learning" data-href="Model-Free Reinforcement Learning" href="the-guide/machine-learning/reinforcement-learning/model-free-reinforcement-learning.html" class="internal-link" target="_self" rel="noopener nofollow">off-policy</a> and <a data-tooltip-position="top" aria-label="- Reinforcement Learning -" data-href="- Reinforcement Learning -" href="the-guide/machine-learning/reinforcement-learning/-reinforcement-learning-.html" class="internal-link" target="_self" rel="noopener nofollow">online RL</a> algorithms that learns the <a data-tooltip-position="top" aria-label="Q-Function" data-href="Q-Function" href="the-guide/machine-learning/reinforcement-learning/q-function.html" class="internal-link" target="_self" rel="noopener nofollow">Q-function</a> from experiences collected via a behavioral policy. The online estimation is then performed by another estimation policy, in most cases <a data-tooltip-position="top" aria-label="Exploration-Exploitation Trade-Off" data-href="Exploration-Exploitation Trade-Off" href="the-guide/machine-learning/reinforcement-learning/exploration-exploitation-trade-off.html" class="internal-link" target="_self" rel="noopener nofollow">epsilon-greedy</a>.
<br><br>When learning the Q-function via <a data-tooltip-position="top" aria-label="Temporal Difference Learning" data-href="Temporal Difference Learning" href="the-guide/machine-learning/reinforcement-learning/temporal-difference-learning.html" class="internal-link" target="_self" rel="noopener nofollow">TD</a> updates, how do we compute the following equations ?<br>
<br>
<br><br>
The problem is choosing the action  to estimate the future reward after taking our behavioral action.
<br>Q-Learning
Q-Learning uses a <a data-tooltip-position="top" aria-label="Temporal Difference Learning" data-href="Temporal Difference Learning" href="the-guide/machine-learning/reinforcement-learning/temporal-difference-learning.html" class="internal-link" target="_self" rel="noopener nofollow">TD</a> approach via

<br>Take action according to behavior <a data-tooltip-position="top" aria-label="Policy" data-href="Policy" href="the-guide/machine-learning/reinforcement-learning/policy.html" class="internal-link" target="_self" rel="noopener nofollow">policy</a> , e.g. <a data-tooltip-position="top" aria-label="Exploration-Exploitation Trade-Off" data-href="Exploration-Exploitation Trade-Off" href="the-guide/machine-learning/reinforcement-learning/exploration-exploitation-trade-off.html" class="internal-link" target="_self" rel="noopener nofollow">Epsilon-Greedy</a> 
<br>Estimate Q-function based on action from another greedy policy

<br>
<br>Leads to optimal Q function

<br>Allows to learn from sub-optimal experiences.
<br>Can be more sample efficient but less stable


<br>Curse of Dimensionality
Curse of Dimensionality makes standard methods of <a data-tooltip-position="top" aria-label="- Reinforcement Learning -" data-href="- Reinforcement Learning -" href="the-guide/machine-learning/reinforcement-learning/-reinforcement-learning-.html" class="internal-link" target="_self" rel="noopener nofollow">RL</a> infeasible in many settings. The above would require a lookup-table of dimension . If one of them is high dimensional or continuous, we need to learn a function instead.
<br><br>Deep Q-Learning (DQN)
Approximate the <a data-tooltip-position="top" aria-label="Q-Function" data-href="Q-Function" href="the-guide/machine-learning/reinforcement-learning/q-function.html" class="internal-link" target="_self" rel="noopener nofollow">state-action value-function</a> using a deep <a data-tooltip-position="top" aria-label="Artificial Neural Network" data-href="Artificial Neural Network" href="the-guide/machine-learning/deep-learning/artificial-neural-network.html" class="internal-link" target="_self" rel="noopener nofollow">neural network</a> with parameters :Minimize the loss 
<br>Problems

<br>Offline dataset unavailable due to complexity
<br>Loss therefore contains bootstrapping, off-policy and function approximation, see<a data-tooltip-position="top" aria-label="The Deadly Triad" data-href="The Deadly Triad" href="the-guide/machine-learning/reinforcement-learning/theorems/the-deadly-triad.html" class="internal-link" target="_self" rel="noopener nofollow">Deadly Triad</a>
<br>Data has to be collected online, can lead to catastrophic forgetting

<br><br><br>
<br>Replay Buffer

<br>Collect and store finite amount of past transitions to reuse them<img alt="center" src="lib/media/pasted-image-20230627121555.png" style="width: 150px; max-width: 100%;">
<br>Reduces negative impact of distribution shift.


<br>Target Network 

<br>Keep a copy  of the online <a data-tooltip-position="top" aria-label="Artificial Neural Network" data-href="Artificial Neural Network" href="the-guide/machine-learning/deep-learning/artificial-neural-network.html" class="internal-link" target="_self" rel="noopener nofollow">neural network</a> parameters and only update this second network every  steps. When computing the error, we can use both networks inThis can help to avoid instability due to function approximation.


<br>Minibatch Updates

<br>At each step uniformly sample a minibatch of  transitions from replay buffer 
<br>Improves efficiency


<br>Reward and Target Clipping

<br>Clip reward and error term between  and 

<br>Sufficient to use Huber Loss (less steep)<img alt="center" src="lib/media/pasted-image-20230627150159.png" style="width: 200px; max-width: 100%;">


<br>Improves stability


<br>Algorithm
Init replay buffer ,  with random weights, same for both networks

<br>while true do

<br>Sample action using epsilon greedy and 
<br>Execute action and observe reward and next state
<br>Store transition in 
<br>Sample minibatch of size  from  
<br>For every sample in minibatch

<br>Compute loss
<br>Perform gradient step


<br>Every  steps update 



<br>Problems
Above tricks can help, but come at the cost of DQN ...

<br>Requiring many samples
<br>Being highly sensitive to hyperparameter tuning
<br>Yielding high computation time

<br><br><br>
<br>Double Deep Q-Learning

<br>Maximum in squared error can lead to overestimation, can lead to sub-optimal performance
<br>Evaluate online network with arg max, use action in target network 


<br>Prioritized Replay Buffer

<br>Transitions in replay buffer that result in high <a data-tooltip-position="top" aria-label="Temporal Difference Learning" data-href="Temporal Difference Learning" href="the-guide/machine-learning/reinforcement-learning/temporal-difference-learning.html" class="internal-link" target="_self" rel="noopener nofollow">TD</a> error  are more informative, scale 

<br> with  to avoid degenerate case
<br> regulates prioritization ( is uniform sampling)
<br>Introduces <a data-tooltip-position="top" aria-label="Bias-Variance Tradeoff" data-href="Bias-Variance Tradeoff" href="the-guide/machine-learning/reinforcement-learning/theorems/bias-variance-tradeoff.html" class="internal-link" target="_self" rel="noopener nofollow">Bias</a>


<br>Correct bias using <a data-tooltip-position="top" aria-label="Importance Sampling" data-href="Importance Sampling" href="the-guide/mathematics/probability-theory/importance-sampling.html" class="internal-link" target="_self" rel="noopener nofollow">importance sampling</a>

<br>Last factor is normalization for stability
<br>If ,  cancels impact of sampling from replay buffer according to , but samples are still generated by drawing from 
<br>In practice start with small  and increase towards .




<br>Dueling DQN

<br>Split <a data-tooltip-position="top" aria-label="Q-Function" data-href="Q-Function" href="the-guide/machine-learning/reinforcement-learning/q-function.html" class="internal-link" target="_self" rel="noopener nofollow">Q-function</a> into <a data-tooltip-position="top" aria-label="Value Functions" data-href="Value Functions" href="the-guide/machine-learning/reinforcement-learning/value-functions.html" class="internal-link" target="_self" rel="noopener nofollow">value function</a> and <a data-tooltip-position="top" aria-label="Advantage Function" data-href="Advantage Function" href="the-guide/machine-learning/reinforcement-learning/advantage-function.html" class="internal-link" target="_self" rel="noopener nofollow">advantage function</a>
<br>...


<br>Noisy DQN

<br>Use NN with noisy parameters (in practice only at last layer) to enforce <a data-tooltip-position="top" aria-label="Exploration-Exploitation Trade-Off" data-href="Exploration-Exploitation Trade-Off" href="the-guide/machine-learning/reinforcement-learning/exploration-exploitation-trade-off.html" class="internal-link" target="_self" rel="noopener nofollow">exploration</a>where  are learnable and  is fixed zero-<a data-tooltip-position="top" aria-label="Expectations" data-href="Expectations" href="the-guide/mathematics/statistics/expectations.html" class="internal-link" target="_self" rel="noopener nofollow">mean</a> noise. This leads to an optimization target that is an <a data-tooltip-position="top" aria-label="Expectations" data-href="Expectations" href="the-guide/mathematics/statistics/expectations.html" class="internal-link" target="_self" rel="noopener nofollow">expectation</a> over the noise  


<br>Distributional DQN

<br>Instead of modelling <a data-tooltip-position="top" aria-label="Expectations" data-href="Expectations" href="the-guide/mathematics/statistics/expectations.html" class="internal-link" target="_self" rel="noopener nofollow">expected</a> return in <a data-tooltip-position="top" aria-label="Q-Function" data-href="Q-Function" href="the-guide/machine-learning/reinforcement-learning/q-function.html" class="internal-link" target="_self" rel="noopener nofollow">Q-function</a>, model whole return <a data-tooltip-position="top" aria-label="Probability Distribution" data-href="Probability Distribution" href="the-guide/mathematics/probability-theory/probability-distribution.html" class="internal-link" target="_self" rel="noopener nofollow">distribution</a> in distributional value function , describes randomness in environment
<br>Reward is random variable 
<br>Transition operator  can be applied to distributional value function, yielding distribution for next state when following <a data-tooltip-position="top" aria-label="Policy" data-href="Policy" href="the-guide/machine-learning/reinforcement-learning/policy.html" class="internal-link" target="_self" rel="noopener nofollow">policy</a> :with  and .
<br>In practice modeled as discrete distribution, elements of support denoted atoms
<br>Distributional <a data-tooltip-position="top" aria-label="Bellman's Equation" data-href="Bellman's Equation" href="the-guide/machine-learning/reinforcement-learning/theorems/bellman's-equation.html" class="internal-link" target="_self" rel="noopener nofollow">Bellman operator</a>changes support of distribution (needs to be projected back).


<br>Rainbow combines all of the above with -step returns.
<br><br><br>
<br><a rel="noopener nofollow" class="external-link" href="http://karpathy.github.io/2016/05/31/rl/" target="_blank">http://karpathy.github.io/2016/05/31/rl/</a>
<br><a rel="noopener nofollow" class="external-link" href="https://www.youtube.com/watch?v=IUiKAD6cuTA&amp;t=60s" target="_blank">https://www.youtube.com/watch?v=IUiKAD6cuTA&amp;t=60s</a>
]]></description><link>the-guide/machine-learning/reinforcement-learning/approaches/(deep)-q-learning.html</link><guid isPermaLink="false">The Guide/Machine Learning/Reinforcement Learning/Approaches/(Deep) Q-Learning.md</guid><pubDate>Thu, 20 Feb 2025 16:36:30 GMT</pubDate><enclosure url="lib/media/pasted-image-20230627121555.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src=&quot;lib/media/pasted-image-20230627121555.png&quot;&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[A2C - Advantage Actor-Critic]]></title><description><![CDATA[ 
 <br>In a Nutshell
On-policy approach for <a data-tooltip-position="top" aria-label="Actor-Critic Methods" data-href="Actor-Critic Methods" href="the-guide/machine-learning/reinforcement-learning/actor-critic-methods.html" class="internal-link" target="_self" rel="noopener nofollow">deep actor-critic methods</a>. Simple in that it only incorporates surrogate loss, uses shortened chunks of the trajectory and <a data-tooltip-position="top" aria-label="Monte Carlo Integration" data-href="Monte Carlo Integration" href="the-guide/mathematics/probability-theory/monte-carlo-integration.html" class="internal-link" target="_self" rel="noopener nofollow">MC</a>-estimates the <a data-tooltip-position="top" aria-label="Value Functions" data-href="Value Functions" href="the-guide/machine-learning/reinforcement-learning/value-functions.html" class="internal-link" target="_self" rel="noopener nofollow">value</a> and <a data-tooltip-position="top" aria-label="Advantage Function" data-href="Advantage Function" href="the-guide/machine-learning/reinforcement-learning/advantage-function.html" class="internal-link" target="_self" rel="noopener nofollow">advantage</a> function. 
<br><br>The resulting <a data-tooltip-position="top" aria-label="Estimator" data-href="Estimator" href="the-guide/mathematics/statistics/estimator.html" class="internal-link" target="_self" rel="noopener nofollow">estimator</a> for the gradient is This is somewhat similiar to the <a data-tooltip-position="top" aria-label="Policy Gradient Theorem" data-href="Policy Gradient Theorem" href="the-guide/machine-learning/reinforcement-learning/theorems/policy-gradient-theorem.html" class="internal-link" target="_self" rel="noopener nofollow">PGT</a>, but it does not use the discounted state <a data-tooltip-position="top" aria-label="Probability Distribution" data-href="Probability Distribution" href="the-guide/mathematics/probability-theory/probability-distribution.html" class="internal-link" target="_self" rel="noopener nofollow">distribution</a> and uses the advantage function instead of the <a data-tooltip-position="top" aria-label="Q-Function" data-href="Q-Function" href="the-guide/machine-learning/reinforcement-learning/q-function.html" class="internal-link" target="_self" rel="noopener nofollow">Q-function</a> according to the surrogate loss.<br><br><br><img alt="center" src="lib/media/pasted-image-20230716174812.png" style="width: 450px; max-width: 100%;">]]></description><link>the-guide/machine-learning/reinforcement-learning/approaches/a2c-advantage-actor-critic.html</link><guid isPermaLink="false">The Guide/Machine Learning/Reinforcement Learning/Approaches/A2C - Advantage Actor-Critic.md</guid><pubDate>Tue, 10 Dec 2024 14:09:50 GMT</pubDate><enclosure url="lib/media/pasted-image-20230716174812.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src=&quot;lib/media/pasted-image-20230716174812.png&quot;&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[CEM - Cross-Entropy Method]]></title><description><![CDATA[ 
 <br><a data-tooltip-position="top" aria-label="Model-Based Reinforcement Learning" data-href="Model-Based Reinforcement Learning" href="the-guide/machine-learning/reinforcement-learning/model-based-reinforcement-learning.html" class="internal-link" target="_self" rel="noopener nofollow">Model-</a> and sampling-based online <a data-tooltip-position="top" aria-label="- Reinforcement Learning -" data-href="- Reinforcement Learning -" href="the-guide/machine-learning/reinforcement-learning/-reinforcement-learning-.html" class="internal-link" target="_self" rel="noopener nofollow">RL</a> approach. Gradient-free and population-based (e.g. genetic algorithms), samples random parameters and evaluates them. Then the best are used to recompute the <a data-tooltip-position="top" aria-label="Probability Distribution" data-href="Probability Distribution" href="the-guide/mathematics/probability-theory/probability-distribution.html" class="internal-link" target="_self" rel="noopener nofollow">sampling distribution</a>. The resulting algorithm is easy to implement and parallelize, but fails for high-dimensional action spaces and needs a lot of samples.<img alt="center" src="lib/media/pasted-image-20230717105919.png" style="width: 300px; max-width: 100%;"><br>
<img alt="center" src="lib/media/pasted-image-20230717105950.png" style="width: 400px; max-width: 100%;">]]></description><link>the-guide/machine-learning/reinforcement-learning/approaches/cem-cross-entropy-method.html</link><guid isPermaLink="false">The Guide/Machine Learning/Reinforcement Learning/Approaches/CEM - Cross-Entropy Method.md</guid><pubDate>Thu, 20 Feb 2025 16:36:30 GMT</pubDate><enclosure url="lib/media/pasted-image-20230717105919.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src=&quot;lib/media/pasted-image-20230717105919.png&quot;&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[DDPG - Deep Deterministic Policy Gradient]]></title><description><![CDATA[ 
 <br>In a Nutshell
Essentially deep <a data-tooltip-position="top" aria-label="(Deep) Q-Learning" data-href="(Deep) Q-Learning" href="the-guide/machine-learning/reinforcement-learning/approaches/(deep)-q-learning.html" class="internal-link" target="_self" rel="noopener nofollow">Q-learning</a> for continuous action spaces. Results in an off-policy approach for <a data-tooltip-position="top" aria-label="Actor-Critic Methods" data-href="Actor-Critic Methods" href="the-guide/machine-learning/reinforcement-learning/actor-critic-methods.html" class="internal-link" target="_self" rel="noopener nofollow">deep actor-critic methods</a>. Directly implements the <a data-href="Deterministic Policy Gradient Theorem" href="the-guide/machine-learning/reinforcement-learning/theorems/deterministic-policy-gradient-theorem.html" class="internal-link" target="_self" rel="noopener nofollow">Deterministic Policy Gradient Theorem</a> and therefore results in a deterministic policy. 
<br><img alt="center" src="lib/media/pasted-image-20230716173851.png" style="width: 550px; max-width: 100%;"><br><br>The maximum operation in <a data-tooltip-position="top" aria-label="Markov Decision Process" data-href="Markov Decision Process" href="the-guide/machine-learning/reinforcement-learning/markov-decision-process.html" class="internal-link" target="_self" rel="noopener nofollow">continuous action spaces</a> is usually unacceptable computation-wise. The target has to be estimated differently.<br>DDPG
In DDPG we simply use the deterministic action chosen by the target critic network We can then compute the loss by computing the look-ahead with the target actor estimation with the values estimated by the online actor. This approach suffers from the same problems as the <a data-tooltip-position="top" aria-label="Policy Gradient Theorem" data-href="Policy Gradient Theorem" href="the-guide/machine-learning/reinforcement-learning/theorems/policy-gradient-theorem.html" class="internal-link" target="_self" rel="noopener nofollow">PGT</a>, mainly assuming knowledge of the true <a data-tooltip-position="top" aria-label="Q-Function" data-href="Q-Function" href="the-guide/machine-learning/reinforcement-learning/q-function.html" class="internal-link" target="_self" rel="noopener nofollow">Q-function</a>, while in reality it has to be <a data-tooltip-position="top" aria-label="Estimator" data-href="Estimator" href="the-guide/mathematics/statistics/estimator.html" class="internal-link" target="_self" rel="noopener nofollow">estimated</a>. DDPG therefore exploits replay memory and target networks for actor and critic to improve stability.
<br>
<br>Short

<br>online policy / actor for 
<br>target policy / actor for  in estimation
<br>target critic for td error estimation using 
<br>online critic for loss function


<br>Problems ...
All caused by estimation of <a data-tooltip-position="top" aria-label="Q-Function" data-href="Q-Function" href="the-guide/machine-learning/reinforcement-learning/q-function.html" class="internal-link" target="_self" rel="noopener nofollow">Q-function</a>, tackled in <a data-tooltip-position="top" aria-label="TD3 - Twin Delayed DDPG" data-href="TD3 - Twin Delayed DDPG" href="the-guide/machine-learning/reinforcement-learning/approaches/td3-twin-delayed-ddpg.html" class="internal-link" target="_self" rel="noopener nofollow">TD3</a>

<br>-function is often overestimated
<br>High <a data-tooltip-position="top" aria-label="Covariance and Variance" data-href="Covariance and Variance" href="the-guide/mathematics/statistics/covariance-and-variance.html" class="internal-link" target="_self" rel="noopener nofollow">variance</a>
<br>Tends to overfit <a data-tooltip-position="top" aria-label="Q-Function" data-href="Q-Function" href="the-guide/machine-learning/reinforcement-learning/q-function.html" class="internal-link" target="_self" rel="noopener nofollow">Q-function</a> peaks, which often depend on poor estimates.

]]></description><link>the-guide/machine-learning/reinforcement-learning/approaches/ddpg-deep-deterministic-policy-gradient.html</link><guid isPermaLink="false">The Guide/Machine Learning/Reinforcement Learning/Approaches/DDPG - Deep Deterministic Policy Gradient.md</guid><pubDate>Thu, 20 Feb 2025 16:30:43 GMT</pubDate><enclosure url="lib/media/pasted-image-20230716173851.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src=&quot;lib/media/pasted-image-20230716173851.png&quot;&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[eNAC - Episodic Natural Actor-Critic]]></title><description><![CDATA[ 
 <br>Info
Classical approach for <a data-tooltip-position="top" aria-label="Actor-Critic Methods" data-href="Actor-Critic Methods" href="the-guide/machine-learning/reinforcement-learning/actor-critic-methods.html" class="internal-link" target="_self" rel="noopener nofollow">actor-critic methods</a>. Combines <a data-tooltip-position="top" aria-label="Compatible Function Approximation Theorem" data-href="Compatible Function Approximation Theorem" href="the-guide/machine-learning/reinforcement-learning/theorems/compatible-function-approximation-theorem.html" class="internal-link" target="_self" rel="noopener nofollow">compatible function approximation</a> (cfa) and <a data-tooltip-position="top" aria-label="Policy Search" data-href="Policy Search" href="the-guide/machine-learning/reinforcement-learning/policy-search.html" class="internal-link" target="_self" rel="noopener nofollow">natural gradient</a> by estimating the <a data-tooltip-position="top" aria-label="Advantage Function" data-href="Advantage Function" href="the-guide/machine-learning/reinforcement-learning/advantage-function.html" class="internal-link" target="_self" rel="noopener nofollow">advantage function</a> via <a data-tooltip-position="top" aria-label="Compatible Function Approximation Theorem" data-href="Compatible Function Approximation Theorem" href="the-guide/machine-learning/reinforcement-learning/theorems/compatible-function-approximation-theorem.html" class="internal-link" target="_self" rel="noopener nofollow">compatible function approximation</a> With the natural gradient  and the cfa gradient , the combination yields To eliminate bias (?), use estimated value function as baseline If single initial state the above is just a constant. The resulting equation which can be written in <a data-tooltip-position="top" aria-label="Matrices" data-href="Matrices" href="the-guide/mathematics/linear-algebra/matrices.html" class="internal-link" target="_self" rel="noopener nofollow">matrix</a> form  and solved via <a data-tooltip-position="top" aria-label="Linear Least Squares and Ridge Regression" data-href="Linear Least Squares and Ridge Regression" href="the-guide/mathematics/optimization/linear-least-squares-and-ridge-regression.html" class="internal-link" target="_self" rel="noopener nofollow">LLS</a>.
<br><br><img alt="center" src="lib/media/pasted-image-20230909105743.png" style="width: 500px; max-width: 100%;">]]></description><link>the-guide/machine-learning/reinforcement-learning/approaches/enac-episodic-natural-actor-critic.html</link><guid isPermaLink="false">The Guide/Machine Learning/Reinforcement Learning/Approaches/eNAC - Episodic Natural Actor-Critic.md</guid><pubDate>Mon, 12 Aug 2024 17:06:50 GMT</pubDate><enclosure url="lib/media/pasted-image-20230909105743.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src=&quot;lib/media/pasted-image-20230909105743.png&quot;&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[GPOMDP]]></title><description><![CDATA[ 
 <br>TODO - something is wrong here, what is the difference to REINFORCE ?<br>Info
<a data-tooltip-position="top" aria-label="Policy Gradient Methods" data-href="Policy Gradient Methods" href="the-guide/machine-learning/reinforcement-learning/policy-gradient-methods.html" class="internal-link" target="_self" rel="noopener nofollow">Policy Gradient method</a> for <a data-href="Policy Search" href="the-guide/machine-learning/reinforcement-learning/policy-search.html" class="internal-link" target="_self" rel="noopener nofollow">Policy Search</a> in <a data-tooltip-position="top" aria-label="Model-Free Reinforcement Learning" data-href="Model-Free Reinforcement Learning" href="the-guide/machine-learning/reinforcement-learning/model-free-reinforcement-learning.html" class="internal-link" target="_self" rel="noopener nofollow">model-free</a> <a data-tooltip-position="top" aria-label="- Reinforcement Learning -" data-href="- Reinforcement Learning -" href="the-guide/machine-learning/reinforcement-learning/-reinforcement-learning-.html" class="internal-link" target="_self" rel="noopener nofollow">RL</a> setting. Instead of using Monte Carlo rollouts as the <a data-href="REINFORCE" href="the-guide/machine-learning/reinforcement-learning/approaches/reinforce.html" class="internal-link" target="_self" rel="noopener nofollow">REINFORCE</a> algorithm does, use step-based approach. For this, the <a data-tooltip-position="top" aria-label="Markov Decision Process" data-href="Markov Decision Process" href="the-guide/machine-learning/reinforcement-learning/markov-decision-process.html" class="internal-link" target="_self" rel="noopener nofollow">reward</a> is decomposed into step-based rewards . <a data-href="REINFORCE" href="the-guide/machine-learning/reinforcement-learning/approaches/reinforce.html" class="internal-link" target="_self" rel="noopener nofollow">REINFORCE</a> weights each step with the reward of the whole rollout, but by decomposing this rewards, we can see that at each past time step , the reward at  does not depend on actions in the future  With this, we can compute the GPOMDP gradientwith optimal time-dependent baseline. This reduces the variance.
]]></description><link>the-guide/machine-learning/reinforcement-learning/approaches/gpomdp.html</link><guid isPermaLink="false">The Guide/Machine Learning/Reinforcement Learning/Approaches/GPOMDP.md</guid><pubDate>Thu, 20 Feb 2025 16:36:30 GMT</pubDate></item><item><title><![CDATA[MBPO - Model-Based Policy Optimization]]></title><description><![CDATA[ 
 <br><a data-tooltip-position="top" aria-label="Model-Based Reinforcement Learning" data-href="Model-Based Reinforcement Learning" href="the-guide/machine-learning/reinforcement-learning/model-based-reinforcement-learning.html" class="internal-link" target="_self" rel="noopener nofollow">Model-</a> and sampling-based offline <a data-tooltip-position="top" aria-label="- Reinforcement Learning -" data-href="- Reinforcement Learning -" href="the-guide/machine-learning/reinforcement-learning/-reinforcement-learning-.html" class="internal-link" target="_self" rel="noopener nofollow">RL</a> approach. Handles <a data-tooltip-position="top" aria-label="- Reinforcement Learning -" data-href="- Reinforcement Learning -" href="the-guide/machine-learning/reinforcement-learning/-reinforcement-learning-.html" class="internal-link" target="_self" rel="noopener nofollow">aleatoric uncertainty</a> by using an approximate model with a <a data-tooltip-position="top" aria-label="Gaussian Distribution" data-href="Gaussian Distribution" href="the-guide/mathematics/probability-theory/gaussian-distribution.html" class="internal-link" target="_self" rel="noopener nofollow">Gaussian distribution</a> and parameters  for each state-action pair:<br>
<br>For new observations, the model is trained via <a data-tooltip-position="top" aria-label="Maximum Likelihood Estimator" data-href="Maximum Likelihood Estimator" href="the-guide/mathematics/statistics/maximum-likelihood-estimator.html" class="internal-link" target="_self" rel="noopener nofollow">maximum likelihood</a> . 
<br><a data-tooltip-position="top" aria-label="- Reinforcement Learning -" data-href="- Reinforcement Learning -" href="the-guide/machine-learning/reinforcement-learning/-reinforcement-learning-.html" class="internal-link" target="_self" rel="noopener nofollow">Epistemic uncertainty</a> is handled by using  probabilistic models  and sampling transitions from a randomly picked .
<br>Two datasets

<br>one from the environment to learn the model 
<br>one from the learned model to learn the <a data-tooltip-position="top" aria-label="Policy" data-href="Policy" href="the-guide/machine-learning/reinforcement-learning/policy.html" class="internal-link" target="_self" rel="noopener nofollow">policy</a> via <a data-tooltip-position="top" aria-label="SAC - Soft Actor-Critic" data-href="SAC - Soft Actor-Critic" href="the-guide/machine-learning/reinforcement-learning/approaches/sac-soft-actor-critic.html" class="internal-link" target="_self" rel="noopener nofollow">SAC</a>, but any algorithm can be used


<br><br><br><img alt="center" src="lib/media/pasted-image-20230717101311.png" style="width: 450px; max-width: 100%;">]]></description><link>the-guide/machine-learning/reinforcement-learning/approaches/mbpo-model-based-policy-optimization.html</link><guid isPermaLink="false">The Guide/Machine Learning/Reinforcement Learning/Approaches/MBPO - Model-Based Policy Optimization.md</guid><pubDate>Thu, 20 Feb 2025 16:36:30 GMT</pubDate><enclosure url="lib/media/pasted-image-20230717101311.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src=&quot;lib/media/pasted-image-20230717101311.png&quot;&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[PPO - Proximal Policy Optimization]]></title><description><![CDATA[ 
 <br>In a Nutshell
Enforce the same trustregion behavior as <a data-tooltip-position="top" aria-label="TRPO - Trust Region Policy Optimization" data-href="TRPO - Trust Region Policy Optimization" href="the-guide/machine-learning/reinforcement-learning/approaches/trpo-trust-region-policy-optimization.html" class="internal-link" target="_self" rel="noopener nofollow">TRPO</a>, but reduces computational complexity by avoiding to compute the <a data-tooltip-position="top" aria-label="Derivative, Gradient, Jacobian and Hessian > The Hessian" data-href="Derivative, Gradient, Jacobian and Hessian#The Hessian" href="the-guide/mathematics/analysis-and-calculus/derivative,-gradient,-jacobian-and-hessian.html#The_Hessian" class="internal-link" target="_self" rel="noopener nofollow">Hessian</a> via reward clipping.
<br><br>Info
On-policy approach for <a data-tooltip-position="top" aria-label="Actor-Critic Methods" data-href="Actor-Critic Methods" href="the-guide/machine-learning/reinforcement-learning/actor-critic-methods.html" class="internal-link" target="_self" rel="noopener nofollow">deep actor-critic methods</a> and <a data-tooltip-position="top" aria-label="Policy Gradient Methods" data-href="Policy Gradient Methods" href="the-guide/machine-learning/reinforcement-learning/policy-gradient-methods.html" class="internal-link" target="_self" rel="noopener nofollow">policy gradient methods</a> that uses the surrogate loss and enforces a trust region via clipping of the <a data-tooltip-position="top" aria-label="Importance Sampling" data-href="Importance Sampling" href="the-guide/mathematics/probability-theory/importance-sampling.html" class="internal-link" target="_self" rel="noopener nofollow">importance sampling</a> and <a data-tooltip-position="top" aria-label="Generalized Advantage Estimation (GAE)" data-href="Generalized Advantage Estimation (GAE)" href="the-guide/machine-learning/reinforcement-learning/generalized-advantage-estimation-(gae).html" class="internal-link" target="_self" rel="noopener nofollow">GAE</a> estimate:  where clipping is performed with boundaries .
<br><img alt="center" src="lib/media/pasted-image-20230716174723.png" style="width: 450px; max-width: 100%;"><br>
<br>Dataset has to be computed using previous policy  
<br><br>Intuition
The clipping limits the update step in the following way:

<br>If the new action yielded a better than expected effect on the outcome (left) and a high ratio of the importance sampling term indicates that the action became a lot more likely under the new policy  than it was under the old policy , limit the resulting update step. This is important, because the expectation is done based on a batch of the data and we don't want to "destroy" the policy based on a single estimate.
<br>If the new action yielded a worse than expected outcome (right) and a low ratio of the importance sampling ratio indicates that the action became a lot less likely with , limit the update to not decrease their probability to 0. Also, if the action became a lot more likely with  (to the right of the red dot) and the advantage is negative, "undo" the last update by allowing a negative loss that is proportional.

<br><img alt="center" src="lib/media/pasted-image-20230716153656.png" style="width: 400px; max-width: 100%;">The algorithm therefore yields an implicit trust region enforcement. This is advantageous, as the enforcement of the trust region as in <a data-tooltip-position="top" aria-label="TRPO - Trust Region Policy Optimization" data-href="TRPO - Trust Region Policy Optimization" href="the-guide/machine-learning/reinforcement-learning/approaches/trpo-trust-region-policy-optimization.html" class="internal-link" target="_self" rel="noopener nofollow">TRPO</a> is linked to high computational complexity.]]></description><link>the-guide/machine-learning/reinforcement-learning/approaches/ppo-proximal-policy-optimization.html</link><guid isPermaLink="false">The Guide/Machine Learning/Reinforcement Learning/Approaches/PPO - Proximal Policy Optimization.md</guid><pubDate>Tue, 04 Mar 2025 10:11:01 GMT</pubDate><enclosure url="lib/media/pasted-image-20230716174723.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src=&quot;lib/media/pasted-image-20230716174723.png&quot;&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[REINFORCE]]></title><description><![CDATA[ 
 <br>In a Nutshell
Fundamental <a data-tooltip-position="top" aria-label="Model-Free Reinforcement Learning" data-href="Model-Free Reinforcement Learning" href="the-guide/machine-learning/reinforcement-learning/model-free-reinforcement-learning.html" class="internal-link" target="_self" rel="noopener nofollow">model-free</a> <a data-tooltip-position="top" aria-label="- Reinforcement Learning -" data-href="- Reinforcement Learning -" href="the-guide/machine-learning/reinforcement-learning/-reinforcement-learning-.html" class="internal-link" target="_self" rel="noopener nofollow">RL</a>-algorithm in <a data-tooltip-position="top" aria-label="Policy Search" data-href="Policy Search" href="the-guide/machine-learning/reinforcement-learning/policy-search.html" class="internal-link" target="_self" rel="noopener nofollow">policy search</a> that directly adapts a policy to maximize returns.
<br><br>Simple <a data-tooltip-position="top" aria-label="Policy Gradient Methods" data-href="Policy Gradient Methods" href="the-guide/machine-learning/reinforcement-learning/policy-gradient-methods.html" class="internal-link" target="_self" rel="noopener nofollow">policy gradient method</a> that uses the <a data-tooltip-position="top" aria-label="Bayes Theorem" data-href="Bayes Theorem" href="the-guide/mathematics/statistics/bayes-theorem.html" class="internal-link" target="_self" rel="noopener nofollow">likelihood</a> for adapting parameters in <a data-tooltip-position="top" aria-label="Policy Search" data-href="Policy Search" href="the-guide/machine-learning/reinforcement-learning/policy-search.html" class="internal-link" target="_self" rel="noopener nofollow">policy search</a>. Using the <a data-tooltip-position="top" aria-label="Policy Gradient Theorem" data-href="Policy Gradient Theorem" href="the-guide/machine-learning/reinforcement-learning/theorems/policy-gradient-theorem.html" class="internal-link" target="_self" rel="noopener nofollow">policy gradient theorem</a> to rewrite the gradient, observe that the logarithmic gradient only depends on the <a data-tooltip-position="top" aria-label="Policy" data-href="Policy" href="the-guide/machine-learning/reinforcement-learning/policy.html" class="internal-link" target="_self" rel="noopener nofollow">policy</a> itself and therefore the parameters . For a rollout, we can writeFor the full loss we need to scale each likelihood by the expected future return at that point in the trajectory. For that , we first collect all rewards and then iterate through them backwards.  <br>Definition
With this and the <a data-tooltip-position="top" aria-label="Expectations" data-href="Expectations" href="the-guide/mathematics/statistics/expectations.html" class="internal-link" target="_self" rel="noopener nofollow">expectation</a> rewritten using <a data-tooltip-position="top" aria-label="Monte Carlo Integration" data-href="Monte Carlo Integration" href="the-guide/mathematics/probability-theory/monte-carlo-integration.html" class="internal-link" target="_self" rel="noopener nofollow">MC integration</a>, the gradient <a data-tooltip-position="top" aria-label="Estimator" data-href="Estimator" href="the-guide/mathematics/statistics/estimator.html" class="internal-link" target="_self" rel="noopener nofollow">estimator</a> can be written as where  is the estimated expected future reward at time-step . 
<br><br>We can also add an optimal baseline that does not introduce a bias while reducing the variance:<br><img alt="center" src="lib/media/pasted-image-20230908173126.png" style="width: 500px; max-width: 100%;"><br>Extensions
Decomposition of reward into steps yields <a data-href="GPOMDP" href="the-guide/machine-learning/reinforcement-learning/approaches/gpomdp.html" class="internal-link" target="_self" rel="noopener nofollow">GPOMDP</a>.
<br><br>Problems
High <a data-tooltip-position="top" aria-label="Covariance and Variance" data-href="Covariance and Variance" href="the-guide/mathematics/statistics/covariance-and-variance.html" class="internal-link" target="_self" rel="noopener nofollow">variance</a>, even with the optimal baseline
]]></description><link>the-guide/machine-learning/reinforcement-learning/approaches/reinforce.html</link><guid isPermaLink="false">The Guide/Machine Learning/Reinforcement Learning/Approaches/REINFORCE.md</guid><pubDate>Thu, 20 Feb 2025 16:36:30 GMT</pubDate><enclosure url="lib/media/pasted-image-20230908173126.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src=&quot;lib/media/pasted-image-20230908173126.png&quot;&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[SAC - Soft Actor-Critic]]></title><description><![CDATA[ 
 <br>Info
Off-policy approach for <a data-tooltip-position="top" aria-label="Actor-Critic Methods" data-href="Actor-Critic Methods" href="the-guide/machine-learning/reinforcement-learning/actor-critic-methods.html" class="internal-link" target="_self" rel="noopener nofollow">deep actor-critic methods</a>. Optimizes a <a data-tooltip-position="top" aria-label="Policy" data-href="Policy" href="the-guide/machine-learning/reinforcement-learning/policy.html" class="internal-link" target="_self" rel="noopener nofollow">stochastic policy</a> under the objective where  is the policy <a data-tooltip-position="top" aria-label="Shannon Entropy" data-href="Shannon Entropy" href="the-guide/information-theory/information-theory-1/information/shannon-entropy.html" class="internal-link" target="_self" rel="noopener nofollow">entropy</a>. 
<br><br>In contrast to <a data-tooltip-position="top" aria-label="DDPG - Deep Deterministic Policy Gradient" data-href="DDPG - Deep Deterministic Policy Gradient" href="the-guide/machine-learning/reinforcement-learning/approaches/ddpg-deep-deterministic-policy-gradient.html" class="internal-link" target="_self" rel="noopener nofollow">DDPG</a>/<a data-tooltip-position="top" aria-label="TD3 - Twin Delayed DDPG" data-href="TD3 - Twin Delayed DDPG" href="the-guide/machine-learning/reinforcement-learning/approaches/td3-twin-delayed-ddpg.html" class="internal-link" target="_self" rel="noopener nofollow">TD3</a>, SAC has no target actor and the surrogate loss is computed based on a soft value functionwhere the action is sampled from the policy.<br>
Distributions with higher uncertainty artificially increase the estimate and are therefore favored, leading to policies with higher entropy.<br>
The following tricks are used<br>
<br><a data-tooltip-position="top" aria-label="The Reparametrization Trick" data-href="The Reparametrization Trick" href="the-guide/mathematics/probability-theory/the-reparametrization-trick.html" class="internal-link" target="_self" rel="noopener nofollow">The Reparametrization Trick</a>

<br>In order to reduce variance of the <a data-tooltip-position="top" aria-label="Estimator" data-href="Estimator" href="the-guide/mathematics/statistics/estimator.html" class="internal-link" target="_self" rel="noopener nofollow">estimator</a>, rewrite the action as , effectively splitting random components  from deterministic ones (). Then, the gradient can be written using the chain rule via 


<br>Automatic tuning of the Entropy Parameter

<br>Tuning  is notoriously hard, better to specify target entropy  and choose according to optimization for each batch of data.


<br>Squashed Gaussians

<br>To ensure the actions are sampled within bounds, use squashed <a data-tooltip-position="top" aria-label="Gaussian Distribution" data-href="Gaussian Distribution" href="the-guide/mathematics/probability-theory/gaussian-distribution.html" class="internal-link" target="_self" rel="noopener nofollow">gaussians</a>and use online actors choosing state-dependent <a data-tooltip-position="top" aria-label="Expectations" data-href="Expectations" href="the-guide/mathematics/statistics/expectations.html" class="internal-link" target="_self" rel="noopener nofollow">mean</a> and <a data-tooltip-position="top" aria-label="Covariance and Variance" data-href="Covariance and Variance" href="the-guide/mathematics/statistics/covariance-and-variance.html" class="internal-link" target="_self" rel="noopener nofollow">standard deviation</a> (implement using two <a data-tooltip-position="top" aria-label="Artificial Neural Network" data-href="Artificial Neural Network" href="the-guide/machine-learning/deep-learning/artificial-neural-network.html" class="internal-link" target="_self" rel="noopener nofollow">NN</a>).<img alt="center" src="lib/media/pasted-image-20230716180104.png" style="width: 400px; max-width: 100%;">


<br><br><br><img alt="center" src="lib/media/pasted-image-20230802182613.png" style="width: 550px; max-width: 100%;"><br> <img alt="center" src="lib/media/pasted-image-20230802182642.png" style="width: 550px; max-width: 100%;">]]></description><link>the-guide/machine-learning/reinforcement-learning/approaches/sac-soft-actor-critic.html</link><guid isPermaLink="false">The Guide/Machine Learning/Reinforcement Learning/Approaches/SAC - Soft Actor-Critic.md</guid><pubDate>Tue, 25 Feb 2025 11:18:08 GMT</pubDate><enclosure url="lib/media/pasted-image-20230716180104.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src=&quot;lib/media/pasted-image-20230716180104.png&quot;&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[SARSA]]></title><description><![CDATA[ 
 <br>In a Nutshell
Class of <a data-tooltip-position="top" aria-label="Model-Free Reinforcement Learning" data-href="Model-Free Reinforcement Learning" href="the-guide/machine-learning/reinforcement-learning/model-free-reinforcement-learning.html" class="internal-link" target="_self" rel="noopener nofollow">on-policy</a> and <a data-tooltip-position="top" aria-label="- Reinforcement Learning -" data-href="- Reinforcement Learning -" href="the-guide/machine-learning/reinforcement-learning/-reinforcement-learning-.html" class="internal-link" target="_self" rel="noopener nofollow">online RL</a> algorithms that learns the <a data-tooltip-position="top" aria-label="Q-Function" data-href="Q-Function" href="the-guide/machine-learning/reinforcement-learning/q-function.html" class="internal-link" target="_self" rel="noopener nofollow">Q-function</a> from experiences collected via a given policy. The online estimation is then performed by the same policy.
<br><br>When learning the Q-function via <a data-tooltip-position="top" aria-label="Temporal Difference Learning" data-href="Temporal Difference Learning" href="the-guide/machine-learning/reinforcement-learning/temporal-difference-learning.html" class="internal-link" target="_self" rel="noopener nofollow">TD</a> updates, how do we compute the following equations ?<br>
<br>
<br><br>
The problem is choosing the action  to estimate the future reward after taking our behavioral action.
<br>SARSA 
SARSA uses a <a data-tooltip-position="top" aria-label="Temporal Difference Learning" data-href="Temporal Difference Learning" href="the-guide/machine-learning/reinforcement-learning/temporal-difference-learning.html" class="internal-link" target="_self" rel="noopener nofollow">TD</a> approach by simply sticking to a given policy  that enforces <a data-tooltip-position="top" aria-label="Exploration-Exploitation Trade-Off" data-href="Exploration-Exploitation Trade-Off" href="the-guide/machine-learning/reinforcement-learning/exploration-exploitation-trade-off.html" class="internal-link" target="_self" rel="noopener nofollow">exploration</a>An easy example would be to use - <a data-tooltip-position="top" aria-label="Exploration-Exploitation Trade-Off" data-href="Exploration-Exploitation Trade-Off" href="the-guide/machine-learning/reinforcement-learning/exploration-exploitation-trade-off.html" class="internal-link" target="_self" rel="noopener nofollow">greedy</a>.
<br>Theorem
To reach the optimal <a data-href="Q-Function" href="the-guide/machine-learning/reinforcement-learning/q-function.html" class="internal-link" target="_self" rel="noopener nofollow">Q-Function</a> , SARSA needs to employ a <a data-href="GLIE" href="the-guide/machine-learning/reinforcement-learning/glie.html" class="internal-link" target="_self" rel="noopener nofollow">GLIE</a> sequence of policies , while the learning rate  has to fulfill the <a data-tooltip-position="top" aria-label="Stochastic Optimization" data-href="Stochastic Optimization" href="the-guide/mathematics/optimization/stochastic-optimization.html" class="internal-link" target="_self" rel="noopener nofollow">Robbins-Monroe</a> condition
<br>Can be extended in the same way as base <a data-tooltip-position="top" aria-label="Temporal Difference Learning" data-href="Temporal Difference Learning" href="the-guide/machine-learning/reinforcement-learning/temporal-difference-learning.html" class="internal-link" target="_self" rel="noopener nofollow">TD-learning</a>:<br>
<br>-step SARSA

<br>
<br>


<br>SARSA

<br>Forward - Update <a data-tooltip-position="top" aria-label="Q-Function" data-href="Q-Function" href="the-guide/machine-learning/reinforcement-learning/q-function.html" class="internal-link" target="_self" rel="noopener nofollow">Q-function</a> to -return 
<br>Backward - Use eligibility trace


<br>Semi-Gradient SARSA

<br>


<br>]]></description><link>the-guide/machine-learning/reinforcement-learning/approaches/sarsa.html</link><guid isPermaLink="false">The Guide/Machine Learning/Reinforcement Learning/Approaches/SARSA.md</guid><pubDate>Thu, 20 Feb 2025 16:36:31 GMT</pubDate></item><item><title><![CDATA[TD3 - Twin Delayed DDPG]]></title><description><![CDATA[ 
 <br>In a Nutshell
Extension of <a data-tooltip-position="top" aria-label="DDPG - Deep Deterministic Policy Gradient" data-href="DDPG - Deep Deterministic Policy Gradient" href="the-guide/machine-learning/reinforcement-learning/approaches/ddpg-deep-deterministic-policy-gradient.html" class="internal-link" target="_self" rel="noopener nofollow">DDPG</a> that tries to overcome it's issues using three modifications: multiple critics, delayed <a data-tooltip-position="top" aria-label="Policy" data-href="Policy" href="the-guide/machine-learning/reinforcement-learning/policy.html" class="internal-link" target="_self" rel="noopener nofollow">policy</a> updates and noisy actions.
<br><img alt="center" src="lib/media/pasted-image-20230716174351.png" style="width: 550px; max-width: 100%;"><br><br>TD 3
Off-policy approach for <a data-tooltip-position="top" aria-label="Actor-Critic Methods" data-href="Actor-Critic Methods" href="the-guide/machine-learning/reinforcement-learning/actor-critic-methods.html" class="internal-link" target="_self" rel="noopener nofollow">deep actor-critic methods</a> that extends the <a data-tooltip-position="top" aria-label="DDPG - Deep Deterministic Policy Gradient" data-href="DDPG - Deep Deterministic Policy Gradient" href="the-guide/machine-learning/reinforcement-learning/approaches/ddpg-deep-deterministic-policy-gradient.html" class="internal-link" target="_self" rel="noopener nofollow">DDPG</a> algorithm to work against its issues. The modifications are:

<br>Twin

<br>Trains two online and target critics / <a data-tooltip-position="top" aria-label="Q-Function" data-href="Q-Function" href="the-guide/machine-learning/reinforcement-learning/q-function.html" class="internal-link" target="_self" rel="noopener nofollow">Q-function</a> approximators  and takes their minimum for state-action value <a data-tooltip-position="top" aria-label="Estimator" data-href="Estimator" href="the-guide/mathematics/statistics/estimator.html" class="internal-link" target="_self" rel="noopener nofollow">estimation</a>. This works against the overestimation and overfitting issues.


<br>Delayed

<br>Only updates the policy after fixed amount of iterations. This works against the variance issue.


<br>Adds noise to the clipped action used to compute next -value

<br>This works against overfitting of peaks.



This results in a new update for the next state value with  and . The resulting policy is still deterministic.
<br>
<br>Short

<br>Compute state action value estimation  using target critic networks (minimum) with actions chosen by:

<br>deterministic actor  with artificial clipped noise


<br>Fit online critic networks to minibatch  and 
<br>in every delay iteration

<br>Compute loss using first online critic and online actor
<br>Optimize online actor network using loss


<br>Update target networks


<br><br><br>
<br><a rel="noopener nofollow" class="external-link" href="https://saashanair.com/blog/blog-posts/twin-delayed-ddpg-td3-how-does-the-algorithm-work" target="_blank">https://saashanair.com/blog/blog-posts/twin-delayed-ddpg-td3-how-does-the-algorithm-work</a>
]]></description><link>the-guide/machine-learning/reinforcement-learning/approaches/td3-twin-delayed-ddpg.html</link><guid isPermaLink="false">The Guide/Machine Learning/Reinforcement Learning/Approaches/TD3 - Twin Delayed DDPG.md</guid><pubDate>Thu, 20 Feb 2025 16:30:33 GMT</pubDate><enclosure url="lib/media/pasted-image-20230716174351.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src=&quot;lib/media/pasted-image-20230716174351.png&quot;&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[TRPO - Trust Region Policy Optimization]]></title><description><![CDATA[ 
 <br>In a Nushell
On-policy approach for <a data-tooltip-position="top" aria-label="Actor-Critic Methods" data-href="Actor-Critic Methods" href="the-guide/machine-learning/reinforcement-learning/actor-critic-methods.html" class="internal-link" target="_self" rel="noopener nofollow">deep actor-critic methods</a> and <a data-tooltip-position="top" aria-label="Policy Gradient Methods" data-href="Policy Gradient Methods" href="the-guide/machine-learning/reinforcement-learning/policy-gradient-methods.html" class="internal-link" target="_self" rel="noopener nofollow">policy gradient methods</a> that uses the surrogate loss and enforces a trust region via a <a data-tooltip-position="top" aria-label="Kullback-Leibler Divergence" data-href="Kullback-Leibler Divergence" href="the-guide/information-theory/information-theory-1/information/kullback-leibler-divergence.html" class="internal-link" target="_self" rel="noopener nofollow">KL</a> constrain. Very effective, but computationally intensive. Modern SOTA is the simpler <a data-tooltip-position="top" aria-label="PPO - Proximal Policy Optimization" data-href="PPO - Proximal Policy Optimization" href="the-guide/machine-learning/reinforcement-learning/approaches/ppo-proximal-policy-optimization.html" class="internal-link" target="_self" rel="noopener nofollow">PPO</a>.
<br><img alt="center" src="lib/media/pasted-image-20230716174854.png" style="width: 450px; max-width: 100%;"><br><br>In practice, the <a data-tooltip-position="top" aria-label="Kullback-Leibler Divergence" data-href="Kullback-Leibler Divergence" href="the-guide/information-theory/information-theory-1/information/kullback-leibler-divergence.html" class="internal-link" target="_self" rel="noopener nofollow">KL-Divergence</a> can be approximated using the <a data-tooltip-position="top" aria-label="Fisher Information Matrix" data-href="Fisher Information Matrix" href="the-guide/mathematics/statistics/fisher-information-matrix.html" class="internal-link" target="_self" rel="noopener nofollow">Fisher information</a> <a data-tooltip-position="top" aria-label="Matrices" data-href="Matrices" href="the-guide/mathematics/linear-algebra/matrices.html" class="internal-link" target="_self" rel="noopener nofollow">matrix</a> In case of a high dimensional parameter space, especially considering <a data-tooltip-position="top" aria-label="Artificial Neural Network" data-href="Artificial Neural Network" href="the-guide/machine-learning/deep-learning/artificial-neural-network.html" class="internal-link" target="_self" rel="noopener nofollow">neural networks</a>, the resulting <a data-tooltip-position="top" aria-label="Matrices" data-href="Matrices" href="the-guide/mathematics/linear-algebra/matrices.html" class="internal-link" target="_self" rel="noopener nofollow">matrix</a> is very large and inverting becomes too expensive ()). Instead, a linear system of the form ca be solved using the <a data-tooltip-position="top" aria-label="Conjugate Gradient Method" data-href="Conjugate Gradient Method" href="the-guide/mathematics/linear-algebra/conjugate-gradient-method.html" class="internal-link" target="_self" rel="noopener nofollow">CG method</a> to find an optimization direction. This algorithm is particularly well-suited, because it only needs the effect of the <a data-tooltip-position="top" aria-label="Fisher Information Matrix" data-href="Fisher Information Matrix" href="the-guide/mathematics/statistics/fisher-information-matrix.html" class="internal-link" target="_self" rel="noopener nofollow">Fisher matrix</a> on the vector, which can be computed using <a data-tooltip-position="top" aria-label="Automatic Differentiation" data-href="Automatic Differentiation" href="the-guide/machine-learning/automatic-differentiation.html" class="internal-link" target="_self" rel="noopener nofollow">automatic differentiation</a>. After that, the step size is computed using line search.]]></description><link>the-guide/machine-learning/reinforcement-learning/approaches/trpo-trust-region-policy-optimization.html</link><guid isPermaLink="false">The Guide/Machine Learning/Reinforcement Learning/Approaches/TRPO - Trust Region Policy Optimization.md</guid><pubDate>Tue, 04 Mar 2025 10:09:50 GMT</pubDate><enclosure url="lib/media/pasted-image-20230716174854.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src=&quot;lib/media/pasted-image-20230716174854.png&quot;&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[Bellman's Equation]]></title><description><![CDATA[ 
 <br>Principle of Optimality
"An optimal sequence of controls in a multi stage optimization problem has the property that whatever the initial stage, state and controls are, the remaining controls must constitute an optimal sequence of decisions for the remaining problem with stage and state resulting from previous controls considered as initial conditions."
<br><br>Bellmann Expectation
Solve multi-step optimization problems by breaking them into smaller recursive sub-problems. The Bellman Expectation Equation in <a data-tooltip-position="top" aria-label="Matrices" data-href="Matrices" href="the-guide/mathematics/linear-algebra/matrices.html" class="internal-link" target="_self" rel="noopener nofollow">matrix</a> form reads

<br> are column vectors with one entry per state
<br> is <a data-tooltip-position="top" aria-label="Markov Decision Process" data-href="Markov Decision Process" href="the-guide/machine-learning/reinforcement-learning/markov-decision-process.html" class="internal-link" target="_self" rel="noopener nofollow">transition matrix</a>

<br><br><br>
<br>Operator for <a data-tooltip-position="top" aria-label="Value Functions" data-href="Value Functions" href="the-guide/machine-learning/reinforcement-learning/value-functions.html" class="internal-link" target="_self" rel="noopener nofollow">Value Function</a> 

<br>General <a data-tooltip-position="top" aria-label="Policy" data-href="Policy" href="the-guide/machine-learning/reinforcement-learning/policy.html" class="internal-link" target="_self" rel="noopener nofollow">policies</a> 
<br>Optimal <a data-tooltip-position="top" aria-label="Policy" data-href="Policy" href="the-guide/machine-learning/reinforcement-learning/policy.html" class="internal-link" target="_self" rel="noopener nofollow">policies</a>
<br>Bellman Equation via 

<br> is unique (?) fixed point




<br>Operator for <a data-href="Q-Function" href="the-guide/machine-learning/reinforcement-learning/q-function.html" class="internal-link" target="_self" rel="noopener nofollow">Q-Function</a> 

<br>General <a data-tooltip-position="top" aria-label="Policy" data-href="Policy" href="the-guide/machine-learning/reinforcement-learning/policy.html" class="internal-link" target="_self" rel="noopener nofollow">policies</a> 
<br>Optimal <a data-tooltip-position="top" aria-label="Policy" data-href="Policy" href="the-guide/machine-learning/reinforcement-learning/policy.html" class="internal-link" target="_self" rel="noopener nofollow">policies</a>
<br>Bellman Equation via 

<br> is unique (?) fixed point




<br>Properties with 

<br>If , the Operator is a <a data-tooltip-position="top" aria-label="Contraction Mapping" data-href="Contraction Mapping" href="the-guide/mathematics/general-stuff/contraction-mapping.html" class="internal-link" target="_self" rel="noopener nofollow">contraction mapping</a> w.r.t. the <a data-tooltip-position="top" aria-label="Norms" data-href="Norms" href="the-guide/mathematics/linear-algebra/norms.html" class="internal-link" target="_self" rel="noopener nofollow">max-norm</a>
<br>Monotonicity 
<br>Limit 


<br><br><br>
<br>Bellmann equation for <a data-tooltip-position="top" aria-label="Value Functions" data-href="Value Functions" href="the-guide/machine-learning/reinforcement-learning/value-functions.html" class="internal-link" target="_self" rel="noopener nofollow">value functions</a>
<br>Bellmann equation for <a data-tooltip-position="top" aria-label="Q-Function" data-href="Q-Function" href="the-guide/machine-learning/reinforcement-learning/q-function.html" class="internal-link" target="_self" rel="noopener nofollow">Q-functions</a>
<br>Bellmann equation for optimal value functions
<br>Bellmann equation for optimal Q-functions
]]></description><link>the-guide/machine-learning/reinforcement-learning/theorems/bellman&apos;s-equation.html</link><guid isPermaLink="false">The Guide/Machine Learning/Reinforcement Learning/Theorems/Bellman&apos;s Equation.md</guid><pubDate>Sun, 02 Feb 2025 17:14:46 GMT</pubDate></item><item><title><![CDATA[Bias-Variance Tradeoff]]></title><description><![CDATA[<a class="tag" href="?query=tag:Machine-Learning" style="background-color: rgb(4, 108, 116); color: white; font-weight: 700; border: none; border-radius: 1em; padding: 0.2em 0.5em;">#Machine-Learning</a> <a class="tag" href="?query=tag:Statistics" style="background-color: rgb(4, 108, 116); color: white; font-weight: 700; border: none; border-radius: 1em; padding: 0.2em 0.5em;">#Statistics</a> 
 <br>In a Nutshell
Fundamental dilemma in <a href=".?query=tag:Machine-Learning" class="tag" target="_blank" rel="noopener nofollow">#Machine-Learning</a> and <a href=".?query=tag:Statistics" class="tag" target="_blank" rel="noopener nofollow">#Statistics</a> . More complex models can adapt better to complex data (lower bias), but risk to also adapt to noise (higher variance).
<br><br><img alt="center" src="lib/media/pasted-image-20230518192130.png" style="width: 300px; max-width: 100%;"> <br><a data-tooltip-position="top" aria-label="Estimator" data-href="Estimator" href="the-guide/mathematics/statistics/estimator.html" class="internal-link" target="_self" rel="noopener nofollow">Bias</a>
Error from erroneous assumptions in the learning algorithm or a model that cannot grasp all aspects of the real system . High bias can cause an algorithm to miss the relevant relations between features and target outputs (Underfitting).
<br><a data-tooltip-position="top" aria-label="Estimator" data-href="Estimator" href="the-guide/mathematics/statistics/estimator.html" class="internal-link" target="_self" rel="noopener nofollow">Variance</a>
Error from sensitivity to small fluctuations in the training set. High variance may result from an algorithm modeling the random <a data-tooltip-position="top" aria-label="https://en.wikipedia.org/wiki/Noise_(signal_processing)" rel="noopener nofollow" class="external-link" title="Noise (signal processing)" href="https://en.wikipedia.org/wiki/Noise_(signal_processing)" target="_blank">noise</a> in the training data (<a data-tooltip-position="top" aria-label="https://en.wikipedia.org/wiki/Overfitting" rel="noopener nofollow" class="external-link" title="Overfitting" href="https://en.wikipedia.org/wiki/Overfitting" target="_blank">Overfitting</a>).
<br><br><br>
<br><a data-tooltip-position="top" aria-label="Estimator" data-href="Estimator" href="the-guide/mathematics/statistics/estimator.html" class="internal-link" target="_self" rel="noopener nofollow">Estimator</a>  from training data , data generated by 
<br><a data-tooltip-position="top" aria-label="Expectations" data-href="Expectations" href="the-guide/mathematics/statistics/expectations.html" class="internal-link" target="_self" rel="noopener nofollow">Expected</a> Squared error

<br>Irreducible error 
<br>Bias 
<br>Variance 


]]></description><link>the-guide/machine-learning/reinforcement-learning/theorems/bias-variance-tradeoff.html</link><guid isPermaLink="false">The Guide/Machine Learning/Reinforcement Learning/Theorems/Bias-Variance Tradeoff.md</guid><pubDate>Sun, 02 Feb 2025 17:12:59 GMT</pubDate><enclosure url="lib/media/pasted-image-20230518192130.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src=&quot;lib/media/pasted-image-20230518192130.png&quot;&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[Compatible Function Approximation Theorem]]></title><description><![CDATA[ 
 <br>Info
The <a data-tooltip-position="top" aria-label="Estimator" data-href="Estimator" href="the-guide/mathematics/statistics/estimator.html" class="internal-link" target="_self" rel="noopener nofollow">estimate</a> of the policy gradient according to the <a data-tooltip-position="top" aria-label="Policy Gradient Theorem" data-href="Policy Gradient Theorem" href="the-guide/machine-learning/reinforcement-learning/theorems/policy-gradient-theorem.html" class="internal-link" target="_self" rel="noopener nofollow">policy gradient theorem</a> is <a data-tooltip-position="top" aria-label="Estimator" data-href="Estimator" href="the-guide/mathematics/statistics/estimator.html" class="internal-link" target="_self" rel="noopener nofollow">unbiased</a>, meaning that with <a data-tooltip-position="top" aria-label="Q-Function" data-href="Q-Function" href="the-guide/machine-learning/reinforcement-learning/q-function.html" class="internal-link" target="_self" rel="noopener nofollow">Q-function</a> approximation , it holds thatif the following conditions hold:

<br>The -function approximation is compatible to the <a data-tooltip-position="top" aria-label="Policy" data-href="Policy" href="the-guide/machine-learning/reinforcement-learning/policy.html" class="internal-link" target="_self" rel="noopener nofollow">policy</a> 
<br>The parameters of the -function  are chosen such that they minimize the mean-squared error 

<br><br><br>
<br>We cannot use TD learning and have an <a data-tooltip-position="top" aria-label="Estimator" data-href="Estimator" href="the-guide/mathematics/statistics/estimator.html" class="internal-link" target="_self" rel="noopener nofollow">unbiased estimate</a> of the gradient, because we need full rollouts for an unbiased estimate
<br>Linear weights lead to
<br>An algorithm that combines compatible function approximation with <a data-tooltip-position="top" aria-label="The Natural Gradient" data-href="The Natural Gradient" href="the-guide/machine-learning/reinforcement-learning/the-natural-gradient.html" class="internal-link" target="_self" rel="noopener nofollow">the natural gradient</a> is the <a data-tooltip-position="top" aria-label="Actor-Critic Methods" data-href="Actor-Critic Methods" href="the-guide/machine-learning/reinforcement-learning/actor-critic-methods.html" class="internal-link" target="_self" rel="noopener nofollow">actor-critic</a> method <a data-tooltip-position="top" aria-label="eNAC - Episodic Natural Actor-Critic" data-href="eNAC - Episodic Natural Actor-Critic" href="the-guide/machine-learning/reinforcement-learning/approaches/enac-episodic-natural-actor-critic.html" class="internal-link" target="_self" rel="noopener nofollow">eNAC</a>.
]]></description><link>the-guide/machine-learning/reinforcement-learning/theorems/compatible-function-approximation-theorem.html</link><guid isPermaLink="false">The Guide/Machine Learning/Reinforcement Learning/Theorems/Compatible Function Approximation Theorem.md</guid><pubDate>Tue, 10 Dec 2024 14:41:12 GMT</pubDate></item><item><title><![CDATA[Deterministic Policy Gradient Theorem]]></title><description><![CDATA[ 
 <br>Theorem 1
Considering a deterministic <a data-tooltip-position="top" aria-label="Policy" data-href="Policy" href="the-guide/machine-learning/reinforcement-learning/policy.html" class="internal-link" target="_self" rel="noopener nofollow">policy</a>  and considering mild assumptions on the <a data-tooltip-position="top" aria-label="Markov Decision Process" data-href="Markov Decision Process" href="the-guide/machine-learning/reinforcement-learning/markov-decision-process.html" class="internal-link" target="_self" rel="noopener nofollow">MDP</a>, the policy gradient can be written as Using the discounted state <a data-tooltip-position="top" aria-label="Probability Distribution" data-href="Probability Distribution" href="the-guide/mathematics/probability-theory/probability-distribution.html" class="internal-link" target="_self" rel="noopener nofollow">distribution</a>  instead of the occupancy metric results in 
<br><br>Theorem 2 - Limit
If  is a stochastic policy, where  is a parameter controlling the <a data-tooltip-position="top" aria-label="Covariance and Variance" data-href="Covariance and Variance" href="the-guide/mathematics/statistics/covariance-and-variance.html" class="internal-link" target="_self" rel="noopener nofollow">variance</a>, then uner mild assumptions on the resulting distribution, the Deterministic Policy gradient Theorem is the limit of the <a data-tooltip-position="top" aria-label="Policy Gradient Theorem" data-href="Policy Gradient Theorem" href="the-guide/machine-learning/reinforcement-learning/theorems/policy-gradient-theorem.html" class="internal-link" target="_self" rel="noopener nofollow">PGT</a> 
]]></description><link>the-guide/machine-learning/reinforcement-learning/theorems/deterministic-policy-gradient-theorem.html</link><guid isPermaLink="false">The Guide/Machine Learning/Reinforcement Learning/Theorems/Deterministic Policy Gradient Theorem.md</guid><pubDate>Tue, 12 Nov 2024 11:23:32 GMT</pubDate></item><item><title><![CDATA[Policy Gradient Theorem]]></title><description><![CDATA[ 
 <br>In a Nutshell
Fundamental theorem of <a data-tooltip-position="top" aria-label="Policy Gradient Methods" data-href="Policy Gradient Methods" href="the-guide/machine-learning/reinforcement-learning/policy-gradient-methods.html" class="internal-link" target="_self" rel="noopener nofollow">Policy Gradient methods</a>. Allows us to directly optimize a parametrized <a data-tooltip-position="top" aria-label="Policy" data-href="Policy" href="the-guide/machine-learning/reinforcement-learning/policy.html" class="internal-link" target="_self" rel="noopener nofollow">policy</a> based on <a data-tooltip-position="top" aria-label="Estimator" data-href="Estimator" href="the-guide/mathematics/statistics/estimator.html" class="internal-link" target="_self" rel="noopener nofollow">estimations</a> of the <a data-href="Q-Function" href="the-guide/machine-learning/reinforcement-learning/q-function.html" class="internal-link" target="_self" rel="noopener nofollow">Q-Function</a>.
<br>The episodic and continuing cases define the performance measure , differently<br>
and thus have to be treated separately to some extent.<br><br><br>In the episodic case (finite structure, no endless interaction), a performance measure  can be defined via the true <a data-tooltip-position="top" aria-label="Value Functions" data-href="Value Functions" href="the-guide/machine-learning/reinforcement-learning/value-functions.html" class="internal-link" target="_self" rel="noopener nofollow">value function</a> for the policy . In reality, we don't have access to this, but we can use <a data-tooltip-position="top" aria-label="Estimator" data-href="Estimator" href="the-guide/mathematics/statistics/estimator.html" class="internal-link" target="_self" rel="noopener nofollow">estimates</a>. However, at first glance, this problem still seems rather challenging:<br>Problem
The problem is that performance depends on both the action selections and the distribution of states in which those selections are made, and that both of these are affected by the policy parameter.
<br>The trajectory probability depends on the <a data-tooltip-position="top" aria-label="Markov Decision Process" data-href="Markov Decision Process" href="the-guide/machine-learning/reinforcement-learning/markov-decision-process.html" class="internal-link" target="_self" rel="noopener nofollow">transition model</a> and the performance depends on action selection and state <a data-tooltip-position="top" aria-label="Probability Distribution" data-href="Probability Distribution" href="the-guide/mathematics/probability-theory/probability-distribution.html" class="internal-link" target="_self" rel="noopener nofollow">distribution</a>, both of which are dependent on the policy and therefore : <br>The term above is reformulated using the following tricks:<br>
<br>Using the <a data-tooltip-position="top" aria-label="Logarithm and Exponential" data-href="Logarithm and Exponential" href="the-guide/mathematics/general-stuff/logarithm-and-exponential.html" class="internal-link" target="_self" rel="noopener nofollow">log-trick</a>, we can "pull" the gradient onto the logarithm of the trajectory distributionBecause of the log-rules for products, the resulting gradient only depends on the policy 
<br>The term above is an <a data-tooltip-position="top" aria-label="Expectations" data-href="Expectations" href="the-guide/mathematics/statistics/expectations.html" class="internal-link" target="_self" rel="noopener nofollow">expectation</a> w.r.t. to the trajectories, so it can be approximated using <a data-tooltip-position="top" aria-label="Monte Carlo Integration" data-href="Monte Carlo Integration" href="the-guide/mathematics/probability-theory/monte-carlo-integration.html" class="internal-link" target="_self" rel="noopener nofollow">MC integration</a> and samples !
<br>Policy Gradient Theorem
For any <a data-tooltip-position="top" aria-label="Markov Decision Process" data-href="Markov Decision Process" href="the-guide/machine-learning/reinforcement-learning/markov-decision-process.html" class="internal-link" target="_self" rel="noopener nofollow">MDP</a>, the <a data-tooltip-position="top" aria-label="Policy Search" data-href="Policy Search" href="the-guide/machine-learning/reinforcement-learning/policy-search.html" class="internal-link" target="_self" rel="noopener nofollow">policy gradient</a> of the episodic case can be computed via where  is the occupancy <a data-tooltip-position="top" aria-label="Measure" data-href="Measure" href="the-guide/mathematics/measure-theory/measure.html" class="internal-link" target="_self" rel="noopener nofollow">measure</a> under a given <a data-tooltip-position="top" aria-label="Policy" data-href="Policy" href="the-guide/machine-learning/reinforcement-learning/policy.html" class="internal-link" target="_self" rel="noopener nofollow">policy</a> , which denotes the discounted frequency of encountering the state . Using the discounted state distribution , the theorem can be stated as 
<br>Occupancy Measure
The occupancy measure is not a <a data-tooltip-position="top" aria-label="Probability Distribution" data-href="Probability Distribution" href="the-guide/mathematics/probability-theory/probability-distribution.html" class="internal-link" target="_self" rel="noopener nofollow">distribution</a>, as it does not sum up to one (because we consider the discounted case) ! This is why we had to introduce the discounted frequency to write the theorem as an <a data-tooltip-position="top" aria-label="Expectations" data-href="Expectations" href="the-guide/mathematics/statistics/expectations.html" class="internal-link" target="_self" rel="noopener nofollow">expectation</a>.
<br>Why is this remarkable ?
The terms above do not involve the derivative of the state distribution and only depend on values we can handle:

<br>The gradient of the policy can be computed, as we know the parametrization.
<br>The <a data-tooltip-position="top" aria-label="Q-Function" data-href="Q-Function" href="the-guide/machine-learning/reinforcement-learning/q-function.html" class="internal-link" target="_self" rel="noopener nofollow">Q-function</a> can be estimated, although this leads to <a data-tooltip-position="top" aria-label="Estimator" data-href="Estimator" href="the-guide/mathematics/statistics/estimator.html" class="internal-link" target="_self" rel="noopener nofollow">bias</a>  use <a data-tooltip-position="top" aria-label="Compatible Function Approximation Theorem" data-href="Compatible Function Approximation Theorem" href="the-guide/machine-learning/reinforcement-learning/theorems/compatible-function-approximation-theorem.html" class="internal-link" target="_self" rel="noopener nofollow">compatible function approximation</a> !

<br><br><br>
<br>If we estimate the <a data-tooltip-position="top" aria-label="Q-Function" data-href="Q-Function" href="the-guide/machine-learning/reinforcement-learning/q-function.html" class="internal-link" target="_self" rel="noopener nofollow">Q-function</a> above using MC rollouts, the expression is equivalent to <a data-href="GPOMDP" href="the-guide/machine-learning/reinforcement-learning/approaches/gpomdp.html" class="internal-link" target="_self" rel="noopener nofollow">GPOMDP</a>.
<br>If we estimate <a data-tooltip-position="top" aria-label="Q-Function" data-href="Q-Function" href="the-guide/machine-learning/reinforcement-learning/q-function.html" class="internal-link" target="_self" rel="noopener nofollow">Q-function</a> above using <a data-tooltip-position="top" aria-label="Temporal Difference Learning" data-href="Temporal Difference Learning" href="the-guide/machine-learning/reinforcement-learning/temporal-difference-learning.html" class="internal-link" target="_self" rel="noopener nofollow">TD methods</a>, the expression is an <a data-tooltip-position="top" aria-label="Actor-Critic Methods" data-href="Actor-Critic Methods" href="the-guide/machine-learning/reinforcement-learning/actor-critic-methods.html" class="internal-link" target="_self" rel="noopener nofollow">actor-critic algorithm</a>.
<br><br><br>
<br><a rel="noopener nofollow" class="external-link" href="https://www.youtube.com/watch?v=e20EY4tFC_Q&amp;t=220s" target="_blank">https://www.youtube.com/watch?v=e20EY4tFC_Q&amp;t=220s</a>
]]></description><link>the-guide/machine-learning/reinforcement-learning/theorems/policy-gradient-theorem.html</link><guid isPermaLink="false">The Guide/Machine Learning/Reinforcement Learning/Theorems/Policy Gradient Theorem.md</guid><pubDate>Sun, 02 Feb 2025 17:08:37 GMT</pubDate></item><item><title><![CDATA[The Deadly Triad]]></title><description><![CDATA[ 
 <br>In a Nutshell
Refers to a concept in model-free RL in which a combination of three elements, when used together, can lead to instability and divergence in learning algorithms. This instability typically manifests as oscillations, divergence, or failure to converge to an optimal policy. The term "deadly triad" highlights the fact that these three components, although individually common and often beneficial, create significant issues when combined.
<br><br>The Deadly Triad
In <a data-tooltip-position="top" aria-label="Model-Free Reinforcement Learning" data-href="Model-Free Reinforcement Learning" href="the-guide/machine-learning/reinforcement-learning/model-free-reinforcement-learning.html" class="internal-link" target="_self" rel="noopener nofollow">model-free</a> <a data-tooltip-position="top" aria-label="- Reinforcement Learning -" data-href="- Reinforcement Learning -" href="the-guide/machine-learning/reinforcement-learning/-reinforcement-learning-.html" class="internal-link" target="_self" rel="noopener nofollow">RL</a>, one often faces problems of instability and divergence for methods using the elements of

<br><a data-tooltip-position="top" aria-label="Reinforcement Learning with Function Approximation" data-href="Reinforcement Learning with Function Approximation" href="the-guide/machine-learning/reinforcement-learning/reinforcement-learning-with-function-approximation.html" class="internal-link" target="_self" rel="noopener nofollow">Function Approximation</a>

<br>Necessary for large scale problems


<br>Bootstrapping

<br>Necessary for data efficiency


<br><a data-tooltip-position="top" aria-label="- Reinforcement Learning -" data-href="- Reinforcement Learning -" href="the-guide/machine-learning/reinforcement-learning/-reinforcement-learning-.html" class="internal-link" target="_self" rel="noopener nofollow">Off-Policy</a> Training

<br>Necessary for heterogeneous experience



]]></description><link>the-guide/machine-learning/reinforcement-learning/theorems/the-deadly-triad.html</link><guid isPermaLink="false">The Guide/Machine Learning/Reinforcement Learning/Theorems/The Deadly Triad.md</guid><pubDate>Thu, 20 Feb 2025 16:36:31 GMT</pubDate></item><item><title><![CDATA[- Reinforcement Learning -]]></title><description><![CDATA[ 
 <br>In a Nutshell
Area of machine learning dealing with how intelligent agents ought to take actions in an unknown environment in order to maximize a reward. 
<br><img alt="center" src="lib/media/pasted-image-20230904175602.png" style="width: 200px; max-width: 100%;"><br><br><br>Environment
Simulation or real world, is usually modeled as an <a data-tooltip-position="top" aria-label="Markov Decision Process" data-href="Markov Decision Process" href="the-guide/machine-learning/reinforcement-learning/markov-decision-process.html" class="internal-link" target="_self" rel="noopener nofollow">MDP</a>.
<br>Episodic vs. Non-Episodic

<br>Episodic - Interaction between agent and environment, starts at initial state sampled according to <a data-tooltip-position="top" aria-label="Markov Decision Process" data-href="Markov Decision Process" href="the-guide/machine-learning/reinforcement-learning/markov-decision-process.html" class="internal-link" target="_self" rel="noopener nofollow">initial state distribution</a>. Terminates after fixed amount of steps (horizon) or when reaching an absorbing state.
<br>Non-Episodic - interaction is modeled in an infinite time-frame

<br>Return 

<br>Sum of future reward collected after  steps, introduce <a data-tooltip-position="top" aria-label="Markov Decision Process" data-href="Markov Decision Process" href="the-guide/machine-learning/reinforcement-learning/markov-decision-process.html" class="internal-link" target="_self" rel="noopener nofollow">discount factor</a> to avoid infinite returns in un-terminated settings or to model uncertainty about the future.
<br>Recursive structure 
Sutton
Goal or purpose is maximization of cumulative sum of reward signal.



<br>Theorem - Reward Function Expression
The reward <a data-tooltip-position="top" aria-label="Mathematical Relations" data-href="Mathematical Relations" href="the-guide/mathematics/general-stuff/mathematical-relations.html" class="internal-link" target="_self" rel="noopener nofollow">reward function</a> can be expresse via the <a data-tooltip-position="top" aria-label="Markov Chain" data-href="Markov Chain" href="the-guide/computational-statistics/data-assimilation/markov-chain.html" class="internal-link" target="_self" rel="noopener nofollow">stationary distribution</a>  of the Markov Chain resulting from transitions according to the policy  via  
<br>Optimism vs Pessimism

<br>Optimism - Similar to greediness, aggressive pursuit of optimal solution. Can lead to impossible physics scenarios and constraint violations, which is especially dangerous for robots.
<br>Pessimism - Strategies to work against the problems of too much optimism. Introduce inductive biases to impose additional constraints, e.g. energy conservation, re-plan everything over shorter horizons or regularize the optimizer used to compute the policy.

<br><a data-tooltip-position="top" aria-label="Uncertainty Estimation" data-href="Uncertainty Estimation" href="the-guide/machine-learning/generative-models/uncertainty-estimation.html" class="internal-link" target="_self" rel="noopener nofollow">Uncertainty</a>

<br>Aleatoric - Inherent, due to noise and stochastic environment
<br>Epistemic - Arising from lack of data / information  

<br>Online vs. Offline Algorithms

<br>Online - Update happens during rollout
<br>Offline - Update happens between rollouts

<br>On- vs. Off-Policy Algorithms

<br>On-Policy Learning "Learn in the job"

<br>Evaluate and improve <a data-tooltip-position="top" aria-label="Policy" data-href="Policy" href="the-guide/machine-learning/reinforcement-learning/policy.html" class="internal-link" target="_self" rel="noopener nofollow">policy</a>  that agent is using for action selection


<br>Off-Policy Learning "Look over someones shoulder"

<br>Evaluate and improve target <a data-tooltip-position="top" aria-label="Policy" data-href="Policy" href="the-guide/machine-learning/reinforcement-learning/policy.html" class="internal-link" target="_self" rel="noopener nofollow">policy</a>  by following a different behavior policy 



<br>Infinite- vs. Finite Horizon Objectives
In general, we want to find a <a data-tooltip-position="top" aria-label="Policy" data-href="Policy" href="the-guide/machine-learning/reinforcement-learning/policy.html" class="internal-link" target="_self" rel="noopener nofollow">policy</a>  that maximizes its expected return . We differentiate between the general long term return and the expected return  for a finite time horizon.

<br>Infinite Horizon - The accumulated expected reward iswith

<br>Discount Factor  (trades off long term vs. immediate reward)


<br>Finite Horizon - The accumulated expected reward for T steps iswhere now the time index is part of the state (it matters how many steps are left, optimal <a data-tooltip-position="top" aria-label="Policy" data-href="Policy" href="the-guide/machine-learning/reinforcement-learning/policy.html" class="internal-link" target="_self" rel="noopener nofollow">policy</a> is time-dependent). Apart from that, we have

<br>final reward  
<br><a data-tooltip-position="top" aria-label="Markov Decision Process" data-href="Markov Decision Process" href="the-guide/machine-learning/reinforcement-learning/markov-decision-process.html" class="internal-link" target="_self" rel="noopener nofollow">initial distribution</a> 
<br><a data-tooltip-position="top" aria-label="Markov Decision Process" data-href="Markov Decision Process" href="the-guide/machine-learning/reinforcement-learning/markov-decision-process.html" class="internal-link" target="_self" rel="noopener nofollow">transition probability function</a>  



<br><br><br><img alt="center" src="lib/media/pasted-image-20230223105916.png" style="width: 300px; max-width: 100%;"><br>How to model the world ? - two main approaches

<br><a data-href="Model-Free Reinforcement Learning" href="the-guide/machine-learning/reinforcement-learning/model-free-reinforcement-learning.html" class="internal-link" target="_self" rel="noopener nofollow">Model-Free Reinforcement Learning</a>

<br>Model-free Prediction - Estimate <a data-tooltip-position="top" aria-label="Value Functions" data-href="Value Functions" href="the-guide/machine-learning/reinforcement-learning/value-functions.html" class="internal-link" target="_self" rel="noopener nofollow">value function</a> of unknown <a data-tooltip-position="top" aria-label="Markov Decision Process" data-href="Markov Decision Process" href="the-guide/machine-learning/reinforcement-learning/markov-decision-process.html" class="internal-link" target="_self" rel="noopener nofollow">MDP</a> + <a data-tooltip-position="top" aria-label="Policy" data-href="Policy" href="the-guide/machine-learning/reinforcement-learning/policy.html" class="internal-link" target="_self" rel="noopener nofollow">policy</a>
<br>Model-free Control - Optimize <a data-tooltip-position="top" aria-label="Value Functions" data-href="Value Functions" href="the-guide/machine-learning/reinforcement-learning/value-functions.html" class="internal-link" target="_self" rel="noopener nofollow">value function</a> of an unknown <a data-tooltip-position="top" aria-label="Markov Decision Process" data-href="Markov Decision Process" href="the-guide/machine-learning/reinforcement-learning/markov-decision-process.html" class="internal-link" target="_self" rel="noopener nofollow">MDP</a>


<br><a data-href="Model-Based Reinforcement Learning" href="the-guide/machine-learning/reinforcement-learning/model-based-reinforcement-learning.html" class="internal-link" target="_self" rel="noopener nofollow">Model-Based Reinforcement Learning</a>

<br>Learn a dynamics model from data and use it to optimize policy
<br>Model can introduce unwanted bias


<br>(<a data-href="Actor-Critic Methods" href="the-guide/machine-learning/reinforcement-learning/actor-critic-methods.html" class="internal-link" target="_self" rel="noopener nofollow">Actor-Critic Methods</a>)

<br>Usually counted as model-free, but can sometimes take a special role



<br><img alt="center" src="lib/media/pasted-image-20241112121804.png"><br>Remarks

<br>Distinction is not clean

<br><a data-tooltip-position="top" aria-label="Model-Based Reinforcement Learning" data-href="Model-Based Reinforcement Learning" href="the-guide/machine-learning/reinforcement-learning/model-based-reinforcement-learning.html" class="internal-link" target="_self" rel="noopener nofollow">MBRL</a> can use model-free approaches, e.g. to improve accuracy
<br><a data-tooltip-position="top" aria-label="Model-Free Reinforcement Learning" data-href="Model-Free Reinforcement Learning" href="the-guide/machine-learning/reinforcement-learning/model-free-reinforcement-learning.html" class="internal-link" target="_self" rel="noopener nofollow">MFRL</a> still uses a model, as value and  function encode dynamics in some way



<br>Sample Efficiency - Model Based vs. Model Free
In an <a data-tooltip-position="top" aria-label="Markov Decision Process" data-href="Markov Decision Process" href="the-guide/machine-learning/reinforcement-learning/markov-decision-process.html" class="internal-link" target="_self" rel="noopener nofollow">MDP</a> with state space  and action space , both settings need samples.

<br>In worst case you go from each state to every other state, taking all possible actions
<br><a data-tooltip-position="top" aria-label="Model-Based Reinforcement Learning" data-href="Model-Based Reinforcement Learning" href="the-guide/machine-learning/reinforcement-learning/model-based-reinforcement-learning.html" class="internal-link" target="_self" rel="noopener nofollow">MBRL</a> can benefit from exploiting domain knowledge of the dynamics in a model, e.g. in linear continuous case (<a data-href="LQG Regulator" href="the-guide/robotics,-dynamics-and-control/control/lqg-regulator.html" class="internal-link" target="_self" rel="noopener nofollow">LQG Regulator</a>)
<br>Discrete case always same sample efficiency

<br><br>]]></description><link>the-guide/machine-learning/reinforcement-learning/-reinforcement-learning-.html</link><guid isPermaLink="false">The Guide/Machine Learning/Reinforcement Learning/- Reinforcement Learning -.md</guid><pubDate>Sun, 13 Apr 2025 18:08:06 GMT</pubDate><enclosure url="lib/media/pasted-image-20230904175602.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src=&quot;lib/media/pasted-image-20230904175602.png&quot;&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[Actor-Critic Methods]]></title><description><![CDATA[ 
 <br>In a Nutshell
Methods in <a data-tooltip-position="top" aria-label="Model-Free Reinforcement Learning" data-href="Model-Free Reinforcement Learning" href="the-guide/machine-learning/reinforcement-learning/model-free-reinforcement-learning.html" class="internal-link" target="_self" rel="noopener nofollow">model-free</a> <a data-tooltip-position="top" aria-label="- Reinforcement Learning -" data-href="- Reinforcement Learning -" href="the-guide/machine-learning/reinforcement-learning/-reinforcement-learning-.html" class="internal-link" target="_self" rel="noopener nofollow">RL</a> that learn both an actor in the form of a <a data-tooltip-position="top" aria-label="Policy" data-href="Policy" href="the-guide/machine-learning/reinforcement-learning/policy.html" class="internal-link" target="_self" rel="noopener nofollow">policy</a> and a critic in from of a <a data-tooltip-position="top" aria-label="Value Functions" data-href="Value Functions" href="the-guide/machine-learning/reinforcement-learning/value-functions.html" class="internal-link" target="_self" rel="noopener nofollow">value function</a>. In general, these methods perform estimation of the <a data-tooltip-position="top" aria-label="Q-Function" data-href="Q-Function" href="the-guide/machine-learning/reinforcement-learning/q-function.html" class="internal-link" target="_self" rel="noopener nofollow">Q-function</a> via a temporal difference method, which is much more robust than Monte-Carlo rollouts, but leads to a biased <a data-tooltip-position="top" aria-label="Estimator" data-href="Estimator" href="the-guide/mathematics/statistics/estimator.html" class="internal-link" target="_self" rel="noopener nofollow">estimator</a>.
<br><br><br>Enables function approximation via <a data-tooltip-position="top" aria-label="Compatible Function Approximation Theorem" data-href="Compatible Function Approximation Theorem" href="the-guide/machine-learning/reinforcement-learning/theorems/compatible-function-approximation-theorem.html" class="internal-link" target="_self" rel="noopener nofollow">compatible function approximation theorem</a>.<br>
<br><a data-href="eNAC - Episodic Natural Actor-Critic" href="the-guide/machine-learning/reinforcement-learning/approaches/enac-episodic-natural-actor-critic.html" class="internal-link" target="_self" rel="noopener nofollow">eNAC - Episodic Natural Actor-Critic</a>
<br><br><br>Use <a data-tooltip-position="top" aria-label="Artificial Neural Network" data-href="Artificial Neural Network" href="the-guide/machine-learning/deep-learning/artificial-neural-network.html" class="internal-link" target="_self" rel="noopener nofollow">Neural Networks</a> to model both an actor (the <a data-tooltip-position="top" aria-label="Policy" data-href="Policy" href="the-guide/machine-learning/reinforcement-learning/policy.html" class="internal-link" target="_self" rel="noopener nofollow">policy</a>) and the critic (<a data-tooltip-position="top" aria-label="Value Functions" data-href="Value Functions" href="the-guide/machine-learning/reinforcement-learning/value-functions.html" class="internal-link" target="_self" rel="noopener nofollow">value function</a>). All following methods are approximate, as their gradient estimation is <a data-tooltip-position="top" aria-label="Estimator" data-href="Estimator" href="the-guide/mathematics/statistics/estimator.html" class="internal-link" target="_self" rel="noopener nofollow">biased</a> and they use simplified objective functions.<br><br>On-policy in this case means to only use samples from the previous policy.<br>
<br>Issues with <a data-tooltip-position="top" aria-label="Policy Gradient Theorem" data-href="Policy Gradient Theorem" href="the-guide/machine-learning/reinforcement-learning/theorems/policy-gradient-theorem.html" class="internal-link" target="_self" rel="noopener nofollow">PGT</a> 

<br>Theorem requires true <a data-tooltip-position="top" aria-label="Q-Function" data-href="Q-Function" href="the-guide/machine-learning/reinforcement-learning/q-function.html" class="internal-link" target="_self" rel="noopener nofollow">Q-function</a> for policy being optimized, but we also want off-policy training !
<br>In order to write the theorem as an <a data-tooltip-position="top" aria-label="Expectations" data-href="Expectations" href="the-guide/mathematics/statistics/expectations.html" class="internal-link" target="_self" rel="noopener nofollow">expectation</a>, we had to introduce the discounted state distribution induced by the policy. This resulted in a trajectory that is terminated at each step with probability , which results in sample inefficiency !


<br>The Surrogate Loss 

<br>Original loss function is complex, as state <a data-tooltip-position="top" aria-label="Probability Distribution" data-href="Probability Distribution" href="the-guide/mathematics/probability-theory/probability-distribution.html" class="internal-link" target="_self" rel="noopener nofollow">distribution</a> depends on current policy. Derive simplified loss function via Performance Difference Lemma using baseline policy . The expectation answers the question of how much better it is to take the action that  enforces compared to the expected value (estimated via ) of that state. To further reduce the complexity of computing the gradient, sample the discounted state distribution from  () while still sampling the actions from . Additionally, the first term is constant w.r.t. the policy and can be dropped. The <a data-tooltip-position="top" aria-label="Advantage Function" data-href="Advantage Function" href="the-guide/machine-learning/reinforcement-learning/advantage-function.html" class="internal-link" target="_self" rel="noopener nofollow">advantage function</a> is estimated using . This yields 
<br>In practice, the discounted state distribution is often replaced by the undiscounted one, leading to non-vanishing gradient but wrong estimates. 
<br>You can also rewrite the above loss using <a data-tooltip-position="top" aria-label="Importance Sampling" data-href="Importance Sampling" href="the-guide/mathematics/probability-theory/importance-sampling.html" class="internal-link" target="_self" rel="noopener nofollow">importance sampling</a> via 


<br>Trust Regions

<br>Approximation of objective function via surrogate loss is only good if we stay close to the sampling distribution / old policy . Therefore, limit the policy updates using a trust region !


<br>On-Policy Approaches

<br><a data-href="A2C - Advantage Actor-Critic" href="the-guide/machine-learning/reinforcement-learning/approaches/a2c-advantage-actor-critic.html" class="internal-link" target="_self" rel="noopener nofollow">A2C - Advantage Actor-Critic</a>

<br>Simple approach directly from <a data-tooltip-position="top" aria-label="Policy Gradient Theorem" data-href="Policy Gradient Theorem" href="the-guide/machine-learning/reinforcement-learning/theorems/policy-gradient-theorem.html" class="internal-link" target="_self" rel="noopener nofollow">PGT</a>.


<br><a data-href="TRPO - Trust Region Policy Optimization" href="the-guide/machine-learning/reinforcement-learning/approaches/trpo-trust-region-policy-optimization.html" class="internal-link" target="_self" rel="noopener nofollow">TRPO - Trust Region Policy Optimization</a>

<br>Enforce trust region when optimizing policy.


<br><a data-href="PPO - Proximal Policy Optimization" href="the-guide/machine-learning/reinforcement-learning/approaches/ppo-proximal-policy-optimization.html" class="internal-link" target="_self" rel="noopener nofollow">PPO - Proximal Policy Optimization</a>

<br>Simpler, more efficient way to enforce trust region.




<br><br>Off-policy in this case means to use samples from a replay buffer.<br>
<br>Soft Updates of the Target Network

<br>In <a data-tooltip-position="top" aria-label="(Deep) Q-Learning" data-href="(Deep) Q-Learning" href="the-guide/machine-learning/reinforcement-learning/approaches/(deep)-q-learning.html" class="internal-link" target="_self" rel="noopener nofollow">DQN</a>, the target network is updated after a fixed amount of steps. Off-policy actor critic methods use an exponential moving average to update the weights of the target network  towards the weights of the online network  at each step viawith  being a smoothing factor.


<br>Off-Policy Approaches

<br><a data-href="DDPG - Deep Deterministic Policy Gradient" href="the-guide/machine-learning/reinforcement-learning/approaches/ddpg-deep-deterministic-policy-gradient.html" class="internal-link" target="_self" rel="noopener nofollow">DDPG - Deep Deterministic Policy Gradient</a>

<br>Updates deterministic policy using <a data-tooltip-position="top" aria-label="Deterministic Policy Gradient Theorem" data-href="Deterministic Policy Gradient Theorem" href="the-guide/machine-learning/reinforcement-learning/theorems/deterministic-policy-gradient-theorem.html" class="internal-link" target="_self" rel="noopener nofollow">DPGT</a> to estimate the gradient.


<br><a data-href="TD3 - Twin Delayed DDPG" href="the-guide/machine-learning/reinforcement-learning/approaches/td3-twin-delayed-ddpg.html" class="internal-link" target="_self" rel="noopener nofollow">TD3 - Twin Delayed DDPG</a>

<br>Updates deterministic policy using <a data-tooltip-position="top" aria-label="Deterministic Policy Gradient Theorem" data-href="Deterministic Policy Gradient Theorem" href="the-guide/machine-learning/reinforcement-learning/theorems/deterministic-policy-gradient-theorem.html" class="internal-link" target="_self" rel="noopener nofollow">DPGT</a>, two <a data-tooltip-position="top" aria-label="Q-Function" data-href="Q-Function" href="the-guide/machine-learning/reinforcement-learning/q-function.html" class="internal-link" target="_self" rel="noopener nofollow">Q-function</a> <a data-tooltip-position="top" aria-label="Estimator" data-href="Estimator" href="the-guide/mathematics/statistics/estimator.html" class="internal-link" target="_self" rel="noopener nofollow">estimators</a> and clipping, leading to a more robust estimate.


<br><a data-href="SAC - Soft Actor-Critic" href="the-guide/machine-learning/reinforcement-learning/approaches/sac-soft-actor-critic.html" class="internal-link" target="_self" rel="noopener nofollow">SAC - Soft Actor-Critic</a>

<br>Updates <a data-tooltip-position="top" aria-label="Policy" data-href="Policy" href="the-guide/machine-learning/reinforcement-learning/policy.html" class="internal-link" target="_self" rel="noopener nofollow">stochastic policy</a> by extending deterministic approach using reparameterization trick. Optimizes entropy-regularized objective.




]]></description><link>the-guide/machine-learning/reinforcement-learning/actor-critic-methods.html</link><guid isPermaLink="false">The Guide/Machine Learning/Reinforcement Learning/Actor-Critic Methods.md</guid><pubDate>Thu, 20 Feb 2025 16:36:30 GMT</pubDate></item><item><title><![CDATA[Advantage Function]]></title><description><![CDATA[ 
 <br>Info
Relative performance of an <a data-tooltip-position="top" aria-label="Markov Decision Process" data-href="Markov Decision Process" href="the-guide/machine-learning/reinforcement-learning/markov-decision-process.html" class="internal-link" target="_self" rel="noopener nofollow">action</a>, computed from <a data-tooltip-position="top" aria-label="Value Functions" data-href="Value Functions" href="the-guide/machine-learning/reinforcement-learning/value-functions.html" class="internal-link" target="_self" rel="noopener nofollow">value function</a> and <a data-tooltip-position="top" aria-label="Q-Function" data-href="Q-Function" href="the-guide/machine-learning/reinforcement-learning/q-function.html" class="internal-link" target="_self" rel="noopener nofollow">state-action-value function</a> Describes how much better / worse it is to take action  in state  and then follow  instead of directly following .
<br><br><br>
<br><a data-tooltip-position="top" aria-label="Expectations" data-href="Expectations" href="the-guide/mathematics/statistics/expectations.html" class="internal-link" target="_self" rel="noopener nofollow">Expected</a> advantage when action is sampled from <a data-tooltip-position="top" aria-label="Policy" data-href="Policy" href="the-guide/machine-learning/reinforcement-learning/policy.html" class="internal-link" target="_self" rel="noopener nofollow">policy</a>  is 0 
]]></description><link>the-guide/machine-learning/reinforcement-learning/advantage-function.html</link><guid isPermaLink="false">The Guide/Machine Learning/Reinforcement Learning/Advantage Function.md</guid><pubDate>Mon, 12 Aug 2024 17:06:50 GMT</pubDate></item><item><title><![CDATA[Batch-Mode Reinforcement Learning]]></title><description><![CDATA[ 
 <br>Info
<a data-tooltip-position="top" aria-label="- Reinforcement Learning -" data-href="- Reinforcement Learning -" href="the-guide/machine-learning/reinforcement-learning/-reinforcement-learning-.html" class="internal-link" target="_self" rel="noopener nofollow">Offline</a> RL method used for <a data-tooltip-position="top" aria-label="Reinforcement Learning with Function Approximation" data-href="Reinforcement Learning with Function Approximation" href="the-guide/machine-learning/reinforcement-learning/reinforcement-learning-with-function-approximation.html" class="internal-link" target="_self" rel="noopener nofollow">RL with function approximation</a> because <a data-tooltip-position="top" aria-label="- Reinforcement Learning -" data-href="- Reinforcement Learning -" href="the-guide/machine-learning/reinforcement-learning/-reinforcement-learning-.html" class="internal-link" target="_self" rel="noopener nofollow">online</a> methods like <a data-href="Temporal Difference Learning" href="the-guide/machine-learning/reinforcement-learning/temporal-difference-learning.html" class="internal-link" target="_self" rel="noopener nofollow">Temporal Difference Learning</a> are typically data inefficient, use each data point only once. Batch (Offline) uses batch of previously collected data points and reuse that data to increase data-efficiency. However, they are in general computationally much more expensive.
<br><br>
<br>Least-Squares Temporal Difference (LSTD) Learning 

<br>
<br><a data-href="Linear Least Squares and Ridge Regression" href="the-guide/mathematics/optimization/linear-least-squares-and-ridge-regression.html" class="internal-link" target="_self" rel="noopener nofollow">Linear Least Squares and Ridge Regression</a> - Solution 

<br>
<br>


<br>Stationary point () at 

<br>Same solution as convergence point of TD-learning.
<br>Updates whole <a data-tooltip-position="top" aria-label="Value Functions" data-href="Value Functions" href="the-guide/machine-learning/reinforcement-learning/value-functions.html" class="internal-link" target="_self" rel="noopener nofollow">Value function</a> based on one-step lookahead for every state at once




<br>Fitted -iteration

<br>Like <a data-tooltip-position="top" aria-label="Dynamic Programming" data-href="Dynamic Programming" href="the-guide/machine-learning/reinforcement-learning/dynamic-programming.html" class="internal-link" target="_self" rel="noopener nofollow">Value-Iteration</a>, but uses supervised learning methods to approximate <a data-href="Q-Function" href="the-guide/machine-learning/reinforcement-learning/q-function.html" class="internal-link" target="_self" rel="noopener nofollow">Q-Function</a> at each iteration.<img alt="center" src="lib/media/pasted-image-20230913125212.png" style="width: 600px; max-width: 100%;">


]]></description><link>the-guide/machine-learning/reinforcement-learning/batch-mode-reinforcement-learning.html</link><guid isPermaLink="false">The Guide/Machine Learning/Reinforcement Learning/Batch-Mode Reinforcement Learning.md</guid><pubDate>Thu, 20 Feb 2025 16:36:31 GMT</pubDate><enclosure url="lib/media/pasted-image-20230913125212.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src=&quot;lib/media/pasted-image-20230913125212.png&quot;&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[Dynamic Programming]]></title><description><![CDATA[ 
 <br>In a Nutshell
Collection of algorithms that can be used to compute <a data-tooltip-position="top" aria-label="Policy" data-href="Policy" href="the-guide/machine-learning/reinforcement-learning/policy.html" class="internal-link" target="_self" rel="noopener nofollow">optimal policy</a> given a perfect model of the environment. Problem needs to have optimal structure and overlapping sub-problems. Full known <a data-tooltip-position="top" aria-label="Markov Decision Process" data-href="Markov Decision Process" href="the-guide/machine-learning/reinforcement-learning/markov-decision-process.html" class="internal-link" target="_self" rel="noopener nofollow">MDP</a> satisfies both properties:

<br><a data-href="Bellman's Equation" href="the-guide/machine-learning/reinforcement-learning/theorems/bellman's-equation.html" class="internal-link" target="_self" rel="noopener nofollow">Bellman's Equation</a> gives recursive structure to break down optimal solution in small parts, each part must be optimal.
<br>Reuse solutions that were already computed in <a data-tooltip-position="top" aria-label="Value Functions" data-href="Value Functions" href="the-guide/machine-learning/reinforcement-learning/value-functions.html" class="internal-link" target="_self" rel="noopener nofollow">value function</a>.

<br><img alt="center" src="lib/media/pasted-image-20241112122147.png"><br><br>In general, there are 2 types of problems that can be solved using dynamic programming:<br>
<br>Prediction

<br>Input: <a data-tooltip-position="top" aria-label="Markov Decision Process" data-href="Markov Decision Process" href="the-guide/machine-learning/reinforcement-learning/markov-decision-process.html" class="internal-link" target="_self" rel="noopener nofollow">MDP</a> + <a data-tooltip-position="top" aria-label="Policy" data-href="Policy" href="the-guide/machine-learning/reinforcement-learning/policy.html" class="internal-link" target="_self" rel="noopener nofollow">policy</a>
<br>Output: <a data-tooltip-position="top" aria-label="Value Functions" data-href="Value Functions" href="the-guide/machine-learning/reinforcement-learning/value-functions.html" class="internal-link" target="_self" rel="noopener nofollow">value function</a> 


<br>Control

<br>Input: <a data-tooltip-position="top" aria-label="Markov Decision Process" data-href="Markov Decision Process" href="the-guide/machine-learning/reinforcement-learning/markov-decision-process.html" class="internal-link" target="_self" rel="noopener nofollow">MDP</a> 
<br>Output: <a data-tooltip-position="top" aria-label="Policy" data-href="Policy" href="the-guide/machine-learning/reinforcement-learning/policy.html" class="internal-link" target="_self" rel="noopener nofollow">policy</a> + <a data-tooltip-position="top" aria-label="Value Functions" data-href="Value Functions" href="the-guide/machine-learning/reinforcement-learning/value-functions.html" class="internal-link" target="_self" rel="noopener nofollow">value function</a>


<br><br><br>For a given <a data-tooltip-position="top" aria-label="Policy" data-href="Policy" href="the-guide/machine-learning/reinforcement-learning/policy.html" class="internal-link" target="_self" rel="noopener nofollow">policy</a>, we can simply compute a <a data-tooltip-position="top" aria-label="Value Functions" data-href="Value Functions" href="the-guide/machine-learning/reinforcement-learning/value-functions.html" class="internal-link" target="_self" rel="noopener nofollow">value function</a> by iterating the following algorithm:<br>Algorithm - Policy Evaluation

<br>At each iteration  do

<br>For each state 

<br>





<br><br><br> Policy Improvement Theorem
With pair of deterministic policies  and  such that then the policy  must be as good as or better than  in the sense 
<br>There are two fundamental approaches:<br>Value Iteration
In value iteration, we simply keep iterating <a data-tooltip-position="top" aria-label="Bellman's Equation" data-href="Bellman's Equation" href="the-guide/machine-learning/reinforcement-learning/theorems/bellman's-equation.html" class="internal-link" target="_self" rel="noopener nofollow">Bellman's equation</a> until the value function converges or we reach the pre-set horizon. While this is the easiest approach, the update of the <a data-tooltip-position="top" aria-label="Value Functions" data-href="Value Functions" href="the-guide/machine-learning/reinforcement-learning/value-functions.html" class="internal-link" target="_self" rel="noopener nofollow">value function</a> via the <a data-tooltip-position="top" aria-label="Q-Function" data-href="Q-Function" href="the-guide/machine-learning/reinforcement-learning/q-function.html" class="internal-link" target="_self" rel="noopener nofollow">Q-function</a> requires a maximization over the whole <a data-tooltip-position="top" aria-label="Markov Decision Process" data-href="Markov Decision Process" href="the-guide/machine-learning/reinforcement-learning/markov-decision-process.html" class="internal-link" target="_self" rel="noopener nofollow">action space</a>  for every state in every iteration, which can be inefficient.
<br>Algorithm - Value Iteration

<br>Start with last layer, where u know the value function , iterate backwards in time 
<br>for each  do

<br>Compute <a data-href="Q-Function" href="the-guide/machine-learning/reinforcement-learning/q-function.html" class="internal-link" target="_self" rel="noopener nofollow">Q-Function</a>

<br>


<br>Compute <a data-tooltip-position="top" aria-label="Value Functions" data-href="Value Functions" href="the-guide/machine-learning/reinforcement-learning/value-functions.html" class="internal-link" target="_self" rel="noopener nofollow">V-Function</a> by maximizing <a data-href="Q-Function" href="the-guide/machine-learning/reinforcement-learning/q-function.html" class="internal-link" target="_self" rel="noopener nofollow">Q-Function</a> over possible actions

<br>




<br>end for
<br>extract optimal <a data-tooltip-position="top" aria-label="Policy" data-href="Policy" href="the-guide/machine-learning/reinforcement-learning/policy.html" class="internal-link" target="_self" rel="noopener nofollow">policy</a> via

<br>



<br>The shown pseudo-code is for <a data-tooltip-position="top" aria-label="- Reinforcement Learning -" data-href="- Reinforcement Learning -" href="the-guide/machine-learning/reinforcement-learning/-reinforcement-learning-.html" class="internal-link" target="_self" rel="noopener nofollow">finite horizons</a>, for the <a data-tooltip-position="top" aria-label="- Reinforcement Learning -" data-href="- Reinforcement Learning -" href="the-guide/machine-learning/reinforcement-learning/-reinforcement-learning-.html" class="internal-link" target="_self" rel="noopener nofollow">infinite</a> setting we need a convergence criterion, e.g. if the value function changes compared to prior iterations. Also, the discount  has to be smaller than . <br>Policy Iteration
In policy iteration, we solve the problem of having to maximize at every iteration by instead following a current policy until the value function converges and then update the policy in a separate step. This way, there are less maximization operations overall.
<br><img alt="center" src="lib/media/pasted-image-20230906131322.png" style="width: 250px; max-width: 100%;"><br>Algorithm - Policy Iteration

<br>Init zero <a data-tooltip-position="top" aria-label="Value Functions" data-href="Value Functions" href="the-guide/machine-learning/reinforcement-learning/value-functions.html" class="internal-link" target="_self" rel="noopener nofollow">V-Function</a> and uniform <a data-tooltip-position="top" aria-label="Policy" data-href="Policy" href="the-guide/machine-learning/reinforcement-learning/policy.html" class="internal-link" target="_self" rel="noopener nofollow">policy</a>
<br>repeat

<br>Evaluation - repeat 

<br>
<br>Compute <a data-href="Q-Function" href="the-guide/machine-learning/reinforcement-learning/q-function.html" class="internal-link" target="_self" rel="noopener nofollow">Q-Function</a> for each <a data-tooltip-position="top" aria-label="Markov Decision Process" data-href="Markov Decision Process" href="the-guide/machine-learning/reinforcement-learning/markov-decision-process.html" class="internal-link" target="_self" rel="noopener nofollow">state-action</a> pair

<br>


<br>Compute <a data-tooltip-position="top" aria-label="Value Functions" data-href="Value Functions" href="the-guide/machine-learning/reinforcement-learning/value-functions.html" class="internal-link" target="_self" rel="noopener nofollow">V-Function</a> for each <a data-tooltip-position="top" aria-label="Markov Decision Process" data-href="Markov Decision Process" href="the-guide/machine-learning/reinforcement-learning/markov-decision-process.html" class="internal-link" target="_self" rel="noopener nofollow">state</a>

<br>




<br>until convergence of <a data-tooltip-position="top" aria-label="Value Functions" data-href="Value Functions" href="the-guide/machine-learning/reinforcement-learning/value-functions.html" class="internal-link" target="_self" rel="noopener nofollow">V</a> or horizon
<br>Improvement 


<br>until convergence of <a data-tooltip-position="top" aria-label="Policy" data-href="Policy" href="the-guide/machine-learning/reinforcement-learning/policy.html" class="internal-link" target="_self" rel="noopener nofollow">policy</a>

<br><br><br>
<br>Synchronous vs. Asynchronuous Dynamic Programming

<br>Synchronous complexity is  for value function and  for state-action value function considering  states and  actions

<br>Number of states often grows exponentially, not feasible for large problems


<br>Asynchronous can be parallelised, larger problems possible 


<br>Policy Iteration vs. Value Iteration

<br>Similarities

<br>Both use dynamic programming, Bellman's equation
<br>Both use one-step lookahead
<br>Both converge to optimal policy<img alt="center" src="lib/media/pasted-image-20230911153156.png" style="width: 700px; max-width: 100%;">


<br>Differences<img alt="center" src="lib/media/pasted-image-20230226145429.png" style="width: 450px; max-width: 100%;">

<br>Maximization instead of simply following a policy makes value iteration more costly
<br>Value Iteration does not extract intermediate policies, intermediate value functions might not be meaningful




<br>Problems
Only feasible for two cases and up until a few million states (synchronous)

<br>Discrete Systems

<br>Integrals  sums
<br>But world is not discrete !
<br>Maximization in continuous spaces is difficult


<br>Linear Systems, Quadratic Reward, Gaussian noise (<a data-href="LQG Regulator" href="the-guide/robotics,-dynamics-and-control/control/lqg-regulator.html" class="internal-link" target="_self" rel="noopener nofollow">LQG Regulator</a>)

<br>But world is not linear
<br>Expectation difficult to compute in non-linear case



<br><br><br>
<br><a rel="noopener nofollow" class="external-link" href="https://www.youtube.com/watch?v=sJIFUTITfBc" target="_blank">https://www.youtube.com/watch?v=sJIFUTITfBc</a>
]]></description><link>the-guide/machine-learning/reinforcement-learning/dynamic-programming.html</link><guid isPermaLink="false">The Guide/Machine Learning/Reinforcement Learning/Dynamic Programming.md</guid><pubDate>Sun, 13 Apr 2025 18:08:06 GMT</pubDate><enclosure url="lib/media/pasted-image-20241112122147.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src=&quot;lib/media/pasted-image-20241112122147.png&quot;&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[Exploration-Exploitation Trade-Off]]></title><description><![CDATA[<a class="tag" href="?query=tag:Machine-Learning" style="background-color: rgb(4, 108, 116); color: white; font-weight: 700; border: none; border-radius: 1em; padding: 0.2em 0.5em;">#Machine-Learning</a> <a class="tag" href="?query=tag:Reinforcement-Learning" style="background-color: rgb(4, 108, 116); color: white; font-weight: 700; border: none; border-radius: 1em; padding: 0.2em 0.5em;">#Reinforcement-Learning</a> 
 <br>In a Nutshell
Fundamental concept in decision-making, <a href=".?query=tag:Machine-Learning" class="tag" target="_blank" rel="noopener nofollow">#Machine-Learning</a> , especially <a href=".?query=tag:Reinforcement-Learning" class="tag" target="_blank" rel="noopener nofollow">#Reinforcement-Learning</a> . It is based on the following conflict:

<br>In order to discover optimal <a data-tooltip-position="top" aria-label="Policy" data-href="Policy" href="the-guide/machine-learning/reinforcement-learning/policy.html" class="internal-link" target="_self" rel="noopener nofollow">policies</a> in a <a data-tooltip-position="top" aria-label="Model-Free Reinforcement Learning" data-href="Model-Free Reinforcement Learning" href="the-guide/machine-learning/reinforcement-learning/model-free-reinforcement-learning.html" class="internal-link" target="_self" rel="noopener nofollow">model-free RL</a> context, we must explore all state-action pairs.
<br>In order to get high returns, we must exploit known high value pairs.<br>
Balancing this trade-off effectively is crucial for optimal performance in uncertain or dynamic environments.

<br><br><br>Make best decision given the currently available information of the environment.<br><br><br>We have  to employ strategies to gather new information about the <a data-tooltip-position="top" aria-label="Markov Decision Process" data-href="Markov Decision Process" href="the-guide/machine-learning/reinforcement-learning/markov-decision-process.html" class="internal-link" target="_self" rel="noopener nofollow">MDP</a> if we do not have or trust our model. The most fundamental strategies are:<br>Epsilon-Greedy Policy 
Explore a random action with probability  and take the greedy action with probability :In order to stabilize training, we can also adapt the hyperparameter  to vanish with increasing number of episodes (<a data-href="GLIE" href="the-guide/machine-learning/reinforcement-learning/glie.html" class="internal-link" target="_self" rel="noopener nofollow">GLIE</a>).
<br>Theorem
For any policy , the -greedy policy wrt.  is an improvement.
<br>Soft-Max Policy
In this approach, we choose the next action with probability proportional to the expected reward and thereby bias towards promising actions. The strength of this effect can be scaled by the hyperparameter of computational temperature : The effect can be understood for the two extreme cases:

<br> (uniform)
<br> greedy

<br>In cases where completely random exploration as above is not efficient or dangerous, we have to revert to more advanced techniques, e.g. through <a data-tooltip-position="top" aria-label="Intrinsic Motivation" data-href="Intrinsic Motivation" href="the-guide/machine-learning/reinforcement-learning/intrinsic-motivation.html" class="internal-link" target="_self" rel="noopener nofollow">intrinsic motivation</a>.]]></description><link>the-guide/machine-learning/reinforcement-learning/exploration-exploitation-trade-off.html</link><guid isPermaLink="false">The Guide/Machine Learning/Reinforcement Learning/Exploration-Exploitation Trade-Off.md</guid><pubDate>Tue, 12 Nov 2024 09:55:30 GMT</pubDate></item><item><title><![CDATA[Generalized Advantage Estimation (GAE)]]></title><description><![CDATA[ 
 <br>To <a data-tooltip-position="top" aria-label="Estimator" data-href="Estimator" href="the-guide/mathematics/statistics/estimator.html" class="internal-link" target="_self" rel="noopener nofollow">|estimate</a> the <a data-tooltip-position="top" aria-label="Advantage Function" data-href="Advantage Function" href="the-guide/machine-learning/reinforcement-learning/advantage-function.html" class="internal-link" target="_self" rel="noopener nofollow">advantage function</a>, we can use a <a data-tooltip-position="top" aria-label="Temporal Difference Learning" data-href="Temporal Difference Learning" href="the-guide/machine-learning/reinforcement-learning/temporal-difference-learning.html" class="internal-link" target="_self" rel="noopener nofollow">TD</a> approach, e.g. However, picking any of these results in the classic <a data-tooltip-position="top" aria-label="Bias-Variance Tradeoff" data-href="Bias-Variance Tradeoff" href="the-guide/machine-learning/reinforcement-learning/theorems/bias-variance-tradeoff.html" class="internal-link" target="_self" rel="noopener nofollow">bias-variance-tradeoff</a>: small look-aheads are biased but have low variance, while longer ones approach unbiased <a data-tooltip-position="top" aria-label="Monte-Carlo Reinforcement Learning" data-href="Monte-Carlo Reinforcement Learning" href="the-guide/machine-learning/reinforcement-learning/monte-carlo-reinforcement-learning.html" class="internal-link" target="_self" rel="noopener nofollow">MC rollouts</a> with high variance. <br>Definition
With GAE, we combine all estimators and enforce an exponential decay on them through an additional hyperparameter .
<br>]]></description><link>the-guide/machine-learning/reinforcement-learning/generalized-advantage-estimation-(gae).html</link><guid isPermaLink="false">The Guide/Machine Learning/Reinforcement Learning/Generalized Advantage Estimation (GAE).md</guid><pubDate>Tue, 12 Nov 2024 09:56:11 GMT</pubDate></item><item><title><![CDATA[GLIE]]></title><description><![CDATA[ 
 <br>Greedy in the Limit of Infinite Exploration
Type of <a data-tooltip-position="top" aria-label="Policy" data-href="Policy" href="the-guide/machine-learning/reinforcement-learning/policy.html" class="internal-link" target="_self" rel="noopener nofollow">policy</a> that converges towards a greedy approach with iterations . Assuming that all <a data-tooltip-position="top" aria-label="Markov Decision Process" data-href="Markov Decision Process" href="the-guide/machine-learning/reinforcement-learning/markov-decision-process.html" class="internal-link" target="_self" rel="noopener nofollow">state-action</a> pairs are theoretically explored infinitely many times  a policy  is GLIE, if 
<br><br>
<br>Example

<br><a data-tooltip-position="top" aria-label="Exploration-Exploitation Trade-Off" data-href="Exploration-Exploitation Trade-Off" href="the-guide/machine-learning/reinforcement-learning/exploration-exploitation-trade-off.html" class="internal-link" target="_self" rel="noopener nofollow">Epsilon-Greedy</a> is GLIE, if 


]]></description><link>the-guide/machine-learning/reinforcement-learning/glie.html</link><guid isPermaLink="false">The Guide/Machine Learning/Reinforcement Learning/GLIE.md</guid><pubDate>Mon, 12 Aug 2024 17:06:50 GMT</pubDate></item><item><title><![CDATA[Intrinsic Motivation]]></title><description><![CDATA[ 
 <br>Caution
There are problems when enforcing <a data-tooltip-position="top" aria-label="Exploration-Exploitation Trade-Off" data-href="Exploration-Exploitation Trade-Off" href="the-guide/machine-learning/reinforcement-learning/exploration-exploitation-trade-off.html" class="internal-link" target="_self" rel="noopener nofollow">exploration</a> in <a data-tooltip-position="top" aria-label="- Reinforcement Learning -" data-href="- Reinforcement Learning -" href="the-guide/machine-learning/reinforcement-learning/-reinforcement-learning-.html" class="internal-link" target="_self" rel="noopener nofollow">RL</a>, namely

<br>Random exploration is prone to failure
<br>In certain environments exploration can be deceptive
<br>Agent can get stuck with small reward states

<br><br>Info
An approach to overcome this is to provide an additional intrinsic reward signal  that motivates the agent to explore via where  balances <a data-tooltip-position="top" aria-label="Exploration-Exploitation Trade-Off" data-href="Exploration-Exploitation Trade-Off" href="the-guide/machine-learning/reinforcement-learning/exploration-exploitation-trade-off.html" class="internal-link" target="_self" rel="noopener nofollow">exploration and exploitation</a>.
<br>
<br>Count-Based Exploration

<br>Explore states where there is the most uncertainty / that were visited the least. The intrinsic reward then reads where  counts the visits for each state. If the number of states is too large to be counted, pseudo-counts can be implemented via unsupervised learning, e.g. <a data-tooltip-position="top" aria-label="Kernel Density Estimation" data-href="Kernel Density Estimation" href="the-guide/computational-statistics/kernel-density-estimation.html" class="internal-link" target="_self" rel="noopener nofollow">KDE</a>, clustering, etc.


<br>Curiosity-Driven Exploration

<br>Compute difference between the features of predicted next states and actual next states via  The greater the difference, the more interesting is the state.


<br>Self-Supervised Exploration via Disagreement

<br>Use multiple prediction models and enforce exploration to states / actions where they disagree the most.


<br><br><br>
<br><a data-href="Exploration by Random Network Destillation" href="research-papers/internship/exploration-by-random-network-destillation.html" class="internal-link" target="_self" rel="noopener nofollow">Exploration by Random Network Destillation</a>
]]></description><link>the-guide/machine-learning/reinforcement-learning/intrinsic-motivation.html</link><guid isPermaLink="false">The Guide/Machine Learning/Reinforcement Learning/Intrinsic Motivation.md</guid><pubDate>Thu, 20 Feb 2025 16:36:31 GMT</pubDate></item><item><title><![CDATA[Markov Decision Process]]></title><description><![CDATA[ 
 <br>In a Nutshell
Elementary model e.g. in <a data-href="- Reinforcement Learning -" href="the-guide/machine-learning/reinforcement-learning/-reinforcement-learning-.html" class="internal-link" target="_self" rel="noopener nofollow">- Reinforcement Learning -</a>, based on a <a data-href="Markov Chain" href="the-guide/computational-statistics/data-assimilation/markov-chain.html" class="internal-link" target="_self" rel="noopener nofollow">Markov Chain</a>, defined by the tuple . Via the <a data-tooltip-position="top" aria-label="Markov Chain" data-href="Markov Chain" href="the-guide/computational-statistics/data-assimilation/markov-chain.html" class="internal-link" target="_self" rel="noopener nofollow">Markov property</a> of the state transition, it is assumed that the current state completely characterizes the process and that the environment is fully observable.
<br><img alt="center" src="lib/media/pasted-image-20221107205734.png" style="width: 300px; max-width: 100%;"><br>Definition
The tuple  with the elements

<br>State <a data-tooltip-position="top" aria-label="Space" data-href="Space" href="the-guide/mathematics/general-stuff/space.html" class="internal-link" target="_self" rel="noopener nofollow">space</a> 
<br>Action space 
<br>Transition function  encoding dynamics 

<br>Deterministic 
<br>Stochastic 


<br>Reward function  with  

<br>Deterministic 
<br>Stochastic 


<br>Initial State <a data-tooltip-position="top" aria-label="Probability Distribution" data-href="Probability Distribution" href="the-guide/mathematics/probability-theory/probability-distribution.html" class="internal-link" target="_self" rel="noopener nofollow">distribution</a> 
<br>Discount Factor 

<br>If , agent is myopic



<br>
<br>Types

<br>Discrete MDP

<br>Discrete state and action space


<br>Continuous MDP

<br>Continuous state space 
<br>Continuous or discrete action space


<br>Absorbing

<br>Absorbing state that can be reached from any other state
<br>Absorbing = state where all actions lead to itself with 0 reward


<br><a data-tooltip-position="top" aria-label="Ergodicity" data-href="Ergodicity" href="the-guide/computational-statistics/data-assimilation/ergodicity.html" class="internal-link" target="_self" rel="noopener nofollow">Ergodic</a>

<br>All states recurrent, visited infinite number of times and without any periodic order.


<br>Regular

<br>Some power of transition matrix has only positive entries.




]]></description><link>the-guide/machine-learning/reinforcement-learning/markov-decision-process.html</link><guid isPermaLink="false">The Guide/Machine Learning/Reinforcement Learning/Markov Decision Process.md</guid><pubDate>Thu, 20 Feb 2025 16:36:31 GMT</pubDate><enclosure url="lib/media/pasted-image-20221107205734.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src=&quot;lib/media/pasted-image-20221107205734.png&quot;&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[Model-Based Reinforcement Learning]]></title><description><![CDATA[ 
 <br>In a Nutshell
Learn a <a data-tooltip-position="top" aria-label="Static and Dynamic Systems" data-href="Static and Dynamic Systems" href="the-guide/robotics,-dynamics-and-control/dynamics/static-and-dynamic-systems.html" class="internal-link" target="_self" rel="noopener nofollow">dynamics model</a> from data and optimize the <a data-tooltip-position="top" aria-label="Policy" data-href="Policy" href="the-guide/machine-learning/reinforcement-learning/policy.html" class="internal-link" target="_self" rel="noopener nofollow">policy</a> using the approximate dynamics model by minimizing a cost function / maximizing a reward (as in optimal control).

<br>Good when there is a well-understood underlying physical model / domain knowledge
<br>Bad when physical model is very complex / not well understood or environment is too complex to model (use <a data-tooltip-position="top" aria-label="Model-Free Reinforcement Learning" data-href="Model-Free Reinforcement Learning" href="the-guide/machine-learning/reinforcement-learning/model-free-reinforcement-learning.html" class="internal-link" target="_self" rel="noopener nofollow">MFRL</a> instead)

<br><br>A full model  in <a data-tooltip-position="top" aria-label="- Reinforcement Learning -" data-href="- Reinforcement Learning -" href="the-guide/machine-learning/reinforcement-learning/-reinforcement-learning-.html" class="internal-link" target="_self" rel="noopener nofollow">- Reinforcement Learning -</a> can be stochastic or deterministic and consists of <br>
<br>An approximated <a data-tooltip-position="top" aria-label="Markov Decision Process" data-href="Markov Decision Process" href="the-guide/machine-learning/reinforcement-learning/markov-decision-process.html" class="internal-link" target="_self" rel="noopener nofollow">reward function</a> 
<br>An approximated <a data-tooltip-position="top" aria-label="Markov Decision Process" data-href="Markov Decision Process" href="the-guide/machine-learning/reinforcement-learning/markov-decision-process.html" class="internal-link" target="_self" rel="noopener nofollow">transition dynamics</a> <br>
<img alt="center" src="lib/media/pasted-image-20230717095355.png" style="width: 300px; max-width: 100%;"> 
<br><br><br>In this context, planning is the process of searching through the state space for an optimal policy by using the model<br><br><br>
<br>Reasons

<br>Policy optimizer (e.g. <a data-tooltip-position="top" aria-label="Gradient Descent" data-href="Gradient Descent" href="the-guide/mathematics/optimization/convex-optimization-lecture/algorithms/gradient-descent.html" class="internal-link" target="_self" rel="noopener nofollow">gradient descent</a>) can lead to problems, e.g. through poorly conditioned gradients
<br>Modeling errors due to model missspecifications or limited data, can be fatal

<br>Sim-to-Real Gap, model never reflects reality




<br>Improvements

<br>Better choice of optimizers
<br>More accurate / involved models
<br>Domain Randomization

<br>Artificially add noise to model




<br><br><br>Closely related to divide and conquer approaches. In both contexts it refers to simplifying a complicated problem by breaking it down into simpler sub-problems in a recursive manner (<a data-href="Bellman's Equation" href="the-guide/machine-learning/reinforcement-learning/theorems/bellman's-equation.html" class="internal-link" target="_self" rel="noopener nofollow">Bellman's Equation</a>).<br>
<br>For model-based reinforcement learning two commonly used approaches that rely on <a data-tooltip-position="top" aria-label="Dynamic Programming" data-href="Dynamic Programming" href="the-guide/machine-learning/reinforcement-learning/dynamic-programming.html" class="internal-link" target="_self" rel="noopener nofollow">dynamic programming</a> are

<br><a data-tooltip-position="top" aria-label="Dynamic Programming" data-href="Dynamic Programming" href="the-guide/machine-learning/reinforcement-learning/dynamic-programming.html" class="internal-link" target="_self" rel="noopener nofollow">Value Iteration</a>
<br><a data-tooltip-position="top" aria-label="Dynamic Programming" data-href="Dynamic Programming" href="the-guide/machine-learning/reinforcement-learning/dynamic-programming.html" class="internal-link" target="_self" rel="noopener nofollow">Policy Iteration</a>


<br><br><br>
<br>Two dimensions -&gt; four categories

<br>Are we using the structure of the model or are we sampling from it ?
<br>Do we use the model during the roll-out (online) or between rollouts (offline)


<br>Structured Models

<br>Offline Usage of Structured Models

<br>Optimal Control, e.g. <a data-tooltip-position="top" aria-label="LQG Regulator" data-href="LQG Regulator" href="the-guide/robotics,-dynamics-and-control/control/lqg-regulator.html" class="internal-link" target="_self" rel="noopener nofollow">LQR</a>, <a data-tooltip-position="top" aria-label="Non-Linear Systems" data-href="Non-Linear Systems" href="Non-Linear Systems" class="internal-link" target="_self" rel="noopener nofollow">iLQR</a>
<br>Why ?

<br>No heavy computation since entirely offline
<br>Scales well to high dimensional action spaces


<br>Problems

<br>Only valid along nominal trajectory




<br>Online Usage of Structured Models

<br>Replan every step online
<br><a data-href="Model Predictive Control" href="the-guide/robotics,-dynamics-and-control/control/model-predictive-control.html" class="internal-link" target="_self" rel="noopener nofollow">Model Predictive Control</a>

<br>Solve small horizon problem at every single timestep using e.g. <a data-tooltip-position="top" aria-label="Non-Linear Systems" data-href="Non-Linear Systems" href="Non-Linear Systems" class="internal-link" target="_self" rel="noopener nofollow">iLQR</a>
<br>Use first step of solution for next step in global problem


<br>Why ?

<br>React to disturbances, deviation from nominal trajectory
<br>Scales well to high dimensional action spaces


<br>Problems

<br>Very costly
<br>Often acts overly greedy
<br>Requires structured model






<br>Sample-Based Methods

<br>Why?

<br>Most structured models use local linearizations, which makes them susceptible to local optima
<br>Model errors get amplified in the gradient with local linearizations 


<br>Sampling-Based Online Methods

<br>Sample points around the current solution, enables to wiggle out of local minima
<br><a data-href="CEM - Cross-Entropy Method" href="the-guide/machine-learning/reinforcement-learning/approaches/cem-cross-entropy-method.html" class="internal-link" target="_self" rel="noopener nofollow">CEM - Cross-Entropy Method</a>
<br>Why ?

<br>No structured model required
<br>Perform better regarding local minima
<br>React to disturbances, deviation from nominal trajectory


<br>Problems

<br>Do not scale well to high dimensions 
<br>Costly,  high sample complexity




<br>Sampling-Based Offline Methods

<br><a data-href="MBPO - Model-Based Policy Optimization" href="the-guide/machine-learning/reinforcement-learning/approaches/mbpo-model-based-policy-optimization.html" class="internal-link" target="_self" rel="noopener nofollow">MBPO - Model-Based Policy Optimization</a> 
<br>Why ?

<br>No heavy computations
<br>Policy is not bound to nominal trajectory
<br>No structured model necessary


<br>Problems

<br>Computations between rollouts are heavy






<br>]]></description><link>the-guide/machine-learning/reinforcement-learning/model-based-reinforcement-learning.html</link><guid isPermaLink="false">The Guide/Machine Learning/Reinforcement Learning/Model-Based Reinforcement Learning.md</guid><pubDate>Sun, 13 Apr 2025 18:08:07 GMT</pubDate><enclosure url="lib/media/pasted-image-20230717095355.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src=&quot;lib/media/pasted-image-20230717095355.png&quot;&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[Model-Free Reinforcement Learning]]></title><description><![CDATA[ 
 <br>Info
Problem - The transition <a data-tooltip-position="top" aria-label="Probability Distribution" data-href="Probability Distribution" href="the-guide/mathematics/probability-theory/probability-distribution.html" class="internal-link" target="_self" rel="noopener nofollow">conditional probabilities</a> for the <a data-tooltip-position="top" aria-label="Markov Decision Process" data-href="Markov Decision Process" href="the-guide/machine-learning/reinforcement-learning/markov-decision-process.html" class="internal-link" target="_self" rel="noopener nofollow">MDP</a>-model needed in the <a data-href="Dynamic Programming" href="the-guide/machine-learning/reinforcement-learning/dynamic-programming.html" class="internal-link" target="_self" rel="noopener nofollow">Dynamic Programming</a> algorithm are in general not given and / or hard to get.
Solution - Model-Free approach using samples to learn e.g. <a data-href="Q-Function" href="the-guide/machine-learning/reinforcement-learning/q-function.html" class="internal-link" target="_self" rel="noopener nofollow">Q-Function</a> or <a data-tooltip-position="top" aria-label="Value Functions" data-href="Value Functions" href="the-guide/machine-learning/reinforcement-learning/value-functions.html" class="internal-link" target="_self" rel="noopener nofollow">Value Function</a> instead of the model.
<br><br><br>
<br>Pure <a data-tooltip-position="top" aria-label="Monte-Carlo Reinforcement Learning" data-href="Monte-Carlo Reinforcement Learning" href="the-guide/machine-learning/reinforcement-learning/monte-carlo-reinforcement-learning.html" class="internal-link" target="_self" rel="noopener nofollow">Monte-Carlo Method</a>

<br>Basically just trial and error
<br>Very inefficient, single bad move at the end makes whole sample strategy bad.


<br>Value-Function Methods

<br>Low-dimensional, discrete state-action space

<br>Online  <a data-href="Temporal Difference Learning" href="the-guide/machine-learning/reinforcement-learning/temporal-difference-learning.html" class="internal-link" target="_self" rel="noopener nofollow">Temporal Difference Learning</a>
<br>Offline  <a data-href="Batch-Mode Reinforcement Learning" href="the-guide/machine-learning/reinforcement-learning/batch-mode-reinforcement-learning.html" class="internal-link" target="_self" rel="noopener nofollow">Batch-Mode Reinforcement Learning</a>


<br>High-dimensional, often continuous state space

<br><a data-tooltip-position="top" aria-label="Reinforcement Learning with Function Approximation" data-href="Reinforcement Learning with Function Approximation" href="the-guide/machine-learning/reinforcement-learning/reinforcement-learning-with-function-approximation.html" class="internal-link" target="_self" rel="noopener nofollow">Function Approximation</a>




<br><a data-href="Policy Search" href="the-guide/machine-learning/reinforcement-learning/policy-search.html" class="internal-link" target="_self" rel="noopener nofollow">Policy Search</a>

<br>Advantages  

<br>Work well for high dimensional or continuous action spaces
<br>Can learn stochastic <a data-tooltip-position="top" aria-label="Policy" data-href="Policy" href="the-guide/machine-learning/reinforcement-learning/policy.html" class="internal-link" target="_self" rel="noopener nofollow">policies</a>
<br>Better convergence 


<br>Problems

<br>Usually converge towards local optimum




<br><a data-href="Actor-Critic Methods" href="the-guide/machine-learning/reinforcement-learning/actor-critic-methods.html" class="internal-link" target="_self" rel="noopener nofollow">Actor-Critic Methods</a>

<br>Combine policy search and value function methods


]]></description><link>the-guide/machine-learning/reinforcement-learning/model-free-reinforcement-learning.html</link><guid isPermaLink="false">The Guide/Machine Learning/Reinforcement Learning/Model-Free Reinforcement Learning.md</guid><pubDate>Mon, 12 Aug 2024 17:06:50 GMT</pubDate></item><item><title><![CDATA[Monte-Carlo Reinforcement Learning]]></title><description><![CDATA[ 
 <br>In a Nusthell
Class of <a data-tooltip-position="top" aria-label="- Reinforcement Learning -" data-href="- Reinforcement Learning -" href="the-guide/machine-learning/reinforcement-learning/-reinforcement-learning-.html" class="internal-link" target="_self" rel="noopener nofollow">online</a> methods for <a data-tooltip-position="top" aria-label="Model-Free Reinforcement Learning" data-href="Model-Free Reinforcement Learning" href="the-guide/machine-learning/reinforcement-learning/model-free-reinforcement-learning.html" class="internal-link" target="_self" rel="noopener nofollow">model-free</a> <a data-href="- Reinforcement Learning -" href="the-guide/machine-learning/reinforcement-learning/-reinforcement-learning-.html" class="internal-link" target="_self" rel="noopener nofollow">- Reinforcement Learning -</a>. Learn from complete episodes (no bootstrapping, episodes must terminate) and use the simplest approach of value = <a data-tooltip-position="top" aria-label="Expectations" data-href="Expectations" href="the-guide/mathematics/statistics/expectations.html" class="internal-link" target="_self" rel="noopener nofollow">mean</a>. 

<br>No bias and good convergence, but high <a data-tooltip-position="top" aria-label="Variance" data-href="Variance" href="Variance" class="internal-link" target="_self" rel="noopener nofollow">variance</a> when compared to temporal <a data-tooltip-position="top" aria-label="Temporal Difference Learning" data-href="Temporal Difference Learning" href="the-guide/machine-learning/reinforcement-learning/temporal-difference-learning.html" class="internal-link" target="_self" rel="noopener nofollow">TD learning</a> (<a data-href="Bias-Variance Tradeoff" href="the-guide/machine-learning/reinforcement-learning/theorems/bias-variance-tradeoff.html" class="internal-link" target="_self" rel="noopener nofollow">Bias-Variance Tradeoff</a>)

<br><img alt="center" src="lib/media/pasted-image-20241112122049.png"><br><br>2 Types<br>
<br>Model-free Prediction

<br>I<a class="internal-link" data-href="Covariance and Variance.md" href="the-guide/mathematics/statistics/covariance-and-variance.html" target="_self" rel="noopener nofollow"></a>erience in MDP (policy or random)
<br>Output: <a data-tooltip-position="top" aria-label="Value Functions" data-href="Value Functions" href="the-guide/machine-learning/reinforcement-learning/value-functions.html" class="internal-link" target="_self" rel="noopener nofollow">value function</a> 


<br>Model-free Control

<br>Input: Episodes of experience 
<br>Output: <a data-tooltip-position="top" aria-label="Policy" data-href="Policy" href="the-guide/machine-learning/reinforcement-learning/policy.html" class="internal-link" target="_self" rel="noopener nofollow">policy</a> + <a data-tooltip-position="top" aria-label="Value Functions" data-href="Value Functions" href="the-guide/machine-learning/reinforcement-learning/value-functions.html" class="internal-link" target="_self" rel="noopener nofollow">value function</a>


<br><br><br>
<br>Monte - Carlo Policy Evaluation

<br>Goal: Learn <a data-tooltip-position="top" aria-label="Value Functions" data-href="Value Functions" href="the-guide/machine-learning/reinforcement-learning/value-functions.html" class="internal-link" target="_self" rel="noopener nofollow">value function</a> from experience under <a data-tooltip-position="top" aria-label="Policy" data-href="Policy" href="the-guide/machine-learning/reinforcement-learning/policy.html" class="internal-link" target="_self" rel="noopener nofollow">policy</a>
<br>(A) First - Visit Monte-Carlo Policy Evaluation

<br>Average returns only for first time a <a data-tooltip-position="top" aria-label="Markov Decision Process" data-href="Markov Decision Process" href="the-guide/machine-learning/reinforcement-learning/markov-decision-process.html" class="internal-link" target="_self" rel="noopener nofollow">state</a> is visited  <a data-tooltip-position="top" aria-label="Estimator" data-href="Estimator" href="the-guide/mathematics/statistics/estimator.html" class="internal-link" target="_self" rel="noopener nofollow">unbiased</a>
<br>Algorithm - Input: <a data-tooltip-position="top" aria-label="Policy" data-href="Policy" href="the-guide/machine-learning/reinforcement-learning/policy.html" class="internal-link" target="_self" rel="noopener nofollow">policy</a> , arbitrary <a data-tooltip-position="top" aria-label="Value Functions" data-href="Value Functions" href="the-guide/machine-learning/reinforcement-learning/value-functions.html" class="internal-link" target="_self" rel="noopener nofollow">value function</a> V, empty list for every state Ret(s)
<br>while true do

<br>Generate an episode using 
<br>for each state s in the episode do

<br>R ← return following the first occurrence of s
<br>Append R to Ret(s)
<br>V (s) ← average(Ret(s))


<br>end for


<br>end while


<br>(B) Every - Visit Monte - Carlo Policy Evaluation

<br>Average returns for every time a <a data-tooltip-position="top" aria-label="Markov Decision Process" data-href="Markov Decision Process" href="the-guide/machine-learning/reinforcement-learning/markov-decision-process.html" class="internal-link" target="_self" rel="noopener nofollow">state</a> is visited  <a data-tooltip-position="top" aria-label="Estimator" data-href="Estimator" href="the-guide/mathematics/statistics/estimator.html" class="internal-link" target="_self" rel="noopener nofollow">biased, but consistent</a>
<br>Algorithm - Input: <a data-tooltip-position="top" aria-label="Policy" data-href="Policy" href="the-guide/machine-learning/reinforcement-learning/policy.html" class="internal-link" target="_self" rel="noopener nofollow">policy</a> , arbitrary <a data-tooltip-position="top" aria-label="Value Functions" data-href="Value Functions" href="the-guide/machine-learning/reinforcement-learning/value-functions.html" class="internal-link" target="_self" rel="noopener nofollow">value function</a> V, empty list for every state Ret(s)
<br>while true do

<br>Generate an episode using 
<br>for each state s in the episode do

<br>for each occurrence of the state s do

<br>R ← return following the first occurrence of s
<br>Append R to Ret(s)
<br>V (s) ← average(Ret(s))


<br>end for


<br>end for


<br>end while




<br><br><br>
<br><a data-tooltip-position="top" aria-label="- Reinforcement Learning -" data-href="- Reinforcement Learning -" href="the-guide/machine-learning/reinforcement-learning/-reinforcement-learning-.html" class="internal-link" target="_self" rel="noopener nofollow">On-Policy</a>

<br>On-Policy Monte-Carlo Control<img alt="center" src="lib/media/pasted-image-20230906131732.png" style="width: 250px; max-width: 100%;">

<br>Generalized <a data-tooltip-position="top" aria-label="Dynamic Programming" data-href="Dynamic Programming" href="the-guide/machine-learning/reinforcement-learning/dynamic-programming.html" class="internal-link" target="_self" rel="noopener nofollow">policy iteration</a> with <a data-tooltip-position="top" aria-label="Q-Function" data-href="Q-Function" href="the-guide/machine-learning/reinforcement-learning/q-function.html" class="internal-link" target="_self" rel="noopener nofollow">Q-function</a> instead of <a data-tooltip-position="top" aria-label="Value Functions" data-href="Value Functions" href="the-guide/machine-learning/reinforcement-learning/value-functions.html" class="internal-link" target="_self" rel="noopener nofollow">value function</a>, because greedy policy update based in value function requires <a data-tooltip-position="top" aria-label="Markov Decision Process" data-href="Markov Decision Process" href="the-guide/machine-learning/reinforcement-learning/markov-decision-process.html" class="internal-link" target="_self" rel="noopener nofollow">transition model</a> (assumes best action is taken, but I don't know which on is best / I have to explore them all). 
<br>In order to implement <a data-tooltip-position="top" aria-label="Exploration-Exploitation Trade-Off" data-href="Exploration-Exploitation Trade-Off" href="the-guide/machine-learning/reinforcement-learning/exploration-exploitation-trade-off.html" class="internal-link" target="_self" rel="noopener nofollow">exploration</a>, the policy improvement step is computed using -greedy<img alt="center" src="lib/media/pasted-image-20230906132820.png" style="width: 200px; max-width: 100%;">


<br><a data-href="GLIE" href="the-guide/machine-learning/reinforcement-learning/glie.html" class="internal-link" target="_self" rel="noopener nofollow">GLIE</a> Monte-Carlo Control

<br>Evaluation based on -th episode 

<br>For each state-action pair in the episode do

<br>
<br>




<br>Improve policy using -greedy algorithm with 




<br><a data-tooltip-position="top" aria-label="- Reinforcement Learning -" data-href="- Reinforcement Learning -" href="the-guide/machine-learning/reinforcement-learning/-reinforcement-learning-.html" class="internal-link" target="_self" rel="noopener nofollow">Off-Policy</a>

<br>Monte-Carlo Control via <a data-href="Importance Sampling" href="the-guide/mathematics/probability-theory/importance-sampling.html" class="internal-link" target="_self" rel="noopener nofollow">Importance Sampling</a>

<br>Return in update for Q-function is weighted according to similarity between policies which is now the <a data-tooltip-position="top" aria-label="Expectations" data-href="Expectations" href="the-guide/mathematics/statistics/expectations.html" class="internal-link" target="_self" rel="noopener nofollow">expected</a> (over ) instead of the mean return.
<br>Cannot use if exploration policy  is zero where evaluation policy  is non-zero
<br>Importance sampling can drastically increase <a data-tooltip-position="top" aria-label="Covariance and Variance" data-href="Covariance and Variance" href="the-guide/mathematics/statistics/covariance-and-variance.html" class="internal-link" target="_self" rel="noopener nofollow">variance</a>




]]></description><link>the-guide/machine-learning/reinforcement-learning/monte-carlo-reinforcement-learning.html</link><guid isPermaLink="false">The Guide/Machine Learning/Reinforcement Learning/Monte-Carlo Reinforcement Learning.md</guid><pubDate>Thu, 20 Feb 2025 16:36:31 GMT</pubDate><enclosure url="lib/media/pasted-image-20241112122049.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src=&quot;lib/media/pasted-image-20241112122049.png&quot;&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[Policy]]></title><description><![CDATA[ 
 <br>In a Nutshell
A <a data-tooltip-position="top" aria-label="Mathematical Relations" data-href="Mathematical Relations" href="the-guide/mathematics/general-stuff/mathematical-relations.html" class="internal-link" target="_self" rel="noopener nofollow">function</a> , taking in an element  of an <a data-tooltip-position="top" aria-label="Markov Decision Process" data-href="Markov Decision Process" href="the-guide/machine-learning/reinforcement-learning/markov-decision-process.html" class="internal-link" target="_self" rel="noopener nofollow">MDP's state space</a> and returning an element  of- or a <a data-tooltip-position="top" aria-label="Probability Distribution" data-href="Probability Distribution" href="the-guide/mathematics/probability-theory/probability-distribution.html" class="internal-link" target="_self" rel="noopener nofollow">probability distribution</a> over the associated action space .
<br><br>Definitions 
Assume a <a data-tooltip-position="top" aria-label="Markov Decision Process" data-href="Markov Decision Process" href="the-guide/machine-learning/reinforcement-learning/markov-decision-process.html" class="internal-link" target="_self" rel="noopener nofollow">MDP</a> . A policy assigns an <a data-tooltip-position="top" aria-label="Markov Decision Process" data-href="Markov Decision Process" href="the-guide/machine-learning/reinforcement-learning/markov-decision-process.html" class="internal-link" target="_self" rel="noopener nofollow">action</a>, or a <a data-tooltip-position="top" aria-label="Probability Distribution" data-href="Probability Distribution" href="the-guide/mathematics/probability-theory/probability-distribution.html" class="internal-link" target="_self" rel="noopener nofollow">distribution</a> over actions, to a <a data-tooltip-position="top" aria-label="Markov Decision Process" data-href="Markov Decision Process" href="the-guide/machine-learning/reinforcement-learning/markov-decision-process.html" class="internal-link" target="_self" rel="noopener nofollow">state</a>

<br>Deterministic Policy 
<br>Stochastic Policy 

<br>
<br>A deterministic policy can be written as a stochastic policy, if the deterministic action has probability 1.
<br>Optimal policy  is the policy that maximizes the <a data-tooltip-position="top" aria-label="Value Functions" data-href="Value Functions" href="the-guide/machine-learning/reinforcement-learning/value-functions.html" class="internal-link" target="_self" rel="noopener nofollow">value function</a>/expected return for every <a data-tooltip-position="top" aria-label="Markov Decision Process" data-href="Markov Decision Process" href="the-guide/machine-learning/reinforcement-learning/markov-decision-process.html" class="internal-link" target="_self" rel="noopener nofollow">state</a>

<br>There is always an (or multiple) optimal policy for any <a data-tooltip-position="top" aria-label="Markov Decision Process" data-href="Markov Decision Process" href="the-guide/machine-learning/reinforcement-learning/markov-decision-process.html" class="internal-link" target="_self" rel="noopener nofollow">MDP</a>
<br>Value functions define partial ordering over policies, optimal policy is greater or equal than all other policies
<br>All optimal policies induce optimal <a data-tooltip-position="top" aria-label="Value Functions" data-href="Value Functions" href="the-guide/machine-learning/reinforcement-learning/value-functions.html" class="internal-link" target="_self" rel="noopener nofollow">value function</a> and <a data-tooltip-position="top" aria-label="Q-Function" data-href="Q-Function" href="the-guide/machine-learning/reinforcement-learning/q-function.html" class="internal-link" target="_self" rel="noopener nofollow">Q-function</a>


]]></description><link>the-guide/machine-learning/reinforcement-learning/policy.html</link><guid isPermaLink="false">The Guide/Machine Learning/Reinforcement Learning/Policy.md</guid><pubDate>Mon, 12 Aug 2024 17:06:50 GMT</pubDate></item><item><title><![CDATA[Policy Gradient Methods]]></title><description><![CDATA[ 
 <br>Info
Family of methods for <a data-href="Policy Search" href="the-guide/machine-learning/reinforcement-learning/policy-search.html" class="internal-link" target="_self" rel="noopener nofollow">Policy Search</a> algorithms in <a data-tooltip-position="top" aria-label="Model-Free Reinforcement Learning" data-href="Model-Free Reinforcement Learning" href="the-guide/machine-learning/reinforcement-learning/model-free-reinforcement-learning.html" class="internal-link" target="_self" rel="noopener nofollow">model-free</a> <a data-tooltip-position="top" aria-label="- Reinforcement Learning -" data-href="- Reinforcement Learning -" href="the-guide/machine-learning/reinforcement-learning/-reinforcement-learning-.html" class="internal-link" target="_self" rel="noopener nofollow">RL</a>. The <a data-tooltip-position="top" aria-label="Expectations" data-href="Expectations" href="the-guide/mathematics/statistics/expectations.html" class="internal-link" target="_self" rel="noopener nofollow">expected</a> long term rewards  is written as <a data-tooltip-position="top" aria-label="Expectations" data-href="Expectations" href="the-guide/mathematics/statistics/expectations.html" class="internal-link" target="_self" rel="noopener nofollow">expectation</a> over trajectory distribution via using the probability of a trajectory  and the accumulated <a data-tooltip-position="top" aria-label="Markov Decision Process" data-href="Markov Decision Process" href="the-guide/machine-learning/reinforcement-learning/markov-decision-process.html" class="internal-link" target="_self" rel="noopener nofollow">reward</a>.
<br><br>
<br>
Episode-Based

<br>Finite Differences

<br>Estimate -th partial derivative by perturbing 

<br>


<br>One evaluation per dimension


<br><a data-href="REINFORCE" href="the-guide/machine-learning/reinforcement-learning/approaches/reinforce.html" class="internal-link" target="_self" rel="noopener nofollow">REINFORCE</a>

<br>In general high <a data-tooltip-position="top" aria-label="Covariance and Variance" data-href="Covariance and Variance" href="the-guide/mathematics/statistics/covariance-and-variance.html" class="internal-link" target="_self" rel="noopener nofollow">variance</a>




<br>
Step-Based

<br>Rewards in the past are not correlated with actions in the future. Decompose rewards and neglect derivatives of actions that don't influence that action.
<br><a data-href="GPOMDP" href="the-guide/machine-learning/reinforcement-learning/approaches/gpomdp.html" class="internal-link" target="_self" rel="noopener nofollow">GPOMDP</a>with optimal time-dependent baseline.


<br>
Both <a data-tooltip-position="top" aria-label="TRPO - Trust Region Policy Optimization" data-href="TRPO - Trust Region Policy Optimization" href="the-guide/machine-learning/reinforcement-learning/approaches/trpo-trust-region-policy-optimization.html" class="internal-link" target="_self" rel="noopener nofollow">TRPO</a> and <a data-tooltip-position="top" aria-label="PPO - Proximal Policy Optimization" data-href="PPO - Proximal Policy Optimization" href="the-guide/machine-learning/reinforcement-learning/approaches/ppo-proximal-policy-optimization.html" class="internal-link" target="_self" rel="noopener nofollow">PPO</a> are policy gradient methods, because they directly update their actor policies parameters.

<br><br><br>
<br>In practice the gradient has very high <a data-tooltip-position="top" aria-label="Covariance and Variance" data-href="Covariance and Variance" href="the-guide/mathematics/statistics/covariance-and-variance.html" class="internal-link" target="_self" rel="noopener nofollow">variance</a>, can be reduced using a (entry-wise) baseline leading to the additional term This baseline can therefore reduce the variance (by lowering the impact of extreme rewards?)  without leading to a biased estimate.
<br>For every Policy Gradient Method, there exists an optimal baseline 
]]></description><link>the-guide/machine-learning/reinforcement-learning/policy-gradient-methods.html</link><guid isPermaLink="false">The Guide/Machine Learning/Reinforcement Learning/Policy Gradient Methods.md</guid><pubDate>Tue, 04 Mar 2025 10:15:11 GMT</pubDate></item><item><title><![CDATA[Policy Representations]]></title><description><![CDATA[ 
 <br>
<br>Categorization

<br>Explicit 

<br>Also called parametric, essentially <a data-tooltip-position="top" aria-label="Probability Distribution" data-href="Probability Distribution" href="the-guide/mathematics/probability-theory/probability-distribution.html" class="internal-link" target="_self" rel="noopener nofollow">conditional</a> <a data-tooltip-position="top" aria-label="Probability Distribution" data-href="Probability Distribution" href="the-guide/mathematics/probability-theory/probability-distribution.html" class="internal-link" target="_self" rel="noopener nofollow">probability distributions</a>

<br>

<br>


<br>

<br>




<br>Advantages

<br>Naturally incorporate continuous actions
<br><a data-href="- Imitation Learning -" href="the-guide/machine-learning/imitation-learning/-imitation-learning-.html" class="internal-link" target="_self" rel="noopener nofollow">- Imitation Learning -</a>

<br>Generalize to unseen situations


<br><a data-href="- Reinforcement Learning -" href="the-guide/machine-learning/reinforcement-learning/-reinforcement-learning-.html" class="internal-link" target="_self" rel="noopener nofollow">- Reinforcement Learning -</a>

<br>Autonomous self Improvement






<br>Implicit

<br><a data-tooltip-position="top" aria-label="Temporal Difference Learning" data-href="Temporal Difference Learning" href="the-guide/machine-learning/reinforcement-learning/temporal-difference-learning.html" class="internal-link" target="_self" rel="noopener nofollow">Epsilon-Greedy or Soft-Max</a>


<br><a data-tooltip-position="top" aria-label="Policy" data-href="Policy" href="the-guide/machine-learning/reinforcement-learning/policy.html" class="internal-link" target="_self" rel="noopener nofollow">Stochastic or Deterministic</a>

<br>Encodes variability in movements and exploration




<br>Vanilla Policy Representation

<br>Examples (global)

<br>RBF
<br><a data-href="Artificial Neural Network" href="the-guide/machine-learning/deep-learning/artificial-neural-network.html" class="internal-link" target="_self" rel="noopener nofollow">Artificial Neural Network</a>
<br>Linear <a data-tooltip-position="top" aria-label="Model-Based Control" data-href="Model-Based Control" href="the-guide/robotics,-dynamics-and-control/control/model-based-control.html" class="internal-link" target="_self" rel="noopener nofollow">Controller</a> 


<br>Advantages

<br>Compact representation
<br>Easy to learn


<br>Problems

<br>Bad Generalization for states where no data was available
<br>Not Robust to small changes in system
<br>Sample Efficiency bad for large <a data-tooltip-position="top" aria-label="Markov Decision Process" data-href="Markov Decision Process" href="the-guide/machine-learning/reinforcement-learning/markov-decision-process.html" class="internal-link" target="_self" rel="noopener nofollow">state-action spaces</a>




<br>Dynamic System Policy Representation

<br>Controller with time and state - dependant trajectory to follow 
<br>Examples

<br>Trajectory following controller
<br>System state depends on environmental state / context 

<br>






<br>Movement Primitives

<br>Compact representations of a movement

<br>Often parametrized as trajectory generator

<br>


<br>Can encode stroke and rythmic movements


<br>Add forcing function  to system dynamics

<br>

<br>Encodes desired additional acceleration profile
<br>Stable by construction, as it vanishes for 
<br>Temporal Scaling by using model phase  with  instead of time
<br>Multi-DoF settings by individual DMP per DoF
<br>Can be represented e.g. by normalized RBF




<br>Probabilistic Movement Primitives

<br>Stochastic trajectory representation 

<br>Single trajectory represented via 


<br>

<br>	


<br>

<br>Use Gaussians for 


<br>For multiple DOF use block diagonal matrix with concatenated vectors<img alt="center" src="lib/media/pasted-image-20230302162803.png" style="width: 350px; max-width: 100%;">

<br>
<br>


<br>Advantages

<br>Can represent unvertainty and with it importance of time points






]]></description><link>the-guide/machine-learning/reinforcement-learning/policy-representations.html</link><guid isPermaLink="false">The Guide/Machine Learning/Reinforcement Learning/Policy Representations.md</guid><pubDate>Thu, 17 Apr 2025 07:39:47 GMT</pubDate><enclosure url="lib/media/pasted-image-20230302162803.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src=&quot;lib/media/pasted-image-20230302162803.png&quot;&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[Policy Search]]></title><description><![CDATA[ 
 <br>Info
Instead of using <a data-tooltip-position="top" aria-label="Value Functions" data-href="Value Functions" href="the-guide/machine-learning/reinforcement-learning/value-functions.html" class="internal-link" target="_self" rel="noopener nofollow">value functions</a> or <a data-tooltip-position="top" aria-label="Q-Function" data-href="Q-Function" href="the-guide/machine-learning/reinforcement-learning/q-function.html" class="internal-link" target="_self" rel="noopener nofollow">Q-functions</a> to learn a <a data-tooltip-position="top" aria-label="Policy" data-href="Policy" href="the-guide/machine-learning/reinforcement-learning/policy.html" class="internal-link" target="_self" rel="noopener nofollow">policy</a>, directly update the policy based in experience. Suitable for continuous or discrete action-spaces.

<br>Question

<br>How to find good parameters  ? Optimize policy in a given parametric policy family where .



<br>
<br>Motivation

<br>Value-based algorithms (e.g. <a data-tooltip-position="top" aria-label="Temporal Difference Learning" data-href="Temporal Difference Learning" href="the-guide/machine-learning/reinforcement-learning/temporal-difference-learning.html" class="internal-link" target="_self" rel="noopener nofollow">TD-Learning</a>) difficult to handle in continuous  or high-dimensional case, use parameterized policies  for action selection !
<br><a data-tooltip-position="top" aria-label="Exploration-Exploitation Trade-Off" data-href="Exploration-Exploitation Trade-Off" href="the-guide/machine-learning/reinforcement-learning/exploration-exploitation-trade-off.html" class="internal-link" target="_self" rel="noopener nofollow">Exploration</a>

<br>Stochastic policies directly enable exploration, most commonly <a data-tooltip-position="top" aria-label="Gaussian Distribution" data-href="Gaussian Distribution" href="the-guide/mathematics/probability-theory/gaussian-distribution.html" class="internal-link" target="_self" rel="noopener nofollow">Gaussians</a>


<br>Advantages

<br>Learning <a data-tooltip-position="top" aria-label="Value Functions" data-href="Value Functions" href="the-guide/machine-learning/reinforcement-learning/value-functions.html" class="internal-link" target="_self" rel="noopener nofollow">Value function</a> may be difficult
<br>Domain Knowledge can be encoded
<br>Demonstrations can be used as init
<br>Max Operators for Value function can be tricky


<br>Problems

<br>No global convergence guarantees
<br>Policy evaluation inefficient and may have high <a data-tooltip-position="top" aria-label="Covariance and Variance" data-href="Covariance and Variance" href="the-guide/mathematics/statistics/covariance-and-variance.html" class="internal-link" target="_self" rel="noopener nofollow">variance</a>




<br><br><img alt="center" src="lib/media/pasted-image-20230908165108.png"><br>
Optimize <a data-tooltip-position="top" aria-label="Policy" data-href="Policy" href="the-guide/machine-learning/reinforcement-learning/policy.html" class="internal-link" target="_self" rel="noopener nofollow">policy</a> in <a data-tooltip-position="top" aria-label="Model-Free Reinforcement Learning" data-href="Model-Free Reinforcement Learning" href="the-guide/machine-learning/reinforcement-learning/model-free-reinforcement-learning.html" class="internal-link" target="_self" rel="noopener nofollow">model-free RL</a> context. Generally two steps:<br><br>
<br>Exploration  Generate a trajectory  

<br>Episode-Based

<br>Exploration in parameter space


<br>Step-Based

<br>Exploration in action space




<br>Evaluation  Assess quality of this trajectory.

<br>Episode-Based

<br>One data point per trajectory for policy update:

<br>

<br>




<br>Learn search distribution (upper-level policy)  for the parameters of the low-level control policy .

<br>


<br>Advantages

<br>Efficient for small amounts of parameters
<br>More sophisticated exploration




<br>Step-Based

<br>One data point per <a data-tooltip-position="top" aria-label="Markov Decision Process" data-href="Markov Decision Process" href="the-guide/machine-learning/reinforcement-learning/markov-decision-process.html" class="internal-link" target="_self" rel="noopener nofollow">action-state</a> pair

<br>
<br>


<br>Explore action space at each time step with stochastic low-level policy 
<br>Advantages 

<br>Less likely to create unstable policies.
<br>Less variance in quality assessment






<br><br>
<br>Update  Compute new (possibly better) policy

<br><a data-href="Policy Gradient Methods" href="the-guide/machine-learning/reinforcement-learning/policy-gradient-methods.html" class="internal-link" target="_self" rel="noopener nofollow">Policy Gradient Methods</a>

<br>Classical methods, assume euclidean geometry


<br><a data-tooltip-position="top" aria-label="The Natural Gradient" data-href="The Natural Gradient" href="the-guide/machine-learning/reinforcement-learning/the-natural-gradient.html" class="internal-link" target="_self" rel="noopener nofollow">Natural Gradients</a>

<br>Uses metric in parameter space




<br><br><br>
<br>Success Matching Principle

<br>Update policy in order to learn how to reproduce successful outcome
<br>Iterate

<br>Create   and evaluate  samples
<br>Compute success probability 

<br>Computing the Weights

<br>Exponential Transformation

<br>
<br> by heuristic






<br>Compute success weighted policy (E-step) 
<br>Fit a new policy (M-step)   that best approximates 

<br>Policy Fitting

<br>

<br>Set 


<br>Closed form solution via Weighted Maximum Likelihood exists for gaussian policy

<br>      










<br>Relative Entropy Policy Search (REPS)

<br>Policy update as constrained optimization problem


]]></description><link>the-guide/machine-learning/reinforcement-learning/policy-search.html</link><guid isPermaLink="false">The Guide/Machine Learning/Reinforcement Learning/Policy Search.md</guid><pubDate>Mon, 12 Aug 2024 17:06:50 GMT</pubDate><enclosure url="lib/media/pasted-image-20230908165108.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src=&quot;lib/media/pasted-image-20230908165108.png&quot;&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[Q-Function]]></title><description><![CDATA[ 
 <br>Info
Function returning the (discounted) <a data-tooltip-position="top" aria-label="Expectations" data-href="Expectations" href="the-guide/mathematics/statistics/expectations.html" class="internal-link" target="_self" rel="noopener nofollow">expected</a> return when starting in <a data-tooltip-position="top" aria-label="Markov Decision Process" data-href="Markov Decision Process" href="the-guide/machine-learning/reinforcement-learning/markov-decision-process.html" class="internal-link" target="_self" rel="noopener nofollow">state</a> , taking <a data-tooltip-position="top" aria-label="Markov Decision Process" data-href="Markov Decision Process" href="the-guide/machine-learning/reinforcement-learning/markov-decision-process.html" class="internal-link" target="_self" rel="noopener nofollow">action</a>  and then following the <a data-tooltip-position="top" aria-label="Policy" data-href="Policy" href="the-guide/machine-learning/reinforcement-learning/policy.html" class="internal-link" target="_self" rel="noopener nofollow">policy</a>  For stochastic environments, we get 
<br><br>
<br><a data-tooltip-position="top" aria-label="- Reinforcement Learning -" data-href="- Reinforcement Learning -" href="the-guide/machine-learning/reinforcement-learning/-reinforcement-learning-.html" class="internal-link" target="_self" rel="noopener nofollow">Finite Horizon</a>

<br>
<br><a data-tooltip-position="top" aria-label="Expectations" data-href="Expectations" href="the-guide/mathematics/statistics/expectations.html" class="internal-link" target="_self" rel="noopener nofollow">Expected</a> (can be stochastic) long-term reward for <a data-tooltip-position="top" aria-label="Markov Chain" data-href="Markov Chain" href="the-guide/computational-statistics/data-assimilation/markov-chain.html" class="internal-link" target="_self" rel="noopener nofollow">state</a>  when following the <a data-tooltip-position="top" aria-label="Policy" data-href="Policy" href="the-guide/machine-learning/reinforcement-learning/policy.html" class="internal-link" target="_self" rel="noopener nofollow">policy</a>  


<br><a data-tooltip-position="top" aria-label="- Reinforcement Learning -" data-href="- Reinforcement Learning -" href="the-guide/machine-learning/reinforcement-learning/-reinforcement-learning-.html" class="internal-link" target="_self" rel="noopener nofollow">Infinite Horizon</a>

<br>
<br>Same principle as finite case, but no timeframe. 


<br>Relationship with Value Function
Can be computed from <a data-tooltip-position="top" aria-label="Value Functions" data-href="Value Functions" href="the-guide/machine-learning/reinforcement-learning/value-functions.html" class="internal-link" target="_self" rel="noopener nofollow">Value Function</a> using <a data-href="Bellman's Equation" href="the-guide/machine-learning/reinforcement-learning/theorems/bellman's-equation.html" class="internal-link" target="_self" rel="noopener nofollow">Bellman's Equation</a> (holds for both horizons):
]]></description><link>the-guide/machine-learning/reinforcement-learning/q-function.html</link><guid isPermaLink="false">The Guide/Machine Learning/Reinforcement Learning/Q-Function.md</guid><pubDate>Thu, 20 Feb 2025 16:36:31 GMT</pubDate></item><item><title><![CDATA[Reinforcement Learning Overview]]></title><description><![CDATA[ 
 <br><a data-href="Model-Based Reinforcement Learning" href="the-guide/machine-learning/reinforcement-learning/model-based-reinforcement-learning.html" class="internal-link" target="_self" rel="noopener nofollow">Model-Based Reinforcement Learning</a><br><a data-href="Model-Free Reinforcement Learning" href="the-guide/machine-learning/reinforcement-learning/model-free-reinforcement-learning.html" class="internal-link" target="_self" rel="noopener nofollow">Model-Free Reinforcement Learning</a><br><a data-href="- Reinforcement Learning -" href="the-guide/machine-learning/reinforcement-learning/-reinforcement-learning-.html" class="internal-link" target="_self" rel="noopener nofollow">- Reinforcement Learning -</a><br><br>
<br>Value Iteration
<br>Policy Iteration
<br><a data-href="Monte-Carlo Reinforcement Learning" href="the-guide/machine-learning/reinforcement-learning/monte-carlo-reinforcement-learning.html" class="internal-link" target="_self" rel="noopener nofollow">Monte-Carlo Reinforcement Learning</a><br><a data-href="Policy Search" href="the-guide/machine-learning/reinforcement-learning/policy-search.html" class="internal-link" target="_self" rel="noopener nofollow">Policy Search</a><br><a data-href="Actor-Critic Methods" href="the-guide/machine-learning/reinforcement-learning/actor-critic-methods.html" class="internal-link" target="_self" rel="noopener nofollow">Actor-Critic Methods</a><br><br>
<br>Policy Evaluation

<br>TD(n)
<br>TD()


<br>Policy Improvement

<br><a data-href="(Deep) Q-Learning" href="the-guide/machine-learning/reinforcement-learning/approaches/(deep)-q-learning.html" class="internal-link" target="_self" rel="noopener nofollow">(Deep) Q-Learning</a>
<br><a data-href="SARSA" href="the-guide/machine-learning/reinforcement-learning/approaches/sarsa.html" class="internal-link" target="_self" rel="noopener nofollow">SARSA</a>


<br>Value Based Methods<br><a data-href="Reinforcement Learning with Function Approximation" href="the-guide/machine-learning/reinforcement-learning/reinforcement-learning-with-function-approximation.html" class="internal-link" target="_self" rel="noopener nofollow">Reinforcement Learning with Function Approximation</a><br><br>
<br>Vanilla
<br>DQN
<br>Rainbow
<br>
<br><a data-href="Policy Gradient Methods" href="the-guide/machine-learning/reinforcement-learning/policy-gradient-methods.html" class="internal-link" target="_self" rel="noopener nofollow">Policy Gradient Methods</a>
<br><a data-href="The Natural Gradient" href="the-guide/machine-learning/reinforcement-learning/the-natural-gradient.html" class="internal-link" target="_self" rel="noopener nofollow">The Natural Gradient</a>
low dimhigh dim]]></description><link>the-guide/machine-learning/reinforcement-learning/reinforcement-learning-overview.html</link><guid isPermaLink="false">The Guide/Machine Learning/Reinforcement Learning/Reinforcement Learning Overview.canvas</guid><pubDate>Thu, 20 Feb 2025 16:36:31 GMT</pubDate></item><item><title><![CDATA[Reinforcement Learning with Function Approximation]]></title><description><![CDATA[ 
 <br>In a Nutshell
<a data-href="- Reinforcement Learning -" href="the-guide/machine-learning/reinforcement-learning/-reinforcement-learning-.html" class="internal-link" target="_self" rel="noopener nofollow">- Reinforcement Learning -</a> using <a data-tooltip-position="top" aria-label="Markov Decision Process" data-href="Markov Decision Process" href="the-guide/machine-learning/reinforcement-learning/markov-decision-process.html" class="internal-link" target="_self" rel="noopener nofollow">continuous MDP</a>. Using <a data-tooltip-position="top" aria-label="Artificial Neural Network" data-href="Artificial Neural Network" href="the-guide/machine-learning/deep-learning/artificial-neural-network.html" class="internal-link" target="_self" rel="noopener nofollow">neural networks</a> as the approximator leads to deep RL.
<br><br><br>
<br>Use parametric value <a data-tooltip-position="top" aria-label="Mathematical Relations" data-href="Mathematical Relations" href="the-guide/mathematics/general-stuff/mathematical-relations.html" class="internal-link" target="_self" rel="noopener nofollow">function</a> approximator using weight vector for the basis functions 

<br>Much fewer parameters than states, but change to one parameter affects (and possibly worsens) many states
<br>Error is Mean Squared Value Error

<br> is importance of states, e.g. in on-policy the time spent in this state


<br>Stochastic gradient descent for value estimation


<br>(A) - Exact <a data-tooltip-position="top" aria-label="Value Functions" data-href="Value Functions" href="the-guide/machine-learning/reinforcement-learning/value-functions.html" class="internal-link" target="_self" rel="noopener nofollow">Value function</a> is known 

<br><a data-tooltip-position="top" aria-label="Gradient Descent" data-href="Gradient Descent" href="the-guide/mathematics/optimization/convex-optimization-lecture/algorithms/gradient-descent.html" class="internal-link" target="_self" rel="noopener nofollow">Stochastic gradient descent</a> to minimize mean squared error above
<br>In most cases not known


<br>(B) - Exact <a data-tooltip-position="top" aria-label="Value Functions" data-href="Value Functions" href="the-guide/machine-learning/reinforcement-learning/value-functions.html" class="internal-link" target="_self" rel="noopener nofollow">Value function</a> is unknown

<br>Gradient Monte-Carlo Algorithm

<br>Use <a data-tooltip-position="top" aria-label="Estimator" data-href="Estimator" href="the-guide/mathematics/statistics/estimator.html" class="internal-link" target="_self" rel="noopener nofollow">estimate</a>  of , if unbiased (e.g. ) convergence is guaranteed with properly decaying step size


<br>Semi-Gradient Methods (online)

<br>Based on <a data-tooltip-position="top" aria-label="Dynamic Programming" data-href="Dynamic Programming" href="the-guide/machine-learning/reinforcement-learning/dynamic-programming.html" class="internal-link" target="_self" rel="noopener nofollow">DP</a> or bootstrapped targets (n-step return), enable step-based update instead 
<br>Only use part of the gradient, remove bootstrapped (biased) part
<br>Converge towards fixed point near local optimum, update with each new sample
<br><a data-tooltip-position="top" aria-label="SARSA" data-href="SARSA" href="the-guide/machine-learning/reinforcement-learning/approaches/sarsa.html" class="internal-link" target="_self" rel="noopener nofollow">Semi-Gradient SARSA</a>

<br>Approximate <a data-tooltip-position="top" aria-label="Q-Function" data-href="Q-Function" href="the-guide/machine-learning/reinforcement-learning/q-function.html" class="internal-link" target="_self" rel="noopener nofollow">Q-Function</a> instead of <a data-tooltip-position="top" aria-label="Value Functions" data-href="Value Functions" href="the-guide/machine-learning/reinforcement-learning/value-functions.html" class="internal-link" target="_self" rel="noopener nofollow">Value function</a>




<br>Least-Squares TD (offline)

<br>Compute fixed point using dataset




<br><br><br>
<br><a data-href="Importance Sampling" href="the-guide/mathematics/probability-theory/importance-sampling.html" class="internal-link" target="_self" rel="noopener nofollow">Importance Sampling</a> Ratiowith update 
<br>Can diverge  <a data-href="The Deadly Triad" href="the-guide/machine-learning/reinforcement-learning/theorems/the-deadly-triad.html" class="internal-link" target="_self" rel="noopener nofollow">The Deadly Triad</a>
<br><br><br>
<br>Need to be able to capture complex behavior, e.g.

<br>Polynomial Features 

<br>e.g. second order 
<br>Number of features grows exponentially with number of dimensions of state


<br>Fourier Basis

<br>Weighted sum of sine and cosine


<br>Coarse Coding

<br>Divide state space in  regions, feature vector is 1 if state is inside region, 0 else<img alt="center" src="lib/media/pasted-image-20230906162212.png" style="width: 350px; max-width: 100%;">


<br>Tile Coding

<br>Use  tilings, each with  tiles<img alt="center" src="lib/media/pasted-image-20230906162252.png" style="width: 350px; max-width: 100%;"> 


<br>Radial Basis Functions

<br>Non-normalized <a data-tooltip-position="top" aria-label="Gaussian Distribution" data-href="Gaussian Distribution" href="the-guide/mathematics/probability-theory/gaussian-distribution.html" class="internal-link" target="_self" rel="noopener nofollow">gaussians</a>, generalization of coarse coding<img alt="center" src="lib/media/pasted-image-20230906162825.png" style="width: 350px; max-width: 100%;">


<br><a data-tooltip-position="top" aria-label="Artificial Neural Network" data-href="Artificial Neural Network" href="the-guide/machine-learning/deep-learning/artificial-neural-network.html" class="internal-link" target="_self" rel="noopener nofollow">Neural Networks</a>

<br>Leads to <a data-href="(Deep) Q-Learning" href="the-guide/machine-learning/reinforcement-learning/approaches/(deep)-q-learning.html" class="internal-link" target="_self" rel="noopener nofollow">(Deep) Q-Learning</a>




]]></description><link>the-guide/machine-learning/reinforcement-learning/reinforcement-learning-with-function-approximation.html</link><guid isPermaLink="false">The Guide/Machine Learning/Reinforcement Learning/Reinforcement Learning with Function Approximation.md</guid><pubDate>Thu, 20 Feb 2025 16:36:31 GMT</pubDate><enclosure url="lib/media/pasted-image-20230906162212.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src=&quot;lib/media/pasted-image-20230906162212.png&quot;&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[Temporal Difference Learning]]></title><description><![CDATA[ 
 <br>In a Nutshell
Class of <a data-tooltip-position="top" aria-label="Model-Free Reinforcement Learning" data-href="Model-Free Reinforcement Learning" href="the-guide/machine-learning/reinforcement-learning/model-free-reinforcement-learning.html" class="internal-link" target="_self" rel="noopener nofollow">model-free</a> <a data-href="- Reinforcement Learning -" href="the-guide/machine-learning/reinforcement-learning/-reinforcement-learning-.html" class="internal-link" target="_self" rel="noopener nofollow">- Reinforcement Learning -</a> methods. Instead of pure <a data-tooltip-position="top" aria-label="Monte-Carlo Reinforcement Learning" data-href="Monte-Carlo Reinforcement Learning" href="the-guide/machine-learning/reinforcement-learning/monte-carlo-reinforcement-learning.html" class="internal-link" target="_self" rel="noopener nofollow">Monte-Carlo strategy</a>, use <a data-href="Bellman's Equation" href="the-guide/machine-learning/reinforcement-learning/theorems/bellman's-equation.html" class="internal-link" target="_self" rel="noopener nofollow">Bellman's Equation</a> and exploit <a data-tooltip-position="top" aria-label="Markov Chain" data-href="Markov Chain" href="the-guide/computational-statistics/data-assimilation/markov-chain.html" class="internal-link" target="_self" rel="noopener nofollow">Markov property</a>. Can learn <a data-tooltip-position="top" aria-label="- Reinforcement Learning -" data-href="- Reinforcement Learning -" href="the-guide/machine-learning/reinforcement-learning/-reinforcement-learning-.html" class="internal-link" target="_self" rel="noopener nofollow">online</a> without knowing the final outcome using incomplete sequences and bootstrapping, works in non-terminating environments.

<br>Biased and sensitive to initial values, but low <a data-tooltip-position="top" aria-label="Covariance and Variance" data-href="Covariance and Variance" href="the-guide/mathematics/statistics/covariance-and-variance.html" class="internal-link" target="_self" rel="noopener nofollow">variance</a> and more efficient when compared to <a data-href="Monte-Carlo Reinforcement Learning" href="the-guide/machine-learning/reinforcement-learning/monte-carlo-reinforcement-learning.html" class="internal-link" target="_self" rel="noopener nofollow">Monte-Carlo Reinforcement Learning</a> (<a data-href="Bias-Variance Tradeoff" href="the-guide/machine-learning/reinforcement-learning/theorems/bias-variance-tradeoff.html" class="internal-link" target="_self" rel="noopener nofollow">Bias-Variance Tradeoff</a>)

<br><img alt="center" src="lib/media/pasted-image-20241112122135.png"><br><br><br>
<br>Sample-based version of <a data-tooltip-position="top" aria-label="Dynamic Programming" data-href="Dynamic Programming" href="the-guide/machine-learning/reinforcement-learning/dynamic-programming.html" class="internal-link" target="_self" rel="noopener nofollow">policy evaluation</a> 
<br><a data-tooltip-position="top" aria-label="- Reinforcement Learning -" data-href="- Reinforcement Learning -" href="the-guide/machine-learning/reinforcement-learning/-reinforcement-learning-.html" class="internal-link" target="_self" rel="noopener nofollow">On-Policy</a>

<br>TD(1)

<br>Compare one time step lookahead prediction  with current estimate of <a data-tooltip-position="top" aria-label="Value Functions" data-href="Value Functions" href="the-guide/machine-learning/reinforcement-learning/value-functions.html" class="internal-link" target="_self" rel="noopener nofollow">the Value Function</a> and compute error 


<br>TD(n)

<br>Compare to n-step lookahead return  and compute error 
<br>Converges to pure <a data-tooltip-position="top" aria-label="Monte-Carlo Reinforcement Learning" data-href="Monte-Carlo Reinforcement Learning" href="the-guide/machine-learning/reinforcement-learning/monte-carlo-reinforcement-learning.html" class="internal-link" target="_self" rel="noopener nofollow">Monte-Carlo</a> Learning


<br>TD()

<br>Forward - Combine all n-step returns and weigh them via <img alt="center" src="lib/media/pasted-image-20230518204948.png" style="width: 400px; max-width: 100%;"> to compute error as above.

<br>Works on full episodes like <a data-tooltip-position="top" aria-label="Monte-Carlo Reinforcement Learning" data-href="Monte-Carlo Reinforcement Learning" href="the-guide/machine-learning/reinforcement-learning/monte-carlo-reinforcement-learning.html" class="internal-link" target="_self" rel="noopener nofollow">MC</a>


<br>Backward-view TD()

<br>Update in proportion to TD-error  and eligibility trace  
<br>Eligibility Trace gives credit to frequency and recency<img alt="center" src="lib/media/pasted-image-20230518205907.png" style="width: 200px; max-width: 100%;">


<br>Can be combined 


<br>Algorithm

<br>Init 
<br>Repeat for 

<br>Sample transition 
<br>Compute error 
<br>Update <a data-tooltip-position="top" aria-label="Value Functions" data-href="Value Functions" href="the-guide/machine-learning/reinforcement-learning/value-functions.html" class="internal-link" target="_self" rel="noopener nofollow">Value Function</a> 






<br><a data-tooltip-position="top" aria-label="- Reinforcement Learning -" data-href="- Reinforcement Learning -" href="the-guide/machine-learning/reinforcement-learning/-reinforcement-learning-.html" class="internal-link" target="_self" rel="noopener nofollow">Off-Policy</a>

<br><a data-href="Importance Sampling" href="the-guide/mathematics/probability-theory/importance-sampling.html" class="internal-link" target="_self" rel="noopener nofollow">Importance Sampling</a> for Off-Policy TD

<br>Use TD targets generated from <a data-tooltip-position="top" aria-label="Policy" data-href="Policy" href="the-guide/machine-learning/reinforcement-learning/policy.html" class="internal-link" target="_self" rel="noopener nofollow">policy</a>  instead of , weight by importance sampling
<br>Only needs a single correction, policies only need to be similar over single step

<br>Much lower <a data-tooltip-position="top" aria-label="Covariance and Variance" data-href="Covariance and Variance" href="the-guide/mathematics/statistics/covariance-and-variance.html" class="internal-link" target="_self" rel="noopener nofollow">variance</a> than <a data-tooltip-position="top" aria-label="Monte-Carlo Reinforcement Learning" data-href="Monte-Carlo Reinforcement Learning" href="the-guide/machine-learning/reinforcement-learning/monte-carlo-reinforcement-learning.html" class="internal-link" target="_self" rel="noopener nofollow">Monte-Carlo importance sampling</a>






<br><br><br>
<br>Need to enforce <a data-tooltip-position="top" aria-label="Exploration-Exploitation Trade-Off" data-href="Exploration-Exploitation Trade-Off" href="the-guide/machine-learning/reinforcement-learning/exploration-exploitation-trade-off.html" class="internal-link" target="_self" rel="noopener nofollow">exploration</a> in order to see new states and actions
<br>Algorithm

<br>Repeat

<br>Evaluate current policy, <a data-tooltip-position="top" aria-label="Estimator" data-href="Estimator" href="the-guide/mathematics/statistics/estimator.html" class="internal-link" target="_self" rel="noopener nofollow">estimate</a> the <a data-href="Q-Function" href="the-guide/machine-learning/reinforcement-learning/q-function.html" class="internal-link" target="_self" rel="noopener nofollow">Q-Function</a>

<br>Update equations
<br>
<br>


<br>How to estimate the action ?

<br>Off-Policy - <a data-tooltip-position="top" aria-label="(Deep) Q-Learning" data-href="(Deep) Q-Learning" href="the-guide/machine-learning/reinforcement-learning/approaches/(deep)-q-learning.html" class="internal-link" target="_self" rel="noopener nofollow">Q-Learning</a>
<br>On-Policy - <a data-href="SARSA" href="the-guide/machine-learning/reinforcement-learning/approaches/sarsa.html" class="internal-link" target="_self" rel="noopener nofollow">SARSA</a>-Learning or importance sampling


<br>Update policy




<br><br><br>
<br>Continuous Case  Need to approximate <a data-tooltip-position="top" aria-label="Value Functions" data-href="Value Functions" href="the-guide/machine-learning/reinforcement-learning/value-functions.html" class="internal-link" target="_self" rel="noopener nofollow">V-Function</a> if not <a data-tooltip-position="top" aria-label="LQG Regulator" data-href="LQG Regulator" href="the-guide/robotics,-dynamics-and-control/control/lqg-regulator.html" class="internal-link" target="_self" rel="noopener nofollow">LQR</a>

<br>Here: Linear Model 
<br>How can we find  ?  TD Learning


<br>Use <a data-tooltip-position="top" aria-label="Gradient Descent" data-href="Gradient Descent" href="the-guide/mathematics/optimization/convex-optimization-lecture/algorithms/gradient-descent.html" class="internal-link" target="_self" rel="noopener nofollow">Stochastic Gradient Descent</a> (Online Method)

<br>

<br>
<br>BS = Bootstrapping

<br>Use single data set and resample from it to create simulated samples.




<br>

<br>Here: 

<br>


<br>Update is performed after each sample.
<br>Compuationally efficient, but data-inefficient

<br>At cost of more complexity, one can use whole batch of data at once  <a data-href="Batch-Mode Reinforcement Learning" href="the-guide/machine-learning/reinforcement-learning/batch-mode-reinforcement-learning.html" class="internal-link" target="_self" rel="noopener nofollow">Batch-Mode Reinforcement Learning</a>


<br>Not a "real" stochastic gradient descent, as Value Function changes after each update in TD learning  introduces bias




]]></description><link>the-guide/machine-learning/reinforcement-learning/temporal-difference-learning.html</link><guid isPermaLink="false">The Guide/Machine Learning/Reinforcement Learning/Temporal Difference Learning.md</guid><pubDate>Sun, 13 Apr 2025 18:08:07 GMT</pubDate><enclosure url="lib/media/pasted-image-20241112122135.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src=&quot;lib/media/pasted-image-20241112122135.png&quot;&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[The Natural Gradient]]></title><description><![CDATA[ 
 <br>Caution
Classical gradient methods for <a data-href="Policy Search" href="the-guide/machine-learning/reinforcement-learning/policy-search.html" class="internal-link" target="_self" rel="noopener nofollow">Policy Search</a> such as <a data-href="Policy Gradient Methods" href="the-guide/machine-learning/reinforcement-learning/policy-gradient-methods.html" class="internal-link" target="_self" rel="noopener nofollow">Policy Gradient Methods</a> assume that the space being optimized is euclidean, but this is not true. We need a metric that captures the influence of parameters on the <a data-tooltip-position="top" aria-label="Probability Distribution" data-href="Probability Distribution" href="the-guide/mathematics/probability-theory/probability-distribution.html" class="internal-link" target="_self" rel="noopener nofollow">distribution</a> in order to better control the step size for the policy update.
<br>
<br>Solution

<br>Find an update direction  that is most similar to standard gradient (VG) while limiting the "distance" between update distributions via <a data-href="Kullback-Leibler Divergence" href="the-guide/information-theory/information-theory-1/information/kullback-leibler-divergence.html" class="internal-link" target="_self" rel="noopener nofollow">Kullback-Leibler Divergence</a> using the <a data-href="Fisher Information Matrix" href="the-guide/mathematics/statistics/fisher-information-matrix.html" class="internal-link" target="_self" rel="noopener nofollow">Fisher Information Matrix</a>  as a metric The solution to this problem is , for which the natural gradient can be written as 


<br><br>
<br>Episode-Based

<br>Create   and evaluate  samples
<br>Compute gradient 
<br>Compute <a data-tooltip-position="top" aria-label="Fisher Information Matrix" data-href="Fisher Information Matrix" href="the-guide/mathematics/statistics/fisher-information-matrix.html" class="internal-link" target="_self" rel="noopener nofollow">FIM</a>
<br>Compute natural gradient 
<br>Update parameters 


<br>Step-Based
]]></description><link>the-guide/machine-learning/reinforcement-learning/the-natural-gradient.html</link><guid isPermaLink="false">The Guide/Machine Learning/Reinforcement Learning/The Natural Gradient.md</guid><pubDate>Mon, 12 Aug 2024 17:06:50 GMT</pubDate></item><item><title><![CDATA[Value Functions]]></title><description><![CDATA[ 
 <br>Info
Function returning the (discounted) <a data-tooltip-position="top" aria-label="Expectations" data-href="Expectations" href="the-guide/mathematics/statistics/expectations.html" class="internal-link" target="_self" rel="noopener nofollow">expected</a> return of starting in <a data-tooltip-position="top" aria-label="Markov Decision Process" data-href="Markov Decision Process" href="the-guide/machine-learning/reinforcement-learning/markov-decision-process.html" class="internal-link" target="_self" rel="noopener nofollow">state</a>  and then following policy  In a stochastic environment, we get 
<br><br>
<br><a data-tooltip-position="top" aria-label="- Reinforcement Learning -" data-href="- Reinforcement Learning -" href="the-guide/machine-learning/reinforcement-learning/-reinforcement-learning-.html" class="internal-link" target="_self" rel="noopener nofollow">Finite Horizon</a>

<br>
<br><a data-tooltip-position="top" aria-label="Expectations" data-href="Expectations" href="the-guide/mathematics/statistics/expectations.html" class="internal-link" target="_self" rel="noopener nofollow">Expected</a> (can be stochastic) long-term reward for <a data-tooltip-position="top" aria-label="Markov Decision Process" data-href="Markov Decision Process" href="the-guide/machine-learning/reinforcement-learning/markov-decision-process.html" class="internal-link" target="_self" rel="noopener nofollow">state</a>  when following the <a data-tooltip-position="top" aria-label="Policy" data-href="Policy" href="the-guide/machine-learning/reinforcement-learning/policy.html" class="internal-link" target="_self" rel="noopener nofollow">policy</a>  

<br> is transition probability function




<br><a data-tooltip-position="top" aria-label="- Reinforcement Learning -" data-href="- Reinforcement Learning -" href="the-guide/machine-learning/reinforcement-learning/-reinforcement-learning-.html" class="internal-link" target="_self" rel="noopener nofollow">Infinite Horizon</a>

<br>
<br><a data-tooltip-position="top" aria-label="Expectations" data-href="Expectations" href="the-guide/mathematics/statistics/expectations.html" class="internal-link" target="_self" rel="noopener nofollow">Expected</a> (can be stochastic) long-term reward for <a data-tooltip-position="top" aria-label="Markov Decision Process" data-href="Markov Decision Process" href="the-guide/machine-learning/reinforcement-learning/markov-decision-process.html" class="internal-link" target="_self" rel="noopener nofollow">state</a>  when following the <a data-tooltip-position="top" aria-label="Policy" data-href="Policy" href="the-guide/machine-learning/reinforcement-learning/policy.html" class="internal-link" target="_self" rel="noopener nofollow">policy</a>  
<br>Discount factor  to emphasize short-term reward


<br><br><br>
<br>Bounded  
<br>Can be computed from <a data-href="Q-Function" href="the-guide/machine-learning/reinforcement-learning/q-function.html" class="internal-link" target="_self" rel="noopener nofollow">Q-Function</a> (holds for both horizons) via 
<br>Optimal Value Function  is result of <a data-tooltip-position="top" aria-label="Policy" data-href="Policy" href="the-guide/machine-learning/reinforcement-learning/policy.html" class="internal-link" target="_self" rel="noopener nofollow">optimal policy</a>
]]></description><link>the-guide/machine-learning/reinforcement-learning/value-functions.html</link><guid isPermaLink="false">The Guide/Machine Learning/Reinforcement Learning/Value Functions.md</guid><pubDate>Thu, 20 Feb 2025 16:36:31 GMT</pubDate></item><item><title><![CDATA[- Machine Learning -]]></title><description><![CDATA[ 
 <br><br><br>
<br>
1 - Problem Class<img alt="center" src="lib/media/pasted-image-20230217203655.png" style="width: 400px; max-width: 100%;">

<br>Description

<br>Unsupervised Learning


<br>Predictions

<br>Supervised Learning


<br>Decisions

<br><a data-href="- Reinforcement Learning -" href="the-guide/machine-learning/reinforcement-learning/-reinforcement-learning-.html" class="internal-link" target="_self" rel="noopener nofollow">- Reinforcement Learning -</a>
<br><a data-href="- Imitation Learning -" href="the-guide/machine-learning/imitation-learning/-imitation-learning-.html" class="internal-link" target="_self" rel="noopener nofollow">- Imitation Learning -</a> ?




<br>
2 - Assumptions in ML

<br>Learning is possible (e.g., existence of causal structure)
<br>Frequentist assumption: Existence of a true model
<br>Observability
<br>Process assumputions: <a data-tooltip-position="top" aria-label="Statistical Independence" data-href="Statistical Independence" href="the-guide/mathematics/statistics/statistical-independence.html" class="internal-link" target="_self" rel="noopener nofollow">i.i.d.</a>, <a data-tooltip-position="top" aria-label="Markov Decision Process" data-href="Markov Decision Process" href="the-guide/machine-learning/reinforcement-learning/markov-decision-process.html" class="internal-link" target="_self" rel="noopener nofollow">Markovian</a>
<br>Smoothness, e.g., in regression, or finitely many switches


<br>Evaluation criteria<br>
<br>What is the goal of the system?
<br>How will the overall performance of the system be measured?
<br>How will the answers to individual queries be evaluated?
<br><br><br>General Modeling Approaches<br>
<br>White-Box Strategy - physical model with physical parameters is known from literature etc. We only fine-tune these parameters to improve the model to fit data.

<br>Advantages - Prior work applicable, makes result interpretable and can generalize very well for good models
<br>Problems - Limited int that some phenomena are not well-understood or impossible to model adequately (friction,...) 


<br>Black-Box Strategy - choose a general function approximator, e.g. a neural network, and collect "enough" training data.

<br>Advantages - Requires only training data, quick to implement
<br>Problems - We don't know how much data we need and data can be expensive. The algorithms need to learn all patterns by themselves, which can lead to computationally expensive algorithms. Even after training, the model may perform very poor in new situations. 


<br>Grey-Box Strategy

<br>Combine strategies above


<br>Model Types<br>
Determining the needed type of model depends on how we want to use the model later on to make predictions. For this, we assume an observable variable  and a target varibale .<br>
<br>Discriminative Models model the conditional probability of a target given an observationExamples are ...

<br><a data-tooltip-position="top" aria-label="Support Vector Machine" data-href="Support Vector Machine" href="the-guide/mathematics/optimization/convex-optimization-lecture/applications-in-optimization/support-vector-machine.html" class="internal-link" target="_self" rel="noopener nofollow">SVMs</a>
<br>Basic <a data-tooltip-position="top" aria-label="Artificial Neural Network" data-href="Artificial Neural Network" href="the-guide/machine-learning/deep-learning/artificial-neural-network.html" class="internal-link" target="_self" rel="noopener nofollow">neural networks</a>


<br><a data-href="Generative Models" href="the-guide/machine-learning/generative-models/generative-models.html" class="internal-link" target="_self" rel="noopener nofollow">Generative Models</a> can be used to generate either new observation, target pairs by modelling the underlying <a data-tooltip-position="top" aria-label="Probability Distribution" data-href="Probability Distribution" href="the-guide/mathematics/probability-theory/probability-distribution.html" class="internal-link" target="_self" rel="noopener nofollow">joint distribution</a>  or generate new data provided a label, modelling the conditionalExamples are ...

<br><a data-href="Diffusion Models" href="the-guide/machine-learning/generative-models/diffusion-models.html" class="internal-link" target="_self" rel="noopener nofollow">Diffusion Models</a>
<br><a data-href="Neural Cellular Automata" href="the-guide/machine-learning/generative-models/neural-cellular-automata.html" class="internal-link" target="_self" rel="noopener nofollow">Neural Cellular Automata</a>


<br>In addition to that, we categorize models based on how they use the available training data to model.<br>
<br>Nonparametric Models use the training data directly as the model, data points essentially become parameters. Examples are <a data-tooltip-position="top" aria-label="Kernel Density Estimation" data-href="Kernel Density Estimation" href="the-guide/computational-statistics/kernel-density-estimation.html" class="internal-link" target="_self" rel="noopener nofollow">KDE</a>, <a data-tooltip-position="top" aria-label="Support Vector Machine" data-href="Support Vector Machine" href="the-guide/mathematics/optimization/convex-optimization-lecture/applications-in-optimization/support-vector-machine.html" class="internal-link" target="_self" rel="noopener nofollow">SVM</a>, KNN or <a data-tooltip-position="top" aria-label="Gaussian Process" data-href="Gaussian Process" href="the-guide/mathematics/probability-theory/gaussian-process.html" class="internal-link" target="_self" rel="noopener nofollow">GP</a> 
<br>Parametric Models collapse the training data onto parameters  of a parameterized model class  and then use the resulting mode for predictions / description / decision making. Examples are Mixture Models or <a data-tooltip-position="top" aria-label="Artificial Neural Network" data-href="Artificial Neural Network" href="the-guide/machine-learning/deep-learning/artificial-neural-network.html" class="internal-link" target="_self" rel="noopener nofollow">(Deep) Neural Networks</a>
<br>It is impossible to give a general recipe on which model to choose, but important aspects to consider are ...<br>
<br>Computational Demand - more expressive models usually require much more parameters, making them harder to train simply by their hardware requirements.
<br>Modelling Assumptions - different classes of models make different assumptions on the underlying real system in their derivations. These can be a neckbreaker for certain applications !
<br>Sufficient Expressiveness - too much flexibility can hurt a models performance, see <a data-href="Bias-Variance Tradeoff" href="the-guide/machine-learning/reinforcement-learning/theorems/bias-variance-tradeoff.html" class="internal-link" target="_self" rel="noopener nofollow">Bias-Variance Tradeoff</a> and the problem of over- and underfitting
<br>Occams Razor
Always choose the simplest (= smallest model complexity) model that fits the data !
<br><br><br>Based on parametriuzed model, use ...<br>
<br>
<a data-href="Bayes Theorem" href="the-guide/mathematics/statistics/bayes-theorem.html" class="internal-link" target="_self" rel="noopener nofollow">Bayes Theorem</a>, distribution over parameters

<br>
<a data-href="Maximum Likelihood Estimator" href="the-guide/mathematics/statistics/maximum-likelihood-estimator.html" class="internal-link" target="_self" rel="noopener nofollow">Maximum Likelihood Estimator</a>

<br>
<a data-href="Maximum A Posteriori Estimator" href="the-guide/mathematics/statistics/maximum-a-posteriori-estimator.html" class="internal-link" target="_self" rel="noopener nofollow">Maximum A Posteriori Estimator</a>

<br>
How do i get good data ? 

<br>Active Model Learning

<br>Whats the best signal for learning a model ?

<br>Intuition: Which input to give when we want observe <a data-tooltip-position="top" aria-label="Static and Dynamic Systems" data-href="Static and Dynamic Systems" href="the-guide/robotics,-dynamics-and-control/dynamics/static-and-dynamic-systems.html" class="internal-link" target="_self" rel="noopener nofollow">system</a> response ?


<br>Usually adapts ideas from Information Theory, e.g. <a data-tooltip-position="top" aria-label="Shannon Entropy" data-href="Shannon Entropy" href="the-guide/information-theory/information-theory-1/information/shannon-entropy.html" class="internal-link" target="_self" rel="noopener nofollow">Entropy</a>




<br>
How can I make a black-box model physically safe ?

<br>Inductive Biases, e.g. Deep Lagrangian networks include energy conservation
<br>Low-pass filter the data


]]></description><link>the-guide/machine-learning/-machine-learning-.html</link><guid isPermaLink="false">The Guide/Machine Learning/- Machine Learning -.md</guid><pubDate>Thu, 17 Apr 2025 07:39:44 GMT</pubDate><enclosure url="lib/media/pasted-image-20230217203655.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src=&quot;lib/media/pasted-image-20230217203655.png&quot;&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[Automatic Differentiation]]></title><description><![CDATA[ 
 <br>In a Nutshell
Techniques to evaluate the partial derivative of a function specified by a computer program. Exploits the fact that every program can be decomposed into elementary operations and functions and uses the chain rule repeatedly on these.
<br><br>Computational Graph
Build up a <a data-tooltip-position="top" aria-label="Graph" data-href="Graph" href="the-guide/mathematics/graph-theory/graph.html" class="internal-link" target="_self" rel="noopener nofollow">graph</a> based on the <a data-tooltip-position="top" aria-label="Mathematical Relations" data-href="Mathematical Relations" href="the-guide/mathematics/general-stuff/mathematical-relations.html" class="internal-link" target="_self" rel="noopener nofollow">function</a> with

<br><a data-tooltip-position="top" aria-label="Graph" data-href="Graph" href="the-guide/mathematics/graph-theory/graph.html" class="internal-link" target="_self" rel="noopener nofollow">Edge</a> represents value (scalar, vector, ...)
<br><a data-tooltip-position="top" aria-label="Node or Vertex Set" data-href="Node or Vertex Set" href="the-guide/mathematics/graph-theory/node-or-vertex-set.html" class="internal-link" target="_self" rel="noopener nofollow">Node</a> represents elementary function (multiplication, division, sine/cosine,...)

<br>Two general strategies for function :<br><br>
<br>First fix variable with respect to which differentiation is performed, compute derivative of sub-expressions 
<br>Use chain rule 
<br>Ideal when  <img alt="center" src="lib/media/pasted-image-20230919105356.png" style="width: 500px; max-width: 100%;">
<br><br>
<br>
Fix variable that is differentiated, compute derivative of sub-expressions 

<br>
Use chain rule 

<br>
<a data-tooltip-position="top" aria-label="Forward and Backward Propagation" data-href="Forward and Backward Propagation" href="the-guide/machine-learning/deep-learning/forward-and-backward-propagation.html" class="internal-link" target="_self" rel="noopener nofollow">Backpropagation</a> is a special case

<br>
Ideal when  <img alt="center" src="lib/media/pasted-image-20230919105430.png" style="width: 500px; max-width: 100%;">

<br>
Hybrid Mode

]]></description><link>the-guide/machine-learning/automatic-differentiation.html</link><guid isPermaLink="false">The Guide/Machine Learning/Automatic Differentiation.md</guid><pubDate>Sat, 29 Mar 2025 16:19:05 GMT</pubDate><enclosure url="lib/media/pasted-image-20230919105356.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src=&quot;lib/media/pasted-image-20230919105356.png&quot;&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[Evaluation Metrics in Machine Learning]]></title><description><![CDATA[ 
 <br><br><br>Precision
Accuracy of positive predictions 
<br>Recall
Used to represent how well a model can identify actual positive cases.
<br>F1 score
Combines Precision and Recall in a balanced way
]]></description><link>the-guide/machine-learning/evaluation-metrics-in-machine-learning.html</link><guid isPermaLink="false">The Guide/Machine Learning/Evaluation Metrics in Machine Learning.md</guid><pubDate>Wed, 04 Dec 2024 10:24:13 GMT</pubDate></item><item><title><![CDATA[Fairness in Machine Learning]]></title><description><![CDATA[ 
 <br>In a Nutshell
Area of research that is concerned with correcting algorithmic <a data-tooltip-position="top" aria-label="Bias-Variance Tradeoff" data-href="Bias-Variance Tradeoff" href="the-guide/machine-learning/reinforcement-learning/theorems/bias-variance-tradeoff.html" class="internal-link" target="_self" rel="noopener nofollow">bias</a> in <a data-tooltip-position="top" aria-label="- Machine Learning -" data-href="- Machine Learning -" href="the-guide/machine-learning/-machine-learning-.html" class="internal-link" target="_self" rel="noopener nofollow">machine learning</a> models that are used to make automated decisions. Usually involves data that contains sensitive variables, such as gender, ethnicity etc..
<br><br>In the context of a classification problem, we denote the characteristics as  and the predicted labels as . The dimensions of  that are considered sensitive are encoded in a <a data-tooltip-position="top" aria-label="Random Variable" data-href="Random Variable" href="the-guide/mathematics/probability-theory/random-variable.html" class="internal-link" target="_self" rel="noopener nofollow">random variable</a>  and the actual output of the classifier is .<br>Independence
Criterions that demand the random variables  and  are <a data-tooltip-position="top" aria-label="Statistical Independence" data-href="Statistical Independence" href="the-guide/mathematics/statistics/statistical-independence.html" class="internal-link" target="_self" rel="noopener nofollow">statistical independent</a>, i.e. their <a data-tooltip-position="top" aria-label="Mutual Information" data-href="Mutual Information" href="the-guide/information-theory/information-theory-1/information/mutual-information.html" class="internal-link" target="_self" rel="noopener nofollow">mutual information</a> is zeroIntuitively, this enforces that classification rates for a target class is equal for all sensitive groups. A relaxation of this approach is setting an upper bound  of the mutual information.
<br>Seperation
Weaker constraint that enforces the statistical independence conditioned on the target value, e.g. for a specific  we get A relaxation involves a slack variable  that sets the maximum difference between rates.
<br>Sufficiency
A random variable satisfies sufficiency, if the sensitive characteristics  is <a data-tooltip-position="top" aria-label="Statistical Independence" data-href="Statistical Independence" href="the-guide/mathematics/statistics/statistical-independence.html" class="internal-link" target="_self" rel="noopener nofollow">statistical independent</a> of the target given the predictionThe probability of being in a group is the same for all individuals (and therefore also for individuals with different sensitive characteristics) given that they were predicted to belong to the same group.
]]></description><link>the-guide/machine-learning/fairness-in-machine-learning.html</link><guid isPermaLink="false">The Guide/Machine Learning/Fairness in Machine Learning.md</guid><pubDate>Thu, 27 Feb 2025 12:04:58 GMT</pubDate></item><item><title><![CDATA[Gaussian Process Regression]]></title><description><![CDATA[ 
 <br>In a Nutshell
<a data-href="- Regression -" href="the-guide/mathematics/statistics/-regression-.html" class="internal-link" target="_self" rel="noopener nofollow">- Regression -</a> method based on the <a data-href="Gaussian Process" href="the-guide/mathematics/probability-theory/gaussian-process.html" class="internal-link" target="_self" rel="noopener nofollow">Gaussian Process</a> model.
<br><br>A <a data-tooltip-position="top" aria-label="Gaussian Process" data-href="Gaussian Process" href="the-guide/mathematics/probability-theory/gaussian-process.html" class="internal-link" target="_self" rel="noopener nofollow">GP</a> can be understood as a prior based on a given dataset . We can therefore compute a <a data-tooltip-position="top" aria-label="Bayes Theorem" data-href="Bayes Theorem" href="the-guide/mathematics/statistics/bayes-theorem.html" class="internal-link" target="_self" rel="noopener nofollow">posterior</a> distribution, where the data constitutes support points (possibly still with <a data-tooltip-position="top" aria-label="Covariance and Variance" data-href="Covariance and Variance" href="the-guide/mathematics/statistics/covariance-and-variance.html" class="internal-link" target="_self" rel="noopener nofollow">variance</a> if we do not trust the data). We denote the values we want to estimate as , extending the target vector to This is nothing but a sample from a higher dimensional Gaussian, given by where we computed . Because every distribution we encounter is Gaussian, we can explicitly compute a formula to <a data-tooltip-position="top" aria-label="Marginal Distribution" data-href="Marginal Distribution" href="the-guide/mathematics/probability-theory/marginal-distribution.html" class="internal-link" target="_self" rel="noopener nofollow">marginalize out</a> the data and obtain a mean and a variance for every new point. The desired <a data-tooltip-position="top" aria-label="Probability Distribution" data-href="Probability Distribution" href="the-guide/mathematics/probability-theory/probability-distribution.html" class="internal-link" target="_self" rel="noopener nofollow">conditional</a> distribution is The resulting conditional <a data-tooltip-position="top" aria-label="Gaussian Distribution" data-href="Gaussian Distribution" href="the-guide/mathematics/probability-theory/gaussian-distribution.html" class="internal-link" target="_self" rel="noopener nofollow">Gaussian</a> is then given by which is easily extended to multiple desired points . The mean value obtained by the above computations an be combined with the respective diagonal entry of the new covariance <a data-tooltip-position="top" aria-label="Matrices" data-href="Matrices" href="the-guide/mathematics/linear-algebra/matrices.html" class="internal-link" target="_self" rel="noopener nofollow">matrix</a> to indicate a confidence that the true value lies within a confidence interval.<img alt="center" src="lib/media/pasted-image-20240626083148.png" style="width: 300px; max-width: 100%;">If we do not trust our datapoints completely, we can additionally assume a variance  on them (theoretically per sample). This value is then added when computing the <a data-tooltip-position="top" aria-label="Covariance and Variance" data-href="Covariance and Variance" href="the-guide/mathematics/statistics/covariance-and-variance.html" class="internal-link" target="_self" rel="noopener nofollow">covariance</a> entries via The resulting functions will have non-zero variances at the datapoints.<img alt="center" src="lib/media/pasted-image-20240626090730.png" style="width: 300px; max-width: 100%;"><br><br>]]></description><link>the-guide/machine-learning/gaussian-process-regression.html</link><guid isPermaLink="false">The Guide/Machine Learning/Gaussian Process Regression.md</guid><pubDate>Thu, 17 Apr 2025 16:39:15 GMT</pubDate><enclosure url="lib/media/pasted-image-20240626083148.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src=&quot;lib/media/pasted-image-20240626083148.png&quot;&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[Complex Differentiation]]></title><description><![CDATA[ 
 <br>Basis of complex function theory, extends derivative from real to complex arguments.<br><br>Complex Derivative
A complex <a data-tooltip-position="top" aria-label="Mathematical Relations" data-href="Mathematical Relations" href="the-guide/mathematics/general-stuff/mathematical-relations.html" class="internal-link" target="_self" rel="noopener nofollow">function</a>  is differentiable at a point , if for  the following limit exists The important thing to note is that this is a two-dimensional limit, which leads to a deeply different theory compared to the real case.
<br>Considering a complex plane with imaginary axis  and real axis , we can rewrite a complex function  in terms of its components via <br>Cauchy-Riemann Equations
The complex function above has a complex derivative , if and only if real and imaginary part satisfy the equations In this case, the derivative is given by any of the expression 
<br>In many cases, the complex derivative exhibits the same rules as the real derivative.<br><br><br>
<br>The rules for elementary functions coincide, e.g. as well as for the hyperbolic and trigonometric functions.
<br>Product, quotient and chain rule are also applicable
<br>
]]></description><link>the-guide/mathematics/analysis-and-calculus/complex-differentiation.html</link><guid isPermaLink="false">The Guide/Mathematics/Analysis and Calculus/Complex Differentiation.md</guid><pubDate>Thu, 15 Aug 2024 12:51:15 GMT</pubDate></item><item><title><![CDATA[Continuity]]></title><description><![CDATA[ 
 <br>In a Nutshell
Encodes abstract idea of controllable cause and effect for <a data-tooltip-position="top" aria-label="Mathematical Relations" data-href="Mathematical Relations" href="the-guide/mathematics/general-stuff/mathematical-relations.html" class="internal-link" target="_self" rel="noopener nofollow">functions</a> and <a data-tooltip-position="top" aria-label="Measure" data-href="Measure" href="the-guide/mathematics/measure-theory/measure.html" class="internal-link" target="_self" rel="noopener nofollow">measures</a>; output cannot change abruptly when changes to the input are small.
<br><br><br>Pointwise Continuity
A <a data-tooltip-position="top" aria-label="Mathematical Relations" data-href="Mathematical Relations" href="the-guide/mathematics/general-stuff/mathematical-relations.html" class="internal-link" target="_self" rel="noopener nofollow">function</a>  with  <a data-tooltip-position="top" aria-label="Metric Space and Completeness" data-href="Metric Space and Completeness" href="the-guide/mathematics/general-stuff/metric-space-and-completeness.html" class="internal-link" target="_self" rel="noopener nofollow">metric</a> or <a data-tooltip-position="top" aria-label="Topology and Topological Space" data-href="Topology and Topological Space" href="the-guide/mathematics/topology/topology-and-topological-space.html" class="internal-link" target="_self" rel="noopener nofollow">topological</a> <a data-tooltip-position="top" aria-label="Space" data-href="Space" href="the-guide/mathematics/general-stuff/space.html" class="internal-link" target="_self" rel="noopener nofollow">spaces</a> is continuous at a point , if for every  there exists a , such that for all  withit follows that
<br>Intuition
For every region around the output, we can find a limit on the size of the region in the input that maps to it. Essentially guarantees the initial statement in a single point.
<br>Uniform Continuity
A function as above is uniformly continuous, if the choice of  depends only on  and not on the specific point .
<br>Intuition
Pointwise continuity for every point, generalizing the initial statement to the domain.
<br>Absolute Continuity
In real analysis, a function  is absoute continuous if for every , there exists a , such that for any collection of disjoint subintervals  in  with we have
<br>Intuition
Even stronger than uniform, as it includes the cumulative effect of finite collections of disjoint intervals. Functions that are uniform, but not absolutely continuous usually involves constructions based on infinite series.
<br>Lipschitz Continuity
The same  is Lipschitz continuous, if there exists a constant  that is a linear bound on the relative change of  with respect to its input
<br><img alt="center" src="lib/media/pasted-image-20250316155250.png" style="width: 150px; max-width: 100%;"><br><br><br>Continuity from Below
If  is an increasing sequence of <a data-tooltip-position="top" aria-label="Measure" data-href="Measure" href="the-guide/mathematics/measure-theory/measure.html" class="internal-link" target="_self" rel="noopener nofollow">measurable</a> <a data-tooltip-position="top" aria-label="Set" data-href="Set" href="the-guide/mathematics/general-stuff/set.html" class="internal-link" target="_self" rel="noopener nofollow">sets</a> , then the measure  satisfies
<br>Continuity from Above
If  is a decreasing sequence of <a data-tooltip-position="top" aria-label="Measure" data-href="Measure" href="the-guide/mathematics/measure-theory/measure.html" class="internal-link" target="_self" rel="noopener nofollow">measurable</a> <a data-tooltip-position="top" aria-label="Set" data-href="Set" href="the-guide/mathematics/general-stuff/set.html" class="internal-link" target="_self" rel="noopener nofollow">sets</a> , then the measure  satisfies<br>

<br>Absolute Continuity
Let  and  be measures on the same measurable space.  is said to be absolutely continuous with respect to  (), if for every measurable <br>

]]></description><link>the-guide/mathematics/analysis-and-calculus/continuity.html</link><guid isPermaLink="false">The Guide/Mathematics/Analysis and Calculus/Continuity.md</guid><pubDate>Wed, 23 Apr 2025 08:34:32 GMT</pubDate><enclosure url="lib/media/pasted-image-20250316155250.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src=&quot;lib/media/pasted-image-20250316155250.png&quot;&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[Derivative Tricks and Theorems]]></title><description><![CDATA[ 
 <br>In a Nutshell
Collection of tricks, reformulation and theorems I encountered regularly.
<br><br>Rules for elementary functions, products, quotients and compositions are the same for real and <a data-tooltip-position="top" aria-label="Complex Differentiation" data-href="Complex Differentiation" href="the-guide/mathematics/analysis-and-calculus/complex-differentiation.html" class="internal-link" target="_self" rel="noopener nofollow">complex differentiation</a>.<br>]]></description><link>the-guide/mathematics/analysis-and-calculus/derivative-tricks-and-theorems.html</link><guid isPermaLink="false">The Guide/Mathematics/Analysis and Calculus/Derivative Tricks and Theorems.md</guid><pubDate>Mon, 24 Feb 2025 23:32:25 GMT</pubDate></item><item><title><![CDATA[Derivative, Gradient, Jacobian and Hessian]]></title><description><![CDATA[ 
 <br>In a Nutshell
Quantifications of the sensitivity to change of a <a data-tooltip-position="top" aria-label="Mathematical Relations" data-href="Mathematical Relations" href="the-guide/mathematics/general-stuff/mathematical-relations.html" class="internal-link" target="_self" rel="noopener nofollow">functions</a> output with respect to its input. The best linear approximation of a function in a point.
<br><br><br>... as Limits
A <a data-tooltip-position="top" aria-label="Mathematical Relations" data-href="Mathematical Relations" href="the-guide/mathematics/general-stuff/mathematical-relations.html" class="internal-link" target="_self" rel="noopener nofollow">function</a> of a real variable  is differentiable at a point  of its domain, if its domain contains an open interval containing  and the limit exists. Existence means that for every positive real number , we can find another positive real number , that bounds every  via  and yields 
<br>What it really means
For every output that is within a certain distance, we can find a another distance to bound the required input to reach every point in the output region. This essentially means that the functions change is somehow controllable, it will not suddenly jump to arbitrarily distant values.
<br>Total vs. Partial Derivative
The derivative  is a <a data-tooltip-position="top" aria-label="Linear Operator" data-href="Linear Operator" href="Linear Operator" class="internal-link" target="_self" rel="noopener nofollow">linear operator</a> mapping a function to its derivative, which maps a small change  in the input to a small change in the outputOn the other hand, the partial derivative  maps small changes in a single argument of a multi argument function to the corresponding change in the output 
<br>For tricks and good resources, go to <a data-href="Derivative Tricks and Theorems" href="the-guide/mathematics/analysis-and-calculus/derivative-tricks-and-theorems.html" class="internal-link" target="_self" rel="noopener nofollow">Derivative Tricks and Theorems</a>.<br><br><br>Vector field, indicating the direction of fastest increase of a scalar-valued differentiable <a data-tooltip-position="top" aria-label="Mathematical Relations" data-href="Mathematical Relations" href="the-guide/mathematics/general-stuff/mathematical-relations.html" class="internal-link" target="_self" rel="noopener nofollow">function</a> of multiple variables-<br><br><br>Definition of the Jacobian Matrix in 
In vector-calculus, the Jacobian of a vector-valued <a data-tooltip-position="top" aria-label="Mathematical Relations" data-href="Mathematical Relations" href="the-guide/mathematics/general-stuff/mathematical-relations.html" class="internal-link" target="_self" rel="noopener nofollow">function</a>  is the <a data-tooltip-position="top" aria-label="Matrices" data-href="Matrices" href="the-guide/mathematics/linear-algebra/matrices.html" class="internal-link" target="_self" rel="noopener nofollow">matrix</a> of all its first-order partial derivatives. Every column corresponds to the derivative of the function w.r.t. one variable . 
<br>The Jacobian matrix represents the differential of  at every point where it is differentiable, that is the best linear approximation of  in a small neighborhood. It describes the amount of "stretching", "rotating" or "transforming" in general that a function imposes locally.<br>Interpretation as Linear Transformation 
When computing a change of variables, the <a data-tooltip-position="top" aria-label="Matrices" data-href="Matrices" href="the-guide/mathematics/linear-algebra/matrices.html" class="internal-link" target="_self" rel="noopener nofollow">determinant</a> of the Jacobian is exactly the formula of volume for a parallelepiped in the new coordinate system, giving a correction factor for every point in space to account for the distorted infinitesimal elements.
<br><br><br>Definition of the Hessian
Based on the function  and assuming all second order partial derivatives exist, the Hessian is an  <a data-tooltip-position="top" aria-label="Matrices" data-href="Matrices" href="the-guide/mathematics/linear-algebra/matrices.html" class="internal-link" target="_self" rel="noopener nofollow">matrix</a> of the formBy the symmetry of second derivates it is a symmetric matrix. It is related to gradient and jacobian via 
]]></description><link>the-guide/mathematics/analysis-and-calculus/derivative,-gradient,-jacobian-and-hessian.html</link><guid isPermaLink="false">The Guide/Mathematics/Analysis and Calculus/Derivative, Gradient, Jacobian and Hessian.md</guid><pubDate>Mon, 24 Feb 2025 22:38:11 GMT</pubDate></item><item><title><![CDATA[Divergence Theorem]]></title><description><![CDATA[ 
 <br>]]></description><link>the-guide/mathematics/analysis-and-calculus/divergence-theorem.html</link><guid isPermaLink="false">The Guide/Mathematics/Analysis and Calculus/Divergence Theorem.md</guid><pubDate>Thu, 15 Aug 2024 12:51:15 GMT</pubDate></item><item><title><![CDATA[Fundamental Theorem of Calculus]]></title><description><![CDATA[ 
 <br>In a Nutshell
One of the most fundamental theorems in mathematics, linking the concepts of <a data-tooltip-position="top" aria-label="Derivative, Gradient, Jacobian and Hessian" data-href="Derivative, Gradient, Jacobian and Hessian" href="the-guide/mathematics/analysis-and-calculus/derivative,-gradient,-jacobian-and-hessian.html" class="internal-link" target="_self" rel="noopener nofollow">differentiation</a> of a <a data-tooltip-position="top" aria-label="Mathematical Relations" data-href="Mathematical Relations" href="the-guide/mathematics/general-stuff/mathematical-relations.html" class="internal-link" target="_self" rel="noopener nofollow">function</a> and the concept of integration.
<br><br>First Fundamental Theorem of Calculus
Let  be a <a data-tooltip-position="top" aria-label="Continuity" data-href="Continuity" href="the-guide/mathematics/analysis-and-calculus/continuity.html" class="internal-link" target="_self" rel="noopener nofollow">continuous</a>, real-valued <a data-tooltip-position="top" aria-label="Mathematical Relations" data-href="Mathematical Relations" href="the-guide/mathematics/general-stuff/mathematical-relations.html" class="internal-link" target="_self" rel="noopener nofollow">function</a> defined on a closed interval . Let  be the function Then  is <a data-tooltip-position="top" aria-label="Continuity" data-href="Continuity" href="the-guide/mathematics/analysis-and-calculus/continuity.html" class="internal-link" target="_self" rel="noopener nofollow">uniformly continuous</a> on the interval  and <a data-tooltip-position="top" aria-label="Derivative, Gradient, Jacobian and Hessian" data-href="Derivative, Gradient, Jacobian and Hessian" href="the-guide/mathematics/analysis-and-calculus/derivative,-gradient,-jacobian-and-hessian.html" class="internal-link" target="_self" rel="noopener nofollow">differentiable</a> on the open interval . Additionally, it holds that 
<br>Second Fundamental Theorem of Calculus
This result slightly strengthens the above by not assuming  to be continuous. If  is "just" a real-valued function and  is a continuous function on  that is the antiderivative of  on , then 
<br>What it means
This essentially establishes the connection between integration and taking derivatives, enabling to derive all the known rules for both. The second parts waiving of the continuity becomes apparent e.g. for the absolute value function.
]]></description><link>the-guide/mathematics/analysis-and-calculus/fundamental-theorem-of-calculus.html</link><guid isPermaLink="false">The Guide/Mathematics/Analysis and Calculus/Fundamental Theorem of Calculus.md</guid><pubDate>Sat, 25 Jan 2025 18:22:51 GMT</pubDate></item><item><title><![CDATA[Green's Identities]]></title><description><![CDATA[ 
 <br>In a Nutshell
Essential identities in vector calculus that relate the bulk  with the boundary  of a region on which differential operators act.
<br><br>Green's First Identity
Let  and  be scalar <a data-tooltip-position="top" aria-label="Mathematical Relations" data-href="Mathematical Relations" href="the-guide/mathematics/general-stuff/mathematical-relations.html" class="internal-link" target="_self" rel="noopener nofollow">functions</a> on , where  is twice and  once continuously differentiable. The boundary of  is denoted  with normal vector . Using the generalized product rule , we obtain
<br>
<br>
Derived from the <a data-href="Divergence Theorem" href="the-guide/mathematics/analysis-and-calculus/divergence-theorem.html" class="internal-link" target="_self" rel="noopener nofollow">Divergence Theorem</a>

<br>
Green's Second Identity 

]]></description><link>the-guide/mathematics/analysis-and-calculus/green&apos;s-identities.html</link><guid isPermaLink="false">The Guide/Mathematics/Analysis and Calculus/Green&apos;s Identities.md</guid><pubDate>Thu, 15 Aug 2024 12:51:15 GMT</pubDate></item><item><title><![CDATA[Implicit Function Theorem]]></title><description><![CDATA[ 
 <br>In a Nutshell
Under mild conditions on the partial derivatives, the set of zeros of a system of equations (implicit function) are locally the graph of a function.
<br><br>Let  and  be <a data-tooltip-position="top" aria-label="Set" data-href="Set" href="the-guide/mathematics/general-stuff/set.html" class="internal-link" target="_self" rel="noopener nofollow">open sets</a> and a continuously differentiable <a data-tooltip-position="top" aria-label="Mathematical Relations" data-href="Mathematical Relations" href="the-guide/mathematics/general-stuff/mathematical-relations.html" class="internal-link" target="_self" rel="noopener nofollow">function</a> that is stated implicitly.<br>The <a data-tooltip-position="top" aria-label="Derivative, Gradient, Jacobian and Hessian" data-href="Derivative, Gradient, Jacobian and Hessian" href="the-guide/mathematics/analysis-and-calculus/derivative,-gradient,-jacobian-and-hessian.html" class="internal-link" target="_self" rel="noopener nofollow">Jacobian</a> is then split into two parts via the <a data-tooltip-position="top" aria-label="Matrices" data-href="Matrices" href="the-guide/mathematics/linear-algebra/matrices.html" class="internal-link" target="_self" rel="noopener nofollow">matrices</a> andwhere the second one is a square matrix.<br>Theorem
If   is a solution to the implicit equationand if the second matrix  from above is invertible in , then there is an open neighborhood  of  and  of  and a unique continuous differentiable map  with , such that for all  it holds that 
<br>The Jacobian of the function  is given by the <a data-tooltip-position="top" aria-label="Matrices" data-href="Matrices" href="the-guide/mathematics/linear-algebra/matrices.html" class="internal-link" target="_self" rel="noopener nofollow">matrix</a> product ]]></description><link>the-guide/mathematics/analysis-and-calculus/implicit-function-theorem.html</link><guid isPermaLink="false">The Guide/Mathematics/Analysis and Calculus/Implicit Function Theorem.md</guid><pubDate>Sat, 25 Jan 2025 17:52:28 GMT</pubDate></item><item><title><![CDATA[Integration Tricks and Theorems]]></title><description><![CDATA[ 
 <br>In a Nutshell
Collection of tricks, reformulation and theorems I encountered regularly.
<br><br>Integration by Substitution
Let  be an <a data-tooltip-position="top" aria-label="Set" data-href="Set" href="the-guide/mathematics/general-stuff/set.html" class="internal-link" target="_self" rel="noopener nofollow">open set</a> in  and  and <a data-tooltip-position="top" aria-label="Mathematical Relations" data-href="Mathematical Relations" href="the-guide/mathematics/general-stuff/mathematical-relations.html" class="internal-link" target="_self" rel="noopener nofollow">injective</a> <a data-tooltip-position="top" aria-label="Derivative, Gradient, Jacobian and Hessian" data-href="Derivative, Gradient, Jacobian and Hessian" href="the-guide/mathematics/analysis-and-calculus/derivative,-gradient,-jacobian-and-hessian.html" class="internal-link" target="_self" rel="noopener nofollow">differentiable</a> function with <a data-tooltip-position="top" aria-label="Continuity" data-href="Continuity" href="the-guide/mathematics/analysis-and-calculus/continuity.html" class="internal-link" target="_self" rel="noopener nofollow">continuous</a> partial derivatives and non-zero <a data-tooltip-position="top" aria-label="Derivative, Gradient, Jacobian and Hessian" data-href="Derivative, Gradient, Jacobian and Hessian" href="the-guide/mathematics/analysis-and-calculus/derivative,-gradient,-jacobian-and-hessian.html" class="internal-link" target="_self" rel="noopener nofollow">Jacobian</a> for any . Then, for any real-valued, compactly supported and continuous function  on  , it holds that 
<br>Integration by Parts
For any two continuously differentiable <a data-tooltip-position="top" aria-label="Mathematical Relations" data-href="Mathematical Relations" href="the-guide/mathematics/general-stuff/mathematical-relations.html" class="internal-link" target="_self" rel="noopener nofollow">functions</a>  and , it holds that 
<br>Leibniz Integral Rule - Differentiation under the Integral Sign
Let  be a function such that both  and its partial derivative  are continuous in  and  in some region of the -plane, including ,.Also suppose that the functions  and  are both continuous and both have continuous derivatives for . Then, for ,
]]></description><link>the-guide/mathematics/analysis-and-calculus/integration-tricks-and-theorems.html</link><guid isPermaLink="false">The Guide/Mathematics/Analysis and Calculus/Integration Tricks and Theorems.md</guid><pubDate>Wed, 26 Feb 2025 22:49:05 GMT</pubDate></item><item><title><![CDATA[Inverse Function Theorem]]></title><description><![CDATA[ 
 <br>Inverse Function Theorem
Let  be a  function defined on an open set , and let . If the Jacobian <a data-tooltip-position="top" aria-label="Matrices" data-href="Matrices" href="the-guide/mathematics/linear-algebra/matrices.html" class="internal-link" target="_self" rel="noopener nofollow">determinant</a> , then there exist open neighborhoods  of  and  of , such that:

<br> maps  bijectively onto 
<br>the inverse   is 
<br>the <a data-tooltip-position="top" aria-label="Derivative, Gradient, Jacobian and Hessian" data-href="Derivative, Gradient, Jacobian and Hessian" href="the-guide/mathematics/analysis-and-calculus/derivative,-gradient,-jacobian-and-hessian.html" class="internal-link" target="_self" rel="noopener nofollow">Jacobian</a> matrix of  at  is given by

<br>Intuition
If a smooth transformation does not collapse dimensions at a point (determinant not zero), the inverse local behavior is just the undoing of the original linear distortion.
<br>Inverse Function Theorem: Single Variable
Let  be a  function defined on an  <a data-tooltip-position="top" aria-label="Set" data-href="Set" href="the-guide/mathematics/general-stuff/set.html" class="internal-link" target="_self" rel="noopener nofollow">open interval</a>  containing a point . If , then there exist open neighborhoods  of  and  of  such that:

<br> maps  <a data-tooltip-position="top" aria-label="Mathematical Relations" data-href="Mathematical Relations" href="the-guide/mathematics/general-stuff/mathematical-relations.html" class="internal-link" target="_self" rel="noopener nofollow">bijectively</a> onto 
<br>the inverse function  is 
<br>the <a data-tooltip-position="top" aria-label="Derivative, Gradient, Jacobian and Hessian" data-href="Derivative, Gradient, Jacobian and Hessian" href="the-guide/mathematics/analysis-and-calculus/derivative,-gradient,-jacobian-and-hessian.html" class="internal-link" target="_self" rel="noopener nofollow">derivative</a> of  at  is given by 

]]></description><link>the-guide/mathematics/analysis-and-calculus/inverse-function-theorem.html</link><guid isPermaLink="false">The Guide/Mathematics/Analysis and Calculus/Inverse Function Theorem.md</guid><pubDate>Wed, 26 Feb 2025 22:59:38 GMT</pubDate></item><item><title><![CDATA[Matrix Calculus]]></title><description><![CDATA[ 
 <br>The three most widely accepted notations are <br>
<br>Vector by Scalar - Tangent 
<br>Scalar by Vector - <a data-tooltip-position="top" aria-label="Derivative, Gradient, Jacobian and Hessian > The Gradient" data-href="Derivative, Gradient, Jacobian and Hessian#The Gradient" href="the-guide/mathematics/analysis-and-calculus/derivative,-gradient,-jacobian-and-hessian.html#The_Gradient" class="internal-link" target="_self" rel="noopener nofollow">Gradient</a>
<br>Vector by Vector - Pushforward differential / <a data-tooltip-position="top" aria-label="Derivative, Gradient, Jacobian and Hessian" data-href="Derivative, Gradient, Jacobian and Hessian" href="the-guide/mathematics/analysis-and-calculus/derivative,-gradient,-jacobian-and-hessian.html" class="internal-link" target="_self" rel="noopener nofollow">Jacobian</a>
<br><br><br>In general, we have to adopt one of two conventions or mix the two while indicating this. <br>
<br>Jacobian / Numerator Layout - 
<br>All rules can be verified by writing out the matrix derivatives as sums and products of components.<br><br><br><br>Combine above with <a data-tooltip-position="top" aria-label="Complex Differentiation" data-href="Complex Differentiation" href="the-guide/mathematics/analysis-and-calculus/complex-differentiation.html" class="internal-link" target="_self" rel="noopener nofollow">complex differentiation</a> results ]]></description><link>the-guide/mathematics/analysis-and-calculus/matrix-calculus.html</link><guid isPermaLink="false">The Guide/Mathematics/Analysis and Calculus/Matrix Calculus.md</guid><pubDate>Sat, 25 Jan 2025 17:52:28 GMT</pubDate></item><item><title><![CDATA[Numerical Differentiation]]></title><description><![CDATA[ 
 <br>Finite Differences  <a rel="noopener nofollow" class="external-link" href="https://en.wikipedia.org/wiki/Numerical_differentiation" target="_blank">https://en.wikipedia.org/wiki/Numerical_differentiation</a>]]></description><link>the-guide/mathematics/analysis-and-calculus/numerical-differentiation.html</link><guid isPermaLink="false">The Guide/Mathematics/Analysis and Calculus/Numerical Differentiation.md</guid><pubDate>Sun, 09 Feb 2025 21:16:50 GMT</pubDate></item><item><title><![CDATA[Stoke's Theorem]]></title><description><![CDATA[ 
 <br>]]></description><link>the-guide/mathematics/analysis-and-calculus/stoke&apos;s-theorem.html</link><guid isPermaLink="false">The Guide/Mathematics/Analysis and Calculus/Stoke&apos;s Theorem.md</guid><pubDate>Thu, 15 Aug 2024 12:51:15 GMT</pubDate></item><item><title><![CDATA[Category]]></title><description><![CDATA[ 
 <br>In a Nutshell
Provides a general framework for mathematical structures and their relations.
<br><br><img alt="center" src="lib/media/pasted-image-20250303231932.png" style="width: 200px; max-width: 100%;"><br>Definition of a Category
A category  is a triplet of the following mathematical entities

<br>A class , whose elements are called objects.
<br>A class , whose elements are called <a data-tooltip-position="top" aria-label="Morphisms" data-href="Morphisms" href="the-guide/mathematics/category-theory/morphisms.html" class="internal-link" target="_self" rel="noopener nofollow">morphisms</a>, sometimes maps or arrows. Each morphism  maps a source object  to a target object .  denotes the class of all morphisms from  to .
<br>A binary operation , called composition of morphisms, such that for any three morphisms , For two morphisms , the composition is written  and is governed by

<br>Associativity - given morphisms ,  and , it holds that 
<br>Identity Element - for every object, there exists a morphism to itself.



<br><br>Examples<br>
<br>Class <a data-href="Set" href="the-guide/mathematics/general-stuff/set.html" class="internal-link" target="_self" rel="noopener nofollow">Set</a> with morphism <a data-tooltip-position="top" aria-label="Mathematical Relations" data-href="Mathematical Relations" href="the-guide/mathematics/general-stuff/mathematical-relations.html" class="internal-link" target="_self" rel="noopener nofollow">function</a> and regular function composition 
]]></description><link>the-guide/mathematics/category-theory/category.html</link><guid isPermaLink="false">The Guide/Mathematics/Category Theory/Category.md</guid><pubDate>Sun, 30 Mar 2025 12:30:19 GMT</pubDate><enclosure url="lib/media/pasted-image-20250303231932.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src=&quot;lib/media/pasted-image-20250303231932.png&quot;&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[Functors]]></title><description><![CDATA[ 
 <br>In a Nutshell
A functor  is a structure-preserving mapping between <a data-tooltip-position="top" aria-label="Category" data-href="Category" href="the-guide/mathematics/category-theory/category.html" class="internal-link" target="_self" rel="noopener nofollow">categories</a>
<br><br>Let  and  be <a data-tooltip-position="top" aria-label="Category" data-href="Category" href="the-guide/mathematics/category-theory/category.html" class="internal-link" target="_self" rel="noopener nofollow">categories</a>.<br>Covariant Functor
A covariant functor   consists of 

<br>An assignment from every object  in  to another object  in 
<br>An assignment from every <a data-tooltip-position="top" aria-label="Morphisms" data-href="Morphisms" href="the-guide/mathematics/category-theory/morphisms.html" class="internal-link" target="_self" rel="noopener nofollow">morphism</a>  in  to a morphism  in .<br>
subject to the constraintsfor all  and all composable morphisms  in .

<br><img alt="center" src="lib/media/pasted-image-20250304001304.png" style="width: 250px; max-width: 100%;"><br>Contravariant Functor
A contravariant functor   consists of 

<br>An assignment from every object  in  to another object  in 
<br>An assignment from every <a data-tooltip-position="top" aria-label="Morphisms" data-href="Morphisms" href="the-guide/mathematics/category-theory/morphisms.html" class="internal-link" target="_self" rel="noopener nofollow">morphism</a>  in  to a morphism  in .<br>
subject to the constraintsfor all  and all composable morphisms  in .

<br><img alt="center" src="lib/media/pasted-image-20250304001433.png" style="width: 250px; max-width: 100%;"><br>Example
In the category of <a data-tooltip-position="top" aria-label="Vector Space" data-href="Vector Space" href="the-guide/mathematics/general-stuff/vector-space.html" class="internal-link" target="_self" rel="noopener nofollow">vector spaces</a> over a fixed <a data-tooltip-position="top" aria-label="Field" data-href="Field" href="the-guide/mathematics/general-stuff/field.html" class="internal-link" target="_self" rel="noopener nofollow">field</a>, the <a data-tooltip-position="top" aria-label="Dual Vector Space" data-href="Dual Vector Space" href="the-guide/mathematics/general-stuff/dual-vector-space.html" class="internal-link" target="_self" rel="noopener nofollow">dual vector space</a> sends each vector to a linear functional. For a linear map between vector spaces , the dual mapreverses the direction.
]]></description><link>the-guide/mathematics/category-theory/functors.html</link><guid isPermaLink="false">The Guide/Mathematics/Category Theory/Functors.md</guid><pubDate>Mon, 03 Mar 2025 23:17:33 GMT</pubDate><enclosure url="lib/media/pasted-image-20250304001304.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src=&quot;lib/media/pasted-image-20250304001304.png&quot;&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[Morphisms]]></title><description><![CDATA[ 
 <br>Definition
A morphism is a mapping between <a data-tooltip-position="top" aria-label="Category" data-href="Category" href="the-guide/mathematics/category-theory/category.html" class="internal-link" target="_self" rel="noopener nofollow">objects of a category</a>. Each morphism  maps a source object  to a target object .  denotes the class of all morphisms from  to .
<br>For different branches of mathematics, we encounter specialized names of morphisms depending on the structure. Some examples are ...<br>
<br>Homomorphisms in Algebra
<br>Continuous Maps in Topology
<br>Linear maps for <a data-tooltip-position="top" aria-label="Vector Space" data-href="Vector Space" href="the-guide/mathematics/general-stuff/vector-space.html" class="internal-link" target="_self" rel="noopener nofollow">Vector Spaces</a>
<br>General types of morphisms from the perspective of <a data-href="Category" href="the-guide/mathematics/category-theory/category.html" class="internal-link" target="_self" rel="noopener nofollow">Category</a> Theory are ....<br><br><br>Monomorphism (one-to-one)
A morphism  is a monomorphism if for every object  and for any pair of morphisms ,
<br><img alt="center" src="lib/media/pasted-image-20250303233516.png" style="width: 300px; max-width: 100%;"><br>Intuition
A monomorphism is like an injective (one-to-one) function. It means that the morphism does not “collapse” distinct elements; if two different maps into its domain become equal when composed with it, then those two maps were already identical.
Example (category <a data-tooltip-position="top" aria-label="Set" data-href="Set" href="the-guide/mathematics/general-stuff/set.html" class="internal-link" target="_self" rel="noopener nofollow">set</a>):
<br><br><br>Epimorphism (onto)
A morphism  is an epimorphism if for every object  and for any pair of morphisms ,
<br><img alt="center" src="lib/media/pasted-image-20250303233756.png" style="width: 300px; max-width: 100%;"><br>Intuition
An epimorphism is like a surjective (onto) function. It ensures that if two maps out of the codomain are equal when pre-composed with the epimorphism, then they are already equal. In other words, the morphism “covers” the whole codomain.
Example (category <a data-tooltip-position="top" aria-label="Set" data-href="Set" href="the-guide/mathematics/general-stuff/set.html" class="internal-link" target="_self" rel="noopener nofollow">set</a>):
<br><br><br>Isomorphism (reversible)
A morphism  is an isomorphism if there exists a morphism  such that
<br><img alt="center" src="lib/media/pasted-image-20250303234656.png" style="width: 300px; max-width: 100%;"><br>Intuition
Example (category <a data-tooltip-position="top" aria-label="Set" data-href="Set" href="the-guide/mathematics/general-stuff/set.html" class="internal-link" target="_self" rel="noopener nofollow">set</a>):
<br>In many branches of mathematics, the study of isomorphisms is of special interest and they are therefore given special names.<br><br><br><br>Endomorphism (to itself)
A morphism  is called an endomorphism of . Its diagram is:
<br><img alt="center" src="lib/media/pasted-image-20250303234540.png" style="width: 300px; max-width: 100%;"><br>Intuition
Exampe (category <a data-tooltip-position="top" aria-label="Vector Space" data-href="Vector Space" href="the-guide/mathematics/general-stuff/vector-space.html" class="internal-link" target="_self" rel="noopener nofollow">vector space</a>):
<br><br><br>Automorphism (invertible to itself)
An automorphism is an isomorphism from an object to itself. That is, an endomorphism  is an automorphism if there exists an inverse  such that
<br><img alt="center" src="lib/media/pasted-image-20250303234406.png" style="width: 300px; max-width: 100%;"><br>Intuition
An automorphism is an endomorphism that is invertible. It represents a symmetry of the object, a way to “rearrange” the object without changing its inherent structure.
Example (category <a data-tooltip-position="top" aria-label="Group" data-href="Group" href="the-guide/mathematics/group-theory/group.html" class="internal-link" target="_self" rel="noopener nofollow">group</a>):
<br><br><br>Retraction
A morphism  is called a retraction if there exists a morphism  (called a section) such that
<br><img alt="center" src="lib/media/pasted-image-20250303234328.png" style="width: 300px; max-width: 100%;"><br>Intuition
A retraction is a map that “retracts” an object onto a subobject. There exists a corresponding section (a kind of “inclusion” map) such that when you first include the subobject into the larger object and then retract, you get the identity on the subobject.<br>
Example: Based on  and  with the mapwith the retraction
<br><br><br>Section
A morphism  is called a section if there exists a morphism  (called a retraction) such that
<br><img alt="center" src="lib/media/pasted-image-20250303234335.png" style="width: 300px; max-width: 100%;"><br>Intuition
A section is the map that serves as a left-inverse to a retraction. It “selects” or “includes” the subobject into the larger object, ensuring that when followed by the retraction, you recover the original element of the subobject.
Example: see above.
]]></description><link>the-guide/mathematics/category-theory/morphisms.html</link><guid isPermaLink="false">The Guide/Mathematics/Category Theory/Morphisms.md</guid><pubDate>Wed, 23 Apr 2025 08:34:32 GMT</pubDate><enclosure url="lib/media/pasted-image-20250303233516.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src=&quot;lib/media/pasted-image-20250303233516.png&quot;&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[Autonomous ODEs]]></title><description><![CDATA[ 
 <br>In a Nutshell
Special category of <a data-tooltip-position="top" aria-label="ODEs - Ordinary Differential Equations" data-href="ODEs - Ordinary Differential Equations" href="the-guide/mathematics/differential-equations/odes-ordinary-differential-equations.html" class="internal-link" target="_self" rel="noopener nofollow">ordinary differential equations</a> that does not depend on the independent variable. When considering  and therefore derivatives of , th this means that 
<br>Why are they special ?
In physics, the autonomous variable  is usually replaced by time . Since the laws of nature are assumed to be unaffected by time, autonomous systems play a special role in many applications.
<br><br><br>Theorem
Solutions to autonomous ODEs are invariant under translations. If  is a solution, then  with  is also a solution.
<br>Proof
Simply follows by replacing  by another variable and verifying initial condition and unaffectedness of the original equation.
<br><br><br>In general, any autonomous ODE or order  can be reduced to an -dimensional system of order  (see <a data-tooltip-position="top" aria-label="ODEs - Ordinary Differential Equations" data-href="ODEs - Ordinary Differential Equations" href="the-guide/mathematics/differential-equations/odes-ordinary-differential-equations.html" class="internal-link" target="_self" rel="noopener nofollow">Theorem</a>). It therefore suffices to consider solution techniques of one dimensional examples. However, there are only a handful of special cases that can always be solved explicitly:<br><br>Details on these techniques can e.g. be found on Wikipedia under <a rel="noopener nofollow" class="external-link" href="https://en.wikipedia.org/wiki/Autonomous_system_(mathematics)" target="_blank">https://en.wikipedia.org/wiki/Autonomous_system_(mathematics)</a>.<br>Any autonomous equation of order  can only bes solved in exactly if they have general favorable properties such as linearity.<br>Intuition
This fact is not surprising considering that many <a data-tooltip-position="top" aria-label="Static and Dynamic Systems" data-href="Static and Dynamic Systems" href="the-guide/robotics,-dynamics-and-control/dynamics/static-and-dynamic-systems.html" class="internal-link" target="_self" rel="noopener nofollow">dynamical systems</a> (which are autonomous) in  dimensions can be chaotic.
]]></description><link>the-guide/mathematics/differential-equations/autonomous-odes.html</link><guid isPermaLink="false">The Guide/Mathematics/Differential Equations/Autonomous ODEs.md</guid><pubDate>Mon, 03 Mar 2025 15:30:25 GMT</pubDate></item><item><title><![CDATA[Explicit Runge Kutta Methods]]></title><description><![CDATA[ 
 <br>In a Nutshell
Subcategory of <a data-href="Runge-Kutta Methods" href="the-guide/mathematics/differential-equations/runge-kutta-methods.html" class="internal-link" target="_self" rel="noopener nofollow">Runge-Kutta Methods</a> to numerically solve <a data-tooltip-position="top" aria-label="ODEs - Ordinary Differential Equations" data-href="ODEs - Ordinary Differential Equations" href="the-guide/mathematics/differential-equations/odes-ordinary-differential-equations.html" class="internal-link" target="_self" rel="noopener nofollow">systems of ODEs</a>. Efficient to compute, but may suffer from stability problems.
<br><br><br>For explicit methods, the coefficients satisfy so that the stages can be computed sequentially. Some important explicit methods are ...<br>Explicit Euler
The Butcher tableau iswhich yields the equations
<br>Midpoint Method
The Butcher tableau iswhich yields the equations
<br>Heun's Method
The Butcher tableau is which yiels the equations 
<br>Classic RK4
The Butcher tableau iswhich yields the equations
]]></description><link>the-guide/mathematics/differential-equations/explicit-runge-kutta-methods.html</link><guid isPermaLink="false">The Guide/Mathematics/Differential Equations/Explicit Runge Kutta Methods.md</guid><pubDate>Sun, 09 Feb 2025 12:49:20 GMT</pubDate></item><item><title><![CDATA[Implicit Runge-Kutta Methods]]></title><description><![CDATA[ 
 <br>In a Nutshell
Subcategory of <a data-href="Runge-Kutta Methods" href="the-guide/mathematics/differential-equations/runge-kutta-methods.html" class="internal-link" target="_self" rel="noopener nofollow">Runge-Kutta Methods</a> to numerically solve <a data-tooltip-position="top" aria-label="ODEs - Ordinary Differential Equations" data-href="ODEs - Ordinary Differential Equations" href="the-guide/mathematics/differential-equations/odes-ordinary-differential-equations.html" class="internal-link" target="_self" rel="noopener nofollow">systems of ODEs</a>. increased stability over <a data-href="Explicit Runge Kutta Methods" href="the-guide/mathematics/differential-equations/explicit-runge-kutta-methods.html" class="internal-link" target="_self" rel="noopener nofollow">Explicit Runge Kutta Methods</a>, especially for <a data-tooltip-position="top" aria-label="Stiff Differential Equations" data-href="Stiff Differential Equations" href="the-guide/mathematics/differential-equations/stiff-differential-equations.html" class="internal-link" target="_self" rel="noopener nofollow">stiff problems</a> at the cost of more computational complexity.
<br><br><br>In implicit methods, the stage equations are generally coupled. Therefore, we have to solve a non-linear system for each stage . Some examples are ...<br>Implicit Euler Method
The Butcher tableau iswhich yields the equations
<br>Implicit Midpoint Method
The Butcher tableau iswhich yields the equations
<br>2-Stage Gauss-Legendre Method
The Butcher tableau is  Its stage equations are 
<br><br><br>DIRK methods are a subclass of implicit methods where the Butcher tableau is chosen to be lower triangular with nonzero diagonal entries: This structure allows the stage equations to be solved sequentially, reducing the computational cost compared to fully implicit methods. DIRK methods thereby combine improved stability properties (for stiff problems) with a relatively efficient solution process.<br>Examples are ...<br>2-stage Singly Diagonally Implicit Runge--Kutta (SDIRK)
Given by the Butcher tableau  with  The algorithm is then 
]]></description><link>the-guide/mathematics/differential-equations/implicit-runge-kutta-methods.html</link><guid isPermaLink="false">The Guide/Mathematics/Differential Equations/Implicit Runge-Kutta Methods.md</guid><pubDate>Sun, 09 Feb 2025 12:44:32 GMT</pubDate></item><item><title><![CDATA[Lax-Richtmyer and Dahlquist Equivalence Theorems]]></title><description><![CDATA[ 
 <br>In a Nutshell
Fundamental theorems for numerical analysis of differential equations. Establish thatfor finite differences for <a data-tooltip-position="top" aria-label="PDEs - Partial Differential Equations" data-href="PDEs - Partial Differential Equations" href="PDEs - Partial Differential Equations" class="internal-link" target="_self" rel="noopener nofollow">PDEs</a> (Lax-Richtmyer) and <a data-tooltip-position="top" aria-label="ODEs - Ordinary Differential Equations" data-href="ODEs - Ordinary Differential Equations" href="the-guide/mathematics/differential-equations/odes-ordinary-differential-equations.html" class="internal-link" target="_self" rel="noopener nofollow">ODEs</a> (Dahlquist).
<br><br>Lax-Richtmyer Theorem
For a consistent finite difference scheme applied to a well-posed linear initial value problem, stability is a necessary and sufficient condition for convergence.
<br>Dahlquist Theorem
A linear multistep method is <a data-tooltip-position="top" aria-label="Accuracy of Numerical Methods" data-href="Accuracy of Numerical Methods" href="the-guide/mathematics/general-stuff/accuracy-of-numerical-methods.html" class="internal-link" target="_self" rel="noopener nofollow">convergent</a> if and only if it is consistent and <a data-tooltip-position="top" aria-label="Stability of Numerical Methods" data-href="Stability of Numerical Methods" href="the-guide/mathematics/general-stuff/stability-of-numerical-methods.html" class="internal-link" target="_self" rel="noopener nofollow">zero-stable</a>.
]]></description><link>the-guide/mathematics/differential-equations/lax-richtmyer-and-dahlquist-equivalence-theorems.html</link><guid isPermaLink="false">The Guide/Mathematics/Differential Equations/Lax-Richtmyer and Dahlquist Equivalence Theorems.md</guid><pubDate>Sun, 09 Feb 2025 14:20:29 GMT</pubDate></item><item><title><![CDATA[ODEs - Ordinary Differential Equations]]></title><description><![CDATA[ 
 <br>In a Nutshell
Differential equation dependent on only a single independent variable, named to underline the contrast to <a data-tooltip-position="top" aria-label="PDEs - Partial Differential Equations" data-href="PDEs - Partial Differential Equations" href="PDEs - Partial Differential Equations" class="internal-link" target="_self" rel="noopener nofollow">PDEs</a> and <a data-tooltip-position="top" aria-label="Stochastic Differential Equation" data-href="Stochastic Differential Equation" href="the-guide/mathematics/probability-theory/stochastic-differential-equation.html" class="internal-link" target="_self" rel="noopener nofollow">SDEs</a>. 
<br>We consider a <a data-tooltip-position="top" aria-label="Mathematical Relations" data-href="Mathematical Relations" href="the-guide/mathematics/general-stuff/mathematical-relations.html" class="internal-link" target="_self" rel="noopener nofollow">function</a>  with  of the form meaning  (in physics this is time  in most cases, see <a data-href="- Notation - ODEs, Robotics, Dynamics and Control -" href="notation-world/-notation-odes,-robotics,-dynamics-and-control-.html" class="internal-link" target="_self" rel="noopener nofollow">- Notation - ODEs, Robotics, Dynamics and Control -</a> for different notations) is the independent variable.<br>Finding the function  given initial values is considered an initial value problem.<br><br><br>Let  be a functional of ,  and <a data-tooltip-position="top" aria-label="Derivative, Gradient, Jacobian and Hessian" data-href="Derivative, Gradient, Jacobian and Hessian" href="the-guide/mathematics/analysis-and-calculus/derivative,-gradient,-jacobian-and-hessian.html" class="internal-link" target="_self" rel="noopener nofollow">derviatives</a> of the latter. An equation of the formis denoted an (explicit) ordinary differential equation.<br>Order of an ODE
The order of the highest derivative with non-zero coefficient (here: ).
<br>Explicit vs. Implicit
Conventions, explicit separates the highest derivative on the right sideimplicit absorbs it into the equation
<br>Autonomous
No dependence on the independent variableFor additional information, see <a data-tooltip-position="top" aria-label="Autonomous ODEs" data-href="Autonomous ODEs" href="the-guide/mathematics/differential-equations/autonomous-odes.html" class="internal-link" target="_self" rel="noopener nofollow">note on autonomous ODEs</a>.
<br>Linear vs. Nonlinear
Linear if  can be written as a linear combination of derivativeswhere  and  are continuous <a data-tooltip-position="top" aria-label="Mathematical Relations" data-href="Mathematical Relations" href="the-guide/mathematics/general-stuff/mathematical-relations.html" class="internal-link" target="_self" rel="noopener nofollow">functions</a>. Conversely, non-linear ODEs are relations involving e.g. trigonometric, quadratic or exponential relationships between derivatives.
<br>Homogeneous vs. Nonhomogenous
Additionally, a linear ODE is homogeneous, if 
<br><br><br>Systems of ODEs
As mentioned in the beginning, all definitions above naturally extend to vector-valued variables yielding a <a data-href="Static and Dynamic Systems" href="the-guide/robotics,-dynamics-and-control/dynamics/static-and-dynamic-systems.html" class="internal-link" target="_self" rel="noopener nofollow">Static and Dynamic Systems</a> of coupled ODEsConsequently, an explicit system of ODEs has the form
<br>The analysis of such systems is essential in the study of <a data-tooltip-position="top" aria-label="Static and Dynamic Systems" data-href="Static and Dynamic Systems" href="the-guide/robotics,-dynamics-and-control/dynamics/static-and-dynamic-systems.html" class="internal-link" target="_self" rel="noopener nofollow">dynamical systems</a> and many other fields. The behavior is usually visualized in <a data-tooltip-position="top" aria-label="Phase Space" data-href="Phase Space" href="the-guide/mathematics/differential-equations/phase-space.html" class="internal-link" target="_self" rel="noopener nofollow">phase space</a>.<br><br><br>Solution of a Differential Equation
Given a differential equationa <a data-tooltip-position="top" aria-label="Mathematical Relations" data-href="Mathematical Relations" href="the-guide/mathematics/general-stuff/mathematical-relations.html" class="internal-link" target="_self" rel="noopener nofollow">function</a>  is called a solution, if  is -times differentiable on  and 
<br>The central theorems regarding solutions of such systems are ...<br>
<br><a data-href="Peano Existence Theorem" href="the-guide/mathematics/differential-equations/peano-existence-theorem.html" class="internal-link" target="_self" rel="noopener nofollow">Peano Existence Theorem</a>, which guarantees existence of solutions to certain initial values
<br><a data-href="Picard-Lindelöf Theorem" href="the-guide/mathematics/differential-equations/picard-lindelöf-theorem.html" class="internal-link" target="_self" rel="noopener nofollow">Picard-Lindelöf Theorem</a>, which extends these guarantees to the existence of a unique solution under additional conditions
<br><br><br>A central theorem for the solution of ODEs, especially in numerics is the following.<br>Theorem
Any explicit ODE of order , namely can be rewritten as a system of  first-order differential equations. This is acheived by defining a new family of unknown functions The resulting system iswith
<br>Why is this useful ?
This fact allows to simplify a  lot of literature by showing properties for these systems instead of many different forms, greatly simplifying many theorems. Additionally, these systems can be represented or approximated locally by <a data-href="Matrices" href="the-guide/mathematics/linear-algebra/matrices.html" class="internal-link" target="_self" rel="noopener nofollow">Matrices</a>, which opens the door for efficient numerical algorithms.
]]></description><link>the-guide/mathematics/differential-equations/odes-ordinary-differential-equations.html</link><guid isPermaLink="false">The Guide/Mathematics/Differential Equations/ODEs - Ordinary Differential Equations.md</guid><pubDate>Wed, 23 Apr 2025 21:55:34 GMT</pubDate></item><item><title><![CDATA[Peano Existence Theorem]]></title><description><![CDATA[ 
 <br>In a Nutshell
Fundamental theorem for <a data-tooltip-position="top" aria-label="ODEs - Ordinary Differential Equations" data-href="ODEs - Ordinary Differential Equations" href="the-guide/mathematics/differential-equations/odes-ordinary-differential-equations.html" class="internal-link" target="_self" rel="noopener nofollow">ordinary differential equations</a>, which guarantees the existence of possible multiple solutions to certain initial value problems.
<br><br>Theorem
Let  be an open <a data-tooltip-position="top" aria-label="Set" data-href="Set" href="the-guide/mathematics/general-stuff/set.html" class="internal-link" target="_self" rel="noopener nofollow">subset</a> of  with  a <a data-tooltip-position="top" aria-label="Continuity" data-href="Continuity" href="the-guide/mathematics/analysis-and-calculus/continuity.html" class="internal-link" target="_self" rel="noopener nofollow">continuous</a> <a data-tooltip-position="top" aria-label="Mathematical Relations" data-href="Mathematical Relations" href="the-guide/mathematics/general-stuff/mathematical-relations.html" class="internal-link" target="_self" rel="noopener nofollow">function</a> and  a continuous, <a data-tooltip-position="top" aria-label="ODEs - Ordinary Differential Equations" data-href="ODEs - Ordinary Differential Equations" href="the-guide/mathematics/differential-equations/odes-ordinary-differential-equations.html" class="internal-link" target="_self" rel="noopener nofollow">explicit ODE</a> defined on . Then every initial value problem  for  with  has a (non-unique) local solution  where  is a neighborhood of  in , such that  .
]]></description><link>the-guide/mathematics/differential-equations/peano-existence-theorem.html</link><guid isPermaLink="false">The Guide/Mathematics/Differential Equations/Peano Existence Theorem.md</guid><pubDate>Sat, 25 Jan 2025 18:42:39 GMT</pubDate></item><item><title><![CDATA[Phase Space]]></title><description><![CDATA[ 
 <br>In a Nutshell
Abstract <a data-tooltip-position="top" aria-label="Space" data-href="Space" href="the-guide/mathematics/general-stuff/space.html" class="internal-link" target="_self" rel="noopener nofollow">space</a> in which every axis corresponds to a degree of freedom of an associated system of <a data-tooltip-position="top" aria-label="ODEs - Ordinary Differential Equations" data-href="ODEs - Ordinary Differential Equations" href="the-guide/mathematics/differential-equations/odes-ordinary-differential-equations.html" class="internal-link" target="_self" rel="noopener nofollow">ODEs</a>. In physics, this is the <a data-tooltip-position="top" aria-label="Set" data-href="Set" href="the-guide/mathematics/general-stuff/set.html" class="internal-link" target="_self" rel="noopener nofollow">set</a> of all possible states when described by a given parametrization.
<br>]]></description><link>the-guide/mathematics/differential-equations/phase-space.html</link><guid isPermaLink="false">The Guide/Mathematics/Differential Equations/Phase Space.md</guid><pubDate>Sat, 25 Jan 2025 17:31:26 GMT</pubDate></item><item><title><![CDATA[Picard-Lindelöf Theorem]]></title><description><![CDATA[ 
 <br>In a Nutshell
Together with <a data-tooltip-position="top" aria-label="Peano Existence Theorem" data-href="Peano Existence Theorem" href="the-guide/mathematics/differential-equations/peano-existence-theorem.html" class="internal-link" target="_self" rel="noopener nofollow">Peano's Theorem</a>, forms the basis of existence and uniqueness of initial value problems. Extends the former to the existence of a unique solution under additional conditions.
<br><br>Why does Lipschitz make it unique ?<br>Theorem
Let  be a closed rectangle with  in its interior. Let  be a function that is <a data-tooltip-position="top" aria-label="Continuity" data-href="Continuity" href="the-guide/mathematics/analysis-and-calculus/continuity.html" class="internal-link" target="_self" rel="noopener nofollow">continuous</a> in  and <a data-tooltip-position="top" aria-label="Lipschitz Continuity" data-href="Lipschitz Continuity" href="Lipschitz Continuity" class="internal-link" target="_self" rel="noopener nofollow">Lipschitz</a> in  with Lipschitz constant independent of . Then there exists some , such that the <a data-tooltip-position="top" aria-label="ODEs - Ordinary Differential Equations" data-href="ODEs - Ordinary Differential Equations" href="the-guide/mathematics/differential-equations/odes-ordinary-differential-equations.html" class="internal-link" target="_self" rel="noopener nofollow">initial value problem</a>has a unique solution on the interval 
]]></description><link>the-guide/mathematics/differential-equations/picard-lindelöf-theorem.html</link><guid isPermaLink="false">The Guide/Mathematics/Differential Equations/Picard-Lindelöf Theorem.md</guid><pubDate>Sat, 25 Jan 2025 18:47:19 GMT</pubDate></item><item><title><![CDATA[Runge-Kutta Methods]]></title><description><![CDATA[ 
 <br>In a Nutshell
Family of iterative methods to solve non-linear differential equations.
<br><br>We follow our notation for <a data-tooltip-position="top" aria-label="ODEs - Ordinary Differential Equations" data-href="ODEs - Ordinary Differential Equations" href="the-guide/mathematics/differential-equations/odes-ordinary-differential-equations.html" class="internal-link" target="_self" rel="noopener nofollow">ODEs</a> that is based on a <a data-tooltip-position="top" aria-label="Mathematical Relations" data-href="Mathematical Relations" href="the-guide/mathematics/general-stuff/mathematical-relations.html" class="internal-link" target="_self" rel="noopener nofollow">function</a>  with  and its <a data-tooltip-position="top" aria-label="Derivative, Gradient, Jacobian and Hessian" data-href="Derivative, Gradient, Jacobian and Hessian" href="the-guide/mathematics/analysis-and-calculus/derivative,-gradient,-jacobian-and-hessian.html" class="internal-link" target="_self" rel="noopener nofollow">derviatives</a>. Additionally, the basic setting for numerical analysis of ODEs usually assumes a <a data-tooltip-position="top" aria-label="ODEs - Ordinary Differential Equations > Reduction to first Order System" data-href="ODEs - Ordinary Differential Equations#Reduction to first Order System" href="the-guide/mathematics/differential-equations/odes-ordinary-differential-equations.html#Reduction_to_first_Order_System" class="internal-link" target="_self" rel="noopener nofollow">reformulation to a first order system</a> without loss of generality. We can thereby consider the initial value problem<br>Runge-Kutta Methods
An -stage Runge--Kutta method approximates the solution of the initial value problem with step-size  via  with the stage values defined by 
<br><br><br>A compact way to present the coefficients of a Runge--Kutta method is via the Butcher tableau: Here, the  indicate the time fractions, the  are the weights for the stage derivatives, and the  are used to compute the final update.<br><br><br>Order of Runge-Kutta Methods
A method is said to be of order  if its local truncation error is , which typically results in a global error of  over the interval.
<br>Explicit Methods
For explicit Runge-Kutta methods, the coefficients satisfy so that each  depends only on the previously computed stages . For more information, see <a data-href="Explicit Runge Kutta Methods" href="the-guide/mathematics/differential-equations/explicit-runge-kutta-methods.html" class="internal-link" target="_self" rel="noopener nofollow">Explicit Runge Kutta Methods</a>. 
<br>Implicit Methods
Otherwise, In implicit methods, the stage equations are generally coupled. This enhances stability (especially for <a data-tooltip-position="top" aria-label="Stiff Differential Equations" data-href="Stiff Differential Equations" href="the-guide/mathematics/differential-equations/stiff-differential-equations.html" class="internal-link" target="_self" rel="noopener nofollow">stiff problems</a>) at the cost of solving <a data-tooltip-position="top" aria-label="ODEs - Ordinary Differential Equations" data-href="ODEs - Ordinary Differential Equations" href="the-guide/mathematics/differential-equations/odes-ordinary-differential-equations.html" class="internal-link" target="_self" rel="noopener nofollow">non-linear systems of ODEs</a>. An important subcategory of this are Diagonally Implicit Runge-Kutta Methods, which allows more efficient algorithms while still offering benefits of implicit methods. For more information, see <a data-href="Implicit Runge-Kutta Methods" href="the-guide/mathematics/differential-equations/implicit-runge-kutta-methods.html" class="internal-link" target="_self" rel="noopener nofollow">Implicit Runge-Kutta Methods</a>.
]]></description><link>the-guide/mathematics/differential-equations/runge-kutta-methods.html</link><guid isPermaLink="false">The Guide/Mathematics/Differential Equations/Runge-Kutta Methods.md</guid><pubDate>Sun, 09 Feb 2025 14:13:57 GMT</pubDate></item><item><title><![CDATA[Seperation of Variables for ODEs and PDEs]]></title><description><![CDATA[ 
 <br>In a Nutshell
Collection of techniques that allow to solve special forms of <a data-tooltip-position="top" aria-label="ODEs - Ordinary Differential Equations" data-href="ODEs - Ordinary Differential Equations" href="the-guide/mathematics/differential-equations/odes-ordinary-differential-equations.html" class="internal-link" target="_self" rel="noopener nofollow">ODEs</a> and <a data-tooltip-position="top" aria-label="PDEs - Partial Differential Equations" data-href="PDEs - Partial Differential Equations" href="PDEs - Partial Differential Equations" class="internal-link" target="_self" rel="noopener nofollow">PDEs</a>. In general, the techniques rely on algebraically manipulate the equation until the two variables appear on seperate sides.
<br><br><a rel="noopener nofollow" class="external-link" href="https://en.wikipedia.org/wiki/Separation_of_variables" target="_blank">https://en.wikipedia.org/wiki/Separation_of_variables</a>]]></description><link>the-guide/mathematics/differential-equations/seperation-of-variables-for-odes-and-pdes.html</link><guid isPermaLink="false">The Guide/Mathematics/Differential Equations/Seperation of Variables for ODEs and PDEs.md</guid><pubDate>Sat, 25 Jan 2025 16:50:09 GMT</pubDate></item><item><title><![CDATA[Stiff Differential Equations]]></title><description><![CDATA[ 
 <br>In a Nutshell
Family of <a data-tooltip-position="top" aria-label="ODEs - Ordinary Differential Equations" data-href="ODEs - Ordinary Differential Equations" href="the-guide/mathematics/differential-equations/odes-ordinary-differential-equations.html" class="internal-link" target="_self" rel="noopener nofollow">ODEs</a> for which stability requirements enforce a much smaller required time-step than accuracy requirements.
<br><br>Consider the initial value problem<br><br>Assume that (F) is sufficiently smooth and <a data-tooltip-position="top" aria-label="Continuity" data-href="Continuity" href="the-guide/mathematics/analysis-and-calculus/continuity.html" class="internal-link" target="_self" rel="noopener nofollow">Lipschitz continuous</a> in . Linearizing about a solution (or considering a linear test equation) yields<br><br>where the <a data-tooltip-position="top" aria-label="Derivative, Gradient, Jacobian and Hessian" data-href="Derivative, Gradient, Jacobian and Hessian" href="the-guide/mathematics/analysis-and-calculus/derivative,-gradient,-jacobian-and-hessian.html" class="internal-link" target="_self" rel="noopener nofollow">Jacobian</a><br><br>has <a data-tooltip-position="top" aria-label="Eigenvalue Problem" data-href="Eigenvalue Problem" href="the-guide/mathematics/linear-algebra/eigenvalue-problem.html" class="internal-link" target="_self" rel="noopener nofollow">eigenvalues</a> . <br>Stiff Problems
If, for a representative , there exists a large disparity in the magnitudes of the real parts of these eigenvalues, i.e.,then the system exhibits fast decaying modes (associated with eigenvalues having large ) alongside slow modes.
Based on the results in <a data-href="Stability of Numerical Methods" href="the-guide/mathematics/general-stuff/stability-of-numerical-methods.html" class="internal-link" target="_self" rel="noopener nofollow">Stability of Numerical Methods</a> and <a data-href="Accuracy of Numerical Methods" href="the-guide/mathematics/general-stuff/accuracy-of-numerical-methods.html" class="internal-link" target="_self" rel="noopener nofollow">Accuracy of Numerical Methods</a>, this means that the step size required for stability is much smaller than that dictated by the accuracy considerations. 
<br><a data-tooltip-position="top" aria-label="Implicit Runge-Kutta Methods" data-href="Implicit Runge-Kutta Methods" href="the-guide/mathematics/differential-equations/implicit-runge-kutta-methods.html" class="internal-link" target="_self" rel="noopener nofollow">Implicit methods</a>, which typically have larger stability regions, are often preferred for stiff problems because they permit larger step sizes while maintaining numerical stability.<br><br><br>For instance, consider the scalar linear test equation<br><br><a data-tooltip-position="top" aria-label="Explicit Runge Kutta Methods" data-href="Explicit Runge Kutta Methods" href="the-guide/mathematics/differential-equations/explicit-runge-kutta-methods.html" class="internal-link" target="_self" rel="noopener nofollow">Explicit methods</a>, such as the forward Euler method, require that the step size  satisfies<br><br>When  is large, this condition forces  to be extremely small to maintain stability—even if the overall solution varies much slower.]]></description><link>the-guide/mathematics/differential-equations/stiff-differential-equations.html</link><guid isPermaLink="false">The Guide/Mathematics/Differential Equations/Stiff Differential Equations.md</guid><pubDate>Sun, 09 Feb 2025 13:21:04 GMT</pubDate></item><item><title><![CDATA[Eingebettete Weingarten-Abbildung]]></title><description><![CDATA[ 
 <br>Man betrachte die Projektoren  in den Normalraum mit Basis via Bei orthogonaler Basis  ergibt sich damit die Projektion in den <a data-href="Tangentialraum" href="the-guide/mathematics/differential-geometry/riemannian-geometry/mannigfaltigkeiten-und-differenzierbare-abbildungen/tangentialraum.html" class="internal-link" target="_self" rel="noopener nofollow">Tangentialraum</a> als  mit <a data-tooltip-position="top" aria-label="Matrices" data-href="Matrices" href="the-guide/mathematics/linear-algebra/matrices.html" class="internal-link" target="_self" rel="noopener nofollow">symmetrischer Matrix</a> . <br>Eingebettete Weingarten-Abbildung
Definiere die <a data-tooltip-position="top" aria-label="Matrices" data-href="Matrices" href="the-guide/mathematics/linear-algebra/matrices.html" class="internal-link" target="_self" rel="noopener nofollow">symmetrische Matrix</a> mit <a data-tooltip-position="top" aria-label="Moore-Penrose Pseudoinverse" data-href="Moore-Penrose Pseudoinverse" href="the-guide/mathematics/linear-algebra/moore-penrose-pseudoinverse.html" class="internal-link" target="_self" rel="noopener nofollow">Pseudoinverse</a> , <a data-tooltip-position="top" aria-label="1. Fundamentalform" data-href="1. Fundamentalform" href="the-guide/mathematics/differential-geometry/elementary-differential-geometry/1.-fundamentalform.html" class="internal-link" target="_self" rel="noopener nofollow">1.FF</a>  und <a data-tooltip-position="top" aria-label="2. Fundamentalform" data-href="2. Fundamentalform" href="the-guide/mathematics/differential-geometry/elementary-differential-geometry/2.-fundamentalform.html" class="internal-link" target="_self" rel="noopener nofollow">2.FF</a> .
<br>
<br>Die <a data-tooltip-position="top" aria-label="Hauptkrümmung und Hauptkrümmungsrichtung" data-href="Hauptkrümmung und Hauptkrümmungsrichtung" href="the-guide/mathematics/differential-geometry/elementary-differential-geometry/extrinsische-geometrie-von-hyperflächen/hauptkrümmung-und-hauptkrümmungsrichtung.html" class="internal-link" target="_self" rel="noopener nofollow">HK</a>  sind die <a data-tooltip-position="top" aria-label="Eigenvalue Problem" data-href="Eigenvalue Problem" href="the-guide/mathematics/linear-algebra/eigenvalue-problem.html" class="internal-link" target="_self" rel="noopener nofollow">Eigenwerte</a> von  zu den <a data-tooltip-position="top" aria-label="Eigenvalue Problem" data-href="Eigenvalue Problem" href="the-guide/mathematics/linear-algebra/eigenvalue-problem.html" class="internal-link" target="_self" rel="noopener nofollow">Eigenvektoren</a>  im <a data-href="Tangentialraum" href="the-guide/mathematics/differential-geometry/riemannian-geometry/mannigfaltigkeiten-und-differenzierbare-abbildungen/tangentialraum.html" class="internal-link" target="_self" rel="noopener nofollow">Tangentialraum</a>
<br>Zusätzlich ist <a data-tooltip-position="top" aria-label="Gauß-Abbildung" data-href="Gauß-Abbildung" href="the-guide/mathematics/differential-geometry/elementary-differential-geometry/extrinsische-geometrie-von-hyperflächen/gauß-abbildung.html" class="internal-link" target="_self" rel="noopener nofollow">Normalenvektor</a>  Eigenvektor zu .
<br>Lebt in Raum, in den Fläche eingebettet ist 
<br> ist <a data-tooltip-position="top" aria-label="Flächen" data-href="Flächen" href="the-guide/mathematics/differential-geometry/elementary-differential-geometry/flächen.html" class="internal-link" target="_self" rel="noopener nofollow">Eigenschaft der Klasse</a> 
<br>Für die <a data-tooltip-position="top" aria-label="Hauptkrümmung und Hauptkrümmungsrichtung" data-href="Hauptkrümmung und Hauptkrümmungsrichtung" href="the-guide/mathematics/differential-geometry/elementary-differential-geometry/extrinsische-geometrie-von-hyperflächen/hauptkrümmung-und-hauptkrümmungsrichtung.html" class="internal-link" target="_self" rel="noopener nofollow">mittlere Krümmung</a> gilt 
<br>Es gilt  und  mit <a data-tooltip-position="top" aria-label="Hauptkrümmung und Hauptkrümmungsrichtung" data-href="Hauptkrümmung und Hauptkrümmungsrichtung" href="the-guide/mathematics/differential-geometry/elementary-differential-geometry/extrinsische-geometrie-von-hyperflächen/hauptkrümmung-und-hauptkrümmungsrichtung.html" class="internal-link" target="_self" rel="noopener nofollow">Gauß-Krümmung</a> .
<br>Während die <a data-href="Weingarten-Abbildung" href="the-guide/mathematics/differential-geometry/elementary-differential-geometry/extrinsische-geometrie-von-hyperflächen/weingarten-abbildung.html" class="internal-link" target="_self" rel="noopener nofollow">Weingarten-Abbildung</a> die Relation zwischen Zeilen von  und  beschreibt, so beschreibt  dies für die Spalten 
]]></description><link>the-guide/mathematics/differential-geometry/elementary-differential-geometry/extrinsische-geometrie-von-hyperflächen/eingebettete-weingarten-abbildung.html</link><guid isPermaLink="false">The Guide/Mathematics/Differential Geometry/Elementary Differential Geometry/Extrinsische Geometrie von Hyperflächen/Eingebettete Weingarten-Abbildung.md</guid><pubDate>Mon, 12 Aug 2024 17:05:42 GMT</pubDate></item><item><title><![CDATA[Euler-Formel]]></title><description><![CDATA[ 
 <br>Für den Sonderfall , also <a data-tooltip-position="top" aria-label="Flächenkurve" data-href="Flächenkurve" href="the-guide/mathematics/differential-geometry/elementary-differential-geometry/flächenkurve.html" class="internal-link" target="_self" rel="noopener nofollow">Flächenkurven</a> auf <a data-href="Hyperflächen" href="the-guide/mathematics/differential-geometry/elementary-differential-geometry/hyperflächen.html" class="internal-link" target="_self" rel="noopener nofollow">Hyperflächen</a> in drei Dimensionen lässt sich zusätzlich die Euler-Formel angeben. Man betrachte hierzu eine Richtung mit bzgl. der <a data-tooltip-position="top" aria-label="1. Fundamentalform" data-href="1. Fundamentalform" href="the-guide/mathematics/differential-geometry/elementary-differential-geometry/1.-fundamentalform.html" class="internal-link" target="_self" rel="noopener nofollow">Riemannschen Norm</a> normierten <a data-tooltip-position="top" aria-label="Hauptkrümmung und Hauptkrümmungsrichtung" data-href="Hauptkrümmung und Hauptkrümmungsrichtung" href="the-guide/mathematics/differential-geometry/elementary-differential-geometry/extrinsische-geometrie-von-hyperflächen/hauptkrümmung-und-hauptkrümmungsrichtung.html" class="internal-link" target="_self" rel="noopener nofollow">HKR</a>  zu den <a data-tooltip-position="top" aria-label="Hauptkrümmung und Hauptkrümmungsrichtung" data-href="Hauptkrümmung und Hauptkrümmungsrichtung" href="the-guide/mathematics/differential-geometry/elementary-differential-geometry/extrinsische-geometrie-von-hyperflächen/hauptkrümmung-und-hauptkrümmungsrichtung.html" class="internal-link" target="_self" rel="noopener nofollow">HK</a> . Die dazugehörigen Vektoren im <a data-href="Tangentialraum" href="the-guide/mathematics/differential-geometry/riemannian-geometry/mannigfaltigkeiten-und-differenzierbare-abbildungen/tangentialraum.html" class="internal-link" target="_self" rel="noopener nofollow">Tangentialraum</a> sind und stellen somit den Einheitskreis in diesem Raum dar. Die <a data-href="Normalkrümmung" href="the-guide/mathematics/differential-geometry/elementary-differential-geometry/extrinsische-geometrie-von-hyperflächen/normalkrümmung.html" class="internal-link" target="_self" rel="noopener nofollow">Normalkrümmung</a> berechnet sich nun mit dem <a data-href="Satz von Meusnier" href="the-guide/mathematics/differential-geometry/elementary-differential-geometry/extrinsische-geometrie-von-hyperflächen/satz-von-meusnier.html" class="internal-link" target="_self" rel="noopener nofollow">Satz von Meusnier</a> via Die letzen Umformungen folgen aus  bzw. .<br>Euler-Formel in 
Für die <a data-href="Normalkrümmung" href="the-guide/mathematics/differential-geometry/elementary-differential-geometry/extrinsische-geometrie-von-hyperflächen/normalkrümmung.html" class="internal-link" target="_self" rel="noopener nofollow">Normalkrümmung</a> einer Hyperfläche  mit den normierten HKR  gilt für eine beliebige Richtung die Euler-Formel mit den HK .
]]></description><link>the-guide/mathematics/differential-geometry/elementary-differential-geometry/extrinsische-geometrie-von-hyperflächen/euler-formel.html</link><guid isPermaLink="false">The Guide/Mathematics/Differential Geometry/Elementary Differential Geometry/Extrinsische Geometrie von Hyperflächen/Euler-Formel.md</guid><pubDate>Mon, 12 Aug 2024 17:05:42 GMT</pubDate></item><item><title><![CDATA[Gauß-Abbildung]]></title><description><![CDATA[ 
 <br>Definition
Für eine <a data-tooltip-position="top" aria-label="Hyperflächen" data-href="Hyperflächen" href="the-guide/mathematics/differential-geometry/elementary-differential-geometry/hyperflächen.html" class="internal-link" target="_self" rel="noopener nofollow">Hyperfläche</a>  heißt die <a data-tooltip-position="top" aria-label="Mathematical Relations" data-href="Mathematical Relations" href="the-guide/mathematics/general-stuff/mathematical-relations.html" class="internal-link" target="_self" rel="noopener nofollow">Abbildung</a> Normalen- oder Gauß-Abbildung von , wenn 

<br> stetig
<br>, also  bzw.  
<br> heißt Normalenvektor. Dieser ist positiv orientiert, wenn . Bei dieser Wahl ist er Element der <a data-tooltip-position="top" aria-label="Flächen" data-href="Flächen" href="the-guide/mathematics/differential-geometry/elementary-differential-geometry/flächen.html" class="internal-link" target="_self" rel="noopener nofollow">orientierten Äquivalenzklasse</a> .

<br><img alt="center" src="lib/media/pasted-image-20231116183923.png" style="width: 500px; max-width: 100%;"><br><br><br>
<br>Die Gauß-Abbildung ist eine glatte <a data-tooltip-position="top" aria-label="Mathematical Relations" data-href="Mathematical Relations" href="the-guide/mathematics/general-stuff/mathematical-relations.html" class="internal-link" target="_self" rel="noopener nofollow">Abbildung</a> von einer <a data-tooltip-position="top" aria-label="Flächen" data-href="Flächen" href="the-guide/mathematics/differential-geometry/elementary-differential-geometry/flächen.html" class="internal-link" target="_self" rel="noopener nofollow">Fläche</a> auf eine <a data-tooltip-position="top" aria-label="Flächen" data-href="Flächen" href="the-guide/mathematics/differential-geometry/elementary-differential-geometry/flächen.html" class="internal-link" target="_self" rel="noopener nofollow">Fläche</a>. 
<br>
<br> ist Basis des 
<br>

<br>Ableiten führt auf , jede partielle Ableitung  ist also Element des <a data-tooltip-position="top" aria-label="Tangentialraum" data-href="Tangentialraum" href="the-guide/mathematics/differential-geometry/riemannian-geometry/mannigfaltigkeiten-und-differenzierbare-abbildungen/tangentialraum.html" class="internal-link" target="_self" rel="noopener nofollow">Tangentialraums</a> von .
<br>Damit exisitert eine Linearkombination (codiert in <a data-tooltip-position="top" aria-label="Matrices" data-href="Matrices" href="the-guide/mathematics/linear-algebra/matrices.html" class="internal-link" target="_self" rel="noopener nofollow">Matrix</a> ) von Spalten der <a data-tooltip-position="top" aria-label="Matrices" data-href="Matrices" href="the-guide/mathematics/linear-algebra/matrices.html" class="internal-link" target="_self" rel="noopener nofollow">Matrix</a> , sodass  (<a data-href="Weingarten-Abbildung" href="the-guide/mathematics/differential-geometry/elementary-differential-geometry/extrinsische-geometrie-von-hyperflächen/weingarten-abbildung.html" class="internal-link" target="_self" rel="noopener nofollow">Weingarten-Abbildung</a>).


<br>Uneindeutigkeit - Für jede parametrisierte <a data-tooltip-position="top" aria-label="Hyperflächen" data-href="Hyperflächen" href="the-guide/mathematics/differential-geometry/elementary-differential-geometry/hyperflächen.html" class="internal-link" target="_self" rel="noopener nofollow">Hyperfläche</a> existieren genau zwei Gauß-Abbildungen (Orientierung)
<br>Für  lässt sich der Normalenvektor definieren als 

<br>In beliebigen Dimensionen erhält man die Formel 


]]></description><link>the-guide/mathematics/differential-geometry/elementary-differential-geometry/extrinsische-geometrie-von-hyperflächen/gauß-abbildung.html</link><guid isPermaLink="false">The Guide/Mathematics/Differential Geometry/Elementary Differential Geometry/Extrinsische Geometrie von Hyperflächen/Gauß-Abbildung.md</guid><pubDate>Mon, 12 Aug 2024 17:05:42 GMT</pubDate><enclosure url="lib/media/pasted-image-20231116183923.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src=&quot;lib/media/pasted-image-20231116183923.png&quot;&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[Hauptkrümmung und Hauptkrümmungsrichtung]]></title><description><![CDATA[ 
 <br>Definition
Untersucht man die Darstellung der <a data-href="Normalkrümmung" href="the-guide/mathematics/differential-geometry/elementary-differential-geometry/extrinsische-geometrie-von-hyperflächen/normalkrümmung.html" class="internal-link" target="_self" rel="noopener nofollow">Normalkrümmung</a> einer <a data-tooltip-position="top" aria-label="Hyperflächen" data-href="Hyperflächen" href="the-guide/mathematics/differential-geometry/elementary-differential-geometry/hyperflächen.html" class="internal-link" target="_self" rel="noopener nofollow">Hyperfläche</a> mittels der <a data-tooltip-position="top" aria-label="1. Fundamentalform" data-href="1. Fundamentalform" href="the-guide/mathematics/differential-geometry/elementary-differential-geometry/1.-fundamentalform.html" class="internal-link" target="_self" rel="noopener nofollow">1. FF</a>  und der <a data-tooltip-position="top" aria-label="2. Fundamentalform" data-href="2. Fundamentalform" href="the-guide/mathematics/differential-geometry/elementary-differential-geometry/2.-fundamentalform.html" class="internal-link" target="_self" rel="noopener nofollow">2. FF</a>   auf Extrema, so führt dies auf das <a data-tooltip-position="top" aria-label="Eigenvalue Problem" data-href="Eigenvalue Problem" href="the-guide/mathematics/linear-algebra/eigenvalue-problem.html" class="internal-link" target="_self" rel="noopener nofollow">Eigenwertproblem</a> mit <a data-href="Weingarten-Abbildung" href="the-guide/mathematics/differential-geometry/elementary-differential-geometry/extrinsische-geometrie-von-hyperflächen/weingarten-abbildung.html" class="internal-link" target="_self" rel="noopener nofollow">Weingarten-Abbildung</a> . Die Eigenwerte  entsprechen dabei den Hauptkrümmungen und die Eigenvektoren  den Hauptkrümmungsrichtungen. Durch die Selbstadjungiertheit der Weingartenabbildung sind diese reell und als g-ONB wählbar. 
<br>Lemma - ONB Basis
Es existiert eine -Orthonormalbasis  aus Eigenvektoren der <a data-href="Weingarten-Abbildung" href="the-guide/mathematics/differential-geometry/elementary-differential-geometry/extrinsische-geometrie-von-hyperflächen/weingarten-abbildung.html" class="internal-link" target="_self" rel="noopener nofollow">Weingarten-Abbildung</a>, d.h. mit dazugehörigen reellen Eigenwerten . Für die Vektoren im Tangentialraum  gilt dann 
<br>Mit der Weingartenabbildung  gilt für die Hauptkrümmungsrichtung  der Zusammenhang die Normale kippt also mit Geschwindigkeit  entlang der Kurve und nicht zur Seite. Bei positivem Vorzeichen der Hauptkrümmung krümmt sich die Fläche zur Normalen hin.<br>
<br>Hauptkrümmungen (HK) sind <a data-tooltip-position="top" aria-label="Flächen" data-href="Flächen" href="the-guide/mathematics/differential-geometry/elementary-differential-geometry/flächen.html" class="internal-link" target="_self" rel="noopener nofollow">Eigenschaft der Klasse</a>  und <a data-tooltip-position="top" aria-label="Geometrische Größen" data-href="Geometrische Größen" href="the-guide/mathematics/differential-geometry/elementary-differential-geometry/lokale-kurventheorie/geometrische-größen.html" class="internal-link" target="_self" rel="noopener nofollow">geometrische Größe</a> von , sie ändern ihr VZ also bei Wahl der entgegengesetzten Normale.
<br>Hauptkrümmungsrichtungen (HKR) sind <a data-tooltip-position="top" aria-label="Flächen" data-href="Flächen" href="the-guide/mathematics/differential-geometry/elementary-differential-geometry/flächen.html" class="internal-link" target="_self" rel="noopener nofollow">Eigenschaft der Klasse</a>  im <a data-href="Tangentialraum" href="the-guide/mathematics/differential-geometry/riemannian-geometry/mannigfaltigkeiten-und-differenzierbare-abbildungen/tangentialraum.html" class="internal-link" target="_self" rel="noopener nofollow">Tangentialraum</a> mit , jedoch nicht im Parameterraum  () ! Sie ändern ihr Vorzeichen im allgemeinen nicht, da im Eigenwertproblem einfach  und  das Vorzeichen wechseln.
<br>Wenn Eigenwerte übereinstimmen, dann sind alle Richtungen Hauptkrümmungsrichtungen.
<br>Es gilt die <a data-href="Euler-Formel" href="the-guide/mathematics/differential-geometry/elementary-differential-geometry/extrinsische-geometrie-von-hyperflächen/euler-formel.html" class="internal-link" target="_self" rel="noopener nofollow">Euler-Formel</a>
<br><br><br>Mit den Hauptkrümmungen und Hauptkrümmungsrichtungen lassen sich zentrale abgeleitete Größen der Differentialgeometrie bestimmen:<br>Mittlere Krümmung H
Die (erste) mittlere Krümmung einer <a data-tooltip-position="top" aria-label="Hyperflächen" data-href="Hyperflächen" href="the-guide/mathematics/differential-geometry/elementary-differential-geometry/hyperflächen.html" class="internal-link" target="_self" rel="noopener nofollow">Hyperfläche</a> ist definiert als 

<br><a data-tooltip-position="top" aria-label="Flächen" data-href="Flächen" href="the-guide/mathematics/differential-geometry/elementary-differential-geometry/flächen.html" class="internal-link" target="_self" rel="noopener nofollow">Eigenschaft der orientierten Fläche</a>  und <a data-tooltip-position="top" aria-label="Geometrische Größen" data-href="Geometrische Größen" href="the-guide/mathematics/differential-geometry/elementary-differential-geometry/lokale-kurventheorie/geometrische-größen.html" class="internal-link" target="_self" rel="noopener nofollow">geometrische Größe</a>**
<br>Das <a data-tooltip-position="top" aria-label="Eigenvalue Problem" data-href="Eigenvalue Problem" href="the-guide/mathematics/linear-algebra/eigenvalue-problem.html" class="internal-link" target="_self" rel="noopener nofollow">charakteristische Polynom</a> ist definiert über 
<br>Ändert VZ bei Wahl der entgegengesetzten Normale

<br>Gauß-Kronecker Krümmung K
Die Gauß-Kronecker-Krümmung einer <a data-tooltip-position="top" aria-label="Hyperflächen" data-href="Hyperflächen" href="the-guide/mathematics/differential-geometry/elementary-differential-geometry/hyperflächen.html" class="internal-link" target="_self" rel="noopener nofollow">Hyperfläche</a> ist definiert als 

<br>Für <a data-href="Hyperflächen" href="the-guide/mathematics/differential-geometry/elementary-differential-geometry/hyperflächen.html" class="internal-link" target="_self" rel="noopener nofollow">Hyperflächen</a> in  ist diese Größe intrinsisch (<a data-href="Theorema Egregium" href="the-guide/mathematics/differential-geometry/elementary-differential-geometry/intrinsische-geometrie-von-hyperflächen/theorema-egregium.html" class="internal-link" target="_self" rel="noopener nofollow">Theorema Egregium</a>) und wird nur als Gauß-Krümmung bezeichnet.
<br><a data-tooltip-position="top" aria-label="Flächen" data-href="Flächen" href="the-guide/mathematics/differential-geometry/elementary-differential-geometry/flächen.html" class="internal-link" target="_self" rel="noopener nofollow">Eigenschaft der Fläche</a>  und <a data-tooltip-position="top" aria-label="Geometrische Größen" data-href="Geometrische Größen" href="the-guide/mathematics/differential-geometry/elementary-differential-geometry/lokale-kurventheorie/geometrische-größen.html" class="internal-link" target="_self" rel="noopener nofollow">geometrische Größe</a>
<br>Änderung der <a data-tooltip-position="top" aria-label="Gauß-Abbildung" data-href="Gauß-Abbildung" href="the-guide/mathematics/differential-geometry/elementary-differential-geometry/extrinsische-geometrie-von-hyperflächen/gauß-abbildung.html" class="internal-link" target="_self" rel="noopener nofollow">Normale</a>  , wenn  ungerade 

<br><br><br>
<br>Flach   ("Ebene")
<br>Umbilisch  ("Kugel")<img alt="center" src="lib/media/pasted-image-20231124135336.png" style="width: 550px; max-width: 100%;">
<br>Elliptisch  ("Kappe") <img alt="center" src="lib/media/pasted-image-20231124135407.png" style="width: 550px; max-width: 100%;">
<br>Hyperbolisch  ("Sattel")<img alt="center" src="lib/media/pasted-image-20231124135432.png" style="width: 550px; max-width: 100%;">
<br>Parabolisch  ("Dachrinne")<img alt="center" src="lib/media/pasted-image-20231124135446.png" style="width: 550px; max-width: 100%;"><br>
Bis aus umbilisch sind alle dies Eigenschaften invariant unter affiner Abbildung.
]]></description><link>the-guide/mathematics/differential-geometry/elementary-differential-geometry/extrinsische-geometrie-von-hyperflächen/hauptkrümmung-und-hauptkrümmungsrichtung.html</link><guid isPermaLink="false">The Guide/Mathematics/Differential Geometry/Elementary Differential Geometry/Extrinsische Geometrie von Hyperflächen/Hauptkrümmung und Hauptkrümmungsrichtung.md</guid><pubDate>Mon, 12 Aug 2024 17:05:42 GMT</pubDate><enclosure url="lib/media/pasted-image-20231124135336.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src=&quot;lib/media/pasted-image-20231124135336.png&quot;&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[Implizite Fläche]]></title><description><![CDATA[ 
 <br>Anstatt eine <a data-tooltip-position="top" aria-label="Flächen" data-href="Flächen" href="the-guide/mathematics/differential-geometry/elementary-differential-geometry/flächen.html" class="internal-link" target="_self" rel="noopener nofollow">Fläche</a> über eine explizite <a data-tooltip-position="top" aria-label="Mathematical Relations" data-href="Mathematical Relations" href="the-guide/mathematics/general-stuff/mathematical-relations.html" class="internal-link" target="_self" rel="noopener nofollow">Abbildung</a> zu definieren, lässt sich dies auch implizit über eine Menge mit bestimmten Eigenschaften erreichen.<br>Definition
Sei  eine glatte <a data-tooltip-position="top" aria-label="Mathematical Relations" data-href="Mathematical Relations" href="the-guide/mathematics/general-stuff/mathematical-relations.html" class="internal-link" target="_self" rel="noopener nofollow">Funktion</a>. Die zugehörige implizite Fläche ist die Menge Diese ist regulär, wenn 
<br>
<br>Auch globale Hyperfläche genannt
<br>Wenn  kompakt ist, so nennt man die Fläche geschlossene <a data-tooltip-position="top" aria-label="Hyperflächen" data-href="Hyperflächen" href="the-guide/mathematics/differential-geometry/elementary-differential-geometry/hyperflächen.html" class="internal-link" target="_self" rel="noopener nofollow">Hyperfläche</a>
<br>Es existiert immer eine geeignete explizite <a data-tooltip-position="top" aria-label="Hyperflächen" data-href="Hyperflächen" href="the-guide/mathematics/differential-geometry/elementary-differential-geometry/hyperflächen.html" class="internal-link" target="_self" rel="noopener nofollow">Hyperfläche</a> zu der gegebenen impliziten Menge:<br>Lemma
Sei  und  regulär wie oben. Setze und ergänze diesen Vektor zu einer ONB in  via d.h. . Es gibt eine parametrisierte Fläche mit einer <a data-tooltip-position="top" aria-label="Mathematical Relations" data-href="Mathematical Relations" href="the-guide/mathematics/general-stuff/mathematical-relations.html" class="internal-link" target="_self" rel="noopener nofollow">Funktion</a>  und , d.h. .
<br>
<br>Beweis - Man betrachte nun . Ableiten nach  an der Stelle  ergibt Nach dem <a data-tooltip-position="top" aria-label="Implicit Function Theorem" data-href="Implicit Function Theorem" href="the-guide/mathematics/analysis-and-calculus/implicit-function-theorem.html" class="internal-link" target="_self" rel="noopener nofollow">Satz über implizite Funktionen</a> muss damit die Funktion   existieren.
<br>Intuition für  als Normale in    wie Potential, maximale Änderung in senkrechter Richtung zu Äquipotentialfläche.

<br>Folgt durch ableiten der selben Darstellung wie im Beweis nach  


<br><br>Man betrachte wieder  und leite zweimal nach  ab. Man erhält nach Umstellen Wegen  sind die <a data-tooltip-position="top" aria-label="Hauptkrümmung und Hauptkrümmungsrichtung" data-href="Hauptkrümmung und Hauptkrümmungsrichtung" href="the-guide/mathematics/differential-geometry/elementary-differential-geometry/extrinsische-geometrie-von-hyperflächen/hauptkrümmung-und-hauptkrümmungsrichtung.html" class="internal-link" target="_self" rel="noopener nofollow">Hauptkrümmungen und Hauptkrümmungsrichtungen</a> der impliziten Fläche im Punkt  die <a data-tooltip-position="top" aria-label="Eigenvalue Problem" data-href="Eigenvalue Problem" href="the-guide/mathematics/linear-algebra/eigenvalue-problem.html" class="internal-link" target="_self" rel="noopener nofollow">Eigenwerte und Eigenvektoren</a> von <br>Satz
Sei  nach obiger Definition regulär und  kompakt. Dann gibt es einen Punkt  mit <a data-tooltip-position="top" aria-label="Hauptkrümmung und Hauptkrümmungsrichtung" data-href="Hauptkrümmung und Hauptkrümmungsrichtung" href="the-guide/mathematics/differential-geometry/elementary-differential-geometry/extrinsische-geometrie-von-hyperflächen/hauptkrümmung-und-hauptkrümmungsrichtung.html" class="internal-link" target="_self" rel="noopener nofollow">Gauß Krümmung</a>
<br><br><br>
<br><img alt="center" src="lib/media/pasted-image-20240221102045.png" style="width: 300px; max-width: 100%;">
]]></description><link>the-guide/mathematics/differential-geometry/elementary-differential-geometry/extrinsische-geometrie-von-hyperflächen/implizite-fläche.html</link><guid isPermaLink="false">The Guide/Mathematics/Differential Geometry/Elementary Differential Geometry/Extrinsische Geometrie von Hyperflächen/Implizite Fläche.md</guid><pubDate>Mon, 12 Aug 2024 17:05:42 GMT</pubDate><enclosure url="lib/media/pasted-image-20240221102045.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src=&quot;lib/media/pasted-image-20240221102045.png&quot;&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[Lokale Normalform]]></title><description><![CDATA[ 
 <br>Zuerst wollen wir eine <a data-tooltip-position="top" aria-label="Hyperflächen" data-href="Hyperflächen" href="the-guide/mathematics/differential-geometry/elementary-differential-geometry/hyperflächen.html" class="internal-link" target="_self" rel="noopener nofollow">Hyperflächen</a> als lokalen Graphen über ihrer <a data-tooltip-position="top" aria-label="Tangentialraum" data-href="Tangentialraum" href="the-guide/mathematics/differential-geometry/riemannian-geometry/mannigfaltigkeiten-und-differenzierbare-abbildungen/tangentialraum.html" class="internal-link" target="_self" rel="noopener nofollow">Tangentialebene</a> darstellen.<br>Lemma 2.10 - Lokaler Graph
Es sei  <a data-tooltip-position="top" aria-label="Hyperflächen" data-href="Hyperflächen" href="the-guide/mathematics/differential-geometry/elementary-differential-geometry/hyperflächen.html" class="internal-link" target="_self" rel="noopener nofollow">Hyperfläche</a> und . Dann gibt es ein Gebiet  mit  und einen Diffeomorphismus  mit , sowie eine <a data-tooltip-position="top" aria-label="Mathematical Relations" data-href="Mathematical Relations" href="the-guide/mathematics/general-stuff/mathematical-relations.html" class="internal-link" target="_self" rel="noopener nofollow">Funktion</a> , sodass  erfüllt Dabei ist  eine beliebige Basis des <a data-tooltip-position="top" aria-label="Tangentialraum" data-href="Tangentialraum" href="the-guide/mathematics/differential-geometry/riemannian-geometry/mannigfaltigkeiten-und-differenzierbare-abbildungen/tangentialraum.html" class="internal-link" target="_self" rel="noopener nofollow">Tangentialraumes</a> im Punkt .
<br>
<br> als Ursprung, laufe  in Tangentialebene weg und erreiche Punkt auf Graphen über <a data-tooltip-position="top" aria-label="Gauß-Abbildung" data-href="Gauß-Abbildung" href="the-guide/mathematics/differential-geometry/elementary-differential-geometry/extrinsische-geometrie-von-hyperflächen/gauß-abbildung.html" class="internal-link" target="_self" rel="noopener nofollow">Normalenrichtung</a>. 
<br><br>Lokale Normalform
Bezüglich einer ONB  des <a data-tooltip-position="top" aria-label="Tangentialraum" data-href="Tangentialraum" href="the-guide/mathematics/differential-geometry/riemannian-geometry/mannigfaltigkeiten-und-differenzierbare-abbildungen/tangentialraum.html" class="internal-link" target="_self" rel="noopener nofollow">Tangentialraumes</a> lässt sich dies umschreiben als Durch Weglassen des kubischen Termes definiert man damit das oskulierende Paraboloid / die lokale Normalform von  in .
<br>
<br>Wenn alle <a data-tooltip-position="top" aria-label="Hauptkrümmung und Hauptkrümmungsrichtung" data-href="Hauptkrümmung und Hauptkrümmungsrichtung" href="the-guide/mathematics/differential-geometry/elementary-differential-geometry/extrinsische-geometrie-von-hyperflächen/hauptkrümmung-und-hauptkrümmungsrichtung.html" class="internal-link" target="_self" rel="noopener nofollow">HK</a> gleiches VZ haben, dann ist der einzige Schnittpunkt der Spur von  in einer hinreichend kleinen Umgebung von  mit der <a data-tooltip-position="top" aria-label="Tangentialraum" data-href="Tangentialraum" href="the-guide/mathematics/differential-geometry/riemannian-geometry/mannigfaltigkeiten-und-differenzierbare-abbildungen/tangentialraum.html" class="internal-link" target="_self" rel="noopener nofollow">Tangentialebene</a> der Punkt  selbst.
<br>Wenn sich die VZ unterscheiden, so schneidet die Spur von  die <a data-tooltip-position="top" aria-label="Tangentialraum" data-href="Tangentialraum" href="the-guide/mathematics/differential-geometry/riemannian-geometry/mannigfaltigkeiten-und-differenzierbare-abbildungen/tangentialraum.html" class="internal-link" target="_self" rel="noopener nofollow">Tangentialebene</a> in jeder noch so kleinen Umgebung von .
]]></description><link>the-guide/mathematics/differential-geometry/elementary-differential-geometry/extrinsische-geometrie-von-hyperflächen/lokale-normalform.html</link><guid isPermaLink="false">The Guide/Mathematics/Differential Geometry/Elementary Differential Geometry/Extrinsische Geometrie von Hyperflächen/Lokale Normalform.md</guid><pubDate>Mon, 12 Aug 2024 17:05:42 GMT</pubDate></item><item><title><![CDATA[Normalkrümmung]]></title><description><![CDATA[ 
 <br> Nun betrachte man eine <a data-tooltip-position="top" aria-label="Flächenkurve" data-href="Flächenkurve" href="the-guide/mathematics/differential-geometry/elementary-differential-geometry/flächenkurve.html" class="internal-link" target="_self" rel="noopener nofollow">Flächenkurve</a>  mit , die eine <a data-href="BL-Kurve" href="the-guide/mathematics/differential-geometry/elementary-differential-geometry/lokale-kurventheorie/bl-kurve.html" class="internal-link" target="_self" rel="noopener nofollow">BL-Kurve</a> ist, also  überall. Die zweite Ableitung der Kurve lässt sich darstellen als Unter Verwendung der <a data-href="Gauß-Abbildung" href="the-guide/mathematics/differential-geometry/elementary-differential-geometry/extrinsische-geometrie-von-hyperflächen/gauß-abbildung.html" class="internal-link" target="_self" rel="noopener nofollow">Gauß-Abbildung</a>  der <a data-tooltip-position="top" aria-label="Hyperflächen" data-href="Hyperflächen" href="the-guide/mathematics/differential-geometry/elementary-differential-geometry/hyperflächen.html" class="internal-link" target="_self" rel="noopener nofollow">Hyperfläche</a>  lassen sich folgende Projektoren definieren:<br>
<br> beschreibt den Anteil der <a data-tooltip-position="top" aria-label="Krümmung einer Kurve" data-href="Krümmung einer Kurve" href="the-guide/mathematics/differential-geometry/elementary-differential-geometry/lokale-kurventheorie/krümmung-einer-kurve.html" class="internal-link" target="_self" rel="noopener nofollow">Krümmung einer Kurve</a> / des Änderungsvektors, der die Fläche aus der Tangentialebene "hinaushebt". 

<br> ist senkrechte Projektion auf Normalengerade für beliebigen Punkt  via 


<br> beschreibt den Anteil der <a data-href="Krümmung einer Kurve" href="the-guide/mathematics/differential-geometry/elementary-differential-geometry/lokale-kurventheorie/krümmung-einer-kurve.html" class="internal-link" target="_self" rel="noopener nofollow">Krümmung einer Kurve</a> / des Änderungsvektors, der die Kurve in der <a data-tooltip-position="top" aria-label="Ebene Kurven" data-href="Ebene Kurven" href="the-guide/mathematics/differential-geometry/elementary-differential-geometry/lokale-kurventheorie/ebene-kurven.html" class="internal-link" target="_self" rel="noopener nofollow">Tangentialebene</a> krümmt.<br>
-  ist senkrechte Projektion in den <a data-href="Tangentialraum" href="the-guide/mathematics/differential-geometry/riemannian-geometry/mannigfaltigkeiten-und-differenzierbare-abbildungen/tangentialraum.html" class="internal-link" target="_self" rel="noopener nofollow">Tangentialraum</a> für beliebigen Punkt  via <img alt="center" src="lib/media/pasted-image-20240219113004.png" style="width: 400px; max-width: 100%;">
<br><br><br>Normalenkrümmung von Flächenkurven
Der Anteil der Krümmung einer <a data-href="Flächenkurve" href="the-guide/mathematics/differential-geometry/elementary-differential-geometry/flächenkurve.html" class="internal-link" target="_self" rel="noopener nofollow">Flächenkurve</a>  in Normalenrichtung lässt sich schreiben als wobei der Faktor  als Normalenkrümmung der <a data-tooltip-position="top" aria-label="Flächenkurve" data-href="Flächenkurve" href="the-guide/mathematics/differential-geometry/elementary-differential-geometry/flächenkurve.html" class="internal-link" target="_self" rel="noopener nofollow">Flächenkurve</a>  bezeichnet wird. Im Falle einer <a data-tooltip-position="top" aria-label="BL-Kurve" data-href="BL-Kurve" href="the-guide/mathematics/differential-geometry/elementary-differential-geometry/lokale-kurventheorie/bl-kurve.html" class="internal-link" target="_self" rel="noopener nofollow">BL-Parametrisierung</a> giltwobei die zweite Formel als <a data-href="Satz von Meusnier" href="the-guide/mathematics/differential-geometry/elementary-differential-geometry/extrinsische-geometrie-von-hyperflächen/satz-von-meusnier.html" class="internal-link" target="_self" rel="noopener nofollow">Satz von Meusnier</a> bekannt ist. Für allgemeine Flächenkurven folgt durch vorherige Umparametrisierungmit der <a data-tooltip-position="top" aria-label="1. Fundamentalform" data-href="1. Fundamentalform" href="the-guide/mathematics/differential-geometry/elementary-differential-geometry/1.-fundamentalform.html" class="internal-link" target="_self" rel="noopener nofollow">1 FF</a>  und der <a data-tooltip-position="top" aria-label="2. Fundamentalform" data-href="2. Fundamentalform" href="the-guide/mathematics/differential-geometry/elementary-differential-geometry/2.-fundamentalform.html" class="internal-link" target="_self" rel="noopener nofollow">2 FF</a> .
<br><br><br>
<br>Mit  gilt zudem 

<br>, da 
<br>


<br>Falls die Normalenkrümmung und damit die <a data-tooltip-position="top" aria-label="2. Fundamentalform" data-href="2. Fundamentalform" href="the-guide/mathematics/differential-geometry/elementary-differential-geometry/2.-fundamentalform.html" class="internal-link" target="_self" rel="noopener nofollow">2 FF</a> verschwinden, so heißt das dazugehörige  <a data-tooltip-position="top" aria-label="Flächenkurve" data-href="Flächenkurve" href="the-guide/mathematics/differential-geometry/elementary-differential-geometry/flächenkurve.html" class="internal-link" target="_self" rel="noopener nofollow">Asymptotenrichtung</a>
<br><br><br><img alt="center" src="lib/media/pasted-image-20231116200113.png" style="width: 500px; max-width: 100%;"><br>
Sei  die Schnittkurve der Fläche mit der Ebene, die durch Normalenvektor  und einen Tangentialvektor  aufgespannt wird. Dann gilt ]]></description><link>the-guide/mathematics/differential-geometry/elementary-differential-geometry/extrinsische-geometrie-von-hyperflächen/normalkrümmung.html</link><guid isPermaLink="false">The Guide/Mathematics/Differential Geometry/Elementary Differential Geometry/Extrinsische Geometrie von Hyperflächen/Normalkrümmung.md</guid><pubDate>Mon, 12 Aug 2024 17:05:42 GMT</pubDate><enclosure url="lib/media/pasted-image-20240219113004.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src=&quot;lib/media/pasted-image-20240219113004.png&quot;&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[Oskulierendes Paraboloid]]></title><description><![CDATA[ 
 <br>Mehrdimensionales pendant zur <a data-href="Ebene Kurven" href="the-guide/mathematics/differential-geometry/elementary-differential-geometry/lokale-kurventheorie/ebene-kurven.html" class="internal-link" target="_self" rel="noopener nofollow">Ebene Kurven</a> für <a data-href="Hyperflächen" href="the-guide/mathematics/differential-geometry/elementary-differential-geometry/hyperflächen.html" class="internal-link" target="_self" rel="noopener nofollow">Hyperflächen</a>. Basierend auf dem Lemma <br>Lemma
In einem Punkt  der Hyperfläche  seien <a data-tooltip-position="top" aria-label="Hauptkrümmung und Hauptkrümmungsrichtung" data-href="Hauptkrümmung und Hauptkrümmungsrichtung" href="the-guide/mathematics/differential-geometry/elementary-differential-geometry/extrinsische-geometrie-von-hyperflächen/hauptkrümmung-und-hauptkrümmungsrichtung.html" class="internal-link" target="_self" rel="noopener nofollow">Hauptkrümmungen</a>  sowie <a data-tooltip-position="top" aria-label="Hauptkrümmung und Hauptkrümmungsrichtung" data-href="Hauptkrümmung und Hauptkrümmungsrichtung" href="the-guide/mathematics/differential-geometry/elementary-differential-geometry/extrinsische-geometrie-von-hyperflächen/hauptkrümmung-und-hauptkrümmungsrichtung.html" class="internal-link" target="_self" rel="noopener nofollow">Hauptkrümmungsrichtungen</a>  gegeben. Dann gibt es eine <a data-tooltip-position="top" aria-label="Flächen" data-href="Flächen" href="the-guide/mathematics/differential-geometry/elementary-differential-geometry/flächen.html" class="internal-link" target="_self" rel="noopener nofollow">Umparametrisierung</a> wobei  eine Kugel mit Radius  um den Ursprung von  ist. Dazu gibt es eine Function , sodass mit  (HKR sind g-ONB von ) gilt für .
<br>Nun gilt es ein geeigetes  zu finden. Dazu wird zunächst eine Umparametrisierung vorgenommen, um eine reine -Abhängigkeit zu erreichen. Die resultierende Parametrisierung ist . Die resultierende Fläche wird nun so verschoben, dass ,  und  (W ist g-ONB, Verschiebung ändert Eigenschaften der Fläche nicht). Die entstehende Fläche hat die Form womit  und . Da die HK unverändert sind und wir die HKR ausgerichtet haben, gilt in  für die <a data-href="Weingarten-Abbildung" href="the-guide/mathematics/differential-geometry/elementary-differential-geometry/extrinsische-geometrie-von-hyperflächen/weingarten-abbildung.html" class="internal-link" target="_self" rel="noopener nofollow">Weingarten-Abbildung</a>  Damit ist die Taylor-Reige von  im urpsrung gegeben via <br>Definition
Das oskulierende Paraboloid der parametrisierten <a data-tooltip-position="top" aria-label="Hyperflächen" data-href="Hyperflächen" href="the-guide/mathematics/differential-geometry/elementary-differential-geometry/hyperflächen.html" class="internal-link" target="_self" rel="noopener nofollow">Hyperfläche</a>  im Punkt  ist gegeben durch 
<br>
<br>Haben alle <a data-tooltip-position="top" aria-label="Hauptkrümmung und Hauptkrümmungsrichtung" data-href="Hauptkrümmung und Hauptkrümmungsrichtung" href="the-guide/mathematics/differential-geometry/elementary-differential-geometry/extrinsische-geometrie-von-hyperflächen/hauptkrümmung-und-hauptkrümmungsrichtung.html" class="internal-link" target="_self" rel="noopener nofollow">Hauptkrümmungen</a> das gleiche Vorzeichen, so ist der Punkt  der einzige Schnittpunkt in der kleinen Umgebung, auf der die Näherung aufgebaut ist.
<br>Sind die Vorzeichen unterschiedlich, so schneidet die <a data-tooltip-position="top" aria-label="Flächen" data-href="Flächen" href="the-guide/mathematics/differential-geometry/elementary-differential-geometry/flächen.html" class="internal-link" target="_self" rel="noopener nofollow">Spur</a> von  die <a data-tooltip-position="top" aria-label="Tangentialraum" data-href="Tangentialraum" href="the-guide/mathematics/differential-geometry/riemannian-geometry/mannigfaltigkeiten-und-differenzierbare-abbildungen/tangentialraum.html" class="internal-link" target="_self" rel="noopener nofollow">Tangentialebene</a> in jeder noch so kleinen Umgebung des Punktes.
]]></description><link>the-guide/mathematics/differential-geometry/elementary-differential-geometry/extrinsische-geometrie-von-hyperflächen/oskulierendes-paraboloid.html</link><guid isPermaLink="false">The Guide/Mathematics/Differential Geometry/Elementary Differential Geometry/Extrinsische Geometrie von Hyperflächen/Oskulierendes Paraboloid.md</guid><pubDate>Mon, 12 Aug 2024 17:05:42 GMT</pubDate></item><item><title><![CDATA[Satz von Meusnier]]></title><description><![CDATA[ 
 <br>Satz
Für eine <a data-tooltip-position="top" aria-label="BL-Kurve" data-href="BL-Kurve" href="the-guide/mathematics/differential-geometry/elementary-differential-geometry/lokale-kurventheorie/bl-kurve.html" class="internal-link" target="_self" rel="noopener nofollow">nach Bogenlänge</a> parametrisierte <a data-href="Flächenkurve" href="the-guide/mathematics/differential-geometry/elementary-differential-geometry/flächenkurve.html" class="internal-link" target="_self" rel="noopener nofollow">Flächenkurve</a> () in der <a data-tooltip-position="top" aria-label="Hyperflächen" data-href="Hyperflächen" href="the-guide/mathematics/differential-geometry/elementary-differential-geometry/hyperflächen.html" class="internal-link" target="_self" rel="noopener nofollow">Hyperfläche</a>  mit <a data-href="Gauß-Abbildung" href="the-guide/mathematics/differential-geometry/elementary-differential-geometry/extrinsische-geometrie-von-hyperflächen/gauß-abbildung.html" class="internal-link" target="_self" rel="noopener nofollow">Gauß-Abbildung</a>  ist die <a data-tooltip-position="top" aria-label="Normalkrümmung" data-href="Normalkrümmung" href="the-guide/mathematics/differential-geometry/elementary-differential-geometry/extrinsische-geometrie-von-hyperflächen/normalkrümmung.html" class="internal-link" target="_self" rel="noopener nofollow">Normalenkrümmung</a> durch die Tangentenrichtung , die <a data-tooltip-position="top" aria-label="1. Fundamentalform" data-href="1. Fundamentalform" href="the-guide/mathematics/differential-geometry/elementary-differential-geometry/1.-fundamentalform.html" class="internal-link" target="_self" rel="noopener nofollow">1. FF</a>  und die <a data-href="Weingarten-Abbildung" href="the-guide/mathematics/differential-geometry/elementary-differential-geometry/extrinsische-geometrie-von-hyperflächen/weingarten-abbildung.html" class="internal-link" target="_self" rel="noopener nofollow">Weingarten-Abbildung</a>  der <a data-tooltip-position="top" aria-label="Hyperflächen" data-href="Hyperflächen" href="the-guide/mathematics/differential-geometry/elementary-differential-geometry/hyperflächen.html" class="internal-link" target="_self" rel="noopener nofollow">Fläche</a> bestimmt 
]]></description><link>the-guide/mathematics/differential-geometry/elementary-differential-geometry/extrinsische-geometrie-von-hyperflächen/satz-von-meusnier.html</link><guid isPermaLink="false">The Guide/Mathematics/Differential Geometry/Elementary Differential Geometry/Extrinsische Geometrie von Hyperflächen/Satz von Meusnier.md</guid><pubDate>Mon, 12 Aug 2024 17:05:42 GMT</pubDate></item><item><title><![CDATA[Weingarten-Abbildung]]></title><description><![CDATA[ 
 <br>Wenn man entlang einer <a data-href="Flächenkurve" href="the-guide/mathematics/differential-geometry/elementary-differential-geometry/flächenkurve.html" class="internal-link" target="_self" rel="noopener nofollow">Flächenkurve</a>  in  läuft, wie ändert sich dabei die <a data-tooltip-position="top" aria-label="Gauß-Abbildung" data-href="Gauß-Abbildung" href="the-guide/mathematics/differential-geometry/elementary-differential-geometry/extrinsische-geometrie-von-hyperflächen/gauß-abbildung.html" class="internal-link" target="_self" rel="noopener nofollow">Flächennormale</a>  ?<br><br>Lemma 2.4
Sei  eine parametrisierte <a data-tooltip-position="top" aria-label="Hyperflächen" data-href="Hyperflächen" href="the-guide/mathematics/differential-geometry/elementary-differential-geometry/hyperflächen.html" class="internal-link" target="_self" rel="noopener nofollow">Hyperfläche</a>. Dann gilt für alle  und ,  ist also <a data-tooltip-position="top" aria-label="Mathematical Relations" data-href="Mathematical Relations" href="the-guide/mathematics/general-stuff/mathematical-relations.html" class="internal-link" target="_self" rel="noopener nofollow">lineare Abbildung</a>. 
<br>
<br>Die Änderung des Normalenvektors von <a data-tooltip-position="top" aria-label="Hyperflächen" data-href="Hyperflächen" href="the-guide/mathematics/differential-geometry/elementary-differential-geometry/hyperflächen.html" class="internal-link" target="_self" rel="noopener nofollow">Hyperflächen</a> findet immer in Tangentialrichtung statt.<br>
<img alt="center" src="lib/media/pasted-image-20240306180043.png" style="width: 450px; max-width: 100%;">
<br><br><br>Weingarten-Abbildung
Besser bekannt als shape operator, bezeichnet eine eindeutige lineare Abbildung einer <a data-tooltip-position="top" aria-label="Hyperflächen" data-href="Hyperflächen" href="the-guide/mathematics/differential-geometry/elementary-differential-geometry/hyperflächen.html" class="internal-link" target="_self" rel="noopener nofollow">Hyperfläche</a>  bezüglich des <a data-tooltip-position="top" aria-label="Gauß-Abbildung" data-href="Gauß-Abbildung" href="the-guide/mathematics/differential-geometry/elementary-differential-geometry/extrinsische-geometrie-von-hyperflächen/gauß-abbildung.html" class="internal-link" target="_self" rel="noopener nofollow">Normalenvektors</a> . Diese lautet Mit den Einträgen  entspricht dies in <a data-tooltip-position="top" aria-label="Matrices" data-href="Matrices" href="the-guide/mathematics/linear-algebra/matrices.html" class="internal-link" target="_self" rel="noopener nofollow">Matrix</a>-Schreibweise mit  und beschreibt die Änderung der <a data-href="Gauß-Abbildung" href="the-guide/mathematics/differential-geometry/elementary-differential-geometry/extrinsische-geometrie-von-hyperflächen/gauß-abbildung.html" class="internal-link" target="_self" rel="noopener nofollow">Gauß-Abbildung</a>  der <a data-tooltip-position="top" aria-label="Hyperflächen" data-href="Hyperflächen" href="the-guide/mathematics/differential-geometry/elementary-differential-geometry/hyperflächen.html" class="internal-link" target="_self" rel="noopener nofollow">Hyperfläche</a>  im Punkt . Da es sich um <a data-href="Hyperflächen" href="the-guide/mathematics/differential-geometry/elementary-differential-geometry/hyperflächen.html" class="internal-link" target="_self" rel="noopener nofollow">Hyperflächen</a> handelt ist diese Änderung mit der durch den <a data-href="Tangentialraum" href="the-guide/mathematics/differential-geometry/riemannian-geometry/mannigfaltigkeiten-und-differenzierbare-abbildungen/tangentialraum.html" class="internal-link" target="_self" rel="noopener nofollow">Tangentialraum</a> gegebenen Basis darstellbar.
<br><br><br>
<br>Ändert Vorzeichen mit , ist für  bestimmt.
<br>Matrix  gibt Zusammenhang zwischen <a data-tooltip-position="top" aria-label="1. Fundamentalform" data-href="1. Fundamentalform" href="the-guide/mathematics/differential-geometry/elementary-differential-geometry/1.-fundamentalform.html" class="internal-link" target="_self" rel="noopener nofollow">1. FF</a> und <a data-tooltip-position="top" aria-label="2. Fundamentalform" data-href="2. Fundamentalform" href="the-guide/mathematics/differential-geometry/elementary-differential-geometry/2.-fundamentalform.html" class="internal-link" target="_self" rel="noopener nofollow">2. FF</a>ist aber insbesondere nicht symmetrisch.
<br>Weingarten-Abbildung nach Umparametrisierung  ist <a data-tooltip-position="top" aria-label="Matrix Similarity" data-href="Matrix Similarity" href="the-guide/mathematics/linear-algebra/matrix-similarity.html" class="internal-link" target="_self" rel="noopener nofollow">ähnlich</a> zu  gemäß 
<br>Bewegungsinvarianz,  und  haben die gleiche Weingarten-Abbildung
<br>Lemma
Die Weingarten-Abbildung ist selbstadjungiert bezüglich des Skalarproduktes, welches durch die <a data-href="1. Fundamentalform" href="the-guide/mathematics/differential-geometry/elementary-differential-geometry/1.-fundamentalform.html" class="internal-link" target="_self" rel="noopener nofollow">1. Fundamentalform</a> impliziert wird Damit exisitert eine g-Orthonormalbasis  mit  aus <a data-tooltip-position="top" aria-label="Eigenvalue Problem" data-href="Eigenvalue Problem" href="the-guide/mathematics/linear-algebra/eigenvalue-problem.html" class="internal-link" target="_self" rel="noopener nofollow">Eigenvektoren</a> der Wingartenabbildung  und dazugehörige Eigenwerte sind reel.
]]></description><link>the-guide/mathematics/differential-geometry/elementary-differential-geometry/extrinsische-geometrie-von-hyperflächen/weingarten-abbildung.html</link><guid isPermaLink="false">The Guide/Mathematics/Differential Geometry/Elementary Differential Geometry/Extrinsische Geometrie von Hyperflächen/Weingarten-Abbildung.md</guid><pubDate>Mon, 12 Aug 2024 17:05:42 GMT</pubDate><enclosure url="lib/media/pasted-image-20240306180043.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src=&quot;lib/media/pasted-image-20240306180043.png&quot;&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[Euler-Charakteristik]]></title><description><![CDATA[ 
 <br><br>Satz für einfach zusammenhängende Gebiete
Für die <a data-href="Triangulierung" href="the-guide/mathematics/differential-geometry/elementary-differential-geometry/globale-kurven-und-flächentheorie/triangulierung.html" class="internal-link" target="_self" rel="noopener nofollow">Triangulierung</a> eines ebenen einfach zusammenhängenden Gebietes gilt für die Eckenzahl , die Flächenzahl  und die Kantenzahl  der Zusammenhang . Die Zahl heißt Euler-Charakteristik der Triangulierung.
<br>
<br>Euler-Charakteristik einer <a data-href="Triangulierung" href="the-guide/mathematics/differential-geometry/elementary-differential-geometry/globale-kurven-und-flächentheorie/triangulierung.html" class="internal-link" target="_self" rel="noopener nofollow">Triangulierung</a> eines ebenen Gebietes ist stets Anzahl der Löcher
<br>Beweisskizze

<br>Induktionsanfang mit Dreieck 
<br>Induktionsschluss - Setze ich ein neues Dreieck ein neues hinzu, so ergeben sich zwei Fälle

<br>Eine neue Kante, keine neue Ecke, eine neue Fläche
<br>Zwei neue Kanten, ein neuer Ecke, eine neue Fläche
<br>Drei neue Kanten, zwei neue Ecken, eine neue Fläche 




<br><br><br>Euler'scher Polyedersatz
Die Euler-Charakteristik einer <a data-tooltip-position="top" aria-label="Geschlossene Flächen" data-href="Geschlossene Flächen" href="the-guide/mathematics/differential-geometry/elementary-differential-geometry/globale-kurven-und-flächentheorie/geschlossene-flächen.html" class="internal-link" target="_self" rel="noopener nofollow">geschlossenen Fläche</a> aus Polyedern  in  mit  Löchern (Geschlecht der <a data-tooltip-position="top" aria-label="Flächen" data-href="Flächen" href="the-guide/mathematics/differential-geometry/elementary-differential-geometry/flächen.html" class="internal-link" target="_self" rel="noopener nofollow">Fläche</a>) ist stets 
<br>
<br>Gilt allgemeiner für -Ecke

<br><a data-tooltip-position="top" aria-label="Triangulierung" data-href="Triangulierung" href="the-guide/mathematics/differential-geometry/elementary-differential-geometry/globale-kurven-und-flächentheorie/triangulierung.html" class="internal-link" target="_self" rel="noopener nofollow">Triangulierung</a> 
<br>Tetraeder 
<br>Würfel 
<br>...


<br>Sei  Triangulierung von  und  homöomorph zu einer Kugel. Dann beträgt die Euler-Charakteristik stets .
]]></description><link>the-guide/mathematics/differential-geometry/elementary-differential-geometry/globale-kurven-und-flächentheorie/euler-charakteristik.html</link><guid isPermaLink="false">The Guide/Mathematics/Differential Geometry/Elementary Differential Geometry/Globale Kurven- und Flächentheorie/Euler-Charakteristik.md</guid><pubDate>Mon, 12 Aug 2024 17:05:42 GMT</pubDate></item><item><title><![CDATA[Geschlossene Flächen]]></title><description><![CDATA[ 
 <br>Sei  glatt und regulär, also für alle , für die .<br>Definition
Die Menge ist die durch  definierte implizite Fläche. Falls  beschränkt, dann ist  kompakt und geschlossen und hat keinen Rand.
<br>
<br>Beispieöl 
]]></description><link>the-guide/mathematics/differential-geometry/elementary-differential-geometry/globale-kurven-und-flächentheorie/geschlossene-flächen.html</link><guid isPermaLink="false">The Guide/Mathematics/Differential Geometry/Elementary Differential Geometry/Globale Kurven- und Flächentheorie/Geschlossene Flächen.md</guid><pubDate>Mon, 12 Aug 2024 17:05:42 GMT</pubDate></item><item><title><![CDATA[Geschlossene Kurve]]></title><description><![CDATA[ 
 <br>Definition
Eine <a data-tooltip-position="top" aria-label="Raumkurven" data-href="Raumkurven" href="the-guide/mathematics/differential-geometry/elementary-differential-geometry/raumkurven.html" class="internal-link" target="_self" rel="noopener nofollow">parametrisierte Kurve</a>  heißt geschlossen, wenn es eine glatte -periodische <a data-tooltip-position="top" aria-label="Mathematical Relations" data-href="Mathematical Relations" href="the-guide/mathematics/general-stuff/mathematical-relations.html" class="internal-link" target="_self" rel="noopener nofollow">Funktion</a>/Kurve  gibt, mit 
<br>Einfach geschlossene Kurve
Die geschlossene Kurve heißt einfach , wenn  eingeschränkt auf  <a data-tooltip-position="top" aria-label="Mathematical Relations" data-href="Mathematical Relations" href="the-guide/mathematics/general-stuff/mathematical-relations.html" class="internal-link" target="_self" rel="noopener nofollow">injektiv</a> ist.
<br>
<br>Intervall halb offen, da Anfangs- und Endpunkt per Definition bereits gleich.
<br><br><br>
<br> mit   ist geschlossen und einfach.
<br> mit   ist geschlossen aber nicht.

<br>Doppelter Umlauf zerstört Injektivität


<br> mit   ist weder geschlossen noch einfach.

<br>Anfangs- und Endpunkt stimmen nicht überein


<br>Die halbe Lemniskate ist nicht glatt geschlossen<img alt="center" src="lib/media/pasted-image-20240119145934.png" style="width: 200px; max-width: 100%;">
<br>Die vollständige Lemniskate ist geschlossen, aber nicht einfach (Knoten)<img alt="center" src="lib/media/pasted-image-20240119145957.png" style="width: 200px; max-width: 100%;">
]]></description><link>the-guide/mathematics/differential-geometry/elementary-differential-geometry/globale-kurven-und-flächentheorie/geschlossene-kurve.html</link><guid isPermaLink="false">The Guide/Mathematics/Differential Geometry/Elementary Differential Geometry/Globale Kurven- und Flächentheorie/Geschlossene Kurve.md</guid><pubDate>Mon, 12 Aug 2024 17:05:42 GMT</pubDate><enclosure url="lib/media/pasted-image-20240119145934.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src=&quot;lib/media/pasted-image-20240119145934.png&quot;&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[Holonomie]]></title><description><![CDATA[ 
 <br>Sei  mit Spur  ein einfach zusammenhängendes Flächenstück. Dieses ist berandet durch eine positiv orientierte <a data-href="BL-Kurve" href="the-guide/mathematics/differential-geometry/elementary-differential-geometry/lokale-kurventheorie/bl-kurve.html" class="internal-link" target="_self" rel="noopener nofollow">BL-Kurve</a> , also .<br>
Zudem sei  ein <a data-tooltip-position="top" aria-label="Tangentialraum" data-href="Tangentialraum" href="the-guide/mathematics/differential-geometry/riemannian-geometry/mannigfaltigkeiten-und-differenzierbare-abbildungen/tangentialraum.html" class="internal-link" target="_self" rel="noopener nofollow">tangentiales</a> Einheitsvektorfeld ( und  ein <a data-tooltip-position="top" aria-label="Parallele Vektorfelder und Parallelverschiebung" data-href="Parallele Vektorfelder und Parallelverschiebung" href="the-guide/mathematics/differential-geometry/riemannian-geometry/metriken-und-zusammenhänge/parallele-vektorfelder-und-parallelverschiebung.html" class="internal-link" target="_self" rel="noopener nofollow">paralleles Vektorfeld</a> zu .<br>
<img alt="Pasted image 20240308154630.png" src="lib/media/pasted-image-20240308154630.png"><br>
<br> sieht im Tangentialraum immer gleich aus,  ändert sich
<br>Nach dem <a data-href="Lifting Lemma" href="the-guide/mathematics/differential-geometry/elementary-differential-geometry/lifting-lemma.html" class="internal-link" target="_self" rel="noopener nofollow">Lifting Lemma</a> bzw. dessen <a data-tooltip-position="top" aria-label="Orientierte geodätische Krümmung" data-href="Orientierte geodätische Krümmung" href="the-guide/mathematics/differential-geometry/elementary-differential-geometry/globale-kurven-und-flächentheorie/orientierte-geodätische-krümmung.html" class="internal-link" target="_self" rel="noopener nofollow">Erweiterung</a> exisitiert der Winkel <br>Question
Wie groß ist 
<br>Es gilt der Zusammenhang <img alt="center" src="lib/media/pasted-image-20240304190023.png" style="width: 150px; max-width: 100%;"><br><img alt="center" src="lib/media/pasted-image-20240304185334.png" style="width: 300px; max-width: 100%;">Insgesamt lässt sich damit der gefragte Term umschreiben zu Der erste Teil ergibt immer , da der <a data-tooltip-position="top" aria-label="Index einer geschlossenen, ebenen Kurve" data-href="Index einer geschlossenen, ebenen Kurve" href="the-guide/mathematics/differential-geometry/elementary-differential-geometry/globale-kurven-und-flächentheorie/index-einer-geschlossenen,-ebenen-kurve.html" class="internal-link" target="_self" rel="noopener nofollow">Index</a> nach dem <a data-href="Satz von Hopf" href="the-guide/mathematics/differential-geometry/elementary-differential-geometry/globale-kurven-und-flächentheorie/satz-von-hopf.html" class="internal-link" target="_self" rel="noopener nofollow">Satz von Hopf</a>  sein muss. Für den zweiten Teil folgt mit der interpretation der <a data-tooltip-position="top" aria-label="Krümmung einer Kurve" data-href="Krümmung einer Kurve" href="the-guide/mathematics/differential-geometry/elementary-differential-geometry/lokale-kurventheorie/krümmung-einer-kurve.html" class="internal-link" target="_self" rel="noopener nofollow">Krümmung</a> einer <a data-tooltip-position="top" aria-label="Ebene Kurven" data-href="Ebene Kurven" href="the-guide/mathematics/differential-geometry/elementary-differential-geometry/lokale-kurventheorie/ebene-kurven.html" class="internal-link" target="_self" rel="noopener nofollow">ebenen Kurve</a> als Änderung des <a data-tooltip-position="top" aria-label="Ebene Kurven" data-href="Ebene Kurven" href="the-guide/mathematics/differential-geometry/elementary-differential-geometry/lokale-kurventheorie/ebene-kurven.html" class="internal-link" target="_self" rel="noopener nofollow">Tangentenwinkels</a> Damit folgtDie zweite Umformung folgt direkt aus dem lokalen <a data-href="Satz von Gauß-Bonnet" href="the-guide/mathematics/differential-geometry/elementary-differential-geometry/satz-von-gauß-bonnet.html" class="internal-link" target="_self" rel="noopener nofollow">Satz von Gauß-Bonnet</a>.<br>Satz
Ist  <a data-tooltip-position="top" aria-label="Geschlossene Kurve" data-href="Geschlossene Kurve" href="the-guide/mathematics/differential-geometry/elementary-differential-geometry/globale-kurven-und-flächentheorie/geschlossene-kurve.html" class="internal-link" target="_self" rel="noopener nofollow">einfach geschlossene</a> <a data-href="Flächenkurve" href="the-guide/mathematics/differential-geometry/elementary-differential-geometry/flächenkurve.html" class="internal-link" target="_self" rel="noopener nofollow">Flächenkurve</a>, welche das einfach zusammehängende Gebiet  berandet, so gilt mit <a data-tooltip-position="top" aria-label="Hauptkrümmung und Hauptkrümmungsrichtung" data-href="Hauptkrümmung und Hauptkrümmungsrichtung" href="the-guide/mathematics/differential-geometry/elementary-differential-geometry/extrinsische-geometrie-von-hyperflächen/hauptkrümmung-und-hauptkrümmungsrichtung.html" class="internal-link" target="_self" rel="noopener nofollow">Gauß-Krümmung</a> . Die Holonomie ist damit unabhängig von der Wahl des parallelen Vektorfeldes  und für einfach zusammenhängende Gebiete auch unabhängig von .
<br>
<br>Erweiterung des <a data-tooltip-position="top" aria-label="Index einer geschlossenen, ebenen Kurve" data-href="Index einer geschlossenen, ebenen Kurve" href="the-guide/mathematics/differential-geometry/elementary-differential-geometry/globale-kurven-und-flächentheorie/index-einer-geschlossenen,-ebenen-kurve.html" class="internal-link" target="_self" rel="noopener nofollow">Indexes</a> auf Kurven in gekrümmten Flächen. Da sich der Tangentialraum ständig ändert verwenden wir als Referenz ein beliebiges <a data-tooltip-position="top" aria-label="Parallele Vektorfelder und Parallelverschiebung" data-href="Parallele Vektorfelder und Parallelverschiebung" href="the-guide/mathematics/differential-geometry/riemannian-geometry/metriken-und-zusammenhänge/parallele-vektorfelder-und-parallelverschiebung.html" class="internal-link" target="_self" rel="noopener nofollow">paralleles Vektorfeld</a>. Der neue Zusammenhang folgt dann unter den genannten Voraussetzungen aus dem <a data-href="Satz von Gauß-Bonnet" href="the-guide/mathematics/differential-geometry/elementary-differential-geometry/satz-von-gauß-bonnet.html" class="internal-link" target="_self" rel="noopener nofollow">Satz von Gauß-Bonnet</a>.
<br>Man nimmt urpsrünglich eine orthogonal Parametrisierung an, findet aber später heraus, dass diese nicht notwendig ist.
<br><br><img alt="Pasted image 20240309151111.png" src="lib/media/pasted-image-20240309151111.png"><br>
Für das obige Beispiel ist sowohl das dunkelgrüne  als auch das hellgrüne zulässig. Die Holonomie beträgt jedoch  oder , es ist also von zentraler Bedeutung.]]></description><link>the-guide/mathematics/differential-geometry/elementary-differential-geometry/globale-kurven-und-flächentheorie/holonomie.html</link><guid isPermaLink="false">The Guide/Mathematics/Differential Geometry/Elementary Differential Geometry/Globale Kurven- und Flächentheorie/Holonomie.md</guid><pubDate>Mon, 12 Aug 2024 17:05:42 GMT</pubDate><enclosure url="lib/media/pasted-image-20240308154630.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src=&quot;lib/media/pasted-image-20240308154630.png&quot;&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[Homotope Kurven]]></title><description><![CDATA[ 
 <br>Gegeben seien  und zwei <a data-tooltip-position="top" aria-label="Raumkurven" data-href="Raumkurven" href="the-guide/mathematics/differential-geometry/elementary-differential-geometry/raumkurven.html" class="internal-link" target="_self" rel="noopener nofollow">Kurven</a>  <br>Homotope Kurven
Die <a data-tooltip-position="top" aria-label="Raumkurven" data-href="Raumkurven" href="the-guide/mathematics/differential-geometry/elementary-differential-geometry/raumkurven.html" class="internal-link" target="_self" rel="noopener nofollow">Kurven</a>  heißen homotop, wenn es eine glatte <a data-tooltip-position="top" aria-label="Mathematical Relations" data-href="Mathematical Relations" href="the-guide/mathematics/general-stuff/mathematical-relations.html" class="internal-link" target="_self" rel="noopener nofollow">Funktion</a> gibt mit 
<br>
<br>In  bzw. auf <a data-tooltip-position="top" aria-label="Convexity" data-href="Convexity" href="the-guide/mathematics/optimization/convex-optimization-lecture/convexity.html" class="internal-link" target="_self" rel="noopener nofollow">konvexen Mengen</a> sind alle Kurven homotop, da man immer konstruieren kann.
<br> homotop, aber  nicht : <img alt="center" src="lib/media/pasted-image-20240226120803.png" style="width: 300px; max-width: 100%;">
<br>Kurven müssen nicht geschlossen sein ! Im geschlossenen Fall spricht man daher oft gesondert von Schleifen

<br>Schleifen sind null-homotop, wenn sie homotop zur konstanten Schleife sind.


<br>Satz 
Homotopie bildet eine <a data-tooltip-position="top" aria-label="Equivalence Relation and Class" data-href="Equivalence Relation and Class" href="the-guide/mathematics/general-stuff/equivalence-relation-and-class.html" class="internal-link" target="_self" rel="noopener nofollow">Äquivalenzrelation</a> .
<br><br><br>Regulär Homotope Kurven
Die Kurven  sind regulär homotop, wenn die Kurven regulär sind für alle .
<br>
<br>Verletzt, wenn z.B, eine Kurve ohne Knoten in eine mit überführt wird, da in Übergangskurven dann Knick auftreten muss
<br>Homotop relativ Geschlossenheit
Die Kurven  sind homotop relativ Geschlossenheit, wenn  <a data-tooltip-position="top" aria-label="Geschlossene Kurve" data-href="Geschlossene Kurve" href="the-guide/mathematics/differential-geometry/elementary-differential-geometry/globale-kurven-und-flächentheorie/geschlossene-kurve.html" class="internal-link" target="_self" rel="noopener nofollow">geschlossen</a> ist für .
<br>Homotop relativ der Endpunkte
Die Kurven  sind homotop relativ der Endpunkte, wenn 
<br>Satz - Index ebener homotoper Kurven
Für regulär homotope <a data-tooltip-position="top" aria-label="Ebene Kurven" data-href="Ebene Kurven" href="the-guide/mathematics/differential-geometry/elementary-differential-geometry/lokale-kurventheorie/ebene-kurven.html" class="internal-link" target="_self" rel="noopener nofollow">ebene Kurven</a>  gilt sie haben den selben <a data-tooltip-position="top" aria-label="Index einer geschlossenen, ebenen Kurve" data-href="Index einer geschlossenen, ebenen Kurve" href="the-guide/mathematics/differential-geometry/elementary-differential-geometry/globale-kurven-und-flächentheorie/index-einer-geschlossenen,-ebenen-kurve.html" class="internal-link" target="_self" rel="noopener nofollow">Index</a>.
<br>
<br>Beweis

<br>Index hängt stetig von  ab, ist aber in , muss also konstant bleiben


]]></description><link>the-guide/mathematics/differential-geometry/elementary-differential-geometry/globale-kurven-und-flächentheorie/homotope-kurven.html</link><guid isPermaLink="false">The Guide/Mathematics/Differential Geometry/Elementary Differential Geometry/Globale Kurven- und Flächentheorie/Homotope Kurven.md</guid><pubDate>Sat, 05 Apr 2025 16:23:52 GMT</pubDate><enclosure url="lib/media/pasted-image-20240226120803.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src=&quot;lib/media/pasted-image-20240226120803.png&quot;&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[Index einer geschlossenen, ebenen Kurve]]></title><description><![CDATA[ 
 <br>Definition
Der Index / die Umlaufzahl einer <a data-tooltip-position="top" aria-label="Geschlossene Kurve" data-href="Geschlossene Kurve" href="the-guide/mathematics/differential-geometry/elementary-differential-geometry/globale-kurven-und-flächentheorie/geschlossene-kurve.html" class="internal-link" target="_self" rel="noopener nofollow">geschlossenen</a>, <a data-tooltip-position="top" aria-label="Ebene Kurven" data-href="Ebene Kurven" href="the-guide/mathematics/differential-geometry/elementary-differential-geometry/lokale-kurventheorie/ebene-kurven.html" class="internal-link" target="_self" rel="noopener nofollow">ebenen und regulären Kurve</a>   mit geliftetem Tangentenwinkel  ist definiert via wobei  den Lift gemäß des <a data-tooltip-position="top" aria-label="Lifting Lemma" data-href="Lifting Lemma" href="the-guide/mathematics/differential-geometry/elementary-differential-geometry/lifting-lemma.html" class="internal-link" target="_self" rel="noopener nofollow">Lifting Lemmas</a> bezeichnet.
<br>Satz 4.3
Da die ebene Kurve geschlossen ist, gilt Der Index ist eine <a data-tooltip-position="top" aria-label="Geometrische Größen" data-href="Geometrische Größen" href="the-guide/mathematics/differential-geometry/elementary-differential-geometry/lokale-kurventheorie/geometrische-größen.html" class="internal-link" target="_self" rel="noopener nofollow">geometrische Eigenschaft</a> der <a data-tooltip-position="top" aria-label="Kurven in 3D" data-href="Kurven in 3D" href="the-guide/mathematics/differential-geometry/elementary-differential-geometry/lokale-kurventheorie/kurven-in-3d.html" class="internal-link" target="_self" rel="noopener nofollow">orientierten Klasse</a> , da die Krümmung ebener Kurven Vorzeichenbehaftet ist.
]]></description><link>the-guide/mathematics/differential-geometry/elementary-differential-geometry/globale-kurven-und-flächentheorie/index-einer-geschlossenen,-ebenen-kurve.html</link><guid isPermaLink="false">The Guide/Mathematics/Differential Geometry/Elementary Differential Geometry/Globale Kurven- und Flächentheorie/Index einer geschlossenen, ebenen Kurve.md</guid><pubDate>Mon, 12 Aug 2024 17:05:42 GMT</pubDate></item><item><title><![CDATA[Minimalflächen]]></title><description><![CDATA[ 
 <br>Gesucht ist eine <a data-tooltip-position="top" aria-label="Flächen" data-href="Flächen" href="the-guide/mathematics/differential-geometry/elementary-differential-geometry/flächen.html" class="internal-link" target="_self" rel="noopener nofollow">Fläche</a> , die bei vorgegebenem Rand  den minimalen <a data-tooltip-position="top" aria-label="1. Fundamentalform" data-href="1. Fundamentalform" href="the-guide/mathematics/differential-geometry/elementary-differential-geometry/1.-fundamentalform.html" class="internal-link" target="_self" rel="noopener nofollow">Flächeninhalt</a> erreicht. Man betrachte hierzu die Variation der Fläche  mit Vektorfeld , welches am Rand verschwindet. Man kann zeigen, dass sich ohne einschränkgun der Allgemeinheit ein normales Vektorfeld verwenden lässt, also ...<br><br>Definition
Definiert als eine <a data-tooltip-position="top" aria-label="Implizite Fläche" data-href="Implizite Fläche" href="the-guide/mathematics/differential-geometry/elementary-differential-geometry/extrinsische-geometrie-von-hyperflächen/implizite-fläche.html" class="internal-link" target="_self" rel="noopener nofollow">globale Hyperfläche</a> mit <a data-tooltip-position="top" aria-label="Hauptkrümmung und Hauptkrümmungsrichtung" data-href="Hauptkrümmung und Hauptkrümmungsrichtung" href="the-guide/mathematics/differential-geometry/elementary-differential-geometry/extrinsische-geometrie-von-hyperflächen/hauptkrümmung-und-hauptkrümmungsrichtung.html" class="internal-link" target="_self" rel="noopener nofollow">mittlerer Krümmung</a> 
<br>Korollar 2.14.
Es gibt keine geschlossenene Minimalflächen .
<br>
<br>Es existieren jedoch nicht-kompakte Minimalflächen wie Helikoid oder Katenoid oder Beispiele mit Rand. 
]]></description><link>the-guide/mathematics/differential-geometry/elementary-differential-geometry/globale-kurven-und-flächentheorie/minimalflächen.html</link><guid isPermaLink="false">The Guide/Mathematics/Differential Geometry/Elementary Differential Geometry/Globale Kurven- und Flächentheorie/Minimalflächen.md</guid><pubDate>Mon, 12 Aug 2024 17:05:43 GMT</pubDate></item><item><title><![CDATA[Orientierte geodätische Krümmung]]></title><description><![CDATA[ 
 <br>Im Sonderfall  ist die <a data-tooltip-position="top" aria-label="Geodätische Krümmung" data-href="Geodätische Krümmung" href="the-guide/mathematics/differential-geometry/elementary-differential-geometry/intrinsische-geometrie-von-hyperflächen/geodätische-krümmung.html" class="internal-link" target="_self" rel="noopener nofollow">geodätische Krümmung</a> vorzeichenbehaftet. Für die Definition wollen wir eine Drehung im Tangentialraum herleiten. Dazu bestimme man hinsichtlich der <a data-tooltip-position="top" aria-label="Matrices" data-href="Matrices" href="the-guide/mathematics/linear-algebra/matrices.html" class="internal-link" target="_self" rel="noopener nofollow">Drehmatrix</a> um   im Urbildraum  eine Korrektur, sodass die entstehende Transformation  in  eine Drehung um  für die entsprechenden Vektoren  im Tangentialraum zur Folge hat.<img alt="center" src="lib/media/pasted-image-20240126161929.png" style="width: 500px; max-width: 100%;"><br>
Wir wollen basierend auf  also ein  bestimmen, sodass die<br>
... ??<br>Ebenso wollen wir folgende Eigenschaften aus dem Fall <a data-tooltip-position="top" aria-label="Ebene Kurven" data-href="Ebene Kurven" href="the-guide/mathematics/differential-geometry/elementary-differential-geometry/lokale-kurventheorie/ebene-kurven.html" class="internal-link" target="_self" rel="noopener nofollow">ebener Kurven</a> (hier ) übersetzen: Aus der zweiten Bedingung folgt direkt und damit . Die erste Bedingung liefert wodurch wir mit das gewünschte Äquivalent  im <a data-href="Tangentialraum" href="the-guide/mathematics/differential-geometry/riemannian-geometry/mannigfaltigkeiten-und-differenzierbare-abbildungen/tangentialraum.html" class="internal-link" target="_self" rel="noopener nofollow">Tangentialraum</a>  im Punkt  erhalten.  <br>Definition 
Sei  eine <a data-tooltip-position="top" aria-label="Flächen" data-href="Flächen" href="the-guide/mathematics/differential-geometry/elementary-differential-geometry/flächen.html" class="internal-link" target="_self" rel="noopener nofollow">parametrisierte Fläche</a> und  mit  eine <a data-tooltip-position="top" aria-label="BL-Kurve" data-href="BL-Kurve" href="the-guide/mathematics/differential-geometry/elementary-differential-geometry/lokale-kurventheorie/bl-kurve.html" class="internal-link" target="_self" rel="noopener nofollow">nach Bogenlänge parametrisierte</a> <a data-tooltip-position="top" aria-label="Flächenkurve" data-href="Flächenkurve" href="the-guide/mathematics/differential-geometry/elementary-differential-geometry/flächenkurve.html" class="internal-link" target="_self" rel="noopener nofollow">Flächenkurve</a>. Die orientierte geodätische Krümmung berechnet sich dann über 
<br>
<br>Für allgemeine Flächenkurven  gilt 
<br><br><br>Sei  <a data-href="Flächenkurve" href="the-guide/mathematics/differential-geometry/elementary-differential-geometry/flächenkurve.html" class="internal-link" target="_self" rel="noopener nofollow">Flächenkurve</a> und glatte tangentiale Einheitsvektorfelder, also  und .<br>Lemma
Es gibt eine glatte <a data-tooltip-position="top" aria-label="Mathematical Relations" data-href="Mathematical Relations" href="the-guide/mathematics/general-stuff/mathematical-relations.html" class="internal-link" target="_self" rel="noopener nofollow">Funktion</a> mitWir schreiben wie im <a data-href="Lifting Lemma" href="the-guide/mathematics/differential-geometry/elementary-differential-geometry/lifting-lemma.html" class="internal-link" target="_self" rel="noopener nofollow">Lifting Lemma</a>
<br>Lemma
Sei nun  zusätzlich <a data-href="BL-Kurve" href="the-guide/mathematics/differential-geometry/elementary-differential-geometry/lokale-kurventheorie/bl-kurve.html" class="internal-link" target="_self" rel="noopener nofollow">BL-Kurve</a>. Dann gilt wobei 
]]></description><link>the-guide/mathematics/differential-geometry/elementary-differential-geometry/globale-kurven-und-flächentheorie/orientierte-geodätische-krümmung.html</link><guid isPermaLink="false">The Guide/Mathematics/Differential Geometry/Elementary Differential Geometry/Globale Kurven- und Flächentheorie/Orientierte geodätische Krümmung.md</guid><pubDate>Mon, 12 Aug 2024 17:05:43 GMT</pubDate><enclosure url="lib/media/pasted-image-20240126161929.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src=&quot;lib/media/pasted-image-20240126161929.png&quot;&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[Satz von Hopf]]></title><description><![CDATA[ 
 <br>Im folgenden verstehen wir eine positiv orientierte Kurve als solche, bei der die <a data-tooltip-position="top" aria-label="Raumkurven" data-href="Raumkurven" href="the-guide/mathematics/differential-geometry/elementary-differential-geometry/raumkurven.html" class="internal-link" target="_self" rel="noopener nofollow">Spur</a> stets links des Tangentialvektors liegt (analog zu mathematisch positiv gegen den Uhrzeigersinn).<img alt="center" src="lib/media/pasted-image-20240126142313.png" style="width: 200px; max-width: 100%;"><br><br>Satz von Hopf
Sei  eine ...

<br><a data-tooltip-position="top" aria-label="Ebene Kurven" data-href="Ebene Kurven" href="the-guide/mathematics/differential-geometry/elementary-differential-geometry/lokale-kurventheorie/ebene-kurven.html" class="internal-link" target="_self" rel="noopener nofollow">ebene</a>,
<br><a data-tooltip-position="top" aria-label="Raumkurven" data-href="Raumkurven" href="the-guide/mathematics/differential-geometry/elementary-differential-geometry/raumkurven.html" class="internal-link" target="_self" rel="noopener nofollow">reguläre</a>,
<br><a data-tooltip-position="top" aria-label="Geschlossene Kurve" data-href="Geschlossene Kurve" href="the-guide/mathematics/differential-geometry/elementary-differential-geometry/globale-kurven-und-flächentheorie/geschlossene-kurve.html" class="internal-link" target="_self" rel="noopener nofollow">geschlossene</a>,
<br><a data-tooltip-position="top" aria-label="Geschlossene Kurve" data-href="Geschlossene Kurve" href="the-guide/mathematics/differential-geometry/elementary-differential-geometry/globale-kurven-und-flächentheorie/geschlossene-kurve.html" class="internal-link" target="_self" rel="noopener nofollow">einfache</a>,

und wie oben positiv orientierte <a data-tooltip-position="top" aria-label="Raumkurven" data-href="Raumkurven" href="the-guide/mathematics/differential-geometry/elementary-differential-geometry/raumkurven.html" class="internal-link" target="_self" rel="noopener nofollow">Kurve</a>. Dann gilt für den <a data-tooltip-position="top" aria-label="Index einer geschlossenen, ebenen Kurve" data-href="Index einer geschlossenen, ebenen Kurve" href="the-guide/mathematics/differential-geometry/elementary-differential-geometry/globale-kurven-und-flächentheorie/index-einer-geschlossenen,-ebenen-kurve.html" class="internal-link" target="_self" rel="noopener nofollow">Index</a> dieser Kurve 
<br>
<br>Negativ orientierte führt zu Index 
<br>
<br>...<br>Ein Sonderfall stellen <a data-tooltip-position="top" aria-label="Stückweise differenzierbare Kurven" data-href="Stückweise differenzierbare Kurven" href="the-guide/mathematics/differential-geometry/elementary-differential-geometry/globale-kurven-und-flächentheorie/stückweise-differenzierbare-kurven.html" class="internal-link" target="_self" rel="noopener nofollow">stückweise differenzierbare Kurven</a> dar:<br>Satz von Hopf für stückweise differenzierbare Kurven
Für eine stückweise differenzierbare Kurve  gilt 
]]></description><link>the-guide/mathematics/differential-geometry/elementary-differential-geometry/globale-kurven-und-flächentheorie/satz-von-hopf.html</link><guid isPermaLink="false">The Guide/Mathematics/Differential Geometry/Elementary Differential Geometry/Globale Kurven- und Flächentheorie/Satz von Hopf.md</guid><pubDate>Mon, 12 Aug 2024 17:05:43 GMT</pubDate><enclosure url="lib/media/pasted-image-20240126142313.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src=&quot;lib/media/pasted-image-20240126142313.png&quot;&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[Stückweise differenzierbare Kurven]]></title><description><![CDATA[ 
 <br>In a Nutshell
Stückweise differenzierbare Kurven betrachtet man, da erst diese Klasse von Kurven abgeschlossen bezüglich Hintereinander-hängen ist.
<br><br><br>Definition
Eine stetige <a data-tooltip-position="top" aria-label="Raumkurven" data-href="Raumkurven" href="the-guide/mathematics/differential-geometry/elementary-differential-geometry/raumkurven.html" class="internal-link" target="_self" rel="noopener nofollow">Kurve</a> , die auf jedem Teilintervall / jeder Kante  einer Zerlegung  in  ist heißt stückweise differenzierbar ( mal). Die Knoten bezeichnen wir mit  und die Intervalle seine . Die Menge dieser Kurven schreiben wir als Die stückweise differenzierbare Kurve ist regulär, wenn alle Teilkurven mit Ausnahme der Ecken  regulär sind. Ist , so spricht man von stückweise glatten Kurven. 
<br>
<br>Oft wird ei<a class="internal-link" data-href="Raumkurven.md" href="the-guide/mathematics/differential-geometry/elementary-differential-geometry/raumkurven.html" target="_self" rel="noopener nofollow"></a>g mit  bezeichnet und das Tuple  bestimmt die Kruve.
<br><img alt="center" src="lib/media/pasted-image-20240126150658.png"><br>Die Definition des <a data-tooltip-position="top" aria-label="Index einer geschlossenen, ebenen Kurve" data-href="Index einer geschlossenen, ebenen Kurve" href="the-guide/mathematics/differential-geometry/elementary-differential-geometry/globale-kurven-und-flächentheorie/index-einer-geschlossenen,-ebenen-kurve.html" class="internal-link" target="_self" rel="noopener nofollow">Index</a> kann für eine solche Kurve wie folgt angeglichen werden:<br>Index einer stückweise glatten Kurve
Der Index einer <a data-tooltip-position="top" aria-label="Geschlossene Kurve" data-href="Geschlossene Kurve" href="the-guide/mathematics/differential-geometry/elementary-differential-geometry/globale-kurven-und-flächentheorie/geschlossene-kurve.html" class="internal-link" target="_self" rel="noopener nofollow">geschlossenen</a>, stückweise glatten Kurve  ist definiert über  
<br>Ebenso gilt eine alternative Formulierung des <a data-tooltip-position="top" aria-label="Satz von Hopf" data-href="Satz von Hopf" href="the-guide/mathematics/differential-geometry/elementary-differential-geometry/globale-kurven-und-flächentheorie/satz-von-hopf.html" class="internal-link" target="_self" rel="noopener nofollow">Satzes von Hopf</a>, bei der der Sprungwinkel auf einer Ecke  zwischen Kanten definiert wird als wobei wir  als Kuspe ausschließen.<img alt="center" src="lib/media/pasted-image-20240126151445.png" style="width: 400px; max-width: 100%;"><br>Satz von Hopf für stückweise differenzierbare Kurven
Für eine stückweise differenzierbare Kurve  wie oben gilt 
]]></description><link>the-guide/mathematics/differential-geometry/elementary-differential-geometry/globale-kurven-und-flächentheorie/stückweise-differenzierbare-kurven.html</link><guid isPermaLink="false">The Guide/Mathematics/Differential Geometry/Elementary Differential Geometry/Globale Kurven- und Flächentheorie/Stückweise differenzierbare Kurven.md</guid><pubDate>Mon, 12 Aug 2024 17:05:43 GMT</pubDate><enclosure url="lib/media/pasted-image-20240126150658.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src=&quot;lib/media/pasted-image-20240126150658.png&quot;&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[Theorema Elegantissimum]]></title><description><![CDATA[ 
 <br>Einfache Spezialfälle des <a data-tooltip-position="top" aria-label="Satz von Gauß-Bonnet" data-href="Satz von Gauß-Bonnet" href="the-guide/mathematics/differential-geometry/elementary-differential-geometry/satz-von-gauß-bonnet.html" class="internal-link" target="_self" rel="noopener nofollow">Satzes von Gauß-Bonnet</a>. Sei  eine Fläche berandet durch ein Dreieck. Dann ist  mit  ein Dreieck mit Ecken  und Kanten .<img alt="center" src="lib/media/pasted-image-20240201172705.png" style="width: 450px; max-width: 100%;">Sind hierbei alle Kanten <a data-tooltip-position="top" aria-label="Geodätische" data-href="Geodätische" href="the-guide/mathematics/differential-geometry/elementary-differential-geometry/intrinsische-geometrie-von-hyperflächen/geodätische.html" class="internal-link" target="_self" rel="noopener nofollow">geodätisch</a>, so heißt das Dreieck geodätisches Dreieck.<br>Theorema Elegantissimum
Für die obige Situation eines geodätischen Dreieckes gilt für die <a data-tooltip-position="top" aria-label="Hauptkrümmung und Hauptkrümmungsrichtung" data-href="Hauptkrümmung und Hauptkrümmungsrichtung" href="the-guide/mathematics/differential-geometry/elementary-differential-geometry/extrinsische-geometrie-von-hyperflächen/hauptkrümmung-und-hauptkrümmungsrichtung.html" class="internal-link" target="_self" rel="noopener nofollow">Gauß-Krümmung</a>  im gebiet  der Zusammenhang  wobei die Winkel die 3 Innenwinkel des Dreiecks in  bezeichnen.
<br><br><br><img alt="center" src="lib/media/pasted-image-20240229105017.png" style="width: 200px; max-width: 100%;"><br>
Für ein geodätisches zweieck gilt ganz analog zu oben Da die Winkel positiv sein müssen folgt hieraus insbesondere , weswegen es keine Zweiecke auf <a data-tooltip-position="top" aria-label="Hauptkrümmung und Hauptkrümmungsrichtung" data-href="Hauptkrümmung und Hauptkrümmungsrichtung" href="the-guide/mathematics/differential-geometry/elementary-differential-geometry/extrinsische-geometrie-von-hyperflächen/hauptkrümmung-und-hauptkrümmungsrichtung.html" class="internal-link" target="_self" rel="noopener nofollow">hyperbolischen Flächen</a> geben kann.]]></description><link>the-guide/mathematics/differential-geometry/elementary-differential-geometry/globale-kurven-und-flächentheorie/theorema-elegantissimum.html</link><guid isPermaLink="false">The Guide/Mathematics/Differential Geometry/Elementary Differential Geometry/Globale Kurven- und Flächentheorie/Theorema Elegantissimum.md</guid><pubDate>Mon, 12 Aug 2024 17:05:43 GMT</pubDate><enclosure url="lib/media/pasted-image-20240201172705.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src=&quot;lib/media/pasted-image-20240201172705.png&quot;&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[Totalkrümmung ebener Kurven]]></title><description><![CDATA[ 
 <br>Definition
Die Totalkrümmung einer <a data-tooltip-position="top" aria-label="Raumkurven" data-href="Raumkurven" href="the-guide/mathematics/differential-geometry/elementary-differential-geometry/raumkurven.html" class="internal-link" target="_self" rel="noopener nofollow">regulär parametrisierten</a> <a data-tooltip-position="top" aria-label="Ebene Kurven" data-href="Ebene Kurven" href="the-guide/mathematics/differential-geometry/elementary-differential-geometry/lokale-kurventheorie/ebene-kurven.html" class="internal-link" target="_self" rel="noopener nofollow">ebenen Kurve</a>  ist definiert durch mit der <a data-tooltip-position="top" aria-label="Krümmung einer Kurve" data-href="Krümmung einer Kurve" href="the-guide/mathematics/differential-geometry/elementary-differential-geometry/lokale-kurventheorie/krümmung-einer-kurve.html" class="internal-link" target="_self" rel="noopener nofollow">Kurvenkrümmung</a> .
<br>Satz
Für eine <a data-tooltip-position="top" aria-label="Geschlossene Kurve" data-href="Geschlossene Kurve" href="the-guide/mathematics/differential-geometry/elementary-differential-geometry/globale-kurven-und-flächentheorie/geschlossene-kurve.html" class="internal-link" target="_self" rel="noopener nofollow">geschlossene</a>, reguläre <a data-tooltip-position="top" aria-label="Ebene Kurven" data-href="Ebene Kurven" href="the-guide/mathematics/differential-geometry/elementary-differential-geometry/lokale-kurventheorie/ebene-kurven.html" class="internal-link" target="_self" rel="noopener nofollow">ebene Kurve</a> gilt für den <a data-tooltip-position="top" aria-label="Index einer geschlossenen, ebenen Kurve" data-href="Index einer geschlossenen, ebenen Kurve" href="the-guide/mathematics/differential-geometry/elementary-differential-geometry/globale-kurven-und-flächentheorie/index-einer-geschlossenen,-ebenen-kurve.html" class="internal-link" target="_self" rel="noopener nofollow">Index</a> der Zusammenhang und er ist <a data-tooltip-position="top" aria-label="Geometrische Größen" data-href="Geometrische Größen" href="the-guide/mathematics/differential-geometry/elementary-differential-geometry/lokale-kurventheorie/geometrische-größen.html" class="internal-link" target="_self" rel="noopener nofollow">geometrische Größe</a> auf der orientierten Klasse .
<br><br><br>Betrachte <a data-tooltip-position="top" aria-label="Geschlossene Kurve" data-href="Geschlossene Kurve" href="the-guide/mathematics/differential-geometry/elementary-differential-geometry/globale-kurven-und-flächentheorie/geschlossene-kurve.html" class="internal-link" target="_self" rel="noopener nofollow">geschlossene</a> <a data-href="BL-Kurve" href="the-guide/mathematics/differential-geometry/elementary-differential-geometry/lokale-kurventheorie/bl-kurve.html" class="internal-link" target="_self" rel="noopener nofollow">BL-Kurve</a>  und Parallelkurve <img alt="center" src="lib/media/pasted-image-20240226122601.png" style="width: 300px; max-width: 100%;">Dann gilt Damit lässt sich die <a data-tooltip-position="top" aria-label="Kurvenintegral" data-href="Kurvenintegral" href="the-guide/mathematics/differential-geometry/elementary-differential-geometry/lokale-kurventheorie/kurvenintegral.html" class="internal-link" target="_self" rel="noopener nofollow">Länge</a> der parallelen kruve berechnen über mit Totalkrümmung  und index ind.]]></description><link>the-guide/mathematics/differential-geometry/elementary-differential-geometry/globale-kurven-und-flächentheorie/totalkrümmung-ebener-kurven.html</link><guid isPermaLink="false">The Guide/Mathematics/Differential Geometry/Elementary Differential Geometry/Globale Kurven- und Flächentheorie/Totalkrümmung ebener Kurven.md</guid><pubDate>Mon, 12 Aug 2024 17:05:43 GMT</pubDate><enclosure url="lib/media/pasted-image-20240226122601.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src=&quot;lib/media/pasted-image-20240226122601.png&quot;&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[Triangulierung]]></title><description><![CDATA[ 
 <br>Sei  eine Fläche berandet durch ein Dreieck. Dann ist  mit  ein Dreieck mit Ecken  und Kanten .<img alt="center" src="lib/media/pasted-image-20240201172705.png" style="width: 450px; max-width: 100%;">Das Dreieck heißt <a data-tooltip-position="top" aria-label="Geodätische" data-href="Geodätische" href="the-guide/mathematics/differential-geometry/elementary-differential-geometry/intrinsische-geometrie-von-hyperflächen/geodätische.html" class="internal-link" target="_self" rel="noopener nofollow">geodätisch</a>, wenn alle Kanten geodätisch sind.<br><br>Definition
Sei  eine <a data-tooltip-position="top" aria-label="Geschlossene Flächen" data-href="Geschlossene Flächen" href="the-guide/mathematics/differential-geometry/elementary-differential-geometry/globale-kurven-und-flächentheorie/geschlossene-flächen.html" class="internal-link" target="_self" rel="noopener nofollow">geschlossene Fläche</a>. Die Menge von Dreiecken heißt Triangulierung von , wenn 

<br>
<br>für  die Vereinigung  enweder

<br>leer ist
<br>einer Kante entspricht
<br>einer Ecke entspricht



<br><img alt="center" src="lib/media/pasted-image-20240229105709.png">]]></description><link>the-guide/mathematics/differential-geometry/elementary-differential-geometry/globale-kurven-und-flächentheorie/triangulierung.html</link><guid isPermaLink="false">The Guide/Mathematics/Differential Geometry/Elementary Differential Geometry/Globale Kurven- und Flächentheorie/Triangulierung.md</guid><pubDate>Mon, 12 Aug 2024 17:05:43 GMT</pubDate><enclosure url="lib/media/pasted-image-20240201172705.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src=&quot;lib/media/pasted-image-20240201172705.png&quot;&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[Christoffel-Symbol]]></title><description><![CDATA[ 
 <br>In a Nutshell
<a data-tooltip-position="top" aria-label="Mathematical Relations" data-href="Mathematical Relations" href="the-guide/mathematics/general-stuff/mathematical-relations.html" class="internal-link" target="_self" rel="noopener nofollow">Funktionen</a>, die die Projektion der zweiten Ableitung einer Fläche in den <a data-href="Tangentialraum" href="the-guide/mathematics/differential-geometry/riemannian-geometry/mannigfaltigkeiten-und-differenzierbare-abbildungen/tangentialraum.html" class="internal-link" target="_self" rel="noopener nofollow">Tangentialraum</a> beschreiben. Ermöglichen definition der Differentialgleichungen für <a data-href="Geodätische" href="the-guide/mathematics/differential-geometry/elementary-differential-geometry/intrinsische-geometrie-von-hyperflächen/geodätische.html" class="internal-link" target="_self" rel="noopener nofollow">Geodätische</a>, was diese der Analysis zugänglich macht. 
<br><br><br>Definition der Christoffel-Symbole
Die Christoffel-Symbole einer Parametrisierung  sind die <a data-tooltip-position="top" aria-label="Mathematical Relations" data-href="Mathematical Relations" href="the-guide/mathematics/general-stuff/mathematical-relations.html" class="internal-link" target="_self" rel="noopener nofollow">Funktionen</a>  für  mit Damit sind sie die Vorfaktoren für Linearkombination der Basis des Tangentialraumes, um Projektion der zweiten Ableitung darin darzustellen.<br>
Im Setting der Riemmannschen Geometrie betrachten wir eine Immersion  und eine <a data-tooltip-position="top" aria-label="Charts and Atlas" data-href="Charts and Atlas" href="the-guide/mathematics/differential-geometry/riemannian-geometry/mannigfaltigkeiten-und-differenzierbare-abbildungen/charts-and-atlas.html" class="internal-link" target="_self" rel="noopener nofollow">Karte</a>  und definieren die Funktionen entsprechend
<br><br>Alternative Definition via Stacks
Mit dem in der Vorlesung eingefühten <a data-href="Stack - Formalismus" href="the-guide/mathematics/differential-geometry/elementary-differential-geometry/intrinsische-geometrie-von-hyperflächen/stack-formalismus.html" class="internal-link" target="_self" rel="noopener nofollow">Stack - Formalismus</a> lassen sich die Christoffel-Symbole kompakt schreiben als bestehend aus horizontalen Vektoren  mit der <a data-tooltip-position="top" aria-label="Moore-Penrose Pseudoinverse" data-href="Moore-Penrose Pseudoinverse" href="the-guide/mathematics/linear-algebra/moore-penrose-pseudoinverse.html" class="internal-link" target="_self" rel="noopener nofollow">Pseudoinversen</a> . Damit erhält man einen Vektor von <a data-tooltip-position="top" aria-label="Matrices" data-href="Matrices" href="the-guide/mathematics/linear-algebra/matrices.html" class="internal-link" target="_self" rel="noopener nofollow">Matrizen</a> für die Bilinearformen der jeweiligen Komponenten (hier für ):
<br>Intuition
Der vertikale Vektor  ist die orthogonale Projektion von  in den <a data-href="Tangentialraum" href="the-guide/mathematics/differential-geometry/riemannian-geometry/mannigfaltigkeiten-und-differenzierbare-abbildungen/tangentialraum.html" class="internal-link" target="_self" rel="noopener nofollow">Tangentialraum</a>.
<br>
<br>Eindeutig, da  Basis des <a data-tooltip-position="top" aria-label="Tangentialraum" data-href="Tangentialraum" href="the-guide/mathematics/differential-geometry/riemannian-geometry/mannigfaltigkeiten-und-differenzierbare-abbildungen/tangentialraum.html" class="internal-link" target="_self" rel="noopener nofollow">Tangentialraumes</a>
<br> (Satz von Schwartz), oben entspricht dies 
<br><br><br>Theorem
Für jede <a data-tooltip-position="top" aria-label="Flächen" data-href="Flächen" href="the-guide/mathematics/differential-geometry/elementary-differential-geometry/flächen.html" class="internal-link" target="_self" rel="noopener nofollow">Immersion</a>  sind die Christoffel-Symbole  durch <a data-tooltip-position="top" aria-label="1. Fundamentalform" data-href="1. Fundamentalform" href="the-guide/mathematics/differential-geometry/elementary-differential-geometry/1.-fundamentalform.html" class="internal-link" target="_self" rel="noopener nofollow">erste Fundamentalform</a> bestimmt Dabei sind  die Einträge der <a data-tooltip-position="top" aria-label="1. Fundamentalform" data-href="1. Fundamentalform" href="the-guide/mathematics/differential-geometry/elementary-differential-geometry/1.-fundamentalform.html" class="internal-link" target="_self" rel="noopener nofollow">ersten Fundamentalform</a> und  die ihrer Inversen.
<br>Beispiele<br>
<br>Für eine beliebige Rotationsfläche mit der zusätzlichen Bedingung  erhält man die Christoffel-Symbole  
<br><br><br>Im Riemannschen Fall basieren unsere Definition auf der axiomatischen Einführung des <a data-tooltip-position="top" aria-label="Zusammenhänge" data-href="Zusammenhänge" href="the-guide/mathematics/differential-geometry/riemannian-geometry/metriken-und-zusammenhänge/zusammenhänge.html" class="internal-link" target="_self" rel="noopener nofollow">Zusammenhanges</a>.<br>Christoffel-Symbole über Zusammenhänge
Die Christoffelsymbole eines Zusammenhanges sind für jede <a data-tooltip-position="top" aria-label="Topology and Topological Space" data-href="Topology and Topological Space" href="the-guide/mathematics/topology/topology-and-topological-space.html" class="internal-link" target="_self" rel="noopener nofollow">Karte</a>  die Funktionen  mit Sie messen den Unterschied zwischen der <a data-tooltip-position="top" aria-label="Lie Ableitung" data-href="Lie Ableitung" href="the-guide/mathematics/differential-geometry/riemannian-geometry/mannigfaltigkeiten-und-differenzierbare-abbildungen/lie-ableitung.html" class="internal-link" target="_self" rel="noopener nofollow">Richtungsableitung</a> der Karte und dem Zusammenhang.
<br>Intuition
Christoffel-Symbole kodieren die Änderung der Basisvektoren des Tangentialraumes, wenn wir von einem zum anderen wandern.
<br><br>
<br><a rel="noopener nofollow" class="external-link" href="https://www.youtube.com/watch?v=TvFvL_sMg4g" target="_blank">https://www.youtube.com/watch?v=TvFvL_sMg4g</a>
]]></description><link>the-guide/mathematics/differential-geometry/elementary-differential-geometry/intrinsische-geometrie-von-hyperflächen/christoffel-symbol.html</link><guid isPermaLink="false">The Guide/Mathematics/Differential Geometry/Elementary Differential Geometry/Intrinsische Geometrie von Hyperflächen/Christoffel-Symbol.md</guid><pubDate>Wed, 23 Apr 2025 08:34:33 GMT</pubDate></item><item><title><![CDATA[Geodätische]]></title><description><![CDATA[ 
 <br>Definition der Vorlesung Differentialgeometrie, je nach Quelle leicht abweichend.<img alt="center" src="lib/media/screenshot-from-2023-12-15-14-59-51.png" style="width: 400px; max-width: 100%;"><br><br>Definition
Eine <a data-href="Flächenkurve" href="the-guide/mathematics/differential-geometry/elementary-differential-geometry/flächenkurve.html" class="internal-link" target="_self" rel="noopener nofollow">Flächenkurve</a>  heißt Geodätische, wenn Die Änderung des <a data-tooltip-position="top" aria-label="Tangentialraum" data-href="Tangentialraum" href="the-guide/mathematics/differential-geometry/riemannian-geometry/mannigfaltigkeiten-und-differenzierbare-abbildungen/tangentialraum.html" class="internal-link" target="_self" rel="noopener nofollow">Tangentialvektors</a> findet nur in <a data-tooltip-position="top" aria-label="Gauß-Abbildung" data-href="Gauß-Abbildung" href="the-guide/mathematics/differential-geometry/elementary-differential-geometry/extrinsische-geometrie-von-hyperflächen/gauß-abbildung.html" class="internal-link" target="_self" rel="noopener nofollow">Normalenrichtung</a> statt, man fährt in <a data-tooltip-position="top" aria-label="Flächen" data-href="Flächen" href="the-guide/mathematics/differential-geometry/elementary-differential-geometry/flächen.html" class="internal-link" target="_self" rel="noopener nofollow">Fläche</a> geradeaus.
<br>Man Betrachte eine <a data-tooltip-position="top" aria-label="Flächen" data-href="Flächen" href="the-guide/mathematics/differential-geometry/elementary-differential-geometry/flächen.html" class="internal-link" target="_self" rel="noopener nofollow">Fläche</a>  und zer<a class="internal-link" data-href="Gauß-Abbildung.md" href="the-guide/mathematics/differential-geometry/elementary-differential-geometry/extrinsische-geometrie-von-hyperflächen/gauß-abbildung.html" target="_self" rel="noopener nofollow"></a>lächen.md)]- und Normalenraum Die senkrechte Projektion erfolgt wieder mit den Projektoren <br>
<br>, realisiert durch ,
<br>, realisiert durch ,
<br>wobei wir die <a data-tooltip-position="top" aria-label="Moore-Penrose Pseudoinverse" data-href="Moore-Penrose Pseudoinverse" href="the-guide/mathematics/linear-algebra/moore-penrose-pseudoinverse.html" class="internal-link" target="_self" rel="noopener nofollow">Pseudoinverse</a>  mittels <a data-href="1. Fundamentalform" href="the-guide/mathematics/differential-geometry/elementary-differential-geometry/1.-fundamentalform.html" class="internal-link" target="_self" rel="noopener nofollow">1. Fundamentalform</a> definieren. Hiermit können wir die zweite Ableitung  einer nach <a data-tooltip-position="top" aria-label="BL-Kurve" data-href="BL-Kurve" href="the-guide/mathematics/differential-geometry/elementary-differential-geometry/lokale-kurventheorie/bl-kurve.html" class="internal-link" target="_self" rel="noopener nofollow">Bogenlänge parametrisierten</a> Flächenkurve  mit  <a data-tooltip-position="top" aria-label="Stack - Formalismus" data-href="Stack - Formalismus" href="the-guide/mathematics/differential-geometry/elementary-differential-geometry/intrinsische-geometrie-von-hyperflächen/stack-formalismus.html" class="internal-link" target="_self" rel="noopener nofollow">Stack</a>  in die Tangentialebene projezierenHierbei ist  mit <br>
<br> horizontalen Matrizen 
<br> vertikalen Vektoren <br>
Da der Betrag dieser Größe der <a data-tooltip-position="top" aria-label="Geodätische Krümmung" data-href="Geodätische Krümmung" href="the-guide/mathematics/differential-geometry/elementary-differential-geometry/intrinsische-geometrie-von-hyperflächen/geodätische-krümmung.html" class="internal-link" target="_self" rel="noopener nofollow">geodätischen Krümmung</a> enspricht, die für Geodätische verschwinden soll, gilt der Satz:
<br>Satz - Differentialgleichung für Geodätische
Die <a data-tooltip-position="top" aria-label="Geodätische Krümmung" data-href="Geodätische Krümmung" href="the-guide/mathematics/differential-geometry/elementary-differential-geometry/intrinsische-geometrie-von-hyperflächen/geodätische-krümmung.html" class="internal-link" target="_self" rel="noopener nofollow">geodätische Krümmung</a> verschwindet für alle  genau dann, wenn Hierbei ist  das <a data-href="Christoffel-Symbol" href="the-guide/mathematics/differential-geometry/elementary-differential-geometry/intrinsische-geometrie-von-hyperflächen/christoffel-symbol.html" class="internal-link" target="_self" rel="noopener nofollow">Christoffel-Symbol</a> im <a data-href="Stack - Formalismus" href="the-guide/mathematics/differential-geometry/elementary-differential-geometry/intrinsische-geometrie-von-hyperflächen/stack-formalismus.html" class="internal-link" target="_self" rel="noopener nofollow">Stack - Formalismus</a>. In Summenschreibweise ergibt sich für jede Komponente (Parameter )  die Gleichung
<br>Die<a class="internal-link" data-href="Geodätische Krümmung.md" href="the-guide/mathematics/differential-geometry/elementary-differential-geometry/intrinsische-geometrie-von-hyperflächen/geodätische-krümmung.html" target="_self" rel="noopener nofollow"></a>g überführen, was bei Vorgabe von  und  mit  nach <a data-tooltip-position="top" aria-label="Peano Existence Theorem" data-href="Peano Existence Theorem" href="the-guide/mathematics/differential-equations/peano-existence-theorem.html" class="internal-link" target="_self" rel="noopener nofollow">Peano</a> und <a data-tooltip-position="top" aria-label="Picard-Lindelöf Theorem" data-href="Picard-Lindelöf Theorem" href="the-guide/mathematics/differential-equations/picard-lindelöf-theorem.html" class="internal-link" target="_self" rel="noopener nofollow">Picard-Lindelöff</a> auf eine eindeutige Lösung führt.<br>Satz - Geodätische Flächenkurve
Eine <a data-tooltip-position="top" aria-label="Flächenkurve" data-href="Flächenkurve" href="the-guide/mathematics/differential-geometry/elementary-differential-geometry/flächenkurve.html" class="internal-link" target="_self" rel="noopener nofollow">Flächenkurve</a>  ist geodätisch, genau dann wenn

<br>Die <a data-href="Geodätische Krümmung" href="the-guide/mathematics/differential-geometry/elementary-differential-geometry/intrinsische-geometrie-von-hyperflächen/geodätische-krümmung.html" class="internal-link" target="_self" rel="noopener nofollow">Geodätische Krümmung</a> verschwindet entlang der <a data-tooltip-position="top" aria-label="Flächenkurve" data-href="Flächenkurve" href="the-guide/mathematics/differential-geometry/elementary-differential-geometry/flächenkurve.html" class="internal-link" target="_self" rel="noopener nofollow">Kurve</a> da .
<br>Der Betrag des <a data-tooltip-position="top" aria-label="Raumkurven" data-href="Raumkurven" href="the-guide/mathematics/differential-geometry/elementary-differential-geometry/raumkurven.html" class="internal-link" target="_self" rel="noopener nofollow">Tangentialvektors</a> ist konstant (Beschleunigen<a class="internal-link" data-href="Flächenkurve.md" href="the-guide/mathematics/differential-geometry/elementary-differential-geometry/flächenkurve.html" target="_self" rel="noopener nofollow"></a>ng in Tangentialrichtung verursachen.), woraus offensichtlich  folgt.

<br><br><br>Definition
Der geodätische Abstand zwischen zwei Punkten  und  ist definiert durch 
<br>
<br><a data-tooltip-position="top" aria-label="Set" data-href="Set" href="the-guide/mathematics/general-stuff/set.html" class="internal-link" target="_self" rel="noopener nofollow">Größte untere Schranke</a>, in Praxis meist Minimum, Probleme bei Flächen die lokal nicht konvex sind ( nicht inkludiert): <img alt="center" src="lib/media/pasted-image-20240223092700.png" style="width: 250px; max-width: 100%;">
<br>Satz
Die durch den geodätische Abstand implizierte <a data-tooltip-position="top" aria-label="Mathematical Relations" data-href="Mathematical Relations" href="the-guide/mathematics/general-stuff/mathematical-relations.html" class="internal-link" target="_self" rel="noopener nofollow">Abbildung</a> ist eine <a data-tooltip-position="top" aria-label="Metric Space and Completeness" data-href="Metric Space and Completeness" href="the-guide/mathematics/general-stuff/metric-space-and-completeness.html" class="internal-link" target="_self" rel="noopener nofollow">Metrik</a> auf der Spur von <a data-tooltip-position="top" aria-label="Flächen" data-href="Flächen" href="the-guide/mathematics/differential-geometry/elementary-differential-geometry/flächen.html" class="internal-link" target="_self" rel="noopener nofollow">Fläche</a> .
<br>Kürzeste
Eine Kurve zwischen zwei Punkten  und  auf der <a data-tooltip-position="top" aria-label="Flächen" data-href="Flächen" href="the-guide/mathematics/differential-geometry/elementary-differential-geometry/flächen.html" class="internal-link" target="_self" rel="noopener nofollow">Fläche</a> heißt Kürzeste, wenn 
<br>Satz
Jede Kürzeste ist Geodätische, die Umkehrung gilt  i.d.R. nicht.<br>
Eine uniform parametrisierte () Kürzeste ist geodätisch. 

<br>Für Kürzeste gibt es eine Umparametrisierung, die geodätisch ist

<br><br><br>Definition
Der geodätische Kreis  mit Mittelpunkt  und Radius  ist Im Anschauungsfall verknüpft die Taylorentwicklung den Umfang  dieses Kreises mit der <a data-tooltip-position="top" aria-label="Hauptkrümmung und Hauptkrümmungsrichtung" data-href="Hauptkrümmung und Hauptkrümmungsrichtung" href="the-guide/mathematics/differential-geometry/elementary-differential-geometry/extrinsische-geometrie-von-hyperflächen/hauptkrümmung-und-hauptkrümmungsrichtung.html" class="internal-link" target="_self" rel="noopener nofollow">Gauß-Krümmung</a>: 
<br>
<br>Beispiele mit verschiedenen Kreisumfängen resultieren aus der Krümmung

<br>In  hat der Kreis den Umfang 
<br>Im <a data-tooltip-position="top" aria-label="Hyperbolischer Raum" data-href="Hyperbolischer Raum" href="the-guide/mathematics/differential-geometry/riemannian-geometry/hyperbolischer-raum.html" class="internal-link" target="_self" rel="noopener nofollow">hyperbolischen</a>  ist der Umfang 
<br>Für  ist der Umfang 


]]></description><link>the-guide/mathematics/differential-geometry/elementary-differential-geometry/intrinsische-geometrie-von-hyperflächen/geodätische.html</link><guid isPermaLink="false">The Guide/Mathematics/Differential Geometry/Elementary Differential Geometry/Intrinsische Geometrie von Hyperflächen/Geodätische.md</guid><pubDate>Mon, 24 Feb 2025 23:49:44 GMT</pubDate><enclosure url="lib/media/screenshot-from-2023-12-15-14-59-51.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src=&quot;lib/media/screenshot-from-2023-12-15-14-59-51.png&quot;&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[Geodätische Krümmung]]></title><description><![CDATA[ 
 <br>Nun betrachte man eine <a data-tooltip-position="top" aria-label="Flächenkurve" data-href="Flächenkurve" href="the-guide/mathematics/differential-geometry/elementary-differential-geometry/flächenkurve.html" class="internal-link" target="_self" rel="noopener nofollow">Flächenkurve</a>  mit , die eine <a data-href="BL-Kurve" href="the-guide/mathematics/differential-geometry/elementary-differential-geometry/lokale-kurventheorie/bl-kurve.html" class="internal-link" target="_self" rel="noopener nofollow">BL-Kurve</a> ist, also  überall. Die zweite Ableitung der Kurve lässt sich darstellen als Unter Verwendung der <a data-href="Gauß-Abbildung" href="the-guide/mathematics/differential-geometry/elementary-differential-geometry/extrinsische-geometrie-von-hyperflächen/gauß-abbildung.html" class="internal-link" target="_self" rel="noopener nofollow">Gauß-Abbildung</a>  der <a data-tooltip-position="top" aria-label="Hyperflächen" data-href="Hyperflächen" href="the-guide/mathematics/differential-geometry/elementary-differential-geometry/hyperflächen.html" class="internal-link" target="_self" rel="noopener nofollow">Hyperfläche</a>  lassen sich folgende Projektoren definieren:<br>
<br> beschreibt den Anteil der <a data-tooltip-position="top" aria-label="Krümmung einer Kurve" data-href="Krümmung einer Kurve" href="the-guide/mathematics/differential-geometry/elementary-differential-geometry/lokale-kurventheorie/krümmung-einer-kurve.html" class="internal-link" target="_self" rel="noopener nofollow">Krümmung einer Kurve</a> / des Änderungsvektors, der die Fläche aus der Tangentialebene "hinaushebt". 

<br> ist senkrechte Projektion auf Normalengerade für beliebigen Punkt  via 


<br> beschreibt den Anteil der <a data-href="Krümmung einer Kurve" href="the-guide/mathematics/differential-geometry/elementary-differential-geometry/lokale-kurventheorie/krümmung-einer-kurve.html" class="internal-link" target="_self" rel="noopener nofollow">Krümmung einer Kurve</a> / des Änderungsvektors, der die Kurve in der <a data-tooltip-position="top" aria-label="Ebene Kurven" data-href="Ebene Kurven" href="the-guide/mathematics/differential-geometry/elementary-differential-geometry/lokale-kurventheorie/ebene-kurven.html" class="internal-link" target="_self" rel="noopener nofollow">Tangentialebene</a> krümmt.<br>
-  ist senkrechte Projektion in den <a data-href="Tangentialraum" href="the-guide/mathematics/differential-geometry/riemannian-geometry/mannigfaltigkeiten-und-differenzierbare-abbildungen/tangentialraum.html" class="internal-link" target="_self" rel="noopener nofollow">Tangentialraum</a> für beliebigen Punkt  via <img alt="center" src="lib/media/pasted-image-20240219113004.png" style="width: 400px; max-width: 100%;">
<br><br><br>Geodätische Krümmung
Der Anteil der Krümmung im <a data-href="Tangentialraum" href="the-guide/mathematics/differential-geometry/riemannian-geometry/mannigfaltigkeiten-und-differenzierbare-abbildungen/tangentialraum.html" class="internal-link" target="_self" rel="noopener nofollow">Tangentialraum</a> führt auf den Begriff der geodätischen Krümmung. Mittels des Satzes von Pythagoras und der <a data-href="Normalkrümmung" href="the-guide/mathematics/differential-geometry/elementary-differential-geometry/extrinsische-geometrie-von-hyperflächen/normalkrümmung.html" class="internal-link" target="_self" rel="noopener nofollow">Normalkrümmung</a> erhält man Die Zahl  heißt geodätische Krümmung der Flächenkurve . 
<br><br><br>
<br>Im Sonderfall  lässt sich die geodätische Krümmung vorzeichenbehaftet definieren  <a data-href="Orientierte geodätische Krümmung" href="the-guide/mathematics/differential-geometry/elementary-differential-geometry/globale-kurven-und-flächentheorie/orientierte-geodätische-krümmung.html" class="internal-link" target="_self" rel="noopener nofollow">Orientierte geodätische Krümmung</a>
]]></description><link>the-guide/mathematics/differential-geometry/elementary-differential-geometry/intrinsische-geometrie-von-hyperflächen/geodätische-krümmung.html</link><guid isPermaLink="false">The Guide/Mathematics/Differential Geometry/Elementary Differential Geometry/Intrinsische Geometrie von Hyperflächen/Geodätische Krümmung.md</guid><pubDate>Mon, 12 Aug 2024 17:05:43 GMT</pubDate><enclosure url="lib/media/pasted-image-20240219113004.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src=&quot;lib/media/pasted-image-20240219113004.png&quot;&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[Hyperflächengleichungen]]></title><description><![CDATA[ 
 <br>Sei  parametrisierte <a data-tooltip-position="top" aria-label="Hyperflächen" data-href="Hyperflächen" href="the-guide/mathematics/differential-geometry/elementary-differential-geometry/hyperflächen.html" class="internal-link" target="_self" rel="noopener nofollow">Hyperfläche</a> mit <a data-tooltip-position="top" aria-label="Gauß-Abbildung" data-href="Gauß-Abbildung" href="the-guide/mathematics/differential-geometry/elementary-differential-geometry/extrinsische-geometrie-von-hyperflächen/gauß-abbildung.html" class="internal-link" target="_self" rel="noopener nofollow">Normale</a> , <a data-tooltip-position="top" aria-label="1. Fundamentalform" data-href="1. Fundamentalform" href="the-guide/mathematics/differential-geometry/elementary-differential-geometry/1.-fundamentalform.html" class="internal-link" target="_self" rel="noopener nofollow">erster Fundamentalform</a> , <a data-tooltip-position="top" aria-label="2. Fundamentalform" data-href="2. Fundamentalform" href="the-guide/mathematics/differential-geometry/elementary-differential-geometry/2.-fundamentalform.html" class="internal-link" target="_self" rel="noopener nofollow">zweiter Fundamentalform</a>  sowie <a data-tooltip-position="top" aria-label="Christoffel-Symbol" data-href="Christoffel-Symbol" href="the-guide/mathematics/differential-geometry/elementary-differential-geometry/intrinsische-geometrie-von-hyperflächen/christoffel-symbol.html" class="internal-link" target="_self" rel="noopener nofollow">Christoffel-Symbolen</a> . Dann erfüllt  (Orientierung) das System partieller Differentialgleichungen <br>
<br>Gauß-Formeln 
<br>Weingartenformel Mit dem Satz von Schwarz folgen aus den obigen Gleichungen Bedingungen für die Existenz von Lösungen der Hyperflächengleichungen.
<br><br>Gauß-Gleichungen
Für die Lösungen der obigen Differentialgleichungen gelten die Bedingungen mit Einträgen  der <a data-tooltip-position="top" aria-label="2. Fundamentalform" data-href="2. Fundamentalform" href="the-guide/mathematics/differential-geometry/elementary-differential-geometry/2.-fundamentalform.html" class="internal-link" target="_self" rel="noopener nofollow">2.FF</a>  der <a data-href="Weingarten-Abbildung" href="the-guide/mathematics/differential-geometry/elementary-differential-geometry/extrinsische-geometrie-von-hyperflächen/weingarten-abbildung.html" class="internal-link" target="_self" rel="noopener nofollow">Weingarten-Abbildung</a> und den Einträgen der Inversen <a data-tooltip-position="top" aria-label="1. Fundamentalform" data-href="1. Fundamentalform" href="the-guide/mathematics/differential-geometry/elementary-differential-geometry/1.-fundamentalform.html" class="internal-link" target="_self" rel="noopener nofollow">1.FF</a>.
<br>Mainardi-Codazzi-Gleichungen
Für die Lösungen der obigen Differentialgleichungen gelten die Bedingungen 
<br><br>Zudem lassen sich auf ähnliche Weise Zusammenhänge für die <a data-tooltip-position="top" aria-label="Eingebettete Weingarten-Abbildung" data-href="Eingebettete Weingarten-Abbildung" href="the-guide/mathematics/differential-geometry/elementary-differential-geometry/extrinsische-geometrie-von-hyperflächen/eingebettete-weingarten-abbildung.html" class="internal-link" target="_self" rel="noopener nofollow">eingebettete Weingarten-Abbildung</a> herleiten:<br>Integrabilitätsbedingung der eingebetteten Weingarten-Abbildung
Mit der <a data-tooltip-position="top" aria-label="Moore-Penrose Pseudoinverse" data-href="Moore-Penrose Pseudoinverse" href="the-guide/mathematics/linear-algebra/moore-penrose-pseudoinverse.html" class="internal-link" target="_self" rel="noopener nofollow">Pseudoinversen</a>  definiert über  der <a data-tooltip-position="top" aria-label="Eingebettete Weingarten-Abbildung" data-href="Eingebettete Weingarten-Abbildung" href="the-guide/mathematics/differential-geometry/elementary-differential-geometry/extrinsische-geometrie-von-hyperflächen/eingebettete-weingarten-abbildung.html" class="internal-link" target="_self" rel="noopener nofollow">eingebetteten Weingarten-Abbildung</a> (da ein <a data-tooltip-position="top" aria-label="Eigenvalue Problem" data-href="Eigenvalue Problem" href="the-guide/mathematics/linear-algebra/eigenvalue-problem.html" class="internal-link" target="_self" rel="noopener nofollow">Eigenwert</a> immer ) erfüllen Lösungen der obigen Differentialgleichungen die Gleichungen  
<br>Satz
Wenn die <a data-tooltip-position="top" aria-label="Moore-Penrose Pseudoinverse" data-href="Moore-Penrose Pseudoinverse" href="the-guide/mathematics/linear-algebra/moore-penrose-pseudoinverse.html" class="internal-link" target="_self" rel="noopener nofollow">Pseudoinverse</a>  so vorgegeben wird, das die obigen Gleichungen erfüllt sind, dann exisitert eine zugehörige Fläche.
]]></description><link>the-guide/mathematics/differential-geometry/elementary-differential-geometry/intrinsische-geometrie-von-hyperflächen/hyperflächengleichungen.html</link><guid isPermaLink="false">The Guide/Mathematics/Differential Geometry/Elementary Differential Geometry/Intrinsische Geometrie von Hyperflächen/Hyperflächengleichungen.md</guid><pubDate>Mon, 12 Aug 2024 17:05:43 GMT</pubDate></item><item><title><![CDATA[Stack - Formalismus]]></title><description><![CDATA[ 
 <br>Eingeführt in der Vorlesung Differentialgeometrie für bestimmte Tensoren zur Vermeidung der Summenschreibweise im Skript. Diese resultiert aus zweiten Ableitungen für vektorwertige <a data-tooltip-position="top" aria-label="Mathematical Relations" data-href="Mathematical Relations" href="the-guide/mathematics/general-stuff/mathematical-relations.html" class="internal-link" target="_self" rel="noopener nofollow">Funktionen</a> mehrerer Veränderlicher im Fall von <a data-href="Flächen" href="the-guide/mathematics/differential-geometry/elementary-differential-geometry/flächen.html" class="internal-link" target="_self" rel="noopener nofollow">Flächen</a>.<br><br>Stack
Eine Struktur  heißt Stack und besteht gedanklich aus 

<br> horizontalen <a data-tooltip-position="top" aria-label="Matrices" data-href="Matrices" href="the-guide/mathematics/linear-algebra/matrices.html" class="internal-link" target="_self" rel="noopener nofollow">Matrizen</a>  mit 
<br> vertikalen Vektoren  mit  

Algebra

<br>Produkt von Stack und <a data-tooltip-position="top" aria-label="Matrices" data-href="Matrices" href="the-guide/mathematics/linear-algebra/matrices.html" class="internal-link" target="_self" rel="noopener nofollow">Matrix</a>
<br>Eintragsweise (jeder Eintrag ist selbst Matrix)

<br> ist Zahl,  ist Matrix




<br>
<br>Rechenregeln

<br>Bilinearform basierend auf Stack  mit Vektor  führt auf Vektor, bei dem jeder Eintrag Ergebnis einer Bilinearform der horizontalen Matrizen ist 

<br>Matrix - Bilinearform mit Matrix ,  und  lässt sich umformen zu 


<br>Stack-Vektor Produkt mit Vektor  ergibt <a data-tooltip-position="top" aria-label="Matrices" data-href="Matrices" href="the-guide/mathematics/linear-algebra/matrices.html" class="internal-link" target="_self" rel="noopener nofollow">Matrix</a> 
<br>Mit Matrizen ... gilt 
<br>


]]></description><link>the-guide/mathematics/differential-geometry/elementary-differential-geometry/intrinsische-geometrie-von-hyperflächen/stack-formalismus.html</link><guid isPermaLink="false">The Guide/Mathematics/Differential Geometry/Elementary Differential Geometry/Intrinsische Geometrie von Hyperflächen/Stack - Formalismus.md</guid><pubDate>Mon, 12 Aug 2024 17:05:43 GMT</pubDate></item><item><title><![CDATA[Theorema Egregium]]></title><description><![CDATA[ 
 <br>Satz
Im Sonderfall von <a data-href="Hyperflächen" href="the-guide/mathematics/differential-geometry/elementary-differential-geometry/hyperflächen.html" class="internal-link" target="_self" rel="noopener nofollow">Hyperflächen</a> mit  ist die <a data-tooltip-position="top" aria-label="Hauptkrümmung und Hauptkrümmungsrichtung" data-href="Hauptkrümmung und Hauptkrümmungsrichtung" href="the-guide/mathematics/differential-geometry/elementary-differential-geometry/extrinsische-geometrie-von-hyperflächen/hauptkrümmung-und-hauptkrümmungsrichtung.html" class="internal-link" target="_self" rel="noopener nofollow">Gauß-Krümmung</a>  durch die erste <a data-tooltip-position="top" aria-label="1. Fundamentalform" data-href="1. Fundamentalform" href="the-guide/mathematics/differential-geometry/elementary-differential-geometry/1.-fundamentalform.html" class="internal-link" target="_self" rel="noopener nofollow">erste Fundamentalform</a>  bestimmt, also intrinsisch.
<br>
<br>Für diesen Fall lässt sich die rechte Seite der Gauß-Gleichungen für ,  und  zur <a data-tooltip-position="top" aria-label="Matrices" data-href="Matrices" href="the-guide/mathematics/linear-algebra/matrices.html" class="internal-link" target="_self" rel="noopener nofollow">Determinante</a> der <a data-href="2. Fundamentalform" href="the-guide/mathematics/differential-geometry/elementary-differential-geometry/2.-fundamentalform.html" class="internal-link" target="_self" rel="noopener nofollow">2. Fundamentalform</a> zusammefassen. Diese lässt sich daher über die <a data-tooltip-position="top" aria-label="Christoffel-Symbol" data-href="Christoffel-Symbol" href="the-guide/mathematics/differential-geometry/elementary-differential-geometry/intrinsische-geometrie-von-hyperflächen/christoffel-symbol.html" class="internal-link" target="_self" rel="noopener nofollow">Christoffel-Symbole</a> berechnen, womit sich die Gauß-Krümmung mittels rein intrinsischer Größen berechnen lässt.
]]></description><link>the-guide/mathematics/differential-geometry/elementary-differential-geometry/intrinsische-geometrie-von-hyperflächen/theorema-egregium.html</link><guid isPermaLink="false">The Guide/Mathematics/Differential Geometry/Elementary Differential Geometry/Intrinsische Geometrie von Hyperflächen/Theorema Egregium.md</guid><pubDate>Mon, 12 Aug 2024 17:05:43 GMT</pubDate></item><item><title><![CDATA[(Kurven)-Rahmen]]></title><description><![CDATA[ 
 <br>Hat der Normalenraum einer Kurve mehr als eine Dimension, so stellt sich die Frage nach einer geeigneten Basis dieses Raumes.<br><br>Rahmen
Sei  eine <a data-tooltip-position="top" aria-label="Raumkurven" data-href="Raumkurven" href="the-guide/mathematics/differential-geometry/elementary-differential-geometry/raumkurven.html" class="internal-link" target="_self" rel="noopener nofollow">Raumkurve</a> mit . Eine <a data-tooltip-position="top" aria-label="Mathematical Relations" data-href="Mathematical Relations" href="the-guide/mathematics/general-stuff/mathematical-relations.html" class="internal-link" target="_self" rel="noopener nofollow">Funktion</a> heißt Rahmen. Darüber hinaus gibt es eine schiefsymmetrische Ableitungs-<a data-tooltip-position="top" aria-label="Matrices" data-href="Matrices" href="the-guide/mathematics/linear-algebra/matrices.html" class="internal-link" target="_self" rel="noopener nofollow">Matrix</a> , sodass Es gilt .
<br>
<br>Beweis der Eigenschaft schiefsymmetrisch

<br>Leite Eigenschaft via Produktregel ab 
<br>Setze Definition ein 
<br>


<br>Satz über Eindeutigkeit
Gegeben sein ein , ein Intervall  und ein  und stetig, dann gibt es einen eindeutig bestimmten Rahmen  mit  udn .
<br>
<br>Beweis über Theorie für DGL

<br><a data-href="Peano Existence Theorem" href="the-guide/mathematics/differential-equations/peano-existence-theorem.html" class="internal-link" target="_self" rel="noopener nofollow">Peano Existence Theorem</a> 
<br><a data-href="Picard-Lindelöf Theorem" href="the-guide/mathematics/differential-equations/picard-lindelöf-theorem.html" class="internal-link" target="_self" rel="noopener nofollow">Picard-Lindelöf Theorem</a>


<br><br><br>Kurvenrahmen
Zu einer gegebenen <a data-href="BL-Kurve" href="the-guide/mathematics/differential-geometry/elementary-differential-geometry/lokale-kurventheorie/bl-kurve.html" class="internal-link" target="_self" rel="noopener nofollow">BL-Kurve</a>  heißt der Rahmen  Kurvenrahmen, wenn wobei  der <a data-tooltip-position="top" aria-label="Raumkurven" data-href="Raumkurven" href="the-guide/mathematics/differential-geometry/elementary-differential-geometry/raumkurven.html" class="internal-link" target="_self" rel="noopener nofollow">Tangentenvektor</a> ist. Das Paar  wird als gerahmte Kurve bezeichnet.
<br>Satz 
Bei gegebenem , , ,  und Rahmen  gibt es eine <a data-href="BL-Kurve" href="the-guide/mathematics/differential-geometry/elementary-differential-geometry/lokale-kurventheorie/bl-kurve.html" class="internal-link" target="_self" rel="noopener nofollow">BL-Kurve</a>  mit Kurvenrahmen  und 
<br>Frenet-Rahmen
Zu gegebener <a data-href="BL-Kurve" href="the-guide/mathematics/differential-geometry/elementary-differential-geometry/lokale-kurventheorie/bl-kurve.html" class="internal-link" target="_self" rel="noopener nofollow">BL-Kurve</a>  definiere man den Rahmen . Der sogenannte Frenet-Rahmen  lässt sich hieraus mittels <a data-tooltip-position="top" aria-label="QR-Decomposition" data-href="QR-Decomposition" href="the-guide/mathematics/linear-algebra/qr-decomposition.html" class="internal-link" target="_self" rel="noopener nofollow">QR-Zerlegung</a> der Form berechen. Die dazugehörige Ableitungsmatrix  hat die Form 
<br>
<br>Frenet-Rahmen nur definiert wo zweite Ableitung nicht verschwindet, empfindlich gegenüber Änderungen der Kurve, da z.B. dritte Ableitung Binormale beeinflusst.
<br><br><br>Rahmen mit der Eigenschaft möglichst wenig um  zu rotieren.<br>Paralleler Rahmen
Der Kurvenrahmen heißt parallel, wenn alle  <a data-tooltip-position="top" aria-label="Paralleles Normalfeld" data-href="Paralleles Normalfeld" href="the-guide/mathematics/differential-geometry/elementary-differential-geometry/lokale-kurventheorie/paralleles-normalfeld.html" class="internal-link" target="_self" rel="noopener nofollow">P-Felder</a> sind. Dann gilt  für  und Funktionen . Dies führt zu mit parallelen Krümmungen bezüglich  .
]]></description><link>the-guide/mathematics/differential-geometry/elementary-differential-geometry/lokale-kurventheorie/(kurven)-rahmen.html</link><guid isPermaLink="false">The Guide/Mathematics/Differential Geometry/Elementary Differential Geometry/Lokale Kurventheorie/(Kurven)-Rahmen.md</guid><pubDate>Mon, 12 Aug 2024 17:05:43 GMT</pubDate></item><item><title><![CDATA[BL-Kurve]]></title><description><![CDATA[ 
 <br>Spezialfall der Parametrisierung einer <a data-tooltip-position="top" aria-label="Raumkurven" data-href="Raumkurven" href="the-guide/mathematics/differential-geometry/elementary-differential-geometry/raumkurven.html" class="internal-link" target="_self" rel="noopener nofollow">Raumkurve</a> unter der sich viele Eigenschaften einfacher nachweisen lassen. Durch Existengarantien lässt sich dies dann auf alle Kurven der Klasse ausweiten. Basiert auf der Definition der Bogenlänge.<br>Bogenlänge
Die Bogenlänge einer <a data-tooltip-position="top" aria-label="Raumkurven" data-href="Raumkurven" href="the-guide/mathematics/differential-geometry/elementary-differential-geometry/raumkurven.html" class="internal-link" target="_self" rel="noopener nofollow">Kurve</a>  ist definiert als Sie ist <a data-tooltip-position="top" aria-label="Geometrische Größen" data-href="Geometrische Größen" href="the-guide/mathematics/differential-geometry/elementary-differential-geometry/lokale-kurventheorie/geometrische-größen.html" class="internal-link" target="_self" rel="noopener nofollow">geometrische Eigenschaft</a> der <a data-tooltip-position="top" aria-label="Raumkurven" data-href="Raumkurven" href="the-guide/mathematics/differential-geometry/elementary-differential-geometry/raumkurven.html" class="internal-link" target="_self" rel="noopener nofollow">Klasse</a> . Im orientierten Fall ist sie eindeutig bis auf Verschiebung des Intervalls.
<br><br><br>BL-Kurve
Eine <a data-tooltip-position="top" aria-label="Raumkurven" data-href="Raumkurven" href="the-guide/mathematics/differential-geometry/elementary-differential-geometry/raumkurven.html" class="internal-link" target="_self" rel="noopener nofollow">parametrisierte Kurve</a>  heißt nach Bogenlänge parametrisierte Kurve (BL-Kurve), falls Damit entspricht die Bogenlänge gerade der Intervallänge.
<br>Satz
Zu jeder <a data-tooltip-position="top" aria-label="Raumkurven" data-href="Raumkurven" href="the-guide/mathematics/differential-geometry/elementary-differential-geometry/raumkurven.html" class="internal-link" target="_self" rel="noopener nofollow">regulär parametrisierten p-Kurve</a>  gibt es eine BL-Kurve  in der <a data-tooltip-position="top" aria-label="Equivalence Relation and Class" data-href="Equivalence Relation and Class" href="the-guide/mathematics/general-stuff/equivalence-relation-and-class.html" class="internal-link" target="_self" rel="noopener nofollow">Äquivalenzklasse</a> orientierungserhaltender Umparametrisierungen . 
<br>Diese berechnet sich theoretisch (elliptische Integrale etc. möglich) über die Schritte<br>
<br>Berechne 
<br>Löse  nach  auf, erhalte 
<br>Erhalte die Umparametrisierung mit 
<br>
<br>Eigenschaften

<br>
<br>
<br>Mit <a data-href="Krümmung einer Kurve" href="the-guide/mathematics/differential-geometry/elementary-differential-geometry/lokale-kurventheorie/krümmung-einer-kurve.html" class="internal-link" target="_self" rel="noopener nofollow">Krümmung einer Kurve</a>  und  <a data-tooltip-position="top" aria-label="Raumkurven" data-href="Raumkurven" href="the-guide/mathematics/differential-geometry/elementary-differential-geometry/raumkurven.html" class="internal-link" target="_self" rel="noopener nofollow">Normalenvektor</a>  gilt 


]]></description><link>the-guide/mathematics/differential-geometry/elementary-differential-geometry/lokale-kurventheorie/bl-kurve.html</link><guid isPermaLink="false">The Guide/Mathematics/Differential Geometry/Elementary Differential Geometry/Lokale Kurventheorie/BL-Kurve.md</guid><pubDate>Sat, 05 Apr 2025 16:26:00 GMT</pubDate></item><item><title><![CDATA[Ebene Kurven]]></title><description><![CDATA[ 
 <br><a class="internal-link" data-href="Vector Space.md" href="the-guide/mathematics/general-stuff/vector-space.html" target="_self" rel="noopener nofollow"></a>&gt;[!quote] In a Nutshell<br>
Sonderfall der <a data-tooltip-position="top" aria-label="Raumkurven" data-href="Raumkurven" href="the-guide/mathematics/differential-geometry/elementary-differential-geometry/raumkurven.html" class="internal-link" target="_self" rel="noopener nofollow">Raumkurven</a> mit . Ermöglicht die Formulierung einfacher Formeln für viele Eigenschaften der Kurve. Zudem lässt sich die <a data-tooltip-position="top" aria-label="Krümmung einer Kurve" data-href="Krümmung einer Kurve" href="the-guide/mathematics/differential-geometry/elementary-differential-geometry/lokale-kurventheorie/krümmung-einer-kurve.html" class="internal-link" target="_self" rel="noopener nofollow">Krümmung</a> sehr anschaulich interpretieren.
<br><br>Tangentenvektor in 
Der Tangentenvektor  an eine Kurve  ist definiert als Er ist damit Eigenschaft von .
<br>Normalenvektor in 
Der Normalenvektor  an eine Kurve  ist definiert als Er ist damit Eigenschaft von , da der <a data-tooltip-position="top" aria-label="Raumkurven" data-href="Raumkurven" href="the-guide/mathematics/differential-geometry/elementary-differential-geometry/raumkurven.html" class="internal-link" target="_self" rel="noopener nofollow">Tangentenvektor</a> unter UP die Orientierung ändern kann. 
<br><br><br>Krümmung in 
Sei  die orientierte -Drehung Wird statt  der Raum der komplexen Zahlen gewählt, so entspricht  gerade der Multiplikation mit . Betrachtet man nun eine ebene <a data-href="BL-Kurve" href="the-guide/mathematics/differential-geometry/elementary-differential-geometry/lokale-kurventheorie/bl-kurve.html" class="internal-link" target="_self" rel="noopener nofollow">BL-Kurve</a>, so defineirt man die <a data-tooltip-position="top" aria-label="Krümmung einer Kurve" data-href="Krümmung einer Kurve" href="the-guide/mathematics/differential-geometry/elementary-differential-geometry/lokale-kurventheorie/krümmung-einer-kurve.html" class="internal-link" target="_self" rel="noopener nofollow">Krümmung einer Kurve</a>  mit dem <a data-tooltip-position="top" aria-label="Raumkurven" data-href="Raumkurven" href="the-guide/mathematics/differential-geometry/elementary-differential-geometry/raumkurven.html" class="internal-link" target="_self" rel="noopener nofollow">Normalenvektor</a>  via Mittels Umformungen folgen daraus die Formeln
<br>
<br>Eigenschaften

<br>
<br>


<br>Beispiele

<br>Der ebene Kreis hat überall die Krümmung 


<br>Vorzeichen
Anders als in höheren Dimensionen ist durch die definition über  eine negative Krümmungen möglich !
<br>Die niedrige Dimension erlaubt uns einige anschauliche Interpretationen der Krümmung mithilfe geometrischer Konstruktionen:<br><br>Schmiegekreis
Anschauung der <a data-tooltip-position="top" aria-label="Krümmung einer Kurve" data-href="Krümmung einer Kurve" href="the-guide/mathematics/differential-geometry/elementary-differential-geometry/lokale-kurventheorie/krümmung-einer-kurve.html" class="internal-link" target="_self" rel="noopener nofollow">Krümmung</a>  einer <a data-tooltip-position="top" aria-label="Ebene Kurven" data-href="Ebene Kurven" href="the-guide/mathematics/differential-geometry/elementary-differential-geometry/lokale-kurventheorie/ebene-kurven.html" class="internal-link" target="_self" rel="noopener nofollow">ebenen Kurve</a> , bei der diese als inverser Radius des Kreises gesehen wird, der an die Kurve gelegt im lokalen Punkt den selben Tangentenvektor besitzt. Der Schmiegekreis einer <a data-tooltip-position="top" aria-label="Raumkurven" data-href="Raumkurven" href="the-guide/mathematics/differential-geometry/elementary-differential-geometry/raumkurven.html" class="internal-link" target="_self" rel="noopener nofollow">Kurve</a>  im Punkt  ist definiert als und lässt sich konstruieren via

<br>Radius  über inverse <a data-tooltip-position="top" aria-label="Krümmung einer Kurve" data-href="Krümmung einer Kurve" href="the-guide/mathematics/differential-geometry/elementary-differential-geometry/lokale-kurventheorie/krümmung-einer-kurve.html" class="internal-link" target="_self" rel="noopener nofollow">Krümmung</a>
<br>Mittelpunkt 

<br><img alt="center" src="lib/media/pasted-image-20240217152025.png" style="width: 400px; max-width: 100%;"><br><br>Anschauung der <a data-tooltip-position="top" aria-label="Krümmung einer Kurve" data-href="Krümmung einer Kurve" href="the-guide/mathematics/differential-geometry/elementary-differential-geometry/lokale-kurventheorie/krümmung-einer-kurve.html" class="internal-link" target="_self" rel="noopener nofollow">Krümmung</a>  einer <a data-tooltip-position="top" aria-label="Ebene Kurven" data-href="Ebene Kurven" href="the-guide/mathematics/differential-geometry/elementary-differential-geometry/lokale-kurventheorie/ebene-kurven.html" class="internal-link" target="_self" rel="noopener nofollow">ebenen Kurve</a> , bei der die Kurve insgesamt jeweils lokal als Graph über der jeweiligen Tangentengerade  im betrachteten Punkt  aufgefasst wird. Die Krümmung ist dabei proportional zur Höhe des Graphen der Taylorapproximation zweiter Ordnung im Punkt .<br>Schmiegeparabel
 Die Schmiegeparabel (osculierende Parabel) einer <a data-tooltip-position="top" aria-label="Raumkurven" data-href="Raumkurven" href="the-guide/mathematics/differential-geometry/elementary-differential-geometry/raumkurven.html" class="internal-link" target="_self" rel="noopener nofollow">Kurve</a>  im Punkt  ist gegeben durch da . Sie ist die Taylorapproximation zweiten Grades an den Graphen im Punkt , der in der Ebene von <a data-tooltip-position="top" aria-label="Raumkurven" data-href="Raumkurven" href="the-guide/mathematics/differential-geometry/elementary-differential-geometry/raumkurven.html" class="internal-link" target="_self" rel="noopener nofollow">Tangenten- und Normalenvektor</a>  bzw.  durch die Kurve entsteht. Jede reguläre p-Kurve lässt sich auf diese Weise lokal auffassen.
<br><img alt="center" src="lib/media/pasted-image-20240217144402.png" style="width: 350px; max-width: 100%;"><br>Die <a data-tooltip-position="top" aria-label="Krümmung einer Kurve" data-href="Krümmung einer Kurve" href="the-guide/mathematics/differential-geometry/elementary-differential-geometry/lokale-kurventheorie/krümmung-einer-kurve.html" class="internal-link" target="_self" rel="noopener nofollow">Kurvenkrümmung</a> ist die zweite Ableitung des lokalen Graphen, da wobei der zweite Summand verschwindet. Im Punkt  ist , wo mit  und  die Formel der Krümmung <a data-tooltip-position="top" aria-label="Ebene Kurven" data-href="Ebene Kurven" href="the-guide/mathematics/differential-geometry/elementary-differential-geometry/lokale-kurventheorie/ebene-kurven.html" class="internal-link" target="_self" rel="noopener nofollow">ebener Kurven</a> folgt <br><br>Anschauung der <a data-tooltip-position="top" aria-label="Krümmung einer Kurve" data-href="Krümmung einer Kurve" href="the-guide/mathematics/differential-geometry/elementary-differential-geometry/lokale-kurventheorie/krümmung-einer-kurve.html" class="internal-link" target="_self" rel="noopener nofollow">Krümmung</a>  einer <a data-tooltip-position="top" aria-label="Ebene Kurven" data-href="Ebene Kurven" href="the-guide/mathematics/differential-geometry/elementary-differential-geometry/lokale-kurventheorie/ebene-kurven.html" class="internal-link" target="_self" rel="noopener nofollow">ebenen Kurve</a> , bei der diese als Änderung des Tangentenwinkels betrachtet wird.<br>Tangentenwinkel
Anwendung des <a data-tooltip-position="top" aria-label="Lifting Lemma" data-href="Lifting Lemma" href="the-guide/mathematics/differential-geometry/elementary-differential-geometry/lifting-lemma.html" class="internal-link" target="_self" rel="noopener nofollow">Lifting Lemmas</a> auf (ebene) <a data-href="BL-Kurve" href="the-guide/mathematics/differential-geometry/elementary-differential-geometry/lokale-kurventheorie/bl-kurve.html" class="internal-link" target="_self" rel="noopener nofollow">BL-Kurve</a>  mit . Laut <a data-href="Lifting Lemma" href="the-guide/mathematics/differential-geometry/elementary-differential-geometry/lifting-lemma.html" class="internal-link" target="_self" rel="noopener nofollow">Lifting Lemma</a> gibt es ein glattes  mit Diese <a data-tooltip-position="top" aria-label="Mathematical Relations" data-href="Mathematical Relations" href="the-guide/mathematics/general-stuff/mathematical-relations.html" class="internal-link" target="_self" rel="noopener nofollow">Funktion</a>  heißt Tangentenwinkel. Damit gilt weswegen wir die Krümmung als Änderung dieses Winkels erhalten . Für beliebige <a data-tooltip-position="top" aria-label="Ebene Kurven" data-href="Ebene Kurven" href="the-guide/mathematics/differential-geometry/elementary-differential-geometry/lokale-kurventheorie/ebene-kurven.html" class="internal-link" target="_self" rel="noopener nofollow">ebene Kurve</a> (oben Annahme BL) erhalten wir 
<br><img alt="center" src="lib/media/pasted-image-20240217153738.png" style="width: 450px; max-width: 100%;"><br>Da der Winkel nur bis auf Vielfache von  bestimmt ist, muss mit stetiger Fortsetzung argumentiert werden.<br>
<img alt="Pasted image 20231101094426.png" src="lib/media/pasted-image-20231101094426.png"><br><br><br>Frenet-Rahmen in 
Das Paar orthogonaler <a data-tooltip-position="top" aria-label="Vector Space" data-href="Vector Space" href="the-guide/mathematics/general-stuff/vector-space.html" class="internal-link" target="_self" rel="noopener nofollow">Vektoren</a> (<a data-tooltip-position="top" aria-label="Ebene Kurven" data-href="Ebene Kurven" href="the-guide/mathematics/differential-geometry/elementary-differential-geometry/lokale-kurventheorie/ebene-kurven.html" class="internal-link" target="_self" rel="noopener nofollow">Tangenten- und Normalenvektor</a>) heißt Frenet-<a data-tooltip-position="top" aria-label="(Kurven)-Rahmen" data-href="(Kurven)-Rahmen" href="the-guide/mathematics/differential-geometry/elementary-differential-geometry/lokale-kurventheorie/(kurven)-rahmen.html" class="internal-link" target="_self" rel="noopener nofollow">Rahmen</a> der <a data-href="BL-Kurve" href="the-guide/mathematics/differential-geometry/elementary-differential-geometry/lokale-kurventheorie/bl-kurve.html" class="internal-link" target="_self" rel="noopener nofollow">BL-Kurve</a> . Hieraus folgt die Frenet-Gleichung  mit <a data-tooltip-position="top" aria-label="Krümmung einer Kurve" data-href="Krümmung einer Kurve" href="the-guide/mathematics/differential-geometry/elementary-differential-geometry/lokale-kurventheorie/krümmung-einer-kurve.html" class="internal-link" target="_self" rel="noopener nofollow">Krümmung</a>  der ebenen Kurve. 
]]></description><link>the-guide/mathematics/differential-geometry/elementary-differential-geometry/lokale-kurventheorie/ebene-kurven.html</link><guid isPermaLink="false">The Guide/Mathematics/Differential Geometry/Elementary Differential Geometry/Lokale Kurventheorie/Ebene Kurven.md</guid><pubDate>Mon, 24 Feb 2025 23:32:25 GMT</pubDate><enclosure url="lib/media/pasted-image-20240217152025.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src=&quot;lib/media/pasted-image-20240217152025.png&quot;&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[Frenet-Kurve]]></title><description><![CDATA[ 
 <br>Definition
Sei  eine <a data-href="BL-Kurve" href="the-guide/mathematics/differential-geometry/elementary-differential-geometry/lokale-kurventheorie/bl-kurve.html" class="internal-link" target="_self" rel="noopener nofollow">BL-Kurve</a>. Dann ist Wenn  für alle , dann heißt  Frenet-Kurve. Nur für solche Kurven kann der <a data-tooltip-position="top" aria-label="Raumkurven" data-href="Raumkurven" href="the-guide/mathematics/differential-geometry/elementary-differential-geometry/raumkurven.html" class="internal-link" target="_self" rel="noopener nofollow">Hauptnormalenvektor</a> definiert werden.
]]></description><link>the-guide/mathematics/differential-geometry/elementary-differential-geometry/lokale-kurventheorie/frenet-kurve.html</link><guid isPermaLink="false">The Guide/Mathematics/Differential Geometry/Elementary Differential Geometry/Lokale Kurventheorie/Frenet-Kurve.md</guid><pubDate>Mon, 12 Aug 2024 17:05:43 GMT</pubDate></item><item><title><![CDATA[Geometrische Größen]]></title><description><![CDATA[ 
 <br>Geometrische Größen
<a data-tooltip-position="top" aria-label="Raumkurven" data-href="Raumkurven" href="the-guide/mathematics/differential-geometry/elementary-differential-geometry/raumkurven.html" class="internal-link" target="_self" rel="noopener nofollow">Eigenschaften</a> eines parametrisierten Objektes in , die parametrisierungs- und bewegungsunabhängig sind. Bewegung ist hierbei als affin-<a data-tooltip-position="top" aria-label="Mathematical Relations" data-href="Mathematical Relations" href="the-guide/mathematics/general-stuff/mathematical-relations.html" class="internal-link" target="_self" rel="noopener nofollow">lineare Abbildung</a> der Form  zu verstehen.

<br> ist Translation.
<br> ist verallgemeinerte Drehspiegelung, also Element der <a data-tooltip-position="top" aria-label="Group" data-href="Group" href="the-guide/mathematics/group-theory/group.html" class="internal-link" target="_self" rel="noopener nofollow">Gruppe</a> Sie werden Isometrien genannt und sind als abstandserhaltende Abbildungen des  charakterisiert. Will man nur orientierungstreue (auch rigid oder proper genannt) Bewegungen zulassen, so wählt man  stattdessen aus 

<br>
<br>Beispiele

<br><a data-href="Krümmung einer Kurve" href="the-guide/mathematics/differential-geometry/elementary-differential-geometry/lokale-kurventheorie/krümmung-einer-kurve.html" class="internal-link" target="_self" rel="noopener nofollow">Krümmung einer Kurve</a>
<br><a data-tooltip-position="top" aria-label="BL-Kurve" data-href="BL-Kurve" href="the-guide/mathematics/differential-geometry/elementary-differential-geometry/lokale-kurventheorie/bl-kurve.html" class="internal-link" target="_self" rel="noopener nofollow">Bogenlänge</a>
<br><a data-tooltip-position="top" aria-label="Raumkurven" data-href="Raumkurven" href="the-guide/mathematics/differential-geometry/elementary-differential-geometry/raumkurven.html" class="internal-link" target="_self" rel="noopener nofollow">Torsion</a>


]]></description><link>the-guide/mathematics/differential-geometry/elementary-differential-geometry/lokale-kurventheorie/geometrische-größen.html</link><guid isPermaLink="false">The Guide/Mathematics/Differential Geometry/Elementary Differential Geometry/Lokale Kurventheorie/Geometrische Größen.md</guid><pubDate>Mon, 12 Aug 2024 17:05:43 GMT</pubDate></item><item><title><![CDATA[Hauptsatz über Kurven]]></title><description><![CDATA[ 
 <br>Existenzsätze über <a data-tooltip-position="top" aria-label="Raumkurven" data-href="Raumkurven" href="the-guide/mathematics/differential-geometry/elementary-differential-geometry/raumkurven.html" class="internal-link" target="_self" rel="noopener nofollow">Raumkurven</a> in verschiedenen Dimensionen basierend auf gegebenen Größen wie Tangentenvektor, Krümmung, etc.<br><br>Hauptsatz über Kurven in 
Folgendes Problem ist eindeutig lösbar

<br>Gegeben

<br>Intervall 
<br>Stelle 
<br>Punkt 
<br>Tangente 
<br><a data-tooltip-position="top" aria-label="Mathematical Relations" data-href="Mathematical Relations" href="the-guide/mathematics/general-stuff/mathematical-relations.html" class="internal-link" target="_self" rel="noopener nofollow">Funktion</a> , stetig


<br>Gesucht ist <a data-href="BL-Kurve" href="the-guide/mathematics/differential-geometry/elementary-differential-geometry/lokale-kurventheorie/bl-kurve.html" class="internal-link" target="_self" rel="noopener nofollow">BL-Kurve</a>  mit

<br>
<br>
<br> als <a data-href="Krümmung einer Kurve" href="the-guide/mathematics/differential-geometry/elementary-differential-geometry/lokale-kurventheorie/krümmung-einer-kurve.html" class="internal-link" target="_self" rel="noopener nofollow">Krümmung einer Kurve</a> von 



<br>Hauptsatz über Kurven in 
Folgendes Problem ist eindeutig lösbar

<br>Gegeben

<br>Intervall 
<br>Stelle 
<br>Punkt 
<br>
<br><a data-tooltip-position="top" aria-label="Mathematical Relations" data-href="Mathematical Relations" href="the-guide/mathematics/general-stuff/mathematical-relations.html" class="internal-link" target="_self" rel="noopener nofollow">Funktion</a> , stetig
<br><a data-tooltip-position="top" aria-label="Mathematical Relations" data-href="Mathematical Relations" href="the-guide/mathematics/general-stuff/mathematical-relations.html" class="internal-link" target="_self" rel="noopener nofollow">Funktion</a> , stetig


<br>Finde eine eindeutige <a data-href="BL-Kurve" href="the-guide/mathematics/differential-geometry/elementary-differential-geometry/lokale-kurventheorie/bl-kurve.html" class="internal-link" target="_self" rel="noopener nofollow">BL-Kurve</a>  mit

<br>
<br>
<br> als Krümmung und  als Torsion



]]></description><link>the-guide/mathematics/differential-geometry/elementary-differential-geometry/lokale-kurventheorie/hauptsatz-über-kurven.html</link><guid isPermaLink="false">The Guide/Mathematics/Differential Geometry/Elementary Differential Geometry/Lokale Kurventheorie/Hauptsatz über Kurven.md</guid><pubDate>Mon, 12 Aug 2024 17:05:43 GMT</pubDate></item><item><title><![CDATA[Krümmung einer Kurve]]></title><description><![CDATA[ 
 <br>Die Krümmung einer regulären Kurve ist ein Maß dafür wie stark diese von einer Geraden abweicht.<br><br>Krümmung
Sei  eine <a data-href="BL-Kurve" href="the-guide/mathematics/differential-geometry/elementary-differential-geometry/lokale-kurventheorie/bl-kurve.html" class="internal-link" target="_self" rel="noopener nofollow">BL-Kurve</a> (, regulär) mit <a data-tooltip-position="top" aria-label="Raumkurven" data-href="Raumkurven" href="the-guide/mathematics/differential-geometry/elementary-differential-geometry/raumkurven.html" class="internal-link" target="_self" rel="noopener nofollow">Normalenvektor</a> . Die <a data-tooltip-position="top" aria-label="Mathematical Relations" data-href="Mathematical Relations" href="the-guide/mathematics/general-stuff/mathematical-relations.html" class="internal-link" target="_self" rel="noopener nofollow">Funktion</a>  mit heißt (Frenet-)Krümmung von  und  ist Krümmungsvektor. Für die Krümmung einer beliebigen regulär <a data-tooltip-position="top" aria-label="Raumkurven" data-href="Raumkurven" href="the-guide/mathematics/differential-geometry/elementary-differential-geometry/raumkurven.html" class="internal-link" target="_self" rel="noopener nofollow">param. Kurve</a>  wird mittels orientierungserhaltender Umparametrisierung  die ensprechende BL-Kurve  der Klasse  als Representant gewählt. Die Krümmung  ist eine <a data-tooltip-position="top" aria-label="Geometrische Größen" data-href="Geometrische Größen" href="the-guide/mathematics/differential-geometry/elementary-differential-geometry/lokale-kurventheorie/geometrische-größen.html" class="internal-link" target="_self" rel="noopener nofollow">geometrische Größe</a>, da sie zudem invariant unter Bewegungen ist.
<br><img alt="center" src="lib/media/pasted-image-20240221124139.png" style="width: 350px; max-width: 100%;"><br>
<br>Eigenschaften

<br>Für  ist 
<br>Für  kann die Krümmung negativ sein, da sich hier Rechts und Linkskurven sinnvoll definieren lassen. Dort gilt der Betrag nicht


]]></description><link>the-guide/mathematics/differential-geometry/elementary-differential-geometry/lokale-kurventheorie/krümmung-einer-kurve.html</link><guid isPermaLink="false">The Guide/Mathematics/Differential Geometry/Elementary Differential Geometry/Lokale Kurventheorie/Krümmung einer Kurve.md</guid><pubDate>Mon, 12 Aug 2024 17:05:43 GMT</pubDate><enclosure url="lib/media/pasted-image-20240221124139.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src=&quot;lib/media/pasted-image-20240221124139.png&quot;&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[Kurven in 3D]]></title><description><![CDATA[ 
 <br>Sonderfall der <a data-tooltip-position="top" aria-label="Raumkurven" data-href="Raumkurven" href="the-guide/mathematics/differential-geometry/elementary-differential-geometry/raumkurven.html" class="internal-link" target="_self" rel="noopener nofollow">Raumkurven</a> mit . Ermöglicht die Formulierung einfacher Formeln für bestimmte Eigenschaften der Kurve.<br><br>Tangentenvektor in 
Der Tangentenvektor  an eine Kurve  ist definiert als Er ist damit Eigenschaft von .
<br>(Haupt)-Normalenvektor
Ist eine gegebene <a data-href="BL-Kurve" href="the-guide/mathematics/differential-geometry/elementary-differential-geometry/lokale-kurventheorie/bl-kurve.html" class="internal-link" target="_self" rel="noopener nofollow">BL-Kurve</a>  eine <a data-href="Frenet-Kurve" href="the-guide/mathematics/differential-geometry/elementary-differential-geometry/lokale-kurventheorie/frenet-kurve.html" class="internal-link" target="_self" rel="noopener nofollow">Frenet-Kurve</a>, so heißt Hauptnormalenvektor der Kurve . Er ist Eigenschaft von .
<br>Binormale
Die Binormale einer <a data-href="BL-Kurve" href="the-guide/mathematics/differential-geometry/elementary-differential-geometry/lokale-kurventheorie/bl-kurve.html" class="internal-link" target="_self" rel="noopener nofollow">BL-Kurve</a>  ist definiert als Aus der zweiten Gleichung geht die Annahme . Sie ist damit Eigenschaft von .
<br>Torsion
Die Torsion  einer <a data-href="BL-Kurve" href="the-guide/mathematics/differential-geometry/elementary-differential-geometry/lokale-kurventheorie/bl-kurve.html" class="internal-link" target="_self" rel="noopener nofollow">BL-Kurve</a> in  ist gegeben durch Für eine allgemeine Kurve  berechnet sie sich nach Sie entspricht der lokalen Änderung der Ebene aus Tangente und Normale und gibt daher an, wie sehr die Kurve lokal versucht von einer <a data-tooltip-position="top" aria-label="Ebene Kurven" data-href="Ebene Kurven" href="the-guide/mathematics/differential-geometry/elementary-differential-geometry/lokale-kurventheorie/ebene-kurven.html" class="internal-link" target="_self" rel="noopener nofollow">ebenen Kurve</a> abzuweichen. Sie ist Element der Klasse  sowie <a data-tooltip-position="top" aria-label="Geometrische Größen" data-href="Geometrische Größen" href="the-guide/mathematics/differential-geometry/elementary-differential-geometry/lokale-kurventheorie/geometrische-größen.html" class="internal-link" target="_self" rel="noopener nofollow">geometrische Größe</a>.
<br>Wie bereits für die Binormale verwendet man die Annahme , die Torsion ist bei verschwindender <a data-href="Krümmung einer Kurve" href="the-guide/mathematics/differential-geometry/elementary-differential-geometry/lokale-kurventheorie/krümmung-einer-kurve.html" class="internal-link" target="_self" rel="noopener nofollow">Krümmung einer Kurve</a> nicht definiert.<br><img alt="center" src="lib/media/pasted-image-20240218125304.png" style="width: 350px; max-width: 100%;"><br>
<br>Zusammenhänge 

<br>


<br><br><br>Die <a data-href="Krümmung einer Kurve" href="the-guide/mathematics/differential-geometry/elementary-differential-geometry/lokale-kurventheorie/krümmung-einer-kurve.html" class="internal-link" target="_self" rel="noopener nofollow">Krümmung einer Kurve</a> ist gegeben durch was zur allgemeinen Formel führt. Die Krümmung ist Element der Klasse  sowie <a data-tooltip-position="top" aria-label="Geometrische Größen" data-href="Geometrische Größen" href="the-guide/mathematics/differential-geometry/elementary-differential-geometry/lokale-kurventheorie/geometrische-größen.html" class="internal-link" target="_self" rel="noopener nofollow">geometrische Größe</a>.<br><br><br>Frenet'sches Dreibein
Das Trio orthogonaler <a data-tooltip-position="top" aria-label="Vector Space" data-href="Vector Space" href="the-guide/mathematics/general-stuff/vector-space.html" class="internal-link" target="_self" rel="noopener nofollow">Vekoren</a> (<a data-tooltip-position="top" aria-label="Kurven in 3D" data-href="Kurven in 3D" href="the-guide/mathematics/differential-geometry/elementary-differential-geometry/lokale-kurventheorie/kurven-in-3d.html" class="internal-link" target="_self" rel="noopener nofollow">Tangenten-, Normalen- und Binormalenvektor</a>) heißt Frenet-Rahmen der <a data-tooltip-position="top" aria-label="Frenet-Kurve" data-href="Frenet-Kurve" href="the-guide/mathematics/differential-geometry/elementary-differential-geometry/lokale-kurventheorie/frenet-kurve.html" class="internal-link" target="_self" rel="noopener nofollow">Frenet</a> <a data-href="BL-Kurve" href="the-guide/mathematics/differential-geometry/elementary-differential-geometry/lokale-kurventheorie/bl-kurve.html" class="internal-link" target="_self" rel="noopener nofollow">BL-Kurve</a> . Hieraus folgt die Frenet-Gleichung  mit <a data-href="Krümmung einer Kurve" href="the-guide/mathematics/differential-geometry/elementary-differential-geometry/lokale-kurventheorie/krümmung-einer-kurve.html" class="internal-link" target="_self" rel="noopener nofollow">Krümmung einer Kurve</a>  und <a data-tooltip-position="top" aria-label="Kurven in 3D" data-href="Kurven in 3D" href="the-guide/mathematics/differential-geometry/elementary-differential-geometry/lokale-kurventheorie/kurven-in-3d.html" class="internal-link" target="_self" rel="noopener nofollow">Torsion</a> . 
<br>
<br>Zyklischer Zusammenhang

<br>
<br>
<br>


<br>Schmiegeebene 
]]></description><link>the-guide/mathematics/differential-geometry/elementary-differential-geometry/lokale-kurventheorie/kurven-in-3d.html</link><guid isPermaLink="false">The Guide/Mathematics/Differential Geometry/Elementary Differential Geometry/Lokale Kurventheorie/Kurven in 3D.md</guid><pubDate>Mon, 07 Apr 2025 16:05:42 GMT</pubDate><enclosure url="lib/media/pasted-image-20240218125304.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src=&quot;lib/media/pasted-image-20240218125304.png&quot;&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[Kurvenintegral]]></title><description><![CDATA[ 
 <br>Definition
Sei  eine <a data-tooltip-position="top" aria-label="Raumkurven" data-href="Raumkurven" href="the-guide/mathematics/differential-geometry/elementary-differential-geometry/raumkurven.html" class="internal-link" target="_self" rel="noopener nofollow">p-Kurve</a>. Mit einer <a data-tooltip-position="top" aria-label="Mathematical Relations" data-href="Mathematical Relations" href="the-guide/mathematics/general-stuff/mathematical-relations.html" class="internal-link" target="_self" rel="noopener nofollow">Funktion</a>  ist das Kurvenintegral definiert als Das Kurvenintegral ist <a data-tooltip-position="top" aria-label="Raumkurven" data-href="Raumkurven" href="the-guide/mathematics/differential-geometry/elementary-differential-geometry/raumkurven.html" class="internal-link" target="_self" rel="noopener nofollow">Eigenschaft</a> der <a data-tooltip-position="top" aria-label="Raumkurven" data-href="Raumkurven" href="the-guide/mathematics/differential-geometry/elementary-differential-geometry/raumkurven.html" class="internal-link" target="_self" rel="noopener nofollow">Klasse</a> . 
<br>
<br>Spezialfall 

<br>Kurvenintegral entspricht <a data-tooltip-position="top" aria-label="BL-Kurve" data-href="BL-Kurve" href="the-guide/mathematics/differential-geometry/elementary-differential-geometry/lokale-kurventheorie/bl-kurve.html" class="internal-link" target="_self" rel="noopener nofollow">Bogenlänge</a> 


<br><br><br>
<br>Das Kurvenintegral ist Eigenschaft der <a data-tooltip-position="top" aria-label="Raumkurven" data-href="Raumkurven" href="the-guide/mathematics/differential-geometry/elementary-differential-geometry/raumkurven.html" class="internal-link" target="_self" rel="noopener nofollow">Klasse</a> .

<br>

<br>...




]]></description><link>the-guide/mathematics/differential-geometry/elementary-differential-geometry/lokale-kurventheorie/kurvenintegral.html</link><guid isPermaLink="false">The Guide/Mathematics/Differential Geometry/Elementary Differential Geometry/Lokale Kurventheorie/Kurvenintegral.md</guid><pubDate>Mon, 12 Aug 2024 17:05:43 GMT</pubDate></item><item><title><![CDATA[Paralleles Normalfeld]]></title><description><![CDATA[ 
 <br>Funktion, die jedem Punkt entlang einer <a data-tooltip-position="top" aria-label="Raumkurven" data-href="Raumkurven" href="the-guide/mathematics/differential-geometry/elementary-differential-geometry/raumkurven.html" class="internal-link" target="_self" rel="noopener nofollow">Raumkurve</a> einen Vektor zuordnet, der orthogonal zum <a data-tooltip-position="top" aria-label="Raumkurven" data-href="Raumkurven" href="the-guide/mathematics/differential-geometry/elementary-differential-geometry/raumkurven.html" class="internal-link" target="_self" rel="noopener nofollow">Tangentialvektor</a> ist und sich nur in Tangentenrichtung ändern kann. Die Änderung dient damit nur dem Aufrechterhalten der Normalität<br><br>Definition
Sei  eine reguläre <a data-tooltip-position="top" aria-label="Raumkurven" data-href="Raumkurven" href="the-guide/mathematics/differential-geometry/elementary-differential-geometry/raumkurven.html" class="internal-link" target="_self" rel="noopener nofollow">Raumkurve</a> / p-Kurve. Eine <a data-tooltip-position="top" aria-label="Mathematical Relations" data-href="Mathematical Relations" href="the-guide/mathematics/general-stuff/mathematical-relations.html" class="internal-link" target="_self" rel="noopener nofollow">Funktion</a> heißt paralleles Normalfeld oder P-Feld, wenn 

<br>
<br> bzw.  für eine Funktion  

<br><img alt="Pasted image 20231103172820.png" src="lib/media/pasted-image-20231103172820.png" style="width: 250px; max-width: 100%;"><img alt="Pasted image 20231103172850.png" src="lib/media/pasted-image-20231103172850.png" style="width: 250px; max-width: 100%;"><br><br>Satz über Eindeutigkeit und Existenz
Sei  nach <a data-tooltip-position="top" aria-label="BL-Kurve" data-href="BL-Kurve" href="the-guide/mathematics/differential-geometry/elementary-differential-geometry/lokale-kurventheorie/bl-kurve.html" class="internal-link" target="_self" rel="noopener nofollow">Bogenlänge parametrisiert</a> und . Dann exisitert ein eindeutiges paralleles Normalenfeld zu jedem Anfangswert .
<br>Satz
Seien  zwei P-Felder. Dann ist  konstant. Insbesondere sind auch  konstant. 
<br>
<br>Beweis

<br>


<br>Satz
Wenn  und dann ist P ein P-feld von c.
<br>
<br>Beweis

<br>...


<br>Satz
Sei  eine <a data-href="BL-Kurve" href="the-guide/mathematics/differential-geometry/elementary-differential-geometry/lokale-kurventheorie/bl-kurve.html" class="internal-link" target="_self" rel="noopener nofollow">BL-Kurve</a>. Dann ist ein <a data-tooltip-position="top" aria-label="Paralleles Normalfeld" data-href="Paralleles Normalfeld" href="the-guide/mathematics/differential-geometry/elementary-differential-geometry/lokale-kurventheorie/paralleles-normalfeld.html" class="internal-link" target="_self" rel="noopener nofollow">P-Feld</a> von , wenn 
<br>
<br>Beweis 

<br>


]]></description><link>the-guide/mathematics/differential-geometry/elementary-differential-geometry/lokale-kurventheorie/paralleles-normalfeld.html</link><guid isPermaLink="false">The Guide/Mathematics/Differential Geometry/Elementary Differential Geometry/Lokale Kurventheorie/Paralleles Normalfeld.md</guid><pubDate>Mon, 12 Aug 2024 17:05:43 GMT</pubDate><enclosure url="lib/media/pasted-image-20231103172820.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src=&quot;lib/media/pasted-image-20231103172820.png&quot;&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[Variation einer Raumkurve]]></title><description><![CDATA[ 
 <br>Oft verwendetes Prinzip, um optimale Kurven bezüglich abgeleiteter Größen wie <a data-tooltip-position="top" aria-label="Kurvenintegral" data-href="Kurvenintegral" href="the-guide/mathematics/differential-geometry/elementary-differential-geometry/lokale-kurventheorie/kurvenintegral.html" class="internal-link" target="_self" rel="noopener nofollow">Länge</a> herzuleiten.<br><br>Definition
Sei  Vektorfeld und  eine <a data-tooltip-position="top" aria-label="Raumkurven" data-href="Raumkurven" href="the-guide/mathematics/differential-geometry/elementary-differential-geometry/raumkurven.html" class="internal-link" target="_self" rel="noopener nofollow">Kurve</a>. Dann ist eine Variation von  und  nennt man Variationsfeld.
]]></description><link>the-guide/mathematics/differential-geometry/elementary-differential-geometry/lokale-kurventheorie/variation-einer-raumkurve.html</link><guid isPermaLink="false">The Guide/Mathematics/Differential Geometry/Elementary Differential Geometry/Lokale Kurventheorie/Variation einer Raumkurve.md</guid><pubDate>Mon, 12 Aug 2024 17:05:43 GMT</pubDate></item><item><title><![CDATA[1. Fundamentalform]]></title><description><![CDATA[ 
 <br>Da wir so etwas wie eine Bogenlängenparametrisierung in den meisten Fällen nicht auf Flächen verallgemeinern können, sind wir im <a data-href="Tangentialraum" href="the-guide/mathematics/differential-geometry/riemannian-geometry/mannigfaltigkeiten-und-differenzierbare-abbildungen/tangentialraum.html" class="internal-link" target="_self" rel="noopener nofollow">Tangentialraum</a>  gezwungen mit verzerrenden Abbildungen zu arbeiten. Wir sehen die Längenverhältnisse dort als eine Eigenschaft des Parameterbereichs  an.<br>
<br>"Welche Längen etc. nimmt jemand wahr, der auf der Fläche lebt und nur deren Dimensionen wahrnehmen kann ?"
<br>Größen, die alleine durch die erste Fundamentalform bestimmt sind (z.B. Längen und Inhalte) werden als Intrinsische Größen bezeichnet. Diese kann man sich als jene Eigenschaften vorstellen, die jemand, der auf der Fläche lebt und nur bis DImension  wahrnehmen kann erkennen würden.
<br><img alt="center" src="lib/media/pasted-image-20240219095620.png"><br><br><br>Vektoren  () lassen sich bezüglich der Basis, welche der <a data-href="Tangentialraum" href="the-guide/mathematics/differential-geometry/riemannian-geometry/mannigfaltigkeiten-und-differenzierbare-abbildungen/tangentialraum.html" class="internal-link" target="_self" rel="noopener nofollow">Tangentialraum</a> liefert schreiben als Dies führt auf das Skalarprodukt , welches stetig vom Fußpunkt  abhängt. Dieses enthält Informationen über Längen, Winkel und Inhalte parametrisierter Flächen.<br>Erste Fundamentalform
Sei  <a data-tooltip-position="top" aria-label="Flächen" data-href="Flächen" href="the-guide/mathematics/differential-geometry/elementary-differential-geometry/flächen.html" class="internal-link" target="_self" rel="noopener nofollow">parametrisierte Fläche</a> und . Die Bilinearform heißt erste Fundamentalform von . Für festes  ist  ein <a data-tooltip-position="top" aria-label="Vector Space" data-href="Vector Space" href="the-guide/mathematics/general-stuff/vector-space.html" class="internal-link" target="_self" rel="noopener nofollow">Skalarprodukt</a> auf . Mit  gilt mit  (symmetrisch).
<br><br><br>
<br><a data-tooltip-position="top" aria-label="Vector Space" data-href="Vector Space" href="the-guide/mathematics/general-stuff/vector-space.html" class="internal-link" target="_self" rel="noopener nofollow">Skalarprodukt</a>

<br>Symmetrisch, da Produkt aus <a data-tooltip-position="top" aria-label="Matrices" data-href="Matrices" href="the-guide/mathematics/linear-algebra/matrices.html" class="internal-link" target="_self" rel="noopener nofollow">Matrix</a> mit transponierter immer symmetrisch
<br>Bilinearität per Konstruktion
<br><a data-tooltip-position="top" aria-label="Matrices" data-href="Matrices" href="the-guide/mathematics/linear-algebra/matrices.html" class="internal-link" target="_self" rel="noopener nofollow">Positiv Definit</a>, da für  vollen Rang angenommen (<a data-tooltip-position="top" aria-label="Flächen" data-href="Flächen" href="the-guide/mathematics/differential-geometry/elementary-differential-geometry/flächen.html" class="internal-link" target="_self" rel="noopener nofollow">Immersion</a>)


<br>Invariant unter <a data-tooltip-position="top" aria-label="Flächen" data-href="Flächen" href="the-guide/mathematics/differential-geometry/elementary-differential-geometry/flächen.html" class="internal-link" target="_self" rel="noopener nofollow">Umparametrisierungen</a>erste Fundamentalform ist Element von .
<br>Bewegungsinvarianz,  mit  (Bewegung der Fläche im Bildraum) und  haben die gleiche erste Fundamentalform
<br>Mit dieser Definition der ersten Fundamentalform lassen sich nun Eigenschaften im Parameterbereich  berechnen, da wir die Verzerrung der Parametrisierung berücksichtigen können:<br>Riemannsche Länge
Mit <a data-tooltip-position="top" aria-label="1. Fundamentalform" data-href="1. Fundamentalform" href="the-guide/mathematics/differential-geometry/elementary-differential-geometry/1.-fundamentalform.html" class="internal-link" target="_self" rel="noopener nofollow">erster Fundamentalform</a>  einer <a data-tooltip-position="top" aria-label="Flächen" data-href="Flächen" href="the-guide/mathematics/differential-geometry/elementary-differential-geometry/flächen.html" class="internal-link" target="_self" rel="noopener nofollow">Fläche</a> ist die Riemannsche Länge von  gegeben durch 
<br>Winkel im Tangentialraum
Der Winkel zwischen zwei Vektoren  im <a data-href="Tangentialraum" href="the-guide/mathematics/differential-geometry/riemannian-geometry/mannigfaltigkeiten-und-differenzierbare-abbildungen/tangentialraum.html" class="internal-link" target="_self" rel="noopener nofollow">Tangentialraum</a> () berechnet sich damit über 
<br>Oberflächeninhalt
Der Flächeninhalt einer <a data-tooltip-position="top" aria-label="Flächen" data-href="Flächen" href="the-guide/mathematics/differential-geometry/elementary-differential-geometry/flächen.html" class="internal-link" target="_self" rel="noopener nofollow">Fläche</a>  bzw von deren Spur  mit <a data-tooltip-position="top" aria-label="1. Fundamentalform" data-href="1. Fundamentalform" href="the-guide/mathematics/differential-geometry/elementary-differential-geometry/1.-fundamentalform.html" class="internal-link" target="_self" rel="noopener nofollow">erster Fundamentalform</a>  ist gegeben durch 
]]></description><link>the-guide/mathematics/differential-geometry/elementary-differential-geometry/1.-fundamentalform.html</link><guid isPermaLink="false">The Guide/Mathematics/Differential Geometry/Elementary Differential Geometry/1. Fundamentalform.md</guid><pubDate>Mon, 24 Feb 2025 23:32:25 GMT</pubDate><enclosure url="lib/media/pasted-image-20240219095620.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src=&quot;lib/media/pasted-image-20240219095620.png&quot;&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[2. Fundamentalform]]></title><description><![CDATA[ 
 <br>Ausgehend von der <a data-href="Weingarten-Abbildung" href="the-guide/mathematics/differential-geometry/elementary-differential-geometry/extrinsische-geometrie-von-hyperflächen/weingarten-abbildung.html" class="internal-link" target="_self" rel="noopener nofollow">Weingarten-Abbildung</a> für <a data-href="Hyperflächen" href="the-guide/mathematics/differential-geometry/elementary-differential-geometry/hyperflächen.html" class="internal-link" target="_self" rel="noopener nofollow">Hyperflächen</a>  erhält man mittels Multiplikation mit der <a data-tooltip-position="top" aria-label="Matrices" data-href="Matrices" href="the-guide/mathematics/linear-algebra/matrices.html" class="internal-link" target="_self" rel="noopener nofollow">Matrix</a>  Die zweite Fundamentalform beschreibt den nur von "Außen" (extrinsisch) wahrnehmbaren Anteil der Krümmung, also wie eine Fläche im Raum gekrümmt ist. Die Krümmung im Tangentialraum beschreibt die <a data-tooltip-position="top" aria-label="1. Fundamentalform" data-href="1. Fundamentalform" href="the-guide/mathematics/differential-geometry/elementary-differential-geometry/1.-fundamentalform.html" class="internal-link" target="_self" rel="noopener nofollow">erste Fundamentalform</a>.<br><br><br>Zweite Fundamentalform
Die zweite Fundamentalform einer <a data-tooltip-position="top" aria-label="Hyperflächen" data-href="Hyperflächen" href="the-guide/mathematics/differential-geometry/elementary-differential-geometry/hyperflächen.html" class="internal-link" target="_self" rel="noopener nofollow">Hyperfläche</a>  mit <a data-href="Gauß-Abbildung" href="the-guide/mathematics/differential-geometry/elementary-differential-geometry/extrinsische-geometrie-von-hyperflächen/gauß-abbildung.html" class="internal-link" target="_self" rel="noopener nofollow">Gauß-Abbildung</a>  ist die fußpunktabhängige Bilinearform In  <a data-tooltip-position="top" aria-label="Matrices" data-href="Matrices" href="the-guide/mathematics/linear-algebra/matrices.html" class="internal-link" target="_self" rel="noopener nofollow">Matrix</a>-Schreibweise entspricht dies  mit den Einträgen Beschreibt die Projektion der Änderung des Tangentialraumes auf die Normale, also wie sehr meine Fläche Normal gekrümmt gekrümmt ist (nur von "Außen" wahrnehmbar).
<br><br><br>
<br>Man kann zeigen, dass  gilt,  ist also <a data-tooltip-position="top" aria-label="Matrices" data-href="Matrices" href="the-guide/mathematics/linear-algebra/matrices.html" class="internal-link" target="_self" rel="noopener nofollow">symmetrisch</a>.
<br>Bezüglich der <a data-href="1. Fundamentalform" href="the-guide/mathematics/differential-geometry/elementary-differential-geometry/1.-fundamentalform.html" class="internal-link" target="_self" rel="noopener nofollow">1. Fundamentalform</a> und der <a data-href="Weingarten-Abbildung" href="the-guide/mathematics/differential-geometry/elementary-differential-geometry/extrinsische-geometrie-von-hyperflächen/weingarten-abbildung.html" class="internal-link" target="_self" rel="noopener nofollow">Weingarten-Abbildung</a> gilt 
<br>Bewegungsinvarianz,  mit  (Bildraum) und  haben die gleiche zweite Fundamentalform
]]></description><link>the-guide/mathematics/differential-geometry/elementary-differential-geometry/2.-fundamentalform.html</link><guid isPermaLink="false">The Guide/Mathematics/Differential Geometry/Elementary Differential Geometry/2. Fundamentalform.md</guid><pubDate>Mon, 12 Aug 2024 17:05:42 GMT</pubDate></item><item><title><![CDATA[Flächen]]></title><description><![CDATA[ 
 <br>Setting<br>
- Definitionsgebiet  mit , offen und zusammenhängend<br>
- Bildbereich  mit <br>
<img alt="center" src="lib/media/pasted-image-20240126153019.png" style="width: 450px; max-width: 100%;"><br>Parametrisierte Fläche
Eine parametrisierte -Fläche ist eine <a data-tooltip-position="top" aria-label="Mathematical Relations" data-href="Mathematical Relations" href="the-guide/mathematics/general-stuff/mathematical-relations.html" class="internal-link" target="_self" rel="noopener nofollow">Abbildung</a> wobei wir meist von Glattheit ausgehen. Man schreibt auch  um die Dimensionen zu betonen.

<br>Bildmenge  der Fläche heißt wie bei <a data-tooltip-position="top" aria-label="Raumkurven" data-href="Raumkurven" href="the-guide/mathematics/differential-geometry/elementary-differential-geometry/raumkurven.html" class="internal-link" target="_self" rel="noopener nofollow">Kurven</a> Spur

<br>Immersion
Die Fläche heißt Immersion oder regulär, wenn vollen <a data-tooltip-position="top" aria-label="Matrices" data-href="Matrices" href="the-guide/mathematics/linear-algebra/matrices.html" class="internal-link" target="_self" rel="noopener nofollow">Rang</a> hat, also  für .
<br><br><br>Umparametrisierung
Eine parametrisierte Fläche  heißt Umparametrisierung der Fläche ,wenn durch einen Diffeomorphismus erfüllt wird. Gilt darüber hinaus , so ist die Transformation orientierungstreu.
<br><br>
<br>
Orthogonal, wenn die <a data-tooltip-position="top" aria-label="1. Fundamentalform" data-href="1. Fundamentalform" href="the-guide/mathematics/differential-geometry/elementary-differential-geometry/1.-fundamentalform.html" class="internal-link" target="_self" rel="noopener nofollow">1FF</a> diagonalgestalt hat, also 
Satz
Für eine parametrisierte Fläche  und einen Punkt  gibt es eine Umgebung  und einen Diffeomorphismus , so dass  orthogonal parametrisiert ist. 


<br>
Konform / Winkelerhaltend, wenn die <a data-tooltip-position="top" aria-label="1. Fundamentalform" data-href="1. Fundamentalform" href="the-guide/mathematics/differential-geometry/elementary-differential-geometry/1.-fundamentalform.html" class="internal-link" target="_self" rel="noopener nofollow">1. FF</a> ein Vielfaches der <a data-tooltip-position="top" aria-label="Matrices" data-href="Matrices" href="the-guide/mathematics/linear-algebra/matrices.html" class="internal-link" target="_self" rel="noopener nofollow">Einheitsmatrix</a> ist 

<br>Klasse der (orientierten) -Flächen

<br>Eine -Fläche ist eine Äquivalenzklasse von regulären -Flächen unter der Relation Umparametrisierung. Wir schreiben die Klasse von  als 
<br>Eine orientierte -Fläche ist entsprechend eine Äquivalenzklasse regulärer -Flächen, wobei orientierungserhaltende Umparametrisierungen als Äquivalenzrelation verwendet werden. Wir schreiben die Klasse von  als 
<br>Eine Zahl oder Funktion  ist Eigenschaft der Klasse  bzw. , wenn  für alle  bzw. .

<br>Isometrien
Zwei parametrisierte Flächen  heißen isometrisch, wenn 

<br>ihre <a data-tooltip-position="top" aria-label="1. Fundamentalform" data-href="1. Fundamentalform" href="the-guide/mathematics/differential-geometry/elementary-differential-geometry/1.-fundamentalform.html" class="internal-link" target="_self" rel="noopener nofollow">erste Fundamentalformen</a> in jedem Punkt  übereinstimmen
<br>die <a data-href="Flächenkurve" href="the-guide/mathematics/differential-geometry/elementary-differential-geometry/flächenkurve.html" class="internal-link" target="_self" rel="noopener nofollow">Flächenkurve</a>  übereinstimmt 

<br>
<br>Intuition - Fläche sieht von innen gleich aus, kann aber verschieden eingebettet sein, bspw. Zylinder und Ebene
<br><br><br>Mit dem -ten Einheitsvektor  und zu gegebener Fläche  heißt eine <a data-tooltip-position="top" aria-label="Raumkurven" data-href="Raumkurven" href="the-guide/mathematics/differential-geometry/elementary-differential-geometry/raumkurven.html" class="internal-link" target="_self" rel="noopener nofollow">Kurve</a> der Form Parameterlinie. Die Schnittwinkel  zwischen zwei Parameterlinien hängen mit der <a data-tooltip-position="top" aria-label="1. Fundamentalform" data-href="1. Fundamentalform" href="the-guide/mathematics/differential-geometry/elementary-differential-geometry/1.-fundamentalform.html" class="internal-link" target="_self" rel="noopener nofollow">1. FF</a> zusammen <img alt="center" src="lib/media/pasted-image-20240126153555.png" style="width: 450px; max-width: 100%;"><br><br><br>Satz von Bonnet 1867
Sei  mit  einfach zusammenhängend und ein Punkt  mit

<br><a data-tooltip-position="top" aria-label="Matrices" data-href="Matrices" href="the-guide/mathematics/linear-algebra/matrices.html" class="internal-link" target="_self" rel="noopener nofollow">Symmetrischen Matrizenfunktionen</a>  und  gegeben, wobei  zusätzlich <a data-tooltip-position="top" aria-label="Matrices" data-href="Matrices" href="the-guide/mathematics/linear-algebra/matrices.html" class="internal-link" target="_self" rel="noopener nofollow">positiv definit</a> ist. Diese erfüllen zudem die <a data-tooltip-position="top" aria-label="Hyperflächengleichungen" data-href="Hyperflächengleichungen" href="the-guide/mathematics/differential-geometry/elementary-differential-geometry/intrinsische-geometrie-von-hyperflächen/hyperflächengleichungen.html" class="internal-link" target="_self" rel="noopener nofollow">Gauß- und  Codazzi-Geichungen</a>. 
<br>Anfangsdaten 

<br>
<br> (<a data-href="Gauß-Abbildung" href="the-guide/mathematics/differential-geometry/elementary-differential-geometry/extrinsische-geometrie-von-hyperflächen/gauß-abbildung.html" class="internal-link" target="_self" rel="noopener nofollow">Gauß-Abbildung</a>)
<br>, voller Rang (<a data-href="Tangentialraum" href="the-guide/mathematics/differential-geometry/riemannian-geometry/mannigfaltigkeiten-und-differenzierbare-abbildungen/tangentialraum.html" class="internal-link" target="_self" rel="noopener nofollow">Tangentialraum</a>)



Dann gibt es eine eindeutig bestimmte <a data-tooltip-position="top" aria-label="Hyperflächen" data-href="Hyperflächen" href="the-guide/mathematics/differential-geometry/elementary-differential-geometry/hyperflächen.html" class="internal-link" target="_self" rel="noopener nofollow">Hyperfläche</a>, die die oben gegebenen Daten erfüllen.
]]></description><link>the-guide/mathematics/differential-geometry/elementary-differential-geometry/flächen.html</link><guid isPermaLink="false">The Guide/Mathematics/Differential Geometry/Elementary Differential Geometry/Flächen.md</guid><pubDate>Mon, 12 Aug 2024 17:05:42 GMT</pubDate><enclosure url="lib/media/pasted-image-20240126153019.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src=&quot;lib/media/pasted-image-20240126153019.png&quot;&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[Flächenkurve]]></title><description><![CDATA[ 
 <br>Definition
Eine <a data-tooltip-position="top" aria-label="Raumkurven" data-href="Raumkurven" href="the-guide/mathematics/differential-geometry/elementary-differential-geometry/raumkurven.html" class="internal-link" target="_self" rel="noopener nofollow">Raumkurve</a> der Form  mit Kurve  im Parameterraum , die über die <a data-tooltip-position="top" aria-label="Mathematical Relations" data-href="Mathematical Relations" href="the-guide/mathematics/general-stuff/mathematical-relations.html" class="internal-link" target="_self" rel="noopener nofollow">Abbildung</a> in der <a data-tooltip-position="top" aria-label="Hyperflächen" data-href="Hyperflächen" href="the-guide/mathematics/differential-geometry/elementary-differential-geometry/hyperflächen.html" class="internal-link" target="_self" rel="noopener nofollow">Hyperfläche</a>  im  verläuft. Die <a data-tooltip-position="top" aria-label="Raumkurven" data-href="Raumkurven" href="the-guide/mathematics/differential-geometry/elementary-differential-geometry/raumkurven.html" class="internal-link" target="_self" rel="noopener nofollow">Kurve</a> wird damit beschrieben über 
<br><img alt="Pasted image 20240220085643.png" src="lib/media/pasted-image-20240220085643.png"><br>Eine Flächenkurve ist nach <a data-tooltip-position="top" aria-label="BL-Kurve" data-href="BL-Kurve" href="the-guide/mathematics/differential-geometry/elementary-differential-geometry/lokale-kurventheorie/bl-kurve.html" class="internal-link" target="_self" rel="noopener nofollow">Bogenlänge</a> parametrisiert, wenn für die <a data-tooltip-position="top" aria-label="1. Fundamentalform" data-href="1. Fundamentalform" href="the-guide/mathematics/differential-geometry/elementary-differential-geometry/1.-fundamentalform.html" class="internal-link" target="_self" rel="noopener nofollow">Riemannsche Länge</a>  and jedem Punkt gilt, also das Pendant zur <a data-tooltip-position="top" aria-label="Raumkurven" data-href="Raumkurven" href="the-guide/mathematics/differential-geometry/elementary-differential-geometry/raumkurven.html" class="internal-link" target="_self" rel="noopener nofollow">Kurve</a>. Die konkrete Bogenlänge einer Flächenkurve ist <br><br><br>Krümmungslinie
Eine Flächenkurve  heißt Krümmungslinie, wenn für alle  <a data-tooltip-position="top" aria-label="Hauptkrümmung und Hauptkrümmungsrichtung" data-href="Hauptkrümmung und Hauptkrümmungsrichtung" href="the-guide/mathematics/differential-geometry/elementary-differential-geometry/extrinsische-geometrie-von-hyperflächen/hauptkrümmung-und-hauptkrümmungsrichtung.html" class="internal-link" target="_self" rel="noopener nofollow">Hauptkrümmungsrichtung</a> ist. Die Kurve verläuft also in  stets entlang der Richtungen, die in den Bildbereich gehoben in Richtung extremaler Krümmung zeigt.
<br>
<br>Beispiele

<br>


<br>Asymptotenlinie
Eine Flächenkurve  heißt Asymptotenlinie, wenn die <a data-href="Normalkrümmung" href="the-guide/mathematics/differential-geometry/elementary-differential-geometry/extrinsische-geometrie-von-hyperflächen/normalkrümmung.html" class="internal-link" target="_self" rel="noopener nofollow">Normalkrümmung</a> für alle  verschwindet Die Projektionen der Kurve in den Bildbereich sind also stets nur in den <a data-tooltip-position="top" aria-label="Tangentialraum" data-href="Tangentialraum" href="the-guide/mathematics/differential-geometry/riemannian-geometry/mannigfaltigkeiten-und-differenzierbare-abbildungen/tangentialraum.html" class="internal-link" target="_self" rel="noopener nofollow">Tangentialraum</a> der Fläche gekrümmt.
<br>
<br>Beispiele 

<br>Oberer und unterer Berührkreis eines Torus


]]></description><link>the-guide/mathematics/differential-geometry/elementary-differential-geometry/flächenkurve.html</link><guid isPermaLink="false">The Guide/Mathematics/Differential Geometry/Elementary Differential Geometry/Flächenkurve.md</guid><pubDate>Mon, 12 Aug 2024 17:05:42 GMT</pubDate><enclosure url="lib/media/pasted-image-20240220085643.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src=&quot;lib/media/pasted-image-20240220085643.png&quot;&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[Hyperflächen]]></title><description><![CDATA[ 
 <br>Definition
<a data-tooltip-position="top" aria-label="Flächen" data-href="Flächen" href="the-guide/mathematics/differential-geometry/elementary-differential-geometry/flächen.html" class="internal-link" target="_self" rel="noopener nofollow">Parametrisierte Flächen</a> mit der besonderheit  (Kodimension ), also In diesem Sonderfall ist insbesondere das orthogonale komplement des <a data-tooltip-position="top" aria-label="Tangentialraum" data-href="Tangentialraum" href="the-guide/mathematics/differential-geometry/riemannian-geometry/mannigfaltigkeiten-und-differenzierbare-abbildungen/tangentialraum.html" class="internal-link" target="_self" rel="noopener nofollow">Tangentialraumes</a>  -Dimensional und durch die <a data-href="Gauß-Abbildung" href="the-guide/mathematics/differential-geometry/elementary-differential-geometry/extrinsische-geometrie-von-hyperflächen/gauß-abbildung.html" class="internal-link" target="_self" rel="noopener nofollow">Gauß-Abbildung</a> beschreibbar. Zudem ist die Änderung dieses Vektors durch die <a data-href="Weingarten-Abbildung" href="the-guide/mathematics/differential-geometry/elementary-differential-geometry/extrinsische-geometrie-von-hyperflächen/weingarten-abbildung.html" class="internal-link" target="_self" rel="noopener nofollow">Weingarten-Abbildung</a> in der Basis des <a data-tooltip-position="top" aria-label="Tangentialraum" data-href="Tangentialraum" href="the-guide/mathematics/differential-geometry/riemannian-geometry/mannigfaltigkeiten-und-differenzierbare-abbildungen/tangentialraum.html" class="internal-link" target="_self" rel="noopener nofollow">Tangentialraumes</a> darstellbar.
]]></description><link>the-guide/mathematics/differential-geometry/elementary-differential-geometry/hyperflächen.html</link><guid isPermaLink="false">The Guide/Mathematics/Differential Geometry/Elementary Differential Geometry/Hyperflächen.md</guid><pubDate>Mon, 12 Aug 2024 17:05:43 GMT</pubDate></item><item><title><![CDATA[Lifting Lemma]]></title><description><![CDATA[ 
 <br><img alt="center" src="lib/media/pasted-image-20240126173419.png" style="width: 550px; max-width: 100%;"><br>Lemma für Definitionsgebiet 
Gegeben sei eine -<a data-tooltip-position="top" aria-label="Mathematical Relations" data-href="Mathematical Relations" href="the-guide/mathematics/general-stuff/mathematical-relations.html" class="internal-link" target="_self" rel="noopener nofollow">Funktion</a> Dann gibt es eine -Funktion Wir schreiben  und nennen diesen Winkel den Lift von .
<br>
<br> nur bis auf Vielfache von  bestimmt
<br> kann Tangential- oder Normalenvektor <a data-tooltip-position="top" aria-label="Ebene Kurven" data-href="Ebene Kurven" href="the-guide/mathematics/differential-geometry/elementary-differential-geometry/lokale-kurventheorie/ebene-kurven.html" class="internal-link" target="_self" rel="noopener nofollow">ebener Kurven</a> sein
<br>Gilt allgemeiner , so lässt sich der Lift analog mit definieren.
<br><br><br>Lemma für Definitionsgebiet 
Gegeben sei eine -<a data-tooltip-position="top" aria-label="Mathematical Relations" data-href="Mathematical Relations" href="the-guide/mathematics/general-stuff/mathematical-relations.html" class="internal-link" target="_self" rel="noopener nofollow">Funktion</a> Dann gibt es eine -Funktion falls das Gebiet  einfach zusammenhängend ist. Wir schreiben  und nennen diesen Winkel den Lift von .
<br><br><img alt="center" src="lib/media/pasted-image-20240229083708.png" style="width: 350px; max-width: 100%;"><br>Problemfall
Da der Winkel  auf ganz  definiert sein und in  sein muss exisitert für obige Kurven kein Lift, da der Winkel springen kann. Im Fall  ist dies nicht möglich, da der Winkel nur auf diesem Intervall stetig sein muss. Wäre das Gebiet im Beispiel einfach zusammenhängend, so würde  erst garnicht existierien. 
]]></description><link>the-guide/mathematics/differential-geometry/elementary-differential-geometry/lifting-lemma.html</link><guid isPermaLink="false">The Guide/Mathematics/Differential Geometry/Elementary Differential Geometry/Lifting Lemma.md</guid><pubDate>Mon, 12 Aug 2024 17:05:43 GMT</pubDate><enclosure url="lib/media/pasted-image-20240126173419.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src=&quot;lib/media/pasted-image-20240126173419.png&quot;&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[Raumkurven]]></title><description><![CDATA[ 
 <br>Parametrisierte Kurve
Eine parametrisierte -Kurve ist eine <a data-tooltip-position="top" aria-label="Mathematical Relations" data-href="Mathematical Relations" href="the-guide/mathematics/general-stuff/mathematical-relations.html" class="internal-link" target="_self" rel="noopener nofollow">Abbildung</a>  mit Intervall  und . 

<br>Bild  heißt Spur, Objekt im Raum. 
<br>Parametrisierte Kurve heißt regulär, wenn Tangentialvektor  für kein  verschwindet (kein Richtungswechsel).
<br>Die Kurve  heißt eingebettet, wenn sie regulär und <a data-tooltip-position="top" aria-label="Mathematical Relations" data-href="Mathematical Relations" href="the-guide/mathematics/general-stuff/mathematical-relations.html" class="internal-link" target="_self" rel="noopener nofollow">injektiv</a> ist.

<br><img alt="center" src="lib/media/pasted-image-20240217104900.png" style="width: 400px; max-width: 100%;"><br>
<br>In der Vorlesung fast immer .
<br>In der Physik meist  Zeit und  Bewegung eines Objektes
<br><br><br>Parametertransformation
Eine Parametertransformation einer -Kurve ist ein -Diffeomorphismus  (<a data-tooltip-position="top" aria-label="Set" data-href="Set" href="the-guide/mathematics/general-stuff/set.html" class="internal-link" target="_self" rel="noopener nofollow">bijektive</a>, stetig diff-bare <a data-tooltip-position="top" aria-label="Mathematical Relations" data-href="Mathematical Relations" href="the-guide/mathematics/general-stuff/mathematical-relations.html" class="internal-link" target="_self" rel="noopener nofollow">Funktion</a>, deren Umkehrabbildung stetig diff-bar ist,  und  sind )  von Intervallen. Die dadurch entstehende Kurve  nennt man eine Umparametrisierung von 

<br>Ist  (dreht keine VZ), so ist die Umparametrisierung orientierungserhaltend

<br>Klasse der (orientierten) -Kurve

<br>Eine -Kurve ist eine <a data-tooltip-position="top" aria-label="Equivalence Relation and Class" data-href="Equivalence Relation and Class" href="the-guide/mathematics/general-stuff/equivalence-relation-and-class.html" class="internal-link" target="_self" rel="noopener nofollow">Äquivalenzklasse</a> von regulären -Kurven unter der Relation Umparametrisierung. Wir schreiben die Klasse von  als 
<br>Eine orientierte -Kurve ist entsprechend eine Äquivalenzklasse regulärer -Kurven, wobei orientierungserhaltende Umparametrisierungen als Äquivalenzrelation verwendet werden. Wir schreiben die Klasse von  als 
<br>Eine Zahl oder Funktion  ist Eigenschaft der Klasse  bzw. , wenn  für alle  bzw. .

<br><img alt="center" src="lib/media/pasted-image-20240217110954.png"><br>
<br>Wir wollen geometrisches Objekt losgelöst von Repräsentation oder Notation betrachten, z.B. in welcher Richtung ein Kreis durchlaufen wird oder in welchen Koordinaten er dargestellt wird.
<br>Zusätzlich wollen wir die Position im Raum außer Acht lassen, führt zu <a data-tooltip-position="top" aria-label="Geometrische Größen" data-href="Geometrische Größen" href="the-guide/mathematics/differential-geometry/elementary-differential-geometry/lokale-kurventheorie/geometrische-größen.html" class="internal-link" target="_self" rel="noopener nofollow">geometrischen Größen</a>
<br>Satz
Es gibt eine lokale orientierungserhaltende Umparametrisierung mit  und , sofern  regulär.
<br><br><br>Tangentenvektor in 
Der Tangentenvektor  an eine Kurve  ist definiert als Er ist damit Eigenschaft von .

<br>"In welche Richtung gehe ich ?"

<br><img alt="center" src="lib/media/pasted-image-20240217111914.png" style="width: 400px; max-width: 100%;"><br>
<br>Es gilt allgemein 
<br>Normalenvektor in 
Ist  konstant, bspw. bei einer regulären <a data-href="BL-Kurve" href="the-guide/mathematics/differential-geometry/elementary-differential-geometry/lokale-kurventheorie/bl-kurve.html" class="internal-link" target="_self" rel="noopener nofollow">BL-Kurve</a>, und handelt es sich um eine <a data-href="Frenet-Kurve" href="the-guide/mathematics/differential-geometry/elementary-differential-geometry/lokale-kurventheorie/frenet-kurve.html" class="internal-link" target="_self" rel="noopener nofollow">Frenet-Kurve</a>, so ist die Normale definiert als 

<br>"In welche Richtung ändert sich der Tangentialvektor ?"

<br><img alt="center" src="lib/media/pasted-image-20240217112052.png" style="width: 400px; max-width: 100%;"><br>
<br>Nur in D kann die Krümmung negativ werden, wenn der Punkt in einer Rechtskurve liegt (da die Normale positiv um  gedreht)
]]></description><link>the-guide/mathematics/differential-geometry/elementary-differential-geometry/raumkurven.html</link><guid isPermaLink="false">The Guide/Mathematics/Differential Geometry/Elementary Differential Geometry/Raumkurven.md</guid><pubDate>Sat, 05 Apr 2025 16:23:52 GMT</pubDate><enclosure url="lib/media/pasted-image-20240217104900.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src=&quot;lib/media/pasted-image-20240217104900.png&quot;&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[Satz von Gauß-Bonnet]]></title><description><![CDATA[ 
 <br><br>Satz
Für eine <a data-tooltip-position="top" aria-label="Flächen" data-href="Flächen" href="the-guide/mathematics/differential-geometry/elementary-differential-geometry/flächen.html" class="internal-link" target="_self" rel="noopener nofollow">konform parametrisierte Fläche</a>  mit daraus resultierender <a data-tooltip-position="top" aria-label="1. Fundamentalform" data-href="1. Fundamentalform" href="the-guide/mathematics/differential-geometry/elementary-differential-geometry/1.-fundamentalform.html" class="internal-link" target="_self" rel="noopener nofollow">1. FF</a> , so gilt für die <a data-tooltip-position="top" aria-label="Hauptkrümmung und Hauptkrümmungsrichtung" data-href="Hauptkrümmung und Hauptkrümmungsrichtung" href="the-guide/mathematics/differential-geometry/elementary-differential-geometry/extrinsische-geometrie-von-hyperflächen/hauptkrümmung-und-hauptkrümmungsrichtung.html" class="internal-link" target="_self" rel="noopener nofollow">Gauß-Krümmung</a> mit .
<br><br><br>Lokaler Satz von Gauß-Bonnet
Sei  eine <a data-tooltip-position="top" aria-label="Flächen" data-href="Flächen" href="the-guide/mathematics/differential-geometry/elementary-differential-geometry/flächen.html" class="internal-link" target="_self" rel="noopener nofollow">orthogonal parametrisierte Fläche</a> (lokal immer möglich). Zudem sei  eine einfach zusammenhängende kompakte Menge, die links der Randparametrisierung  (positiv orientierte, <a data-tooltip-position="top" aria-label="Geschlossene Kurve" data-href="Geschlossene Kurve" href="the-guide/mathematics/differential-geometry/elementary-differential-geometry/globale-kurven-und-flächentheorie/geschlossene-kurve.html" class="internal-link" target="_self" rel="noopener nofollow">geschlossene</a> und mindestens <a data-tooltip-position="top" aria-label="Stückweise differenzierbare Kurven" data-href="Stückweise differenzierbare Kurven" href="the-guide/mathematics/differential-geometry/elementary-differential-geometry/globale-kurven-und-flächentheorie/stückweise-differenzierbare-kurven.html" class="internal-link" target="_self" rel="noopener nofollow">stückweise glatte Kurve</a>) liegt. Dann gilt für glatte Rändermit <a data-tooltip-position="top" aria-label="Geodätische Krümmung" data-href="Geodätische Krümmung" href="the-guide/mathematics/differential-geometry/elementary-differential-geometry/intrinsische-geometrie-von-hyperflächen/geodätische-krümmung.html" class="internal-link" target="_self" rel="noopener nofollow">geodätischer Krümmung</a> und bei <a data-tooltip-position="top" aria-label="Stückweise differenzierbare Kurven" data-href="Stückweise differenzierbare Kurven" href="the-guide/mathematics/differential-geometry/elementary-differential-geometry/globale-kurven-und-flächentheorie/stückweise-differenzierbare-kurven.html" class="internal-link" target="_self" rel="noopener nofollow">stückweiser Differenzierbarkeit</a> mit den Sprungwinkeln  der Ecke  und den Teilstücken  des Randes die alternative Formulierung 
<br><img alt="center" src="lib/media/pasted-image-20240126171358.png" style="width: 450px; max-width: 100%;"><br><br><br>Sei nun mindestens  eine geschlossene  Fläche. Mittels einer globalen <a data-href="Triangulierung" href="the-guide/mathematics/differential-geometry/elementary-differential-geometry/globale-kurven-und-flächentheorie/triangulierung.html" class="internal-link" target="_self" rel="noopener nofollow">Triangulierung</a>  lassen sich Zusammenhänge der Ecken, Kanten und Flächen Herstellen und mit der <a data-href="Euler-Charakteristik" href="the-guide/mathematics/differential-geometry/elementary-differential-geometry/globale-kurven-und-flächentheorie/euler-charakteristik.html" class="internal-link" target="_self" rel="noopener nofollow">Euler-Charakteristik</a> verrechnen. Zunächst gilt mit  allgemeinen Integrationsregeln aus der lokalen Variante folgt. Hiermit lässt sich mit dem Innenwinkel   schreiben Da für jede Ecke  ein Gesamtwinkel von  gelten muss folgt Zudem gilt für die geschlossene Fläche der Zusammenhang  und damit , insgesamt alsoHinzu kommt, dass die Kanten für jedes Dreieck durchlaufen werden, also jeweils einmal in jede Richtung, da wir immer mathematisch positiv integrieren. Dadurch kürzen sich alle Wegintegrale der <a data-tooltip-position="top" aria-label="Geodätische Krümmung" data-href="Geodätische Krümmung" href="the-guide/mathematics/differential-geometry/elementary-differential-geometry/intrinsische-geometrie-von-hyperflächen/geodätische-krümmung.html" class="internal-link" target="_self" rel="noopener nofollow">geodätischen Krümmung</a> heraus. <br>Globaler Satz von Gauß-Bonnet
Damit lässt sich der Satz in seiner globalen Variante angeben. Für jede geschlossene oreintirete  Fläche  mit  gilt wobei  das Geschlecht der Fläche ist. Das Ergebnis ist unabhängig von der gewählte Triangulierung.
]]></description><link>the-guide/mathematics/differential-geometry/elementary-differential-geometry/satz-von-gauß-bonnet.html</link><guid isPermaLink="false">The Guide/Mathematics/Differential Geometry/Elementary Differential Geometry/Satz von Gauß-Bonnet.md</guid><pubDate>Mon, 12 Aug 2024 17:05:43 GMT</pubDate><enclosure url="lib/media/pasted-image-20240126171358.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src=&quot;lib/media/pasted-image-20240126171358.png&quot;&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[Bertrand - Puiseux Theorem]]></title><description><![CDATA[ 
 <br><br>Theorem
The length of an <a data-tooltip-position="top" aria-label="Geodätische" data-href="Geodätische" href="the-guide/mathematics/differential-geometry/elementary-differential-geometry/intrinsische-geometrie-von-hyperflächen/geodätische.html" class="internal-link" target="_self" rel="noopener nofollow">geodesic circle</a> of radius  around the point  on a two-dimensional <a data-tooltip-position="top" aria-label="Flächen" data-href="Flächen" href="the-guide/mathematics/differential-geometry/elementary-differential-geometry/flächen.html" class="internal-link" target="_self" rel="noopener nofollow">surface</a> / a two-dimensional <a data-tooltip-position="top" aria-label="Mannigfaltigkeiten" data-href="Mannigfaltigkeiten" href="the-guide/mathematics/differential-geometry/riemannian-geometry/mannigfaltigkeiten-und-differenzierbare-abbildungen/mannigfaltigkeiten.html" class="internal-link" target="_self" rel="noopener nofollow">manifold</a>  with <a data-tooltip-position="top" aria-label="Hauptkrümmung und Hauptkrümmungsrichtung" data-href="Hauptkrümmung und Hauptkrümmungsrichtung" href="the-guide/mathematics/differential-geometry/elementary-differential-geometry/extrinsische-geometrie-von-hyperflächen/hauptkrümmung-und-hauptkrümmungsrichtung.html" class="internal-link" target="_self" rel="noopener nofollow">Gauß-curvature</a>  is 
<br>Proof
For an  sufficiently small, we can assume . Using polar coordinates we can simply integrate and obtain a radial geodesic with constant velocity  for every fixed . We find the proof by finding the Taylor coefficients 

<br>Constant Coefficient - we can simply derive 
<br>Linear Coefficient - differentiating yields  This also means that . We compute 
<br>Quadratic Coefficient - We can use the <a data-tooltip-position="top" aria-label="Jacobi Fields" data-href="Jacobi Fields" href="the-guide/mathematics/differential-geometry/riemannian-geometry/curvature/jacobi-fields.html" class="internal-link" target="_self" rel="noopener nofollow">Jacobi ODE</a> to directly obtainThis follows from the direct relation of  to .
<br>Third-Order Coefficient - we again use the Jacobi ODE in

]]></description><link>the-guide/mathematics/differential-geometry/riemannian-geometry/curvature/bertrand-puiseux-theorem.html</link><guid isPermaLink="false">The Guide/Mathematics/Differential Geometry/Riemannian Geometry/Curvature/Bertrand - Puiseux Theorem.md</guid><pubDate>Mon, 12 Aug 2024 17:05:43 GMT</pubDate></item><item><title><![CDATA[Cartan-Hadamard Theorem]]></title><description><![CDATA[ 
 <br>In a Nutshell
Central result of Riemannian geometry, reveals that manifolds with non-positive curvature have some global simplicity associated with them. They lack complex <a data-tooltip-position="top" aria-label="Topology and Topological Space" data-href="Topology and Topological Space" href="the-guide/mathematics/topology/topology-and-topological-space.html" class="internal-link" target="_self" rel="noopener nofollow">topological</a> features such as holes or handles that can arise from different curvatures.
<br><br>Theorem - Cartan Hadamard (1928)
Let  be a <a data-tooltip-position="top" aria-label="Topology and Topological Space" data-href="Topology and Topological Space" href="the-guide/mathematics/topology/topology-and-topological-space.html" class="internal-link" target="_self" rel="noopener nofollow">complete</a> <a data-tooltip-position="top" aria-label="(Semi-) Riemannsche Metriken" data-href="(Semi-) Riemannsche Metriken" href="the-guide/mathematics/differential-geometry/riemannian-geometry/metriken-und-zusammenhänge/(semi-)-riemannsche-metriken.html" class="internal-link" target="_self" rel="noopener nofollow">Riemannian</a> <a data-tooltip-position="top" aria-label="Mannigfaltigkeiten" data-href="Mannigfaltigkeiten" href="the-guide/mathematics/differential-geometry/riemannian-geometry/mannigfaltigkeiten-und-differenzierbare-abbildungen/mannigfaltigkeiten.html" class="internal-link" target="_self" rel="noopener nofollow">manifold</a> with <a data-tooltip-position="top" aria-label="Sectional Curvature" data-href="Sectional Curvature" href="the-guide/mathematics/differential-geometry/riemannian-geometry/curvature/sectional-curvature.html" class="internal-link" target="_self" rel="noopener nofollow">sectional curvature</a>  for every . Then it holds that

<br> is a differentiable <a data-tooltip-position="top" aria-label="Überlagerung" data-href="Überlagerung" href="the-guide/mathematics/differential-geometry/riemannian-geometry/mannigfaltigkeiten-und-differenzierbare-abbildungen/überlagerung.html" class="internal-link" target="_self" rel="noopener nofollow">covering</a> for every .
<br>If  is also <a data-tooltip-position="top" aria-label="Topology and Topological Space" data-href="Topology and Topological Space" href="the-guide/mathematics/topology/topology-and-topological-space.html" class="internal-link" target="_self" rel="noopener nofollow">simply connected</a>, then  is <a data-tooltip-position="top" aria-label="Mathematical Relations" data-href="Mathematical Relations" href="the-guide/mathematics/general-stuff/mathematical-relations.html" class="internal-link" target="_self" rel="noopener nofollow">diffeomorphic</a> to .

<br>Intuition
The assumptions can be understood in the following way

<br>Non-positive curvature - ensures that geodesics will never converge, their initial separation tends to grow or stay constant
<br>Simply connected manifold - <a data-tooltip-position="top" aria-label="Mannigfaltigkeiten" data-href="Mannigfaltigkeiten" href="the-guide/mathematics/differential-geometry/riemannian-geometry/mannigfaltigkeiten-und-differenzierbare-abbildungen/mannigfaltigkeiten.html" class="internal-link" target="_self" rel="noopener nofollow">manifold</a> has no holes or handles, every loop can be continuously shrunk into a point
<br>Complete manifold - any geodesic can be extended indefinitely, no boundary points that could complicate global structure

Then the <a data-tooltip-position="top" aria-label="Hyperbolischer Raum" data-href="Hyperbolischer Raum" href="the-guide/mathematics/differential-geometry/riemannian-geometry/hyperbolischer-raum.html" class="internal-link" target="_self" rel="noopener nofollow">hyperbolic</a> space is diffeomorph to , it can be continuously transformed without tearing or glueing. The resulting global structure is somewhat simple and predictable, particularly because this also implies uniqueness of geodesics.
<br>The main point of the proof is the following Lemma:<br>Lemma - Local Isometry transmit Completeness
Let  be a <a data-tooltip-position="top" aria-label="Isometrische Mannigfaltigkeiten" data-href="Isometrische Mannigfaltigkeiten" href="the-guide/mathematics/differential-geometry/riemannian-geometry/metriken-und-zusammenhänge/isometrische-mannigfaltigkeiten.html" class="internal-link" target="_self" rel="noopener nofollow">local isometry</a> of connected manifolds. If  is complete  is an isometric <a data-tooltip-position="top" aria-label="Überlagerung" data-href="Überlagerung" href="the-guide/mathematics/differential-geometry/riemannian-geometry/mannigfaltigkeiten-und-differenzierbare-abbildungen/überlagerung.html" class="internal-link" target="_self" rel="noopener nofollow">covering</a> and  is complete as well.
<br>
<br>Lifting Geodesics and Completeness 
<br><a data-tooltip-position="top" aria-label="Mathematical Relations" data-href="Mathematical Relations" href="the-guide/mathematics/general-stuff/mathematical-relations.html" class="internal-link" target="_self" rel="noopener nofollow">Surjectivity</a> (hit every point on ) -  is open in , because it is local diffeomorphism. Also  is bounded, because  is complete. Because  is connected, it follows that 
<br>Differentiable Covering - For every  we have to find a neighborhood , such that  ist und jedes  diffeomorphism. We define ...

<br>Let  be a <a data-tooltip-position="top" aria-label="Normale Bälle" data-href="Normale Bälle" href="the-guide/mathematics/differential-geometry/riemannian-geometry/metriken-und-zusammenhänge/normale-bälle.html" class="internal-link" target="_self" rel="noopener nofollow">normal Ball</a>
<br>
<br>


<br> are disjont - 
<br> - 
<br> is Diffeomorphism - 


<br> is complete by assumption, the <a data-tooltip-position="top" aria-label="Exponentialabbildung" data-href="Exponentialabbildung" href="the-guide/mathematics/differential-geometry/riemannian-geometry/metriken-und-zusammenhänge/exponentialabbildung.html" class="internal-link" target="_self" rel="noopener nofollow">exponential map</a> is defined and local <a data-tooltip-position="top" aria-label="Differenzierbare Abbildung zwischen Mannigfaltigkeiten" data-href="Differenzierbare Abbildung zwischen Mannigfaltigkeiten" href="the-guide/mathematics/differential-geometry/riemannian-geometry/mannigfaltigkeiten-und-differenzierbare-abbildungen/differenzierbare-abbildung-zwischen-mannigfaltigkeiten.html" class="internal-link" target="_self" rel="noopener nofollow">Diffeomorphism</a>.  is a Riemannian manifold if we use the pullback-metricThis way the map  is a local isometry. To use the Lemma above and show that  is a covering, we only have to show that  is complete.<br>Straight lines through the origin of  are mapped to geodesics through  by . We know from above that  is a local isometry, the straight lines are locally geodesics in . Because they are parametrized by arc length and  is complete (infinite length), the lines have infinite length in  as well. The exponential map is thereby defined on , which <a data-tooltip-position="top" aria-label="Riemannsche Mannigfaltigkeiten als Metrische Räume" data-href="Riemannsche Mannigfaltigkeiten als Metrische Räume" href="the-guide/mathematics/differential-geometry/riemannian-geometry/metriken-und-zusammenhänge/riemannsche-mannigfaltigkeiten-als-metrische-räume.html" class="internal-link" target="_self" rel="noopener nofollow">we know to yield completeness</a>.<br>With this, the first part of the theorem follows from the above Lemma (we do not actually want the metric for that, we want exp to start in  with the standard metric, so we can forget it). <br>Since  is complete, we can use that every <a data-tooltip-position="top" aria-label="Überlagerung" data-href="Überlagerung" href="the-guide/mathematics/differential-geometry/riemannian-geometry/mannigfaltigkeiten-und-differenzierbare-abbildungen/überlagerung.html" class="internal-link" target="_self" rel="noopener nofollow">covering of a complete manifold is bijective</a> to obtain the second point.]]></description><link>the-guide/mathematics/differential-geometry/riemannian-geometry/curvature/cartan-hadamard-theorem.html</link><guid isPermaLink="false">The Guide/Mathematics/Differential Geometry/Riemannian Geometry/Curvature/Cartan-Hadamard Theorem.md</guid><pubDate>Wed, 23 Apr 2025 08:34:33 GMT</pubDate></item><item><title><![CDATA[Jacobi Fields]]></title><description><![CDATA[ 
 <br>In a Nutshell
<a data-tooltip-position="top" aria-label="Vektorfelder auf Mannigfaltigkeiten" data-href="Vektorfelder auf Mannigfaltigkeiten" href="the-guide/mathematics/differential-geometry/riemannian-geometry/mannigfaltigkeiten-und-differenzierbare-abbildungen/vektorfelder-auf-mannigfaltigkeiten.html" class="internal-link" target="_self" rel="noopener nofollow">Vector fields</a> along <a data-tooltip-position="top" aria-label="Exponentialabbildung" data-href="Exponentialabbildung" href="the-guide/mathematics/differential-geometry/riemannian-geometry/metriken-und-zusammenhänge/exponentialabbildung.html" class="internal-link" target="_self" rel="noopener nofollow">geodesics</a> that describe infinitesimal variations by other geodesics. These special constructions allow us to extend our knowledge from the <a data-href="Gauß Lemma" href="the-guide/mathematics/differential-geometry/riemannian-geometry/metriken-und-zusammenhänge/gauß-lemma.html" class="internal-link" target="_self" rel="noopener nofollow">Gauß Lemma</a> to understand how geodesics spread apart.<br>
The resulting field describes the first-order distance of neighboring geodesics. 
<br><br>Definition - Jacobi Field
Let  and  with  be a geodesic running through . A Jacobi Field  with initial conditions  and  is a <a data-tooltip-position="top" aria-label="Calculus of Variations" data-href="Calculus of Variations" href="the-guide/mathematics/functional-analysis-and-calculus-of-variations/calculus-of-variations.html" class="internal-link" target="_self" rel="noopener nofollow">variation</a> of making sure that every variation is a geodesic as well. This yields the field with  the <a data-tooltip-position="top" aria-label="Pushforward Operator on Manifolds" data-href="Pushforward Operator on Manifolds" href="the-guide/mathematics/differential-geometry/riemannian-geometry/mannigfaltigkeiten-und-differenzierbare-abbildungen/pushforward-operator-on-manifolds.html" class="internal-link" target="_self" rel="noopener nofollow">differential</a> of the <a data-tooltip-position="top" aria-label="Exponentialabbildung" data-href="Exponentialabbildung" href="the-guide/mathematics/differential-geometry/riemannian-geometry/metriken-und-zusammenhänge/exponentialabbildung.html" class="internal-link" target="_self" rel="noopener nofollow">exponential map</a>.
<br>
<br>There is a formula for the general case  
<br>Theorem - ""
The variational field above satisfies the Jacobi <a data-tooltip-position="top" aria-label="ODEs - Ordinary Differential Equations" data-href="ODEs - Ordinary Differential Equations" href="the-guide/mathematics/differential-equations/odes-ordinary-differential-equations.html" class="internal-link" target="_self" rel="noopener nofollow">ODE</a> where  denotes the <a data-href="Riemann Curvature Tensor" href="the-guide/mathematics/differential-geometry/riemannian-geometry/curvature/riemann-curvature-tensor.html" class="internal-link" target="_self" rel="noopener nofollow">Riemann Curvature Tensor</a>.
<br>Proof
We can simply compute the second <a data-tooltip-position="top" aria-label="Kovariante Ableitung" data-href="Kovariante Ableitung" href="the-guide/mathematics/differential-geometry/riemannian-geometry/metriken-und-zusammenhänge/kovariante-ableitung.html" class="internal-link" target="_self" rel="noopener nofollow">covariant derivative</a> of . We first use the <a data-tooltip-position="top" aria-label="Kovariante Ableitung" data-href="Kovariante Ableitung" href="the-guide/mathematics/differential-geometry/riemannian-geometry/metriken-und-zusammenhänge/kovariante-ableitung.html" class="internal-link" target="_self" rel="noopener nofollow">covariant Schwartz theorem</a> inSwitching two covariant derivatives yields a term with the <a data-tooltip-position="top" aria-label="Riemann Curvature Tensor" data-href="Riemann Curvature Tensor" href="the-guide/mathematics/differential-geometry/riemannian-geometry/curvature/riemann-curvature-tensor.html" class="internal-link" target="_self" rel="noopener nofollow">curvature tensor</a>We based our construction of  on the fact that  is always a geodesic, which is why the first term on the right vanishes.
<br>Additional Information
The <a data-tooltip-position="top" aria-label="ODEs - Ordinary Differential Equations" data-href="ODEs - Ordinary Differential Equations" href="the-guide/mathematics/differential-equations/odes-ordinary-differential-equations.html" class="internal-link" target="_self" rel="noopener nofollow">ODE</a> above can be obtained by linearizing the ODE for geodesics using the definition of the <a data-tooltip-position="top" aria-label="Riemann Curvature Tensor" data-href="Riemann Curvature Tensor" href="the-guide/mathematics/differential-geometry/riemannian-geometry/curvature/riemann-curvature-tensor.html" class="internal-link" target="_self" rel="noopener nofollow">curvature tensor</a> and the <a data-tooltip-position="top" aria-label="Hyperflächengleichungen" data-href="Hyperflächengleichungen" href="the-guide/mathematics/differential-geometry/elementary-differential-geometry/intrinsische-geometrie-von-hyperflächen/hyperflächengleichungen.html" class="internal-link" target="_self" rel="noopener nofollow">Gauß equations</a>.
<br>
<br>Examples

<br>Reparametrization - for any geodesic, we can derive very simple Jacobi fields via reparametrizing (translation and scaling) 
<br>Euclidean Space - in  every straight line is a geodesic, we look at  through the origin. It holds that , we can take any 
<br>Sphere - In , we can derive an explicit formula in using . With an additional , this yields 
<br><a data-tooltip-position="top" aria-label="Hyperbolischer Raum" data-href="Hyperbolischer Raum" href="the-guide/mathematics/differential-geometry/riemannian-geometry/hyperbolischer-raum.html" class="internal-link" target="_self" rel="noopener nofollow">Hyperbolic space</a> - using the hyperboloid model and , we can use a unit vector  and the geodesicsto obtain 


<br>Lemma
The Jacobi ODE is a second order system with unique solution given initial values  and . The <a data-tooltip-position="top" aria-label="Space" data-href="Space" href="the-guide/mathematics/general-stuff/space.html" class="internal-link" target="_self" rel="noopener nofollow">space</a> of Jacobi fields  along a geodesic  is -dimensional.
<br>
<br>Choice of two -dimensional vectors determines unique Jacobi field, therefore -dimensional
<br>Proof
Local representation of a vector field using ONB basis  obtained via <a data-tooltip-position="top" aria-label="Parallele Vektorfelder und Parallelverschiebung" data-href="Parallele Vektorfelder und Parallelverschiebung" href="the-guide/mathematics/differential-geometry/riemannian-geometry/metriken-und-zusammenhänge/parallele-vektorfelder-und-parallelverschiebung.html" class="internal-link" target="_self" rel="noopener nofollow">parallel transport</a>. By that, only  changes tangentially, which is why the covariant derivative only acts on these coefficients. The curvature tensor is multilinear, we can insert our vector field and obtainwhere all coefficients are smooth. We can combine the derivatives with this and the fact that the basis is linear independent to obtain a system of  linear ODEs with smooth coefficienstUnique solution for this follows from <a data-href="Picard-Lindelöf Theorem" href="the-guide/mathematics/differential-equations/picard-lindelöf-theorem.html" class="internal-link" target="_self" rel="noopener nofollow">Picard-Lindelöf Theorem</a> and <a data-href="Peano Existence Theorem" href="the-guide/mathematics/differential-equations/peano-existence-theorem.html" class="internal-link" target="_self" rel="noopener nofollow">Peano Existence Theorem</a>.
<br>We now also show that the inverted direction, namely that any solution to the Jacobi ODE has to be a <a data-tooltip-position="top" aria-label="Calculus of Variations" data-href="Calculus of Variations" href="the-guide/mathematics/functional-analysis-and-calculus-of-variations/calculus-of-variations.html" class="internal-link" target="_self" rel="noopener nofollow">variation</a> of a geodesic by geodesics:<br>Theorem - ""
Given a geodesic  and . The solution to the Jacobi ODE with initial conditions  and  is the <a data-tooltip-position="top" aria-label="Variation der Energie" data-href="Variation der Energie" href="the-guide/mathematics/differential-geometry/riemannian-geometry/variation-der-energie.html" class="internal-link" target="_self" rel="noopener nofollow">variation field</a>and it holds that 
<br>Proof
We only show the proof for the important special case of , but there are extensions to the general setting. The only thing that is not trivial is , which follows fromSince we are stationary in the tangent space at , the covariant derivative of  is the regular derivative, which gives the result. 
<br>With this result, we can state that which extends our knowledge about the behavior of geodesics from the <a data-href="Gauß Lemma" href="the-guide/mathematics/differential-geometry/riemannian-geometry/metriken-und-zusammenhänge/gauß-lemma.html" class="internal-link" target="_self" rel="noopener nofollow">Gauß Lemma</a>. Additionally, we now have a differential equation that has the <a data-tooltip-position="top" aria-label="Riemann Curvature Tensor" data-href="Riemann Curvature Tensor" href="the-guide/mathematics/differential-geometry/riemannian-geometry/curvature/riemann-curvature-tensor.html" class="internal-link" target="_self" rel="noopener nofollow">curvature</a> tensor as an coefficient. If we know geodesics and <a data-tooltip-position="top" aria-label="Kovariante Ableitung" data-href="Kovariante Ableitung" href="the-guide/mathematics/differential-geometry/riemannian-geometry/metriken-und-zusammenhänge/kovariante-ableitung.html" class="internal-link" target="_self" rel="noopener nofollow">covariant derivatives</a> on a space, we can now explicitly compute the curvature.<br><br><br>In these special cases, we can derive explicit formulas. This follows from the special representation of the <a data-tooltip-position="top" aria-label="Riemann Curvature Tensor" data-href="Riemann Curvature Tensor" href="the-guide/mathematics/differential-geometry/riemannian-geometry/curvature/riemann-curvature-tensor.html" class="internal-link" target="_self" rel="noopener nofollow">curvature tensor</a> in this case (<a data-tooltip-position="top" aria-label="Riemann Curvature Tensor > Constant Curvature of Dimension 2" data-href="Riemann Curvature Tensor#Constant Curvature of Dimension 2" href="the-guide/mathematics/differential-geometry/riemannian-geometry/curvature/riemann-curvature-tensor.html#Constant_Curvature_of_Dimension_2" class="internal-link" target="_self" rel="noopener nofollow">Theorem</a>). We obtain <br>
<br> - using the explicit formulas we derived above, we can use the formula for the <a data-tooltip-position="top" aria-label="Sectional Curvature" data-href="Sectional Curvature" href="the-guide/mathematics/differential-geometry/riemannian-geometry/curvature/sectional-curvature.html" class="internal-link" target="_self" rel="noopener nofollow">sectional curvature</a> with  and  to derive For the first equality, we used the fact that  is g-orthogonal to . The  vanishes because by defining the geodesic over  we implicitly assumed <a data-tooltip-position="top" aria-label="BL-Kurve" data-href="BL-Kurve" href="the-guide/mathematics/differential-geometry/elementary-differential-geometry/lokale-kurventheorie/bl-kurve.html" class="internal-link" target="_self" rel="noopener nofollow">arc-length</a> parametrization. The  follows from the fact that  is tangential to  at any point and therefore 
<br> - for the hyperboloid model of ,we can also use the explicit formulas we derived above. The <a data-tooltip-position="top" aria-label="Sectional Curvature" data-href="Sectional Curvature" href="the-guide/mathematics/differential-geometry/riemannian-geometry/curvature/sectional-curvature.html" class="internal-link" target="_self" rel="noopener nofollow">sectional curvature</a> with  and  now is For the first equality, we used the fact that  is g-orthogonal to . The  vanishes because by defining the geodesic over  we implicitly assumed <a data-tooltip-position="top" aria-label="BL-Kurve" data-href="BL-Kurve" href="the-guide/mathematics/differential-geometry/elementary-differential-geometry/lokale-kurventheorie/bl-kurve.html" class="internal-link" target="_self" rel="noopener nofollow">arc-length</a> parametrization. The  follows from the fact that  is tangential to  at any point and therefore 
<br>In general, using the initial conditions and their <a data-tooltip-position="top" aria-label="Paralleles Normalfeld" data-href="Paralleles Normalfeld" href="the-guide/mathematics/differential-geometry/elementary-differential-geometry/lokale-kurventheorie/paralleles-normalfeld.html" class="internal-link" target="_self" rel="noopener nofollow">parallel continuations</a>  and , we arrive at  ]]></description><link>the-guide/mathematics/differential-geometry/riemannian-geometry/curvature/jacobi-fields.html</link><guid isPermaLink="false">The Guide/Mathematics/Differential Geometry/Riemannian Geometry/Curvature/Jacobi Fields.md</guid><pubDate>Wed, 22 Jan 2025 10:40:46 GMT</pubDate></item><item><title><![CDATA[Konjugierte Punkte und Konjugierter Ort]]></title><description><![CDATA[ 
 <br>Definition
Sei  <a data-href="Geodätische" href="the-guide/mathematics/differential-geometry/elementary-differential-geometry/intrinsische-geometrie-von-hyperflächen/geodätische.html" class="internal-link" target="_self" rel="noopener nofollow">Geodätische</a>. Zwei Parameter  sind konjugiert zueinander längs , wenn es ein nicht-triviales <a data-tooltip-position="top" aria-label="Jacobi Fields" data-href="Jacobi Fields" href="the-guide/mathematics/differential-geometry/riemannian-geometry/curvature/jacobi-fields.html" class="internal-link" target="_self" rel="noopener nofollow">Jacobi-Feld</a> längs  gibt mit . Die maximale Anzahl linear unabhängiger Felder mit dieser Eigenschaft wird als Vielfachheit bezeichnet. 
<br>
<br>
Die Vielfachheit kann maximal  sein, da der Raum der Jacobi-Felder mit festen Startwert -dimensional ist, die Felder in Richtung von  aber keine weiteren Nullstellen haben.

<br>
Beispiele

<br>In  sind immer  und  konjugiert mit Vielfachheit 


<br>Die obige Aussage lässt sich auch über kritische Punkte der <a data-href="Exponentialabbildung" href="the-guide/mathematics/differential-geometry/riemannian-geometry/metriken-und-zusammenhänge/exponentialabbildung.html" class="internal-link" target="_self" rel="noopener nofollow">Exponentialabbildung</a> definieren:<br>Satz - Konjugierte Punkte über Exponentialabbildung
Ist obige Geodätische auf dem Intervall  mit  definiert, so ist 

<br> konjugiert zu  genau dann, wenn  den kritischen Punkt ( hat nicht vollen Rang)  hat
<br>Die Vielfachheit von  ist die Dimension des Kerns von .

<br>Ist  konjugiert zu  längs , so muss es nicht unbedingt mehrere Geodätische zwischen diesen Punkten geben, da das Jacobi-Feld Abstände nur in erster Ordnung misst !<br>
Es zeigt nur, dass es eine Schar Geodätischer existiert, die  in den (benachbarten) Punkten  schneiden, sodass alle  für . <br>Erster konjugierter Punkt und konjugierter Ort
Ist ein Punkt  der einzige konjugierte Punkt zu , so bezeichnet man ihn als ersten konjugierten Punkt.<br>
Die Menge aller Punkte, die für einen Startpunkt  über eine Geodätische erster konjugierter Punkt dieses Punktes sind ist der konjugierte Ort.
<br><br><br>Theorem - Geodätische ohne konjugierte Punkte minimiert Energie
Sei  Geodätische ohne konjugierte Punkte und  eigentliche <a data-tooltip-position="top" aria-label="Variation der Energie" data-href="Variation der Energie" href="the-guide/mathematics/differential-geometry/riemannian-geometry/variation-der-energie.html" class="internal-link" target="_self" rel="noopener nofollow">Variation</a> mit  für . Dann gibt es ein , sodass  für alle . 
<br>Proof

<br>Beginne bei Strahl in , hat vollen Rang weil keine konjugierten Punkte
<br>Jeder Punkt auf Strahl hat Umgebung, in der Exponentialabbildung lokaler Diffeomorphismus ist
<br>Kompaktheit des Intervalls bedeutet wir können Umgebung des Strahles mit Unterteilung konstruieren
<br>Für kleine  können wir in dieser Umgebung variierte Kurven liften
<br>Damit ist nach den selben Argumenten wie bei radialen Geodätischen deren Energie für nicht-triviale Variationen aber immer größer 

<br>Für eine Metrik sind alle Geodätischen frei von konjugierten Punkten !<br><br><br>Man kann mit dem konjugierten Ort und der Annahme nicht-positiver Krümmung leicht eine sehr starke Aussage zeigen:<br>Lemma
Ist für eine Riemannsche Mannigfaltigkeit  mit nicht-positiver <a data-tooltip-position="top" aria-label="Sectional Curvature" data-href="Sectional Curvature" href="the-guide/mathematics/differential-geometry/riemannian-geometry/curvature/sectional-curvature.html" class="internal-link" target="_self" rel="noopener nofollow">Schnittkrümmung</a>. Dann ist der konjugierte Ort überall leer (keine konjugierten Punkte) und die <a data-href="Exponentialabbildung" href="the-guide/mathematics/differential-geometry/riemannian-geometry/metriken-und-zusammenhänge/exponentialabbildung.html" class="internal-link" target="_self" rel="noopener nofollow">Exponentialabbildung</a> wo definiert lokaler Diffeomorphismus.
<br>
<br>Unterschied zu <a data-tooltip-position="top" aria-label="Exponentialabbildung" data-href="Exponentialabbildung" href="the-guide/mathematics/differential-geometry/riemannian-geometry/metriken-und-zusammenhänge/exponentialabbildung.html" class="internal-link" target="_self" rel="noopener nofollow">lokaler Existenz</a>:
<br>Proof
Man kann nachrechnen Das lässt sich mit der Definition der <a data-tooltip-position="top" aria-label="Sectional Curvature" data-href="Sectional Curvature" href="the-guide/mathematics/differential-geometry/riemannian-geometry/curvature/sectional-curvature.html" class="internal-link" target="_self" rel="noopener nofollow">Schnittkrümmung</a> weiter Umschreiben wobei die Schranke aus der Krümmung und nicht-Negativität des Nenners folgt (Flächeninhalt). Damit ist  <a data-tooltip-position="top" aria-label="Convexity" data-href="Convexity" href="the-guide/mathematics/optimization/convex-optimization-lecture/convexity.html" class="internal-link" target="_self" rel="noopener nofollow">konvex</a> und im Riemannschen Fall immer positiv. Ist also irgendein weiterer Punkt als  Null, so muss die gesamte Variation verschwinden. 
]]></description><link>the-guide/mathematics/differential-geometry/riemannian-geometry/curvature/konjugierte-punkte-und-konjugierter-ort.html</link><guid isPermaLink="false">The Guide/Mathematics/Differential Geometry/Riemannian Geometry/Curvature/Konjugierte Punkte und Konjugierter Ort.md</guid><pubDate>Mon, 12 Aug 2024 17:05:43 GMT</pubDate></item><item><title><![CDATA[Musikalischer Isomorphismus]]></title><description><![CDATA[ 
 <br>Quote
Definiert Isomoprhismus zwischen Tangentialvektoren und -Formen für Mannigfaltigkeiten.
<br><br>Man bezeichnet -lineare Abbildungen   als <a data-tooltip-position="top" aria-label="Differential Form" data-href="Differential Form" href="the-guide/mathematics/general-stuff/differential-form.html" class="internal-link" target="_self" rel="noopener nofollow">1-Formen</a>, ihre Menge sei . <br>Die per Definition <a data-tooltip-position="top" aria-label="(Semi-) Riemannsche Metriken" data-href="(Semi-) Riemannsche Metriken" href="the-guide/mathematics/differential-geometry/riemannian-geometry/metriken-und-zusammenhänge/(semi-)-riemannsche-metriken.html" class="internal-link" target="_self" rel="noopener nofollow">nicht-ausgeartete Bilinearform</a>  induziert einen kanonischen <a data-tooltip-position="top" aria-label="Mathematical Relations" data-href="Mathematical Relations" href="the-guide/mathematics/general-stuff/mathematical-relations.html" class="internal-link" target="_self" rel="noopener nofollow">Isomorphismus</a> zwischen dem Vektorraum  und seinem Dualraum .<br>Definition
Sei  <a data-tooltip-position="top" aria-label="(Semi-) Riemannsche Metriken" data-href="(Semi-) Riemannsche Metriken" href="the-guide/mathematics/differential-geometry/riemannian-geometry/metriken-und-zusammenhänge/(semi-)-riemannsche-metriken.html" class="internal-link" target="_self" rel="noopener nofollow">semi-Riemannsche Metrik</a> auf einer <a data-tooltip-position="top" aria-label="Mannigfaltigkeiten" data-href="Mannigfaltigkeiten" href="the-guide/mathematics/differential-geometry/riemannian-geometry/mannigfaltigkeiten-und-differenzierbare-abbildungen/mannigfaltigkeiten.html" class="internal-link" target="_self" rel="noopener nofollow">Mannigfaltigkeit</a> . Dann ist das sogenannte Index senken durch ein  linearer <a data-tooltip-position="top" aria-label="Mathematical Relations" data-href="Mathematical Relations" href="the-guide/mathematics/general-stuff/mathematical-relations.html" class="internal-link" target="_self" rel="noopener nofollow">Isomorphismus</a>, bei dem die Koordinaten des Vektors  zu denen der 1-Form gemacht werden. Analog definiert man das Index heben via 
<br>Proof
- Linearität und Homomorphismus sind unmittelbar aus der Definition klar. Es bleibt die Bijektivität zu zeigen.

<br>Injektivität - Man nehme an , erfüllen  für alle . Aus der Bilinearität folgt, dass  gelten muss. Da  zudem nicht ausgeartet ist, muss  gelten, es gibt also für jeden getroffenen output (-Form) nur maximal einen Input.
<br>Surjektivität - Wir können eine -Form über die Komponenten von  ausdrücken. Bezüglich einer Karte  in Standardbasis und einem darin dargestellten Tangentialvektor  gilt Da  nicht ausgeartet ist, lässt sich  zu  invertieren, was die äquivalente Gleichung liefert. Damit ist gezeigt, dass jede -Form getroffen wird. Die Unabhängigkeit von der gewählten Karte folgt aus der Eindeutigkeit von .

]]></description><link>the-guide/mathematics/differential-geometry/riemannian-geometry/curvature/musikalischer-isomorphismus.html</link><guid isPermaLink="false">The Guide/Mathematics/Differential Geometry/Riemannian Geometry/Curvature/Musikalischer Isomorphismus.md</guid><pubDate>Mon, 12 Aug 2024 17:05:43 GMT</pubDate></item><item><title><![CDATA[Ricci- and Scalar Curvature]]></title><description><![CDATA[ 
 <br>In a Nutshell
In Riemannian geoemtry, an intrinsic notion of curvature is achieved by <a data-tooltip-position="top" aria-label="Tensors" data-href="Tensors" href="the-guide/mathematics/general-stuff/tensors.html" class="internal-link" target="_self" rel="noopener nofollow">contraction of a tensor</a>. The following is for the Riemannian case, for <a data-tooltip-position="top" aria-label="(Semi-) Riemannsche Metriken" data-href="(Semi-) Riemannsche Metriken" href="the-guide/mathematics/differential-geometry/riemannian-geometry/metriken-und-zusammenhänge/(semi-)-riemannsche-metriken.html" class="internal-link" target="_self" rel="noopener nofollow">semi-Riemannian</a> <a data-tooltip-position="top" aria-label="Mannigfaltigkeiten" data-href="Mannigfaltigkeiten" href="the-guide/mathematics/differential-geometry/riemannian-geometry/mannigfaltigkeiten-und-differenzierbare-abbildungen/mannigfaltigkeiten.html" class="internal-link" target="_self" rel="noopener nofollow">manifolds</a>, the signs have to be adapted.
<br><br>Ricci-Tensor
The Ricci-<a data-tooltip-position="top" aria-label="Tensors" data-href="Tensors" href="the-guide/mathematics/general-stuff/tensors.html" class="internal-link" target="_self" rel="noopener nofollow">Tensor</a> is a symmetric -tensor where  is an orthonormal basis that is preserved by <a data-tooltip-position="top" aria-label="Parallele Vektorfelder und Parallelverschiebung" data-href="Parallele Vektorfelder und Parallelverschiebung" href="the-guide/mathematics/differential-geometry/riemannian-geometry/metriken-und-zusammenhänge/parallele-vektorfelder-und-parallelverschiebung.html" class="internal-link" target="_self" rel="noopener nofollow">parallel transport</a>. Locally, we can write .
<br>Scalar Curvature
The scalar curvature  is defined as again using two orthonormal bases.
<br>Scalar Curvature as Growth of Spheres
The scalar curvature shows up in the expansion rate of the volume of a <a data-tooltip-position="top" aria-label="Normale Bälle" data-href="Normale Bälle" href="the-guide/mathematics/differential-geometry/riemannian-geometry/metriken-und-zusammenhänge/normale-bälle.html" class="internal-link" target="_self" rel="noopener nofollow">normal sphere</a>
<br>Ricci Curvature
For the Riemannian case, we can define the Ricci Curvature in direction  via 
<br>Ricci Curvature as Cone Volume
Describes the rate of volume growth for cones with axis .
]]></description><link>the-guide/mathematics/differential-geometry/riemannian-geometry/curvature/ricci-and-scalar-curvature.html</link><guid isPermaLink="false">The Guide/Mathematics/Differential Geometry/Riemannian Geometry/Curvature/Ricci- and Scalar Curvature.md</guid><pubDate>Mon, 12 Aug 2024 17:05:43 GMT</pubDate></item><item><title><![CDATA[Riemann Curvature Tensor]]></title><description><![CDATA[ 
 <br>The curvature on a <a data-tooltip-position="top" aria-label="Mannigfaltigkeiten" data-href="Mannigfaltigkeiten" href="the-guide/mathematics/differential-geometry/riemannian-geometry/mannigfaltigkeiten-und-differenzierbare-abbildungen/mannigfaltigkeiten.html" class="internal-link" target="_self" rel="noopener nofollow">Riemannian manifold</a> is described by a -<a data-tooltip-position="top" aria-label="Tensors" data-href="Tensors" href="the-guide/mathematics/general-stuff/tensors.html" class="internal-link" target="_self" rel="noopener nofollow">tensor</a>, mapping three input <a data-tooltip-position="top" aria-label="Vektorfelder auf Mannigfaltigkeiten" data-href="Vektorfelder auf Mannigfaltigkeiten" href="the-guide/mathematics/differential-geometry/riemannian-geometry/mannigfaltigkeiten-und-differenzierbare-abbildungen/vektorfelder-auf-mannigfaltigkeiten.html" class="internal-link" target="_self" rel="noopener nofollow">vector fields</a> to a single one. <br>Definition
Let  be a <a data-tooltip-position="top" aria-label="(Semi-) Riemannsche Metriken" data-href="(Semi-) Riemannsche Metriken" href="the-guide/mathematics/differential-geometry/riemannian-geometry/metriken-und-zusammenhänge/(semi-)-riemannsche-metriken.html" class="internal-link" target="_self" rel="noopener nofollow">semi-Riemannian</a> <a data-tooltip-position="top" aria-label="Mannigfaltigkeiten" data-href="Mannigfaltigkeiten" href="the-guide/mathematics/differential-geometry/riemannian-geometry/mannigfaltigkeiten-und-differenzierbare-abbildungen/mannigfaltigkeiten.html" class="internal-link" target="_self" rel="noopener nofollow">manifold</a> with <a data-tooltip-position="top" aria-label="Levi-Civita-Zusammenhang und Koszul-Formel" data-href="Levi-Civita-Zusammenhang und Koszul-Formel" href="the-guide/mathematics/differential-geometry/riemannian-geometry/metriken-und-zusammenhänge/levi-civita-zusammenhang-und-koszul-formel.html" class="internal-link" target="_self" rel="noopener nofollow">Levi-Civita-Connection</a> . The curvature <a data-tooltip-position="top" aria-label="Tensors" data-href="Tensors" href="the-guide/mathematics/general-stuff/tensors.html" class="internal-link" target="_self" rel="noopener nofollow">tensor</a> is the commutator of the second <a data-tooltip-position="top" aria-label="Kovariante Ableitung" data-href="Kovariante Ableitung" href="the-guide/mathematics/differential-geometry/riemannian-geometry/metriken-und-zusammenhänge/kovariante-ableitung.html" class="internal-link" target="_self" rel="noopener nofollow">covariant derivatives</a> If the tensor vanishes at every point, the manifold is denoted flat. The curvature tensor is invariant under <a data-tooltip-position="top" aria-label="Differenzierbare Abbildung zwischen Mannigfaltigkeiten" data-href="Differenzierbare Abbildung zwischen Mannigfaltigkeiten" href="the-guide/mathematics/differential-geometry/riemannian-geometry/mannigfaltigkeiten-und-differenzierbare-abbildungen/differenzierbare-abbildung-zwischen-mannigfaltigkeiten.html" class="internal-link" target="_self" rel="noopener nofollow">Diffeomorphisms</a>, e.g.  and . 
<br>Let  be a <a data-tooltip-position="top" aria-label="Topology and Topological Space" data-href="Topology and Topological Space" href="the-guide/mathematics/topology/topology-and-topological-space.html" class="internal-link" target="_self" rel="noopener nofollow">chart</a> of a <a data-tooltip-position="top" aria-label="(Semi-) Riemannsche Metriken" data-href="(Semi-) Riemannsche Metriken" href="the-guide/mathematics/differential-geometry/riemannian-geometry/metriken-und-zusammenhänge/(semi-)-riemannsche-metriken.html" class="internal-link" target="_self" rel="noopener nofollow">semi-Riemannian</a> <a data-tooltip-position="top" aria-label="Mannigfaltigkeiten" data-href="Mannigfaltigkeiten" href="the-guide/mathematics/differential-geometry/riemannian-geometry/mannigfaltigkeiten-und-differenzierbare-abbildungen/mannigfaltigkeiten.html" class="internal-link" target="_self" rel="noopener nofollow">manifold</a> with standard basis  and the <a data-tooltip-position="top" aria-label="Levi-Civita-Zusammenhang und Koszul-Formel" data-href="Levi-Civita-Zusammenhang und Koszul-Formel" href="the-guide/mathematics/differential-geometry/riemannian-geometry/metriken-und-zusammenhänge/levi-civita-zusammenhang-und-koszul-formel.html" class="internal-link" target="_self" rel="noopener nofollow">Levi-Civita connection</a> with the <a data-tooltip-position="top" aria-label="Christoffel-Symbol" data-href="Christoffel-Symbol" href="the-guide/mathematics/differential-geometry/elementary-differential-geometry/intrinsische-geometrie-von-hyperflächen/christoffel-symbol.html" class="internal-link" target="_self" rel="noopener nofollow">Christoffel-symbols</a> . In this case, it holds that where  is exactly the left (intrinsic) side of the <a data-tooltip-position="top" aria-label="Hyperflächengleichungen" data-href="Hyperflächengleichungen" href="the-guide/mathematics/differential-geometry/elementary-differential-geometry/intrinsische-geometrie-von-hyperflächen/hyperflächengleichungen.html" class="internal-link" target="_self" rel="noopener nofollow">Gaussian hypersurface equations</a> This can be derived by considering  and inserting back the Christoffel symbols. We can say that the curvature tensor generalizes the difference of derivatives  we encountered when deriving the Gaussian equations for an immersion.<br>Intuition
We can visualize the initial formula by thinking about moving a vector  along an infinitesimal rectangle. We set a <a data-tooltip-position="top" aria-label="Mathematical Relations" data-href="Mathematical Relations" href="the-guide/mathematics/general-stuff/mathematical-relations.html" class="internal-link" target="_self" rel="noopener nofollow">function</a>  in a neighborhood  with We now consider the image of a rectangle with lengths  and  with the lower left corner at . We move the vector  along that rectangle via <a data-tooltip-position="top" aria-label="Parallele Vektorfelder und Parallelverschiebung" data-href="Parallele Vektorfelder und Parallelverschiebung" href="the-guide/mathematics/differential-geometry/riemannian-geometry/metriken-und-zusammenhänge/parallele-vektorfelder-und-parallelverschiebung.html" class="internal-link" target="_self" rel="noopener nofollow">parallel transport</a>, the vector with which we arrive back at the origin will in general point in a different direction (compare <a data-tooltip-position="top" aria-label="Holonomie" data-href="Holonomie" href="the-guide/mathematics/differential-geometry/elementary-differential-geometry/globale-kurven-und-flächentheorie/holonomie.html" class="internal-link" target="_self" rel="noopener nofollow">holonomy</a>). The curvature tensor describes the difference vector of the initial and the transported vector for infinitesimal rectangles, i.e. If we choose the vector fields in an inconvenient way, the parallel transport will not arrive back at the origin. For this case, we need the correction term with the <a data-tooltip-position="top" aria-label="Lie-Klammer" data-href="Lie-Klammer" href="the-guide/mathematics/differential-geometry/riemannian-geometry/mannigfaltigkeiten-und-differenzierbare-abbildungen/lie-klammer.html" class="internal-link" target="_self" rel="noopener nofollow">Lie-bracket</a> as a direction.
<br>Lemma 2 - Linearity
The <a data-tooltip-position="top" aria-label="Mathematical Relations" data-href="Mathematical Relations" href="the-guide/mathematics/general-stuff/mathematical-relations.html" class="internal-link" target="_self" rel="noopener nofollow">mapping</a>  is -linear in every entry:
<br>Lemma 3
In combination with the <a data-tooltip-position="top" aria-label="(Semi-) Riemannsche Metriken" data-href="(Semi-) Riemannsche Metriken" href="the-guide/mathematics/differential-geometry/riemannian-geometry/metriken-und-zusammenhänge/(semi-)-riemannsche-metriken.html" class="internal-link" target="_self" rel="noopener nofollow">metric</a>  of the manifold , we have the following properties:

<br>Bianchi-Identity 
<br>
<br>
<br>

<br>
<br>The identities 2-4 can be seen as switching orientations of the spanned areas
<br>Theorem
If  is a non-degenerate symmetric bilinear form and the tri-linear mapping  has the properties of Lemma 3,  uniquely defines the Riemann curvature tensor.  
<br><br><br>Theorem - Constant Curvature
Let . In this case  has constant curvature, if
<br>
<br> has constant curvature 
<br><br><br>Theorem 
A semi-Riemannian manifold is flat, iff every point has a neighborhood  on which <a data-tooltip-position="top" aria-label="Parallele Vektorfelder und Parallelverschiebung" data-href="Parallele Vektorfelder und Parallelverschiebung" href="the-guide/mathematics/differential-geometry/riemannian-geometry/metriken-und-zusammenhänge/parallele-vektorfelder-und-parallelverschiebung.html" class="internal-link" target="_self" rel="noopener nofollow">the parallel transport</a> of vector fields is path-independent.
<br>Proof
"" If the neighborhood exists, there is a local coordinate field, for which the first covariant derivative in the curvature tensor definition vanishes.
"" We extend a vector in  to a vector field  by parallel transport, first along  and then along . From this, we have that , but the other covariant derivative is only zero along  per construction. However, from the fact that the curvature tensor has to vanish, we know that Because for every fixed , there is a point where  vanishes (along the  axis), the parallel transport of this zero vector also has to vanish. 
<br>Corollary
A semi-Riemannian manifold is localy <a data-tooltip-position="top" aria-label="Isometrische Mannigfaltigkeiten" data-href="Isometrische Mannigfaltigkeiten" href="the-guide/mathematics/differential-geometry/riemannian-geometry/metriken-und-zusammenhänge/isometrische-mannigfaltigkeiten.html" class="internal-link" target="_self" rel="noopener nofollow">isometric</a> to , iff the curvature tensor vanishes .
]]></description><link>the-guide/mathematics/differential-geometry/riemannian-geometry/curvature/riemann-curvature-tensor.html</link><guid isPermaLink="false">The Guide/Mathematics/Differential Geometry/Riemannian Geometry/Curvature/Riemann Curvature Tensor.md</guid><pubDate>Wed, 23 Apr 2025 08:34:33 GMT</pubDate></item><item><title><![CDATA[Riemannian Tensor Constructions and Differential Operators]]></title><description><![CDATA[ 
 <br>In a Nutshell
Generalization of Differential Operators to <a data-tooltip-position="top" aria-label="Mannigfaltigkeiten" data-href="Mannigfaltigkeiten" href="the-guide/mathematics/differential-geometry/riemannian-geometry/mannigfaltigkeiten-und-differenzierbare-abbildungen/mannigfaltigkeiten.html" class="internal-link" target="_self" rel="noopener nofollow">Riemannian manifolds</a>. We can define the known operators of <a data-tooltip-position="top" aria-label="Divergence Theorem" data-href="Divergence Theorem" href="the-guide/mathematics/analysis-and-calculus/divergence-theorem.html" class="internal-link" target="_self" rel="noopener nofollow">divergence</a>, rotation and gradient on a manifold by using the <a data-tooltip-position="top" aria-label="(Semi-) Riemannsche Metriken" data-href="(Semi-) Riemannsche Metriken" href="the-guide/mathematics/differential-geometry/riemannian-geometry/metriken-und-zusammenhänge/(semi-)-riemannsche-metriken.html" class="internal-link" target="_self" rel="noopener nofollow">metric</a>/<a data-tooltip-position="top" aria-label="1. Fundamentalform" data-href="1. Fundamentalform" href="the-guide/mathematics/differential-geometry/elementary-differential-geometry/1.-fundamentalform.html" class="internal-link" target="_self" rel="noopener nofollow">first fundamental form</a>  of our Riemannian manifold . Generally, we simply switch out partial derivatives for <a data-tooltip-position="top" aria-label="Kovariante Ableitung" data-href="Kovariante Ableitung" href="the-guide/mathematics/differential-geometry/riemannian-geometry/metriken-und-zusammenhänge/kovariante-ableitung.html" class="internal-link" target="_self" rel="noopener nofollow">covariant derivatives</a> and components  for <a data-tooltip-position="top" aria-label="Differential Form" data-href="Differential Form" href="the-guide/mathematics/general-stuff/differential-form.html" class="internal-link" target="_self" rel="noopener nofollow">1-forms</a> .
<br>In order to define the operators, Riemann introduced a couple of specific tensor constructions. <br><a data-tooltip-position="top" aria-label="Musikalischer Isomorphismus" data-href="Musikalischer Isomorphismus" href="the-guide/mathematics/differential-geometry/riemannian-geometry/curvature/musikalischer-isomorphismus.html" class="internal-link" target="_self" rel="noopener nofollow">Raising indices</a> can be extended to tensors. Considering , we define   <br>Riemannian Trace 
Let  with . The Riemannian Contraction or Trace is This operation preserves <a data-tooltip-position="top" aria-label="Tensor Fields on Manifolds" data-href="Tensor Fields on Manifolds" href="the-guide/mathematics/differential-geometry/riemannian-geometry/curvature/tensor-fields-on-manifolds.html" class="internal-link" target="_self" rel="noopener nofollow">tensor fields</a> and can be defined in two ways:

<br>The <a data-tooltip-position="top" aria-label="Musikalischer Isomorphismus" data-href="Musikalischer Isomorphismus" href="the-guide/mathematics/differential-geometry/riemannian-geometry/curvature/musikalischer-isomorphismus.html" class="internal-link" target="_self" rel="noopener nofollow">musical isomorphism</a>  applied to the -th argument results in . We then apply the usual <a data-tooltip-position="top" aria-label="Tensors" data-href="Tensors" href="the-guide/mathematics/general-stuff/tensors.html" class="internal-link" target="_self" rel="noopener nofollow">tensor contraction</a>
<br>Again using an orthonormal frame  for  (in general ), it holds for every point in the <a data-tooltip-position="top" aria-label="Set" data-href="Set" href="the-guide/mathematics/general-stuff/set.html" class="internal-link" target="_self" rel="noopener nofollow">open subset</a>  that 

<br><br><br>Riemannian Gradient
In elementary differential geometry, the <a data-tooltip-position="top" aria-label="Derivative, Gradient, Jacobian and Hessian > The Gradient" data-href="Derivative, Gradient, Jacobian and Hessian#The Gradient" href="the-guide/mathematics/analysis-and-calculus/derivative,-gradient,-jacobian-and-hessian.html#The_Gradient" class="internal-link" target="_self" rel="noopener nofollow">gradient</a> of a <a data-tooltip-position="top" aria-label="Mathematical Relations" data-href="Mathematical Relations" href="the-guide/mathematics/general-stuff/mathematical-relations.html" class="internal-link" target="_self" rel="noopener nofollow">function</a>  can be computed via  For more general <a data-tooltip-position="top" aria-label="(Semi-) Riemannsche Metriken" data-href="(Semi-) Riemannsche Metriken" href="the-guide/mathematics/differential-geometry/riemannian-geometry/metriken-und-zusammenhänge/(semi-)-riemannsche-metriken.html" class="internal-link" target="_self" rel="noopener nofollow">semi-Riemannian</a> <a data-tooltip-position="top" aria-label="Mannigfaltigkeiten" data-href="Mannigfaltigkeiten" href="the-guide/mathematics/differential-geometry/riemannian-geometry/mannigfaltigkeiten-und-differenzierbare-abbildungen/mannigfaltigkeiten.html" class="internal-link" target="_self" rel="noopener nofollow">manifolds</a>, the gradient of a function  can be defined using th<a class="internal-link" data-href="Differential Form.md" href="the-guide/mathematics/general-stuff/differential-form.html" target="_self" rel="noopener nofollow"></a>us|musical isomorphism]]. Globally, we obtainLocally, we use write 
<br>Generally, the Riemannian <a data-tooltip-position="top" aria-label="Divergence Theorem" data-href="Divergence Theorem" href="the-guide/mathematics/analysis-and-calculus/divergence-theorem.html" class="internal-link" target="_self" rel="noopener nofollow">divergence</a> is the <a data-tooltip-position="top" aria-label="Tensors" data-href="Tensors" href="the-guide/mathematics/general-stuff/tensors.html" class="internal-link" target="_self" rel="noopener nofollow">contraction</a> of the -<a data-tooltip-position="top" aria-label="Tensors" data-href="Tensors" href="the-guide/mathematics/general-stuff/tensors.html" class="internal-link" target="_self" rel="noopener nofollow">tensor</a> , i.e. This can be reformulated into<br>...<br><br><br>Definition
The Laplace-Beltrami operator is defined as the Riemannian divergence of the Riemannian gradient 
]]></description><link>the-guide/mathematics/differential-geometry/riemannian-geometry/curvature/riemannian-tensor-constructions-and-differential-operators.html</link><guid isPermaLink="false">The Guide/Mathematics/Differential Geometry/Riemannian Geometry/Curvature/Riemannian Tensor Constructions and Differential Operators.md</guid><pubDate>Sat, 25 Jan 2025 17:52:28 GMT</pubDate></item><item><title><![CDATA[Satz von Myers]]></title><description><![CDATA[ 
 <br>In a Nutshell
Zentraler Satz der Riemann'schen Geometrie, der Informationen über die globale Struktur einer Mannigfaltigkeit mit positiver unterer Krümmungs-Schranke liefert.
<br><br>Definition Durchmesser
Der Durchmesser einer <a data-tooltip-position="top" aria-label="(Semi-) Riemannsche Metriken" data-href="(Semi-) Riemannsche Metriken" href="the-guide/mathematics/differential-geometry/riemannian-geometry/metriken-und-zusammenhänge/(semi-)-riemannsche-metriken.html" class="internal-link" target="_self" rel="noopener nofollow">Riemann'schen</a> <a data-tooltip-position="top" aria-label="Mannigfaltigkeiten" data-href="Mannigfaltigkeiten" href="the-guide/mathematics/differential-geometry/riemannian-geometry/mannigfaltigkeiten-und-differenzierbare-abbildungen/mannigfaltigkeiten.html" class="internal-link" target="_self" rel="noopener nofollow">Mannigfaltigkeit</a>  ist definiert über
<br>
<br>
<br> (Identifikation beachten)
<br>Satz von Myers
Sei  eine <a data-tooltip-position="top" aria-label="Mannigfaltigkeiten" data-href="Mannigfaltigkeiten" href="the-guide/mathematics/differential-geometry/riemannian-geometry/mannigfaltigkeiten-und-differenzierbare-abbildungen/mannigfaltigkeiten.html" class="internal-link" target="_self" rel="noopener nofollow">vollständige</a> <a data-tooltip-position="top" aria-label="Topology and Topological Space" data-href="Topology and Topological Space" href="the-guide/mathematics/topology/topology-and-topological-space.html" class="internal-link" target="_self" rel="noopener nofollow">zusammenhängende</a> <a data-tooltip-position="top" aria-label="(Semi-) Riemannsche Metriken" data-href="(Semi-) Riemannsche Metriken" href="the-guide/mathematics/differential-geometry/riemannian-geometry/metriken-und-zusammenhänge/(semi-)-riemannsche-metriken.html" class="internal-link" target="_self" rel="noopener nofollow">Riemann'sche Mannigfaltigkeit</a>. Gibt es eine untere Schranke an die <a data-tooltip-position="top" aria-label="Ricci- and Scalar Curvature" data-href="Ricci- and Scalar Curvature" href="the-guide/mathematics/differential-geometry/riemannian-geometry/curvature/ricci-and-scalar-curvature.html" class="internal-link" target="_self" rel="noopener nofollow">Ricci-Krümmung</a>, hier definiert über ein , sodass dann ist der oben definierte Durchmesser begrenzt durchErgänzt man  zu einer ONB, so erhält man Ebenen im <a data-href="Tangentialraum" href="the-guide/mathematics/differential-geometry/riemannian-geometry/mannigfaltigkeiten-und-differenzierbare-abbildungen/tangentialraum.html" class="internal-link" target="_self" rel="noopener nofollow">Tangentialraum</a>  und damit die  Schranke Die Mannigfaltigkeit kann also nicht unendlich ausgedehnt sein. Außerdem lässt sich zeigen, dass  <a data-tooltip-position="top" aria-label="Topology and Topological Space" data-href="Topology and Topological Space" href="the-guide/mathematics/topology/topology-and-topological-space.html" class="internal-link" target="_self" rel="noopener nofollow">kompakt</a> ist und die <a data-tooltip-position="top" aria-label="Fundamentalgruppe einer Mannigfaltigkeit" data-href="Fundamentalgruppe einer Mannigfaltigkeit" href="the-guide/mathematics/differential-geometry/riemannian-geometry/mannigfaltigkeiten-und-differenzierbare-abbildungen/fundamentalgruppe-einer-mannigfaltigkeit.html" class="internal-link" target="_self" rel="noopener nofollow">Fundamentalgruppe</a> endlich ist (unten). Damit stellt der Satz eine Verbindung zwischen Krümmung und globaler Struktur her.
<br>
<br>Die Schranken sind nicht äquivalent, die zweite Forderung ist stärker. Der beweis lässt sich jedoch auch mit der Summe und richtungsweise durchführen (S.185).
<br>Additional Information

<br>Positive Krümmung alleine reicht nicht, etwa das Rotationsparaboloid hat unendlichen Durchmesser.
<br>Um zu verdeutlichen, dass Vollständigkeit (Geodätische unendlich fortsetzbar) nötig ist betrachten wir einen Steifen , den wir über Kugelkoordinaten auf eine unendlich lange Abwicklung des Äquators abbilden. Mit der pullback-Metrik ist diese Mannigfaltigkeit nicht abgeschlossen, da ich nur begrenzt in Richtung  kann, der Durchmesser ist mit  und  unendlich.

<br>Proof
Der Beweis führt über Widerspruch. Nehmen wir  and, so muss es zwei Punkte  mit größerem Abstand geben. Mit dem <a data-href="Satz von Hopf-Rinow" href="the-guide/mathematics/differential-geometry/riemannian-geometry/metriken-und-zusammenhänge/satz-von-hopf-rinow.html" class="internal-link" target="_self" rel="noopener nofollow">Satz von Hopf-Rinow</a> muss es also auch eine <a data-href="Kürzeste" href="the-guide/mathematics/differential-geometry/riemannian-geometry/metriken-und-zusammenhänge/kürzeste.html" class="internal-link" target="_self" rel="noopener nofollow">Kürzeste</a> von  nach  geben, die logischerweise länger als  ist. Mit der Schranke aus dem Satz erhalten wirFür einen Widerspruch wählen wir eine <a data-href="BL-Kurve" href="the-guide/mathematics/differential-geometry/elementary-differential-geometry/lokale-kurventheorie/bl-kurve.html" class="internal-link" target="_self" rel="noopener nofollow">BL-Kurve</a> und verschieben deren Anfangspunkt  orthogonal mit  mit . Um eine <a data-tooltip-position="top" aria-label="Calculus of Variations" data-href="Calculus of Variations" href="the-guide/mathematics/functional-analysis-and-calculus-of-variations/calculus-of-variations.html" class="internal-link" target="_self" rel="noopener nofollow">Variation</a> mit möglichst wenig Längenzuwachs zu erhalten, wollen wir verhindern, dass sich die neue Kurve um  wickelt. Dazu nutzen wir die <a data-tooltip-position="top" aria-label="Parallele Vektorfelder und Parallelverschiebung" data-href="Parallele Vektorfelder und Parallelverschiebung" href="the-guide/mathematics/differential-geometry/riemannian-geometry/metriken-und-zusammenhänge/parallele-vektorfelder-und-parallelverschiebung.html" class="internal-link" target="_self" rel="noopener nofollow">Parallelverschiebung</a> (keine "links-rechts" Verschiebung) von  entlang der Kurve. Um die Variation richtig abzuschnüren, verwenden wirEinsetzen in die <a data-tooltip-position="top" aria-label="Variation der Energie" data-href="Variation der Energie" href="the-guide/mathematics/differential-geometry/riemannian-geometry/variation-der-energie.html" class="internal-link" target="_self" rel="noopener nofollow">zweite Variation der Energie</a> liefert ein negatives Ergebnis, die Kurve nach Variation hat also niedrigere Energie als , womit  nicht Kürzeste sein kann.
<br><br>Mit dem Ergebnis des Satzes können wir zudem folgendes Lemma zeigen.<br>Lemma
Sei  <a data-tooltip-position="top" aria-label="Mannigfaltigkeiten" data-href="Mannigfaltigkeiten" href="the-guide/mathematics/differential-geometry/riemannian-geometry/mannigfaltigkeiten-und-differenzierbare-abbildungen/mannigfaltigkeiten.html" class="internal-link" target="_self" rel="noopener nofollow">zusammenhängende und vollständige Mannigfaltigkeit</a> mit positiver unterer Schranke wie oben. Dann ist ...

<br> kompakt
<br>die <a data-tooltip-position="top" aria-label="Fundamentalgruppe einer Mannigfaltigkeit" data-href="Fundamentalgruppe einer Mannigfaltigkeit" href="the-guide/mathematics/differential-geometry/riemannian-geometry/mannigfaltigkeiten-und-differenzierbare-abbildungen/fundamentalgruppe-einer-mannigfaltigkeit.html" class="internal-link" target="_self" rel="noopener nofollow">Fundamentalgruppe</a>  endlich

<br>
<br>Durch Vollständigkeit ist <a data-href="Exponentialabbildung" href="the-guide/mathematics/differential-geometry/riemannian-geometry/metriken-und-zusammenhänge/exponentialabbildung.html" class="internal-link" target="_self" rel="noopener nofollow">Exponentialabbildung</a> überall definiert. Mit dem obigen Satz ist damit das Bild der Exponentialabbildung des normalen Balles mit Abschluss und Radius  die gesamte Mannigfaltigkeit. Da der Ball im Tangentialraum kompakt ist und die Exponentialabbildung stetig ist, ist auch die Mannigfaltigkeit kompakt.
<br><a data-tooltip-position="top" aria-label="Überlagerung" data-href="Überlagerung" href="the-guide/mathematics/differential-geometry/riemannian-geometry/mannigfaltigkeiten-und-differenzierbare-abbildungen/überlagerung.html" class="internal-link" target="_self" rel="noopener nofollow">Universelle Überlagerung</a> ist lokal isometrisch zu , sie muss damit ebenfalls die obige Krümmungsschranke erfüllen, womit sie wiederum kompakt ist. 
]]></description><link>the-guide/mathematics/differential-geometry/riemannian-geometry/curvature/satz-von-myers.html</link><guid isPermaLink="false">The Guide/Mathematics/Differential Geometry/Riemannian Geometry/Curvature/Satz von Myers.md</guid><pubDate>Wed, 23 Apr 2025 08:34:33 GMT</pubDate></item><item><title><![CDATA[Sectional Curvature]]></title><description><![CDATA[ 
 <br>In a Nutshell
Only the <a data-tooltip-position="top" aria-label="Hauptkrümmung und Hauptkrümmungsrichtung" data-href="Hauptkrümmung und Hauptkrümmungsrichtung" href="the-guide/mathematics/differential-geometry/elementary-differential-geometry/extrinsische-geometrie-von-hyperflächen/hauptkrümmung-und-hauptkrümmungsrichtung.html" class="internal-link" target="_self" rel="noopener nofollow">Gaussian curvature</a> for two dimensional manifolds is an intrinsic measure per <a data-href="Theorema Egregium" href="the-guide/mathematics/differential-geometry/elementary-differential-geometry/intrinsische-geometrie-von-hyperflächen/theorema-egregium.html" class="internal-link" target="_self" rel="noopener nofollow">Theorema Egregium</a>. To generalize the intrinsic curvature, we specialize the <a data-tooltip-position="top" aria-label="Riemann Curvature Tensor" data-href="Riemann Curvature Tensor" href="the-guide/mathematics/differential-geometry/riemannian-geometry/curvature/riemann-curvature-tensor.html" class="internal-link" target="_self" rel="noopener nofollow">Riemann curvature tensor</a> to planes in the <a data-tooltip-position="top" aria-label="Tangentialraum" data-href="Tangentialraum" href="the-guide/mathematics/differential-geometry/riemannian-geometry/mannigfaltigkeiten-und-differenzierbare-abbildungen/tangentialraum.html" class="internal-link" target="_self" rel="noopener nofollow">tangent space</a>. Defines the <a data-href="Riemann Curvature Tensor" href="the-guide/mathematics/differential-geometry/riemannian-geometry/curvature/riemann-curvature-tensor.html" class="internal-link" target="_self" rel="noopener nofollow">Riemann Curvature Tensor</a> and thereby whether a manifold has positive, negative or no curvature (flat).
<br><br>Definition
Let  be a <a data-tooltip-position="top" aria-label="(Semi-) Riemannsche Metriken" data-href="(Semi-) Riemannsche Metriken" href="the-guide/mathematics/differential-geometry/riemannian-geometry/metriken-und-zusammenhänge/(semi-)-riemannsche-metriken.html" class="internal-link" target="_self" rel="noopener nofollow">semi-Riemannian</a> <a data-tooltip-position="top" aria-label="Mannigfaltigkeiten" data-href="Mannigfaltigkeiten" href="the-guide/mathematics/differential-geometry/riemannian-geometry/mannigfaltigkeiten-und-differenzierbare-abbildungen/mannigfaltigkeiten.html" class="internal-link" target="_self" rel="noopener nofollow">manifold</a> with . For a given  and  non-degenerate (denominator not zero), the sectional curvature is 
<br>For the Riemannian case, the denominator is exactly the squared area of the parallelogram spanned by the two vectors, which is a normalization.<br>Intuition
Can be interpreted as the length-increase of the <a data-tooltip-position="top" aria-label="Exponentialabbildung" data-href="Exponentialabbildung" href="the-guide/mathematics/differential-geometry/riemannian-geometry/metriken-und-zusammenhänge/exponentialabbildung.html" class="internal-link" target="_self" rel="noopener nofollow">exponential map</a> of circles in the respective plane.
<br>Lemma
The sectional curvature is the only local isometry-invariance of a semi-Riemannian manifold of a given index.
<br>Theorem 5

<br>The sectional curvature defines the <a data-tooltip-position="top" aria-label="Riemann Curvature Tensor" data-href="Riemann Curvature Tensor" href="the-guide/mathematics/differential-geometry/riemannian-geometry/curvature/riemann-curvature-tensor.html" class="internal-link" target="_self" rel="noopener nofollow">Riemann curvature tensor</a>
<br> only depends on 

<br>Proof

<br>Follows from the fact that the curvature tensor follows directly from the metric
<br>All basis transformations of  are combinations of ...

<br>, which has no effect on the formula for , since the switching happens twice in the denominator
<br>, which has no effect, since .
<br>, which scales the enumerator and the  denominator by  and has thereby no effect.



<br> For , the above formula is equivalent to the <a data-tooltip-position="top" aria-label="Hauptkrümmung und Hauptkrümmungsrichtung" data-href="Hauptkrümmung und Hauptkrümmungsrichtung" href="the-guide/mathematics/differential-geometry/elementary-differential-geometry/extrinsische-geometrie-von-hyperflächen/hauptkrümmung-und-hauptkrümmungsrichtung.html" class="internal-link" target="_self" rel="noopener nofollow">Gaussian curvature</a>.<br><br><br>Definition
A semi-Riemannian manifold has constant curvature, if there is a , such that  for every possible combination of  and , given that the plane is spanned by non-degenerate vectors.
<br> for Constant Curvature
In this case, it holds that 
<br>Proof
"" Inserting  directly yields the definition of the sectional curvature
"" For the formula above, we can show all properties of the curvature tensor in every point.
<br>For positive curvature, we can derive <a data-tooltip-position="top" aria-label="Satz von Myers" data-href="Satz von Myers" href="the-guide/mathematics/differential-geometry/riemannian-geometry/curvature/satz-von-myers.html" class="internal-link" target="_self" rel="noopener nofollow">Myers Theorem</a> to obtain an upper bound on the length of geodesics, adding additional structure to the manifold.<br>
The negative case turns out to be diffeomorphic to  by the <a data-href="Cartan-Hadamard Theorem" href="the-guide/mathematics/differential-geometry/riemannian-geometry/curvature/cartan-hadamard-theorem.html" class="internal-link" target="_self" rel="noopener nofollow">Cartan-Hadamard Theorem</a>.]]></description><link>the-guide/mathematics/differential-geometry/riemannian-geometry/curvature/sectional-curvature.html</link><guid isPermaLink="false">The Guide/Mathematics/Differential Geometry/Riemannian Geometry/Curvature/Sectional Curvature.md</guid><pubDate>Mon, 12 Aug 2024 17:05:43 GMT</pubDate></item><item><title><![CDATA[Tensor Fields on Manifolds]]></title><description><![CDATA[ 
 <br>Definition
Let  be a <a data-tooltip-position="top" aria-label="Mannigfaltigkeiten" data-href="Mannigfaltigkeiten" href="the-guide/mathematics/differential-geometry/riemannian-geometry/mannigfaltigkeiten-und-differenzierbare-abbildungen/mannigfaltigkeiten.html" class="internal-link" target="_self" rel="noopener nofollow">differentiable manifold</a>. A  tensor-field is a <a data-tooltip-position="top" aria-label="Mathematical Relations" data-href="Mathematical Relations" href="the-guide/mathematics/general-stuff/mathematical-relations.html" class="internal-link" target="_self" rel="noopener nofollow">map</a> that is  -linear in every entry.
<br>
<br>Examples

<br>The <a data-tooltip-position="top" aria-label="(Semi-) Riemannsche Metriken" data-href="(Semi-) Riemannsche Metriken" href="the-guide/mathematics/differential-geometry/riemannian-geometry/metriken-und-zusammenhänge/(semi-)-riemannsche-metriken.html" class="internal-link" target="_self" rel="noopener nofollow">semi-Riemannian metric</a> is a  tensor-field
<br>The <a data-href="Riemann Curvature Tensor" href="the-guide/mathematics/differential-geometry/riemannian-geometry/curvature/riemann-curvature-tensor.html" class="internal-link" target="_self" rel="noopener nofollow">Riemann Curvature Tensor</a> is a  tensor-field
<br>The denominator of the <a data-tooltip-position="top" aria-label="Sectional Curvature" data-href="Sectional Curvature" href="the-guide/mathematics/differential-geometry/riemannian-geometry/curvature/sectional-curvature.html" class="internal-link" target="_self" rel="noopener nofollow">sectional curvature</a> is a  tensor-field


<br>Theorem
For a tensor-field , the value  only depends on each inputs value in , here .  
<br>
<br>Important to express tensor-field via coefficients that depend only on 
<br><a data-tooltip-position="top" aria-label="Tensors" data-href="Tensors" href="the-guide/mathematics/general-stuff/tensors.html" class="internal-link" target="_self" rel="noopener nofollow">Contra- and Covariance</a> describe the behavior of tensor fields when switching <a data-tooltip-position="top" aria-label="Topology and Topological Space" data-href="Topology and Topological Space" href="the-guide/mathematics/topology/topology-and-topological-space.html" class="internal-link" target="_self" rel="noopener nofollow">charts</a>. Given  and  around , the transition is . For the standard basis  of  and  of , a direction is encoded as , the transformation is described via the <a data-tooltip-position="top" aria-label="Derivative, Gradient, Jacobian and Hessian > The Jacobian" data-href="Derivative, Gradient, Jacobian and Hessian#The Jacobian" href="the-guide/mathematics/analysis-and-calculus/derivative,-gradient,-jacobian-and-hessian.html#The_Jacobian" class="internal-link" target="_self" rel="noopener nofollow">Jacobian</a>  <br><br><br>Extending the idea of a <a data-tooltip-position="top" aria-label="Kovariante Ableitung" data-href="Kovariante Ableitung" href="the-guide/mathematics/differential-geometry/riemannian-geometry/metriken-und-zusammenhänge/kovariante-ableitung.html" class="internal-link" target="_self" rel="noopener nofollow">covariant derivative</a> along  to tensor fields as a <a data-tooltip-position="top" aria-label="Mathematical Relations" data-href="Mathematical Relations" href="the-guide/mathematics/general-stuff/mathematical-relations.html" class="internal-link" target="_self" rel="noopener nofollow">map</a> It can be computed by rearranging the terms resulting from the product rule when computing the <a data-tooltip-position="top" aria-label="Lie Ableitung" data-href="Lie Ableitung" href="the-guide/mathematics/differential-geometry/riemannian-geometry/mannigfaltigkeiten-und-differenzierbare-abbildungen/lie-ableitung.html" class="internal-link" target="_self" rel="noopener nofollow">Lie derivative</a>The extension to vector-valued tensors simply swaps  for . In this case, we consider and reach the definition<br>Theorem
A <a data-tooltip-position="top" aria-label="(Semi-) Riemannsche Metriken" data-href="(Semi-) Riemannsche Metriken" href="the-guide/mathematics/differential-geometry/riemannian-geometry/metriken-und-zusammenhänge/(semi-)-riemannsche-metriken.html" class="internal-link" target="_self" rel="noopener nofollow">semi-Riemannian</a> metric  is parallel , iff  is <a data-tooltip-position="top" aria-label="Zusammenhänge" data-href="Zusammenhänge" href="the-guide/mathematics/differential-geometry/riemannian-geometry/metriken-und-zusammenhänge/zusammenhänge.html" class="internal-link" target="_self" rel="noopener nofollow">compatible with the metric</a>.
<br>
<br>Basically the product rule holds
]]></description><link>the-guide/mathematics/differential-geometry/riemannian-geometry/curvature/tensor-fields-on-manifolds.html</link><guid isPermaLink="false">The Guide/Mathematics/Differential Geometry/Riemannian Geometry/Curvature/Tensor Fields on Manifolds.md</guid><pubDate>Wed, 23 Apr 2025 08:34:34 GMT</pubDate></item><item><title><![CDATA[Charts and Atlas]]></title><description><![CDATA[ 
 <br><br>Locally Euclidean Topology
A <a data-tooltip-position="top" aria-label="Topology and Topological Space" data-href="Topology and Topological Space" href="the-guide/mathematics/topology/topology-and-topological-space.html" class="internal-link" target="_self" rel="noopener nofollow">topological space</a> is locally euclidean if every point on  has a neighborhood that is  <a data-tooltip-position="top" aria-label="Mathematical Relations" data-href="Mathematical Relations" href="the-guide/mathematics/general-stuff/mathematical-relations.html" class="internal-link" target="_self" rel="noopener nofollow">homeomorph</a> to . The homeomorphism  with <a data-tooltip-position="top" aria-label="Set" data-href="Set" href="the-guide/mathematics/general-stuff/set.html" class="internal-link" target="_self" rel="noopener nofollow">open subsets</a>  and  is denoted a chart or sometimes local coordinates/ coordinate map.
<br><img alt="center" src="lib/media/pasted-image-20240718180404.png" style="width: 450px; max-width: 100%;"><br><br><br>Eine Überdeckung einer <a data-tooltip-position="top" aria-label="Mannigfaltigkeiten" data-href="Mannigfaltigkeiten" href="the-guide/mathematics/differential-geometry/riemannian-geometry/mannigfaltigkeiten-und-differenzierbare-abbildungen/mannigfaltigkeiten.html" class="internal-link" target="_self" rel="noopener nofollow">Mannigfaltigkeit</a>  ist eine Familie offener <a data-tooltip-position="top" aria-label="Set" data-href="Set" href="the-guide/mathematics/general-stuff/set.html" class="internal-link" target="_self" rel="noopener nofollow">Mengen</a>  mit <br>Definition - Atlas
Die Menge  von <a data-tooltip-position="top" aria-label="Topology and Topological Space" data-href="Topology and Topological Space" href="the-guide/mathematics/topology/topology-and-topological-space.html" class="internal-link" target="_self" rel="noopener nofollow">Karten</a>, deren Definitionsgebiet  überdeckt heißt Atlas.
<br>
<br>Zwei <a data-tooltip-position="top" aria-label="Topology and Topological Space" data-href="Topology and Topological Space" href="the-guide/mathematics/topology/topology-and-topological-space.html" class="internal-link" target="_self" rel="noopener nofollow">Karten</a>  und  sind differenzierbar verträglich wenn der Kartenwechsel ein Diffeomorphismus ist. Insbesondere sind Karten mit disjunktem Bild immer verträglich, da es keine gemeinsamen Punkte gibt.
<br>Ein Atlas ist differenzierbar, wenn alle möglichen Kartenwechsel  differenzierbar sind
<br>Definition - Differenzierbare Struktur
Sei  ein Atlas und .  ist eine differenzierbare Struktur oder ein maximal differenzierbarer Atlas, wenn es zu allen Karten von  verträgliche Karten enthält.
<br><img alt="center" src="lib/media/pasted-image-20240503102315.png" style="width: 500px; max-width: 100%;"><br>Satz 
Eine differenzierbare Struktur ist Atlas und wird eindeutig durch  bestimmt.
<br>Proof
Wir zeigen schlicht, dass aus der Verträglichkeit zweier Karten zu einem Atlas deren Verträglichkeit untereinander folgt. Dazu betrachten wir den Punkt  und die Karten  und . Ein beliebiger Atlas deckt  vollständig ab, es exisitert also mindestens eine Karte  um . Wir nehmen einfach den UmwegDa beide Terme in Klammern differenzierbar sind, gilt dies nach Kettenregel auch für deren Verkettung.
]]></description><link>the-guide/mathematics/differential-geometry/riemannian-geometry/mannigfaltigkeiten-und-differenzierbare-abbildungen/charts-and-atlas.html</link><guid isPermaLink="false">The Guide/Mathematics/Differential Geometry/Riemannian Geometry/Mannigfaltigkeiten und differenzierbare Abbildungen/Charts and Atlas.md</guid><pubDate>Wed, 23 Apr 2025 08:34:34 GMT</pubDate><enclosure url="lib/media/pasted-image-20240718180404.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src=&quot;lib/media/pasted-image-20240718180404.png&quot;&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[Differenzierbare Abbildung zwischen Mannigfaltigkeiten]]></title><description><![CDATA[ 
 <br>Definition
Seien  und  <a data-tooltip-position="top" aria-label="Mannigfaltigkeiten" data-href="Mannigfaltigkeiten" href="the-guide/mathematics/differential-geometry/riemannian-geometry/mannigfaltigkeiten-und-differenzierbare-abbildungen/mannigfaltigkeiten.html" class="internal-link" target="_self" rel="noopener nofollow">differenzierbare Mannigfaltigkeiten</a>. Dann heißt die <a data-tooltip-position="top" aria-label="Mathematical Relations" data-href="Mathematical Relations" href="the-guide/mathematics/general-stuff/mathematical-relations.html" class="internal-link" target="_self" rel="noopener nofollow">Abbildung</a>  differenzierbar in , falls  differenzierbar in  ist mit einer <a data-tooltip-position="top" aria-label="Topology and Topological Space" data-href="Topology and Topological Space" href="the-guide/mathematics/topology/topology-and-topological-space.html" class="internal-link" target="_self" rel="noopener nofollow">Karte</a>  von  um  und einer Karte  um .
<br><img alt="center" src="lib/media/pasted-image-20240718184546.png" style="width: 400px; max-width: 100%;"><br>Diese Definition ist unabhängig von den gewählten <a data-tooltip-position="top" aria-label="Charts and Atlas" data-href="Charts and Atlas" href="the-guide/mathematics/differential-geometry/riemannian-geometry/mannigfaltigkeiten-und-differenzierbare-abbildungen/charts-and-atlas.html" class="internal-link" target="_self" rel="noopener nofollow">Karten</a>, da Kartenwechsel selbst als Differenzierbar vorausgesetzt werden: Bezüglich anderer Karten  um  und  um  kann man schreiben Die äußeren Terme sind als Kartenwechsel differenzierbar, womit die Differenzierbarkeit des mittleren Termes äquivalent zur Differenzierbarkeit der linken Seite ist.<br>
<br>
Immersion
Eine differenzierbare Abbildung zwischen Mannigfaltigkeiten heißt Immersion, wenn das Differential für alle  injektiv ist.


<br>
Ich treffe alle Tangentialvektoren

<br><br>Diffeomorphismus zwischen Mannigfaltigkeiten
Ein <a data-tooltip-position="top" aria-label="Mathematical Relations" data-href="Mathematical Relations" href="the-guide/mathematics/general-stuff/mathematical-relations.html" class="internal-link" target="_self" rel="noopener nofollow">Homöomorphismus</a>  zwischen Mannigfaltigkeiten heißt Diffeomorphismus, wenn  und  differenzierbar sind.
]]></description><link>the-guide/mathematics/differential-geometry/riemannian-geometry/mannigfaltigkeiten-und-differenzierbare-abbildungen/differenzierbare-abbildung-zwischen-mannigfaltigkeiten.html</link><guid isPermaLink="false">The Guide/Mathematics/Differential Geometry/Riemannian Geometry/Mannigfaltigkeiten und differenzierbare Abbildungen/Differenzierbare Abbildung zwischen Mannigfaltigkeiten.md</guid><pubDate>Wed, 23 Apr 2025 08:34:34 GMT</pubDate><enclosure url="lib/media/pasted-image-20240718184546.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src=&quot;lib/media/pasted-image-20240718184546.png&quot;&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[Fundamentalgruppe einer Mannigfaltigkeit]]></title><description><![CDATA[ 
 <br>Das folgende bezieht sich direkt auf zusammenhängende <a data-tooltip-position="top" aria-label="Mannigfaltigkeiten" data-href="Mannigfaltigkeiten" href="the-guide/mathematics/differential-geometry/riemannian-geometry/mannigfaltigkeiten-und-differenzierbare-abbildungen/mannigfaltigkeiten.html" class="internal-link" target="_self" rel="noopener nofollow">topologische Mannigfaltgkeiten</a>, die Fundamentalgruppe kann aber allgemeiner für <a data-tooltip-position="top" aria-label="Topology and Topological Space" data-href="Topology and Topological Space" href="the-guide/mathematics/topology/topology-and-topological-space.html" class="internal-link" target="_self" rel="noopener nofollow">topologische Räume</a> definieren. Der Unterschied liegt darin, dass die hier betrachteten Kurven durch die Annahme automatisch stetig definiert sind. <br>In a Nutshell
Werkzeug, um topologische Struktur einer Mannigfaltigkeit zu klassifizieren. Basiert auf der Art und Weise wie Schleifen in der Mannigfaltigkeit verlaufen können, wie sie verknotet sind und wie sie sich Zusammenziehen lassen.
<br><br>Der Ausgangspunkt sind <a data-tooltip-position="top" aria-label="Homotope Kurven" data-href="Homotope Kurven" href="the-guide/mathematics/differential-geometry/elementary-differential-geometry/globale-kurven-und-flächentheorie/homotope-kurven.html" class="internal-link" target="_self" rel="noopener nofollow">homotope</a> <a data-tooltip-position="top" aria-label="Raumkurven" data-href="Raumkurven" href="the-guide/mathematics/differential-geometry/elementary-differential-geometry/raumkurven.html" class="internal-link" target="_self" rel="noopener nofollow">Kurven</a> auf Mannigfaltigkeiten, genauer Schleifen. In der Riemman'schen Geometrie verwendet man statt des Unterraumes  einfach die jeweilige Mannigfaltigkeit . Wir wissen bereits, dass Homotopie eine <a data-tooltip-position="top" aria-label="Equivalence Relation and Class" data-href="Equivalence Relation and Class" href="the-guide/mathematics/general-stuff/equivalence-relation-and-class.html" class="internal-link" target="_self" rel="noopener nofollow">Äquivalenzrelation</a>  bildet. Wir bezeichnen die Menge der Äquivalenzklassen von Schleifen auf  durch einen Punkt  mit Diesem Raum geben wir nun <a data-tooltip-position="top" aria-label="Group" data-href="Group" href="the-guide/mathematics/group-theory/group.html" class="internal-link" target="_self" rel="noopener nofollow">Gruppenstruktur</a>. Dafür verwenden wir die Operation der Hintereinanderausführung von links nach rechtswas wieder eine Schleife durch  gibt.<br>Lemma
Ist  und , so ist auch . Die folgt daraus, dass die <a data-tooltip-position="top" aria-label="Homotope Kurven" data-href="Homotope Kurven" href="the-guide/mathematics/differential-geometry/elementary-differential-geometry/globale-kurven-und-flächentheorie/homotope-kurven.html" class="internal-link" target="_self" rel="noopener nofollow">Homotopie</a> von  zu  einfach aus der Hintereinanderausführung der einzelnen Homotopien zu erhalten ist.
<br>Intuition
Ich überführe  mittels  zu  und  mittels  zu . Dann kann ich durch Hintereinanderausführen der Abbildungen  genau die Hintereinander ausgeführten Kurven ineinander überführen.
<br>Als neutrales Element lässt sich einfach die konstante Kurve  definiere, die immer im Punkt verharrt. Das inverse Element  erhalten wir durch umgekehrten Durchlauf .<br>Gruppenstruktur von 
Mit der oben definierten Hintereinanderausführung als binärer Operation und dem neutralen und inversen Element wie oben wird  <a data-tooltip-position="top" aria-label="Group" data-href="Group" href="the-guide/mathematics/group-theory/group.html" class="internal-link" target="_self" rel="noopener nofollow">Gruppe</a>.
<br>Für die Definition der Fundamentalgruppe wollen wir nun die Abhängigkeit vom Punkt  auflösen. Da wir Schleifen, die wir via <a data-tooltip-position="top" aria-label="Homotope Kurven" data-href="Homotope Kurven" href="the-guide/mathematics/differential-geometry/elementary-differential-geometry/globale-kurven-und-flächentheorie/homotope-kurven.html" class="internal-link" target="_self" rel="noopener nofollow">Homotopie</a> ineinander überführen können als gleichwertig ansehen (Äquivalenzrelation), betrachten wir das Verschieben von Schleifen:<br>Satz
Sei  eine Kurve von  nach  in . Die Abbildung (Hintereinanderausführung von links nach rechts definiert) ist <a data-tooltip-position="top" aria-label="Mathematical Relations" data-href="Mathematical Relations" href="the-guide/mathematics/general-stuff/mathematical-relations.html" class="internal-link" target="_self" rel="noopener nofollow">Gruppenisomorphismus</a> von  auf .
<br>Intuition
Ich ziehe den Punkt auf ein , wo  definiert ist, führe die Kurve dort wie gewohnt aus und pushe den Punkt wieder weg. Damit vermeide ich die Situation, in der ich  woanders nicht definieren kann.
<br>Proof
Die obige Abbildung ist Homomorphismus (Algebras)Zudem lässt sich die eindeutige Inverse der Abbildung via  definieren. Damit können wir ohne Verlust an Information hin- und hertransformieren,  was einem Isomorphismus entspricht.
<br>Wenn  <a data-tooltip-position="top" aria-label="Topology and Topological Space" data-href="Topology and Topological Space" href="the-guide/mathematics/topology/topology-and-topological-space.html" class="internal-link" target="_self" rel="noopener nofollow">zusammenhängend</a> ist gibt es zwischen allen Punkten  eine solche Kurve , in diesem Fall ist  also wirklich unabhängig von  ! Dies ermöglicht folgende Definition:<br>Fundamentalgruppe einer Mannigfaltigkeit
Die <a data-tooltip-position="top" aria-label="Group" data-href="Group" href="the-guide/mathematics/group-theory/group.html" class="internal-link" target="_self" rel="noopener nofollow">Gruppe</a>  heißt Fundamentalgruppe. ist , so ist  <a data-tooltip-position="top" aria-label="Topology and Topological Space" data-href="Topology and Topological Space" href="the-guide/mathematics/topology/topology-and-topological-space.html" class="internal-link" target="_self" rel="noopener nofollow">einfach zusammenhängend</a>. 
<br>
<br>Beispiele

<br> 
<br> für  (lassen sich immer zusammenziehen), für  ist  (Kurven mit  Schleifen)
<br>Torus , da Kurven mit  Schleifen auf  Arten um den Torus gewickelt werden können.


]]></description><link>the-guide/mathematics/differential-geometry/riemannian-geometry/mannigfaltigkeiten-und-differenzierbare-abbildungen/fundamentalgruppe-einer-mannigfaltigkeit.html</link><guid isPermaLink="false">The Guide/Mathematics/Differential Geometry/Riemannian Geometry/Mannigfaltigkeiten und differenzierbare Abbildungen/Fundamentalgruppe einer Mannigfaltigkeit.md</guid><pubDate>Wed, 23 Apr 2025 08:34:34 GMT</pubDate></item><item><title><![CDATA[Lie Ableitung]]></title><description><![CDATA[ 
 <br>In a Nutshell
Generalization of the <a data-tooltip-position="top" aria-label="Derivative, Gradient, Jacobian and Hessian > The Gradient" data-href="Derivative, Gradient, Jacobian and Hessian#The Gradient" href="the-guide/mathematics/analysis-and-calculus/derivative,-gradient,-jacobian-and-hessian.html#The_Gradient" class="internal-link" target="_self" rel="noopener nofollow">directional derivative</a> to <a data-tooltip-position="top" aria-label="Tangentialraum" data-href="Tangentialraum" href="the-guide/mathematics/differential-geometry/riemannian-geometry/mannigfaltigkeiten-und-differenzierbare-abbildungen/tangentialraum.html" class="internal-link" target="_self" rel="noopener nofollow">tangent vectors</a> of a <a data-tooltip-position="top" aria-label="Mannigfaltigkeiten" data-href="Mannigfaltigkeiten" href="the-guide/mathematics/differential-geometry/riemannian-geometry/mannigfaltigkeiten-und-differenzierbare-abbildungen/mannigfaltigkeiten.html" class="internal-link" target="_self" rel="noopener nofollow">manifold</a>, yielding a first intrinsic notion of a derivative.
<br><br>Im euklidischen lässt sich wie gewohnt die Richtungsableitung  einer skalar-wertigen <a data-tooltip-position="top" aria-label="Mathematical Relations" data-href="Mathematical Relations" href="the-guide/mathematics/general-stuff/mathematical-relations.html" class="internal-link" target="_self" rel="noopener nofollow">Funktion</a>  definieren. Die Lie Ableitung überträgt dieses Konzept auf <a data-href="Mannigfaltigkeiten" href="the-guide/mathematics/differential-geometry/riemannian-geometry/mannigfaltigkeiten-und-differenzierbare-abbildungen/mannigfaltigkeiten.html" class="internal-link" target="_self" rel="noopener nofollow">Mannigfaltigkeiten</a>. Im folgenden sei die Menge der glatten Funktionen auf  mit  bezeichnet.<br><br>Definition
Die Lie Ableitung von  im Punkt  in Richtung  ist wobei  <a data-tooltip-position="top" aria-label="Topology and Topological Space" data-href="Topology and Topological Space" href="the-guide/mathematics/topology/topology-and-topological-space.html" class="internal-link" target="_self" rel="noopener nofollow">Karte</a> um  ist und  mit den allgemeinen Basiskomponenten der Karte . Ist der Vektor  ein Element der Standardbasis  bzgl. der Karte , so schreiben wir Damit lässt sich die Lie Ableitung auf einer Mannigfaltigkeit wörtlich wie die euklidische Richtungsableitung notieren: 
<br><br><img alt="center" src="lib/media/pasted-image-20240429190411.png" style="width: 400px; max-width: 100%;"><br>
Dieses Konzept hat zunächst die selben Probleme wie der Tangentialvektor an sich, da der <a data-tooltip-position="top" aria-label="Tangentialraum" data-href="Tangentialraum" href="the-guide/mathematics/differential-geometry/riemannian-geometry/mannigfaltigkeiten-und-differenzierbare-abbildungen/tangentialraum.html" class="internal-link" target="_self" rel="noopener nofollow">Tangentialvektor</a> von der Wahl der <a data-tooltip-position="top" aria-label="Charts and Atlas" data-href="Charts and Atlas" href="the-guide/mathematics/differential-geometry/riemannian-geometry/mannigfaltigkeiten-und-differenzierbare-abbildungen/charts-and-atlas.html" class="internal-link" target="_self" rel="noopener nofollow">Karte abhängt</a>. Wir müssen daher zeigen, dass die Lie-Ableitung unabhängig von der Wahl der Karte ist.<br>Lemma
Die Lie Ableitung ist unabhängig von der Wahl der Karte.
<br>Man betrachte einen Tangentialvektor  mit Karte . Es gilt Damit haben wir in der Klammer eine Verkettung (Mannigfaltigkeit  Tangentialraum  Mannigfaltigkeit), womit wir den Ausdruck als Kettenregel schreiben können Der Ausdruck in der großen Klammer entspricht genau dem Kartenwechsel von  nach für den <a data-tooltip-position="top" aria-label="Tangentialraum" data-href="Tangentialraum" href="the-guide/mathematics/differential-geometry/riemannian-geometry/mannigfaltigkeiten-und-differenzierbare-abbildungen/tangentialraum.html" class="internal-link" target="_self" rel="noopener nofollow">Tangentialvektor</a>.<br><br><br>Definition
Die obige punktweise definierte Lie-Ableitung erweitert sich für <a data-href="Vektorfelder auf Mannigfaltigkeiten" href="the-guide/mathematics/differential-geometry/riemannian-geometry/mannigfaltigkeiten-und-differenzierbare-abbildungen/vektorfelder-auf-mannigfaltigkeiten.html" class="internal-link" target="_self" rel="noopener nofollow">Vektorfelder auf Mannigfaltigkeiten</a> via wobei die Notation an oben anschließt. Bezüglich dem Vektorfeld  hängt  hängt nur  ab, bezüglich  in einer Umgebung von .
<br>Diese Darstellung ist zwar lokal, also bezüglich einer Karte  um , jedoch ist die Definition insgesamt wieder unabhängig von der Wahl der Karte, da <br>Damit ist  ein Differentialoperator erster Ordnung mit den Eigenschaften ...<br><br>Lemma - Beziehung zum <a data-href="Pushforward Operator on Manifolds" href="the-guide/mathematics/differential-geometry/riemannian-geometry/mannigfaltigkeiten-und-differenzierbare-abbildungen/pushforward-operator-on-manifolds.html" class="internal-link" target="_self" rel="noopener nofollow">Pushforward Operator on Manifolds</a>
Ist  und , so gilt mit <a data-href="Pushforward Operator on Manifolds" href="the-guide/mathematics/differential-geometry/riemannian-geometry/mannigfaltigkeiten-und-differenzierbare-abbildungen/pushforward-operator-on-manifolds.html" class="internal-link" target="_self" rel="noopener nofollow">Pushforward Operator on Manifolds</a> .
<br><br><br>Für ein <a data-tooltip-position="top" aria-label="Vektorfelder auf Mannigfaltigkeiten" data-href="Vektorfelder auf Mannigfaltigkeiten" href="the-guide/mathematics/differential-geometry/riemannian-geometry/mannigfaltigkeiten-und-differenzierbare-abbildungen/vektorfelder-auf-mannigfaltigkeiten.html" class="internal-link" target="_self" rel="noopener nofollow">Vektorfeld</a>  auf einer <a data-tooltip-position="top" aria-label="Mannigfaltigkeiten" data-href="Mannigfaltigkeiten" href="the-guide/mathematics/differential-geometry/riemannian-geometry/mannigfaltigkeiten-und-differenzierbare-abbildungen/mannigfaltigkeiten.html" class="internal-link" target="_self" rel="noopener nofollow">Mannigfaltigkeit</a>  mit Funktion  is auch die <a data-href="Lie Ableitung" href="the-guide/mathematics/differential-geometry/riemannian-geometry/mannigfaltigkeiten-und-differenzierbare-abbildungen/lie-ableitung.html" class="internal-link" target="_self" rel="noopener nofollow">Lie Ableitung</a> Element von . Daher können Richtungsableitungen hintereinander geschaltet werden:Dieser Differentialoperator zweiter Ordnung lässt sich bezüglich einer <a data-tooltip-position="top" aria-label="Topology and Topological Space" data-href="Topology and Topological Space" href="the-guide/mathematics/topology/topology-and-topological-space.html" class="internal-link" target="_self" rel="noopener nofollow">Karte</a>  darstellen als er ist jedoch keine <a data-href="Lie Ableitung" href="the-guide/mathematics/differential-geometry/riemannian-geometry/mannigfaltigkeiten-und-differenzierbare-abbildungen/lie-ableitung.html" class="internal-link" target="_self" rel="noopener nofollow">Lie Ableitung</a> / kein Vektorfeld auf der Mannigfaltigkeit, da auch Änderungen außerhalb des Tangentialraums geschehen können. Um ein Vektorfeld auf der Mannigfaltigkeit zu erhalten verwendet man die <a data-href="Lie-Klammer" href="the-guide/mathematics/differential-geometry/riemannian-geometry/mannigfaltigkeiten-und-differenzierbare-abbildungen/lie-klammer.html" class="internal-link" target="_self" rel="noopener nofollow">Lie-Klammer</a>.]]></description><link>the-guide/mathematics/differential-geometry/riemannian-geometry/mannigfaltigkeiten-und-differenzierbare-abbildungen/lie-ableitung.html</link><guid isPermaLink="false">The Guide/Mathematics/Differential Geometry/Riemannian Geometry/Mannigfaltigkeiten und differenzierbare Abbildungen/Lie Ableitung.md</guid><pubDate>Wed, 23 Apr 2025 08:34:34 GMT</pubDate><enclosure url="lib/media/pasted-image-20240429190411.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src=&quot;lib/media/pasted-image-20240429190411.png&quot;&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[Lie-Klammer]]></title><description><![CDATA[ 
 <br>In a Nutshell
In der Differentialgeometrie kann man <a data-href="Vektorfelder auf Mannigfaltigkeiten" href="the-guide/mathematics/differential-geometry/riemannian-geometry/mannigfaltigkeiten-und-differenzierbare-abbildungen/vektorfelder-auf-mannigfaltigkeiten.html" class="internal-link" target="_self" rel="noopener nofollow">Vektorfelder auf Mannigfaltigkeiten</a> als infinitisimale Generatoren von flows auf diesen Mannigfaltigkeiten verstehen. Durch die nicht-triviale Struktur der Mannigfaltigkeit ist es von Interesse zu verstehen, wie sich flows verhalten, die aus Kombinationen solcher Felder entstehen.
<br><br>Hierbei ist zunächt interessant, ob der entstehende Flow kommutiert, also ob die Reihenfolge der Ausführung Einfluss auf den Endpunkt hat.<br>Satz 6 - Kommutator
Seien . Dann wird durch für alle  ein Vektorfeld Z eindeutig bestimmt. Dieses Vektorfeld heißt Kommutator oder Lie-Klammer und wir schreiben Bezüglich einer <a data-tooltip-position="top" aria-label="Topology and Topological Space" data-href="Topology and Topological Space" href="the-guide/mathematics/topology/topology-and-topological-space.html" class="internal-link" target="_self" rel="noopener nofollow">Karte</a>  gilt  Intuitiv beschreibt dieser wie die Flüsse zweier Vektorfelder infinitisimal kommutieren. 
<br><br>
<br>Beispiele

<br>Man betrachte die Vektorfelder  und  auf . Es folgt mit ,  
<br>Koordinatenfelder kommutieren immer


<br>Intuition
Man stelle sich ein rechteckiges Netz vor, welches die Mannigfaltigkeit überspannt. Entspricht die Addition zweier Vektorfelder einem solchen Netz, so kommutieren sie. Vor allem in gekrümmten Räumen ist ein solches Netz aber unter Umständen nicht einfach zu konstruieren da es sich der Krümmung anpassen muss. Ein einfaches Beispiel ist die Sphäre mit Kugelkoordinaten: Am Äquator sind die -Schritte am größten, gegen die Pole werden sie immer kleiner. 
Geht man nun erst entlang des Äquators einen Schritt und dann z.B. nach Norden, so landet man an einem anderen Punkt als wenn man zuerst nach Norden gehen würde.
<br><br>Es gibt zwei wichtige Sätze, die eine Annahme über die Lie-Klammer als Voraussetzung<br>
haben: Sind  Vektorfelder mit  und sind die Felder linear unabhängig<br>
in , so gilt:<br>
<br>Verschwinden alle Lie-Klammern paarweise, so kann man in einer Umgebung von p eine Karte finden, so dass die gegebenen Vektorfelder tangential an Parameterlinien sind.
<br>Sind alle Lie-Klammern in der linearen Hülle der Vektorfelder enthalten, so kann man in einer Umgebung von p eine k-dimensionale Untermannigfaltigkeit finden, an die die gegebenen Vektorfelder in jedem Punkt tangential sind (Satz von Frobenius).
]]></description><link>the-guide/mathematics/differential-geometry/riemannian-geometry/mannigfaltigkeiten-und-differenzierbare-abbildungen/lie-klammer.html</link><guid isPermaLink="false">The Guide/Mathematics/Differential Geometry/Riemannian Geometry/Mannigfaltigkeiten und differenzierbare Abbildungen/Lie-Klammer.md</guid><pubDate>Wed, 23 Apr 2025 08:34:34 GMT</pubDate></item><item><title><![CDATA[Mannigfaltigkeiten]]></title><description><![CDATA[ 
 <br>In a Nutshell
<a data-tooltip-position="top" aria-label="Space" data-href="Space" href="the-guide/mathematics/general-stuff/space.html" class="internal-link" target="_self" rel="noopener nofollow">Raum</a>, der komplett über Karten abgedeckt werden kann, womit dessen lokale Struktur aussieht wie ein offener Teilraum des  (lokaler <a data-tooltip-position="top" aria-label="Mathematical Relations" data-href="Mathematical Relations" href="the-guide/mathematics/general-stuff/mathematical-relations.html" class="internal-link" target="_self" rel="noopener nofollow">Homöomorphismus</a>). Dahingegen bleibt die globale Struktur offen.
<br><img alt="center" src="lib/media/pasted-image-20240727142149.png" style="width: 500px; max-width: 100%;"><br><br>Topologische Mannigfaltigkeit
Eine topologische Mannigfaltigkeit ist ein  <a data-tooltip-position="top" aria-label="Topology and Topological Space" data-href="Topology and Topological Space" href="the-guide/mathematics/topology/topology-and-topological-space.html" class="internal-link" target="_self" rel="noopener nofollow">Haussdorff</a> <a data-tooltip-position="top" aria-label="Space" data-href="Space" href="the-guide/mathematics/general-stuff/space.html" class="internal-link" target="_self" rel="noopener nofollow">Raum</a>  der Dimension  (um Punkte unterscheiden zu können), der das  <a data-tooltip-position="top" aria-label="Topology and Topological Space" data-href="Topology and Topological Space" href="the-guide/mathematics/topology/topology-and-topological-space.html" class="internal-link" target="_self" rel="noopener nofollow">zweite Abzählbarkeitsaxiom</a> (abzählbare Basis) erfüllt und lokal euklidisch mit Dimension  ist.
<br>Insbesondere respektiert jede Karte die durch die <a data-tooltip-position="top" aria-label="Topology and Topological Space" data-href="Topology and Topological Space" href="the-guide/mathematics/topology/topology-and-topological-space.html" class="internal-link" target="_self" rel="noopener nofollow">Topologie</a> von   und  offenen Mengen, und Kartenwechsel sind, wo definiert, <a data-tooltip-position="top" aria-label="Mathematical Relations" data-href="Mathematical Relations" href="the-guide/mathematics/general-stuff/mathematical-relations.html" class="internal-link" target="_self" rel="noopener nofollow">Homöomorphismen</a> zwischen Teilmengen von .<br>
Mit Begriffen der <a data-tooltip-position="top" aria-label="Topology and Topological Space" data-href="Topology and Topological Space" href="the-guide/mathematics/topology/topology-and-topological-space.html" class="internal-link" target="_self" rel="noopener nofollow">Topologie</a> gibt es folgende weitere mögliche Strukturen:<br>
<br><a data-tooltip-position="top" aria-label="Topology and Topological Space > Connectedness" data-href="Topology and Topological Space#Connectedness" href="the-guide/mathematics/topology/topology-and-topological-space.html#Connectedness" class="internal-link" target="_self" rel="noopener nofollow">Zusammenhängende</a> Mannigfaltigkeiten - kann nicht als Vereinigung disjunkter Teile repräsentiert werden.

<br>Torus


<br>Einfach Zusammenhängende Mannigfaltigkeit - Mannigfaltigkeit hat keine Löcher etc., jede Schleife kann glatt auf Punkt zusammengezogen werden.

<br>Sphäre, Torus geht nicht mehr


<br>Kompakte Mannigfaltigkeit - Mannigfaltigkeit "ohne Rand" 

<br>Sphäre, Torus, etc


<br>Relativtopologie - jede offene Teilmenge einer topologischen Mannigfaltigkeit ist selber Mannigfaltigkeit.
<br>Differenzierbare Mannigfaltigkeit
Eine differenzierbare Mannigfaltigkeit ist eine topologische Mannigfaltigkeit zusammen mit einer <a data-tooltip-position="top" aria-label="Charts and Atlas" data-href="Charts and Atlas" href="the-guide/mathematics/differential-geometry/riemannian-geometry/mannigfaltigkeiten-und-differenzierbare-abbildungen/charts-and-atlas.html" class="internal-link" target="_self" rel="noopener nofollow">differenzierbaren Struktur</a> . Wir schreiben oft kurz  für eine differenzierbare Mannigfaltigkeit der Dimension . 
<br>
<br>Meist wird differenzierbar als  verstanden, man kann das Ganze aber auch für  durchführen
<br>Beispiele

<br>Sphären , <a data-tooltip-position="top" aria-label="Stereographic Projections" data-href="Stereographic Projections" href="Stereographic Projections" class="internal-link" target="_self" rel="noopener nofollow">stereographische Projektionen</a> (2) führen auf Atlas aus zwei Karten


<br>Untermannigfaltigkeit
Gilt für eine Inklusion  dass   und ist  <a data-tooltip-position="top" aria-label="Pushforward Operator on Manifolds" data-href="Pushforward Operator on Manifolds" href="the-guide/mathematics/differential-geometry/riemannian-geometry/mannigfaltigkeiten-und-differenzierbare-abbildungen/pushforward-operator-on-manifolds.html" class="internal-link" target="_self" rel="noopener nofollow">Einbettung</a> (injektives Differential und Homöomorphismus), so ist  Untermannigfaltigkeit von .  
<br><br>Definiert man auf der Mannigfaltigkeit eine <a data-tooltip-position="top" aria-label="(Semi-) Riemannsche Metriken" data-href="(Semi-) Riemannsche Metriken" href="the-guide/mathematics/differential-geometry/riemannian-geometry/metriken-und-zusammenhänge/(semi-)-riemannsche-metriken.html" class="internal-link" target="_self" rel="noopener nofollow">(semi-)Riemannsche</a> <a data-tooltip-position="top" aria-label="Metric Space and Completeness" data-href="Metric Space and Completeness" href="the-guide/mathematics/general-stuff/metric-space-and-completeness.html" class="internal-link" target="_self" rel="noopener nofollow">Metrik</a> , womit sich Begriffe der Geometrie wie Längen und Winkel aus Mannigfaltigkeiten definieren lassen, so erhält man ...<br>(Semi-) Riemannsche Mannigfaltigkeit
Das Tupel  heißt Riemannsche bzw. semi-/pseudo-Riemannsche Mannigfaltigkeit.
<br>
<br><a data-tooltip-position="top" aria-label="Isometrische Mannigfaltigkeiten" data-href="Isometrische Mannigfaltigkeiten" href="the-guide/mathematics/differential-geometry/riemannian-geometry/metriken-und-zusammenhänge/isometrische-mannigfaltigkeiten.html" class="internal-link" target="_self" rel="noopener nofollow">Isometrisch</a> - zwei Mannigfaltigkeiten lassen sich (ggfs. lokal) via Diffeomorphismus ineinander überführen (inkl. Metrik)
<br><a data-tooltip-position="top" aria-label="Isometrische Mannigfaltigkeiten" data-href="Isometrische Mannigfaltigkeiten" href="the-guide/mathematics/differential-geometry/riemannian-geometry/metriken-und-zusammenhänge/isometrische-mannigfaltigkeiten.html" class="internal-link" target="_self" rel="noopener nofollow">Homogen</a>, wenn man immer eine Isometrie findet, die zwei Punkte aufeinander abbildet
<br><a data-tooltip-position="top" aria-label="Isometrische Mannigfaltigkeiten" data-href="Isometrische Mannigfaltigkeiten" href="the-guide/mathematics/differential-geometry/riemannian-geometry/metriken-und-zusammenhänge/isometrische-mannigfaltigkeiten.html" class="internal-link" target="_self" rel="noopener nofollow">Isotrop</a>, wenn man für jeden Punkt immer eine Isometrie findet, die beliebige normierte (nach der jeweiligen Metrik zu beurteilen) Tangentialvektoren aufeinander abbildet (nur Riemannsch)

<br>Beispiel sind  unter Rotationen,  unter allgemeinen Bewegungen oder  unter .
<br>Gegenbeispiel Zylinder


<br>Total Geodätische Untermannigfaltigkeit
Eine semi-Riemannsche Untermannigfaltigkeit   einer semi-Riemannschen Mannigfaltigkeit  heißt total geodätisch, wenn jede Geodätische von  auch eine Geodätische von  ist.
<br>
<br>Beispiel ist Großkreis der Sphäre 
<br>Vollständigkeit

<br>Eine Riemann'sche Mannigfaltigkeit  ist metrisch vollständig, wenn <a data-tooltip-position="top" aria-label="Cauchy Sequence" data-href="Cauchy Sequence" href="the-guide/mathematics/general-stuff/cauchy-sequence.html" class="internal-link" target="_self" rel="noopener nofollow">Cauchy-Folgen</a> bzgl. dem geodätischen Abstand  konvergieren.
<br>Kann jede Geodätische auf  zu einer auf ganz  definierten Geodätischen fortgesetzt werden, so ist  geodätisch vollständig.

<br>
<br>Im Riemannschen Fall sind beide Begriffe äquivalent und man sagt oft einfach vollständige Mannigfaltigkeit
<br><br>Mannigfaltigkeiten konstanter Krümmung
Eine <a data-tooltip-position="top" aria-label="(Semi-) Riemannsche Metriken" data-href="(Semi-) Riemannsche Metriken" href="the-guide/mathematics/differential-geometry/riemannian-geometry/metriken-und-zusammenhänge/(semi-)-riemannsche-metriken.html" class="internal-link" target="_self" rel="noopener nofollow">semi-Riemannsche</a> Mannigfaltigkeit  hat konstante Krümmung, wenn die <a data-tooltip-position="top" aria-label="Sectional Curvature" data-href="Sectional Curvature" href="the-guide/mathematics/differential-geometry/riemannian-geometry/curvature/sectional-curvature.html" class="internal-link" target="_self" rel="noopener nofollow">Schnittkrümmung</a> in jedem Punkt konstant ist . Eine vollständig zusammenhängende Mannigfaltigkeit konstanter Krümmung heißt Raumform / space form.
]]></description><link>the-guide/mathematics/differential-geometry/riemannian-geometry/mannigfaltigkeiten-und-differenzierbare-abbildungen/mannigfaltigkeiten.html</link><guid isPermaLink="false">The Guide/Mathematics/Differential Geometry/Riemannian Geometry/Mannigfaltigkeiten und differenzierbare Abbildungen/Mannigfaltigkeiten.md</guid><pubDate>Wed, 23 Apr 2025 08:34:34 GMT</pubDate><enclosure url="lib/media/pasted-image-20240727142149.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src=&quot;lib/media/pasted-image-20240727142149.png&quot;&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[Pushforward Operator on Manifolds]]></title><description><![CDATA[ 
 <br>Während der Vorlesung als Differential bezeichnet.<br><br>Definition
Sei  eine <a data-tooltip-position="top" aria-label="Differenzierbare Abbildung zwischen Mannigfaltigkeiten" data-href="Differenzierbare Abbildung zwischen Mannigfaltigkeiten" href="the-guide/mathematics/differential-geometry/riemannian-geometry/mannigfaltigkeiten-und-differenzierbare-abbildungen/differenzierbare-abbildung-zwischen-mannigfaltigkeiten.html" class="internal-link" target="_self" rel="noopener nofollow">differenzierbare Abbildung</a> zwischen zwei <a data-href="Mannigfaltigkeiten" href="the-guide/mathematics/differential-geometry/riemannian-geometry/mannigfaltigkeiten-und-differenzierbare-abbildungen/mannigfaltigkeiten.html" class="internal-link" target="_self" rel="noopener nofollow">Mannigfaltigkeiten</a>. Das Differential  (oft auch ) ist die <a data-tooltip-position="top" aria-label="Mathematical Relations" data-href="Mathematical Relations" href="the-guide/mathematics/general-stuff/mathematical-relations.html" class="internal-link" target="_self" rel="noopener nofollow">Abbildung</a> wobei  <a data-tooltip-position="top" aria-label="Topology and Topological Space" data-href="Topology and Topological Space" href="the-guide/mathematics/topology/topology-and-topological-space.html" class="internal-link" target="_self" rel="noopener nofollow">Karte</a> von  um  und  von  um  ist. Wir verwenden auch die Notation Das Differential lässt sich in einem Punkt über die Zuordnung der Hauptteile schreiben als 
<br><img alt="center" src="lib/media/pasted-image-20240723171714.png" style="width: 500px; max-width: 100%;"><br><br>Lemma
Das Differential ist unabhängig von der Wahl der Karte.
<br>...<br><br>Immersion
Eine differenzierbare Abbildung  zwischen zwei <a data-href="Mannigfaltigkeiten" href="the-guide/mathematics/differential-geometry/riemannian-geometry/mannigfaltigkeiten-und-differenzierbare-abbildungen/mannigfaltigkeiten.html" class="internal-link" target="_self" rel="noopener nofollow">Mannigfaltigkeiten</a> heißt Immersion, wenn das <a data-tooltip-position="top" aria-label="Pushforward Operator on Manifolds" data-href="Pushforward Operator on Manifolds" href="the-guide/mathematics/differential-geometry/riemannian-geometry/mannigfaltigkeiten-und-differenzierbare-abbildungen/pushforward-operator-on-manifolds.html" class="internal-link" target="_self" rel="noopener nofollow">Differential</a>  für alle  <a data-tooltip-position="top" aria-label="Mathematical Relations" data-href="Mathematical Relations" href="the-guide/mathematics/general-stuff/mathematical-relations.html" class="internal-link" target="_self" rel="noopener nofollow">injektiv</a> ist.
<br>
<br>In jedem Tangentialraum  von  gibt es für jeden Vektor höchstens einen dazugehörigen Vektor im Tangentialraum 
<br> ist Immersion, da Differential von  (Kopie von ) auf Linie in  injektiv
<br>Einbettung
Immersion (differenzierbare Abbildung mit injektivem Differential), die <a data-tooltip-position="top" aria-label="Mathematical Relations" data-href="Mathematical Relations" href="the-guide/mathematics/general-stuff/mathematical-relations.html" class="internal-link" target="_self" rel="noopener nofollow">Homöomorphismus</a> auf ihr Bild ist. 
<br>
<br> erhält <a data-tooltip-position="top" aria-label="Topology and Topological Space" data-href="Topology and Topological Space" href="the-guide/mathematics/topology/topology-and-topological-space.html" class="internal-link" target="_self" rel="noopener nofollow">topologische Struktur</a>.
<br> keine Einbettung, da Einheitskreis ein Loch hat
]]></description><link>the-guide/mathematics/differential-geometry/riemannian-geometry/mannigfaltigkeiten-und-differenzierbare-abbildungen/pushforward-operator-on-manifolds.html</link><guid isPermaLink="false">The Guide/Mathematics/Differential Geometry/Riemannian Geometry/Mannigfaltigkeiten und differenzierbare Abbildungen/Pushforward Operator on Manifolds.md</guid><pubDate>Wed, 23 Apr 2025 08:34:34 GMT</pubDate><enclosure url="lib/media/pasted-image-20240723171714.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src=&quot;lib/media/pasted-image-20240723171714.png&quot;&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[Tangentialbündel]]></title><description><![CDATA[ 
 <br>In a Nutshell
Für  ist der <a data-href="Tangentialraum" href="the-guide/mathematics/differential-geometry/riemannian-geometry/mannigfaltigkeiten-und-differenzierbare-abbildungen/tangentialraum.html" class="internal-link" target="_self" rel="noopener nofollow">Tangentialraum</a> in jedem Punkt  derselbe . Für Untermannigfaltigkeiten  hat man zu verschiedenen Fußpunkten  dagegen verschiedene <a data-tooltip-position="top" aria-label="Vector Space" data-href="Vector Space" href="the-guide/mathematics/general-stuff/vector-space.html" class="internal-link" target="_self" rel="noopener nofollow">Untervektorräume</a> , Beispielsweise .
<br><br>Daher werden Tangentialvektoren in der Form (Fußpunkt, Vektor)  angegeben.<br>Tangentialbündel
Sei  -dimensionale <a data-tooltip-position="top" aria-label="Mannigfaltigkeiten" data-href="Mannigfaltigkeiten" href="the-guide/mathematics/differential-geometry/riemannian-geometry/mannigfaltigkeiten-und-differenzierbare-abbildungen/mannigfaltigkeiten.html" class="internal-link" target="_self" rel="noopener nofollow">Mannigfaltigkeit</a> mit <a data-tooltip-position="top" aria-label="Charts and Atlas" data-href="Charts and Atlas" href="the-guide/mathematics/differential-geometry/riemannian-geometry/mannigfaltigkeiten-und-differenzierbare-abbildungen/charts-and-atlas.html" class="internal-link" target="_self" rel="noopener nofollow">Atlas</a> . Auf der <a data-tooltip-position="top" aria-label="Set" data-href="Set" href="the-guide/mathematics/general-stuff/set.html" class="internal-link" target="_self" rel="noopener nofollow">Menge</a>  bestimmt dann mit der Abbildung  basierend auf der <a data-tooltip-position="top" aria-label="Topology and Topological Space" data-href="Topology and Topological Space" href="the-guide/mathematics/topology/topology-and-topological-space.html" class="internal-link" target="_self" rel="noopener nofollow">Karte</a>  um  und Tangentialvektor  einen Atlas der -dimensionalen Mannigfaltigkeit  unabhängig vom gewählten Atlas .Diese <a data-tooltip-position="top" aria-label="Mannigfaltigkeiten" data-href="Mannigfaltigkeiten" href="the-guide/mathematics/differential-geometry/riemannian-geometry/mannigfaltigkeiten-und-differenzierbare-abbildungen/mannigfaltigkeiten.html" class="internal-link" target="_self" rel="noopener nofollow">Mannigfaltigkeit</a>  nennen wir Tangentialbündel und schreiben Elemente als .
<br>
<br>Beispiel - im unteren Bild sind links einige Tangentialräume des Kreises, einer -dimensionalen Mannigfaltigkeit zu sehen. Rechts ist das entsprechende -dimensionale Tangentialbündel zu sehen.<br>
<img alt="center" src="lib/media/pasted-image-20240422175405.png" style="width: 400px; max-width: 100%;">
Intuition
Die Dimension  folgt daraus, dass wir für jeden Punkt auf der -dimensionalen Mannigfaltigkeit einen -dimensionalen Tangentialraum haben


]]></description><link>the-guide/mathematics/differential-geometry/riemannian-geometry/mannigfaltigkeiten-und-differenzierbare-abbildungen/tangentialbündel.html</link><guid isPermaLink="false">The Guide/Mathematics/Differential Geometry/Riemannian Geometry/Mannigfaltigkeiten und differenzierbare Abbildungen/Tangentialbündel.md</guid><pubDate>Wed, 23 Apr 2025 08:34:35 GMT</pubDate><enclosure url="lib/media/pasted-image-20240422175405.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src=&quot;lib/media/pasted-image-20240422175405.png&quot;&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[Tangentialraum]]></title><description><![CDATA[ 
 <br><br>Tangentialraum und Tangentialebene
Der Tangentialraum der Fläche  im Punkt  ist wobei die Elemente als Tangentialvektoren bezeichnet werden. Das Differential  ist <a data-tooltip-position="top" aria-label="Vector Space" data-href="Vector Space" href="the-guide/mathematics/general-stuff/vector-space.html" class="internal-link" target="_self" rel="noopener nofollow">Vektorraumisomorphismus</a> und es gilt . Der affine Unterraum von  in  wird dabei als Tangentialebene bezeichnet und ist die formale Verschiebung des Tangentialraumes an den entsprechenden Punkt.
<br><img alt="center" src="lib/media/pasted-image-20231118133554.png" style="width: 200px; max-width: 100%;"><br>Lemma
Der Tangentialraum ist <a data-tooltip-position="top" aria-label="Flächen" data-href="Flächen" href="the-guide/mathematics/differential-geometry/elementary-differential-geometry/flächen.html" class="internal-link" target="_self" rel="noopener nofollow">Eigenschaft</a> der Klasse .
<br><br><br>In der Sprache der Riemann'schen Geometrie hängt der Tangentialraum von der Wahl der <a data-tooltip-position="top" aria-label="Charts and Atlas" data-href="Charts and Atlas" href="the-guide/mathematics/differential-geometry/riemannian-geometry/mannigfaltigkeiten-und-differenzierbare-abbildungen/charts-and-atlas.html" class="internal-link" target="_self" rel="noopener nofollow">Karte</a> ab. Daher identifizieren wir die Tangentialvektoren verschiedener Karten mittels einer <a data-tooltip-position="top" aria-label="Equivalence Relation and Class" data-href="Equivalence Relation and Class" href="the-guide/mathematics/general-stuff/equivalence-relation-and-class.html" class="internal-link" target="_self" rel="noopener nofollow">Äquivalenzrelation</a>.<img alt="center" src="lib/media/pasted-image-20240722170141.png" style="width: 350px; max-width: 100%;"><br>Theorem - Äquivalenzrelation von Tangentialvektoren
Sei  eine -dimensionale <a data-tooltip-position="top" aria-label="Mannigfaltigkeiten" data-href="Mannigfaltigkeiten" href="the-guide/mathematics/differential-geometry/riemannian-geometry/mannigfaltigkeiten-und-differenzierbare-abbildungen/mannigfaltigkeiten.html" class="internal-link" target="_self" rel="noopener nofollow">Mannigfaltigkeit</a> und ,  zwei <a data-tooltip-position="top" aria-label="Topology and Topological Space" data-href="Topology and Topological Space" href="the-guide/mathematics/topology/topology-and-topological-space.html" class="internal-link" target="_self" rel="noopener nofollow">Karten</a> um . Für die Vektoren  im Punkt  erklärteine <a data-tooltip-position="top" aria-label="Equivalence Relation and Class" data-href="Equivalence Relation and Class" href="the-guide/mathematics/general-stuff/equivalence-relation-and-class.html" class="internal-link" target="_self" rel="noopener nofollow">Äquivalenzrelation</a>. In <a data-tooltip-position="top" aria-label="Matrices" data-href="Matrices" href="the-guide/mathematics/linear-algebra/matrices.html" class="internal-link" target="_self" rel="noopener nofollow">Matrix</a>-Schreibweise ist dies wobei  die lokalen Basiskomponenten der Karte sind. Tangentialvektoren transformieren also mit der <a data-tooltip-position="top" aria-label="Derivative, Gradient, Jacobian and Hessian > The Jacobian" data-href="Derivative, Gradient, Jacobian and Hessian#The Jacobian" href="the-guide/mathematics/analysis-and-calculus/derivative,-gradient,-jacobian-and-hessian.html#The_Jacobian" class="internal-link" target="_self" rel="noopener nofollow">Jacobi-Matrix</a> des Kartenwechsels.
<br>Tangentialraum 
Der Tangentialraum einer <a data-tooltip-position="top" aria-label="Mannigfaltigkeiten" data-href="Mannigfaltigkeiten" href="the-guide/mathematics/differential-geometry/riemannian-geometry/mannigfaltigkeiten-und-differenzierbare-abbildungen/mannigfaltigkeiten.html" class="internal-link" target="_self" rel="noopener nofollow">Mannigfaltigkeit</a>  im Punkt  ist der Raum der <a data-tooltip-position="top" aria-label="Equivalence Relation and Class" data-href="Equivalence Relation and Class" href="the-guide/mathematics/general-stuff/equivalence-relation-and-class.html" class="internal-link" target="_self" rel="noopener nofollow">Äquivalenzklassen</a> Ein Tangentialvektor  wird durch  repräsentiert. Wir nennen  dabei den Hauptteil des Tangentialvektors  bezüglich der Karte .
<br>Da die Hauptteile <a data-tooltip-position="top" aria-label="Vector Space" data-href="Vector Space" href="the-guide/mathematics/general-stuff/vector-space.html" class="internal-link" target="_self" rel="noopener nofollow">Vektorraum</a>-Struktur haben wird damit  zum <a data-tooltip-position="top" aria-label="Vector Space" data-href="Vector Space" href="the-guide/mathematics/general-stuff/vector-space.html" class="internal-link" target="_self" rel="noopener nofollow">Vektorraum</a>:<br>Theorem - Vektorraumstruktur des Tangentialraumes
Auf  gibt es eine <a data-tooltip-position="top" aria-label="Vector Space" data-href="Vector Space" href="the-guide/mathematics/general-stuff/vector-space.html" class="internal-link" target="_self" rel="noopener nofollow">Vektorraumstruktur</a>, so dass  für jede <a data-tooltip-position="top" aria-label="Topology and Topological Space" data-href="Topology and Topological Space" href="the-guide/mathematics/topology/topology-and-topological-space.html" class="internal-link" target="_self" rel="noopener nofollow">Karte</a>  <a data-tooltip-position="top" aria-label="Mathematical Relations" data-href="Mathematical Relations" href="the-guide/mathematics/general-stuff/mathematical-relations.html" class="internal-link" target="_self" rel="noopener nofollow">Isomorphismus</a> von -dimensionalen Vektorräumen ist.
<br>Proof
Folgt aus der Definition der Äquivalenzrelation: Die Kartenwechsel über  sind lineare Operationen, damit sind auch Linearkombinationen ÄquivalenzrelationenDa die Kartenwechsel zudem <a data-tooltip-position="top" aria-label="Mathematical Relations" data-href="Mathematical Relations" href="the-guide/mathematics/general-stuff/mathematical-relations.html" class="internal-link" target="_self" rel="noopener nofollow">bijektiv</a> sind, ist obige Abbildung Isomorphismus.
]]></description><link>the-guide/mathematics/differential-geometry/riemannian-geometry/mannigfaltigkeiten-und-differenzierbare-abbildungen/tangentialraum.html</link><guid isPermaLink="false">The Guide/Mathematics/Differential Geometry/Riemannian Geometry/Mannigfaltigkeiten und differenzierbare Abbildungen/Tangentialraum.md</guid><pubDate>Wed, 23 Apr 2025 08:34:35 GMT</pubDate><enclosure url="lib/media/pasted-image-20231118133554.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src=&quot;lib/media/pasted-image-20231118133554.png&quot;&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[Überlagerung]]></title><description><![CDATA[ 
 <br>In a Nutshell
Kann als detailliertere Version einer <a data-tooltip-position="top" aria-label="Mannigfaltigkeiten" data-href="Mannigfaltigkeiten" href="the-guide/mathematics/differential-geometry/riemannian-geometry/mannigfaltigkeiten-und-differenzierbare-abbildungen/mannigfaltigkeiten.html" class="internal-link" target="_self" rel="noopener nofollow">Mannigfaltigkeit</a> verstanden werde, die so strukturiert ist, dass sie die ursprüngliche Mannigfaltigkeit abdeckt. Hierbei haben lokale Nachbarschaften unter Umständen mehrere Kopien in der Überlagerung, sogenannte Blätter. Über den Lift kann man Wege auf die Überlagerung heben, was Eigenschaften der ursprünglichen Mannigfaltigkeit erklären kann.
<br><br>Überlagerung / Covering
Seien  <a data-tooltip-position="top" aria-label="Topology and Topological Space" data-href="Topology and Topological Space" href="the-guide/mathematics/topology/topology-and-topological-space.html" class="internal-link" target="_self" rel="noopener nofollow">zusammenhängende</a> <a data-tooltip-position="top" aria-label="Mannigfaltigkeiten" data-href="Mannigfaltigkeiten" href="the-guide/mathematics/differential-geometry/riemannian-geometry/mannigfaltigkeiten-und-differenzierbare-abbildungen/mannigfaltigkeiten.html" class="internal-link" target="_self" rel="noopener nofollow">topologische/differenzierbare/Riemannsche Mannigfaltigkeiten</a>. Eine <a data-tooltip-position="top" aria-label="Mathematical Relations" data-href="Mathematical Relations" href="the-guide/mathematics/general-stuff/mathematical-relations.html" class="internal-link" target="_self" rel="noopener nofollow">surjektive Abbildung</a>die stetig/differenzierbar/(lokal) isometrisch ist heißt topologische/differenzierbare/isometrische Überlagerung, wenn es für jedes  eine Umgebung  gibt, deren Urbild eine disjunkte Vereinigung istwobei  <a data-tooltip-position="top" aria-label="Mathematical Relations" data-href="Mathematical Relations" href="the-guide/mathematics/general-stuff/mathematical-relations.html" class="internal-link" target="_self" rel="noopener nofollow">Homöomorphismus/Diffeomorphismus/globale Isometrie</a> ist.  ist offene <a data-tooltip-position="top" aria-label="Set" data-href="Set" href="the-guide/mathematics/general-stuff/set.html" class="internal-link" target="_self" rel="noopener nofollow">Teilmenge</a> von  und  eine Indexmenge. Ist zudem  <a data-tooltip-position="top" aria-label="Topology and Topological Space" data-href="Topology and Topological Space" href="the-guide/mathematics/topology/topology-and-topological-space.html" class="internal-link" target="_self" rel="noopener nofollow">einfach zusammenhängend</a>, so heißt die Überlagerung universell.
<br>
<br>Es gilt immer 
<br>Die Relation Überlagerung ist transitiv: wenn   und  Überlagerungen sind, dann ist  Überlagerung.
<br><br>Blätterzahl einer Überlagerung
Die diskrete (aber potentiell unendliche) <a data-tooltip-position="top" aria-label="Set" data-href="Set" href="the-guide/mathematics/general-stuff/set.html" class="internal-link" target="_self" rel="noopener nofollow">Menge</a>wird Blätterzahl oder diskrete Fasern genannt und ist unabhängig von .
<br>
<br>Ist die Blätterzahl nicht diskret, beispielsweise  für , so führt dies auf den Begriff der Faserung
<br>Beispiele

<br>,  ist universelle Überlagerung mit 

<br>Alle Punkte im Abstand 1 werden auf den selben Punkt gemappt


<br> ,  ist -blättrige Überlagerung mit 

<br>Je nach Durchlaufgeschwindigkeit wird der Kreis  mal auf sich selbst gemappt


<br> mit Abbildung via <a data-tooltip-position="top" aria-label="A Hitchhiker's Guide to Rotation Representations > Quaternions" data-href="A Hitchhiker's Guide to Rotation Representations#Quaternions" href="the-guide/robotics,-dynamics-and-control/a-hitchhiker's-guide-to-rotation-representations.html#Quaternions" class="internal-link" target="_self" rel="noopener nofollow">Quaternionen</a> ist universelle zweiblättrige Überlagerung (Double Cover). Hierbei beschreibt für ein   der Realteil in  die Drehachse und der Imaginärteil den Drehwinkel. 


<br>Lift
Eine wichtige Eigenschaft von Überlagerungen verallgemeinert das <a data-href="Lifting Lemma" href="the-guide/mathematics/differential-geometry/elementary-differential-geometry/lifting-lemma.html" class="internal-link" target="_self" rel="noopener nofollow">Lifting Lemma</a>: 

<br>Ist  eine <a data-tooltip-position="top" aria-label="Raumkurven" data-href="Raumkurven" href="the-guide/mathematics/differential-geometry/elementary-differential-geometry/raumkurven.html" class="internal-link" target="_self" rel="noopener nofollow">Kurve</a> mit  und  ein Punkt mit , so gibt es eine eindeutig bestimmte Kurve  mit  und . 
<br>Genauso lassen sich <a data-tooltip-position="top" aria-label="Homotope Kurven" data-href="Homotope Kurven" href="the-guide/mathematics/differential-geometry/elementary-differential-geometry/globale-kurven-und-flächentheorie/homotope-kurven.html" class="internal-link" target="_self" rel="noopener nofollow">Homotopien</a> in  eindeutig liften. Die Kurve  ist der Lift von .

<br>Korollar 23
Ist  <a data-tooltip-position="top" aria-label="Topology and Topological Space" data-href="Topology and Topological Space" href="the-guide/mathematics/topology/topology-and-topological-space.html" class="internal-link" target="_self" rel="noopener nofollow">einfach zusammenhängend</a>, so ist jede Überlagerung  einblättrig,  ist also <a data-tooltip-position="top" aria-label="Mathematical Relations" data-href="Mathematical Relations" href="the-guide/mathematics/general-stuff/mathematical-relations.html" class="internal-link" target="_self" rel="noopener nofollow">bijektiv</a>.
<br>Proof
Für einen Punkt  sei .

<br>Wir beginnen mit einer Kurve  von  nach  in . Diese existiert immer, da  ebenfalls zusammenhängend sein muss wenn  es ist.
<br>Das Bild  der Kurve auf  ist eine Schleife, lässt sich also mittels Homotopie auf den Punkt  zusammenziehen.
<br>Diese Homotopie lässt sich wieder nach  liften, wo per Konstruktion  und  gilt.
<br>Das muss jedoch für jedes  gelten, also auch für , wo wir nur einen Punkt 
<br>liften. Damit ist 

<br><br><br>Satz - Universelle Überlagerung
Ist zusätzlich zu obiger Definition  <a data-tooltip-position="top" aria-label="Topology and Topological Space" data-href="Topology and Topological Space" href="the-guide/mathematics/topology/topology-and-topological-space.html" class="internal-link" target="_self" rel="noopener nofollow">einfach zusammenhängend</a>, so heißt die Überlagerung universell. Die universelle Überlagerung  einer <a data-tooltip-position="top" aria-label="Mannigfaltigkeiten" data-href="Mannigfaltigkeiten" href="the-guide/mathematics/differential-geometry/riemannian-geometry/mannigfaltigkeiten-und-differenzierbare-abbildungen/mannigfaltigkeiten.html" class="internal-link" target="_self" rel="noopener nofollow">Mannigfaltigkeit</a> kann als Raum der Endpunkte <a data-tooltip-position="top" aria-label="Homotope Kurven" data-href="Homotope Kurven" href="the-guide/mathematics/differential-geometry/elementary-differential-geometry/globale-kurven-und-flächentheorie/homotope-kurven.html" class="internal-link" target="_self" rel="noopener nofollow">homotoper Kurven</a> definiert werdenunter der <a data-tooltip-position="top" aria-label="Equivalence Relation and Class" data-href="Equivalence Relation and Class" href="the-guide/mathematics/general-stuff/equivalence-relation-and-class.html" class="internal-link" target="_self" rel="noopener nofollow">Äquivalenzrelation</a>  für homotope . Jede Mannigfaltigkeit besitzt eine solche universelle Überlagerung, die selbst wieder Mannigfaltigkeit ist.
<br>Intuition
Beschreibt die maximal entwirrte Version einer Mannigfaltigkeit, bei der alle möglichen Schleifen aufgehoben werden, indem sie auf offene Linien transformiert werden.
<br>
<br>Beispiele

<br> für , Schleifen werden zu offener Strecken
<br>Für den Torus erhält man eine unendliche Ebene (Gitterplan), man stelle sich den aufgeschnittenen Torus vor, der unendlich wiederholt wird.Jede Schleife wird erneut zur Linie
<br>Für höherdimensionale Sphären ist die universelle Überlagerung die Sphäre selbst, da alle Schleifen zusammengezogen werden können


<br>Damit gilt umgekehrt in Verbindung mit der <a data-tooltip-position="top" aria-label="Fundamentalgruppe einer Mannigfaltigkeit" data-href="Fundamentalgruppe einer Mannigfaltigkeit" href="the-guide/mathematics/differential-geometry/riemannian-geometry/mannigfaltigkeiten-und-differenzierbare-abbildungen/fundamentalgruppe-einer-mannigfaltigkeit.html" class="internal-link" target="_self" rel="noopener nofollow">Fundamentalgruppe</a> :<br>Korollar
Für jede universelle Überlagerung  gilt ...

<br>. 
<br>Ist  kompakt, so muss  endlich sein.

]]></description><link>the-guide/mathematics/differential-geometry/riemannian-geometry/mannigfaltigkeiten-und-differenzierbare-abbildungen/überlagerung.html</link><guid isPermaLink="false">The Guide/Mathematics/Differential Geometry/Riemannian Geometry/Mannigfaltigkeiten und differenzierbare Abbildungen/Überlagerung.md</guid><pubDate>Wed, 23 Apr 2025 08:34:35 GMT</pubDate></item><item><title><![CDATA[Vektorfelder auf Mannigfaltigkeiten]]></title><description><![CDATA[ 
 <br>Definition
Ein (differenzierbares) Vektorfeld  auf einer <a data-tooltip-position="top" aria-label="Mannigfaltigkeiten" data-href="Mannigfaltigkeiten" href="the-guide/mathematics/differential-geometry/riemannian-geometry/mannigfaltigkeiten-und-differenzierbare-abbildungen/mannigfaltigkeiten.html" class="internal-link" target="_self" rel="noopener nofollow">Mannigfaltigkeit</a>  ist eine <a data-tooltip-position="top" aria-label="Differenzierbare Abbildung zwischen Mannigfaltigkeiten" data-href="Differenzierbare Abbildung zwischen Mannigfaltigkeiten" href="the-guide/mathematics/differential-geometry/riemannian-geometry/mannigfaltigkeiten-und-differenzierbare-abbildungen/differenzierbare-abbildung-zwischen-mannigfaltigkeiten.html" class="internal-link" target="_self" rel="noopener nofollow">differenzierbare Abbildung</a> Die <a data-tooltip-position="top" aria-label="Set" data-href="Set" href="the-guide/mathematics/general-stuff/set.html" class="internal-link" target="_self" rel="noopener nofollow">Menge</a> aller Vektorfelder auf  bezeichnen wir mit .
<br>Bezüglich einer <a data-tooltip-position="top" aria-label="Charts and Atlas" data-href="Charts and Atlas" href="the-guide/mathematics/differential-geometry/riemannian-geometry/mannigfaltigkeiten-und-differenzierbare-abbildungen/charts-and-atlas.html" class="internal-link" target="_self" rel="noopener nofollow">Karte</a>  können wir den repräsentierenden Hauptteil des Vektorfeldes als eine Funktion  schreiben. Ist   ein Element der Standardbasis bzgl. der Karte , so können wir schreiben Das Feld  ist differenzierbar, wenn die <a data-tooltip-position="top" aria-label="Mathematical Relations" data-href="Mathematical Relations" href="the-guide/mathematics/general-stuff/mathematical-relations.html" class="internal-link" target="_self" rel="noopener nofollow">Abbildung</a>  für alle Karten differenzierbar ist.<br><img alt="center" src="lib/media/pasted-image-20240722181042.png" style="width: 500px; max-width: 100%;">]]></description><link>the-guide/mathematics/differential-geometry/riemannian-geometry/mannigfaltigkeiten-und-differenzierbare-abbildungen/vektorfelder-auf-mannigfaltigkeiten.html</link><guid isPermaLink="false">The Guide/Mathematics/Differential Geometry/Riemannian Geometry/Mannigfaltigkeiten und differenzierbare Abbildungen/Vektorfelder auf Mannigfaltigkeiten.md</guid><pubDate>Mon, 12 Aug 2024 17:05:43 GMT</pubDate><enclosure url="lib/media/pasted-image-20240722181042.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src=&quot;lib/media/pasted-image-20240722181042.png&quot;&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[(Semi-) Riemannsche Metriken]]></title><description><![CDATA[ 
 <br>In a Nutshell
Verallgemeinerung der <a data-tooltip-position="top" aria-label="1. Fundamentalform" data-href="1. Fundamentalform" href="the-guide/mathematics/differential-geometry/elementary-differential-geometry/1.-fundamentalform.html" class="internal-link" target="_self" rel="noopener nofollow">ersten Fundamentalform</a> auf <a data-href="Mannigfaltigkeiten" href="the-guide/mathematics/differential-geometry/riemannian-geometry/mannigfaltigkeiten-und-differenzierbare-abbildungen/mannigfaltigkeiten.html" class="internal-link" target="_self" rel="noopener nofollow">Mannigfaltigkeiten</a>. Erweitert die Struktur einer Mannigfaltigkeit, indem Sie es ermöglicht Längen und Winkel zu messen.
<br><br>Additional Information
Eine symmetrisch <a data-tooltip-position="top" aria-label="Bilinear Form" data-href="Bilinear Form" href="the-guide/mathematics/linear-algebra/bilinear-form.html" class="internal-link" target="_self" rel="noopener nofollow">Bilinearform</a>  auf einem <a data-tooltip-position="top" aria-label="Vector Space" data-href="Vector Space" href="the-guide/mathematics/general-stuff/vector-space.html" class="internal-link" target="_self" rel="noopener nofollow">Vektorraum</a>  heißt ausgeartet, wenn es ein  gibt, sodass  für alle . Der Index  der Bilinearform bezeichnet die maximale Dimension eines Unterraums, auf dem  gilt. Die Form ist positiv definit, wenn  oder äquivalent  für alle  außer dem Nullvektor.
<br><br>(Semi-) Riemannsche Metrik
Sei  eine <a data-tooltip-position="top" aria-label="Mannigfaltigkeiten" data-href="Mannigfaltigkeiten" href="the-guide/mathematics/differential-geometry/riemannian-geometry/mannigfaltigkeiten-und-differenzierbare-abbildungen/mannigfaltigkeiten.html" class="internal-link" target="_self" rel="noopener nofollow">differenzierbare Mannigfaltigkeit</a>. Eine semi-Riemannsche Metrik vom Index  auf  ist eine <a data-tooltip-position="top" aria-label="Mathematical Relations" data-href="Mathematical Relations" href="the-guide/mathematics/general-stuff/mathematical-relations.html" class="internal-link" target="_self" rel="noopener nofollow">Abbildung</a>so dass in jedem Punkt  die Einschränkung  eine nicht ausgeartete symmetrische Bilinearform mit Index  ist. Ist  positiv definit für alle , so heißt die <a data-tooltip-position="top" aria-label="Metric Space and Completeness" data-href="Metric Space and Completeness" href="the-guide/mathematics/general-stuff/metric-space-and-completeness.html" class="internal-link" target="_self" rel="noopener nofollow">Metrik</a> Riemansch.<br>
Diese definitionen sind äquivalent dazu, dass bezüglich jeder Karte  die Funktionen differenzierbar sind.
<br>
<br>Das Tupel  heißt Riemannsche bzw. semi-/pseudo-Riemannsche Mannigfaltigkeit.
<br>Lokal erhält man mit  und  den Ausdruck 
<br>Für klassische Flächen, also  <a data-tooltip-position="top" aria-label="Flächen" data-href="Flächen" href="the-guide/mathematics/differential-geometry/elementary-differential-geometry/flächen.html" class="internal-link" target="_self" rel="noopener nofollow">Immersion</a> wird auf  eine Riemannsche Metrik durch <a data-tooltip-position="top" aria-label="Mathematical Relations" data-href="Mathematical Relations" href="the-guide/mathematics/general-stuff/mathematical-relations.html" class="internal-link" target="_self" rel="noopener nofollow">pullback</a>  der Standardmetrik via  definiert. Ist  offene <a data-tooltip-position="top" aria-label="Set" data-href="Set" href="the-guide/mathematics/general-stuff/set.html" class="internal-link" target="_self" rel="noopener nofollow">Teilmenge</a> (kein Rand) des , so ist die erklärte Metrik die bekannte <a data-href="1. Fundamentalform" href="the-guide/mathematics/differential-geometry/elementary-differential-geometry/1.-fundamentalform.html" class="internal-link" target="_self" rel="noopener nofollow">1. Fundamentalform</a>.
<br><br><br>Korollar
Auf jeder differenzierbaren Mannigfaltigkeit exisitert eine <a data-tooltip-position="top" aria-label="(Semi-) Riemannsche Metriken" data-href="(Semi-) Riemannsche Metriken" href="the-guide/mathematics/differential-geometry/riemannian-geometry/metriken-und-zusammenhänge/(semi-)-riemannsche-metriken.html" class="internal-link" target="_self" rel="noopener nofollow">Riemannsche Metrig</a> .
<br>Proof 
Für gegebene <a data-tooltip-position="top" aria-label="Charts and Atlas" data-href="Charts and Atlas" href="the-guide/mathematics/differential-geometry/riemannian-geometry/mannigfaltigkeiten-und-differenzierbare-abbildungen/charts-and-atlas.html" class="internal-link" target="_self" rel="noopener nofollow">Karte</a>  eines Atlas  holen wir die Standardmetrik von  zurück. Damit erhalten wir zunächst nur für die Umgebung, auf der die karte definiert warauf dem Rest der Mannigfaltigkeit setzen wir zunächst unstetig die Metrik zu . Betrachtet man nun eine <a data-tooltip-position="top" aria-label="Partition of Unity" data-href="Partition of Unity" href="the-guide/mathematics/general-stuff/partition-of-unity.html" class="internal-link" target="_self" rel="noopener nofollow">Zerlegung der Eins</a>  relativ zum gegebenen Atlas , so ist in jedem Punkt eine endliche summe und differenzierbar von  abhängig. Zudem erhalten sich die Eigenschaften der bilinear, positiv definit und symmetrisch.
<br>gilt nicht für semi Riemansch - gekämmter Igel !<br><br>Konforme Metriken
Eine semi-Riemannsche Metrik  auf einer <a data-tooltip-position="top" aria-label="Mannigfaltigkeiten" data-href="Mannigfaltigkeiten" href="the-guide/mathematics/differential-geometry/riemannian-geometry/mannigfaltigkeiten-und-differenzierbare-abbildungen/mannigfaltigkeiten.html" class="internal-link" target="_self" rel="noopener nofollow">Mannigfaltigkeit</a>  heißt konform zur <a data-tooltip-position="top" aria-label="Metric Space and Completeness" data-href="Metric Space and Completeness" href="the-guide/mathematics/general-stuff/metric-space-and-completeness.html" class="internal-link" target="_self" rel="noopener nofollow">Metrik</a> , wenn eine Funktion  existiert, sodass 
<br>
<br>Winkelerhaltend
<br><br>Analog zur <a data-href="1. Fundamentalform" href="the-guide/mathematics/differential-geometry/elementary-differential-geometry/1.-fundamentalform.html" class="internal-link" target="_self" rel="noopener nofollow">1. Fundamentalform</a> lassen sich nun Eigenschaften intrinsisch berechnen:<br>Riemannsche Länge
Für eine <a data-tooltip-position="top" aria-label="Mannigfaltigkeiten" data-href="Mannigfaltigkeiten" href="the-guide/mathematics/differential-geometry/riemannian-geometry/mannigfaltigkeiten-und-differenzierbare-abbildungen/mannigfaltigkeiten.html" class="internal-link" target="_self" rel="noopener nofollow">Riemannsche Mannigfaltigkeit</a>  ist die Länge definiert durchDies führt auf die definition der Bogenlänge 
<br>Winkel im Tangentialraum
Ebenso definieren wir den Riemannschen Winkel über 
<br>Volumen
Das Volumen des <a data-tooltip-position="top" aria-label="Topology and Topological Space" data-href="Topology and Topological Space" href="the-guide/mathematics/topology/topology-and-topological-space.html" class="internal-link" target="_self" rel="noopener nofollow">Kartenbildes</a>  berechnet sich über 
]]></description><link>the-guide/mathematics/differential-geometry/riemannian-geometry/metriken-und-zusammenhänge/(semi-)-riemannsche-metriken.html</link><guid isPermaLink="false">The Guide/Mathematics/Differential Geometry/Riemannian Geometry/Metriken und Zusammenhänge/(Semi-) Riemannsche Metriken.md</guid><pubDate>Wed, 23 Apr 2025 08:34:35 GMT</pubDate></item><item><title><![CDATA[Exponentialabbildung]]></title><description><![CDATA[ 
 <br>In a Nutshell
Analog zur klassischen Differentialgeometrie lassen sich im Riemannschen Fall <a data-href="Geodätische" href="the-guide/mathematics/differential-geometry/elementary-differential-geometry/intrinsische-geometrie-von-hyperflächen/geodätische.html" class="internal-link" target="_self" rel="noopener nofollow">Geodätische</a> als Kurven ohne intrinsische Änderung der Geschwindigkeit definieren. Wir wollen die daraus resultierende Abbildung studieren, wobei als zentrale Frage die Existenz eines Diffeomorphismus auftreten wird.
<br><br><br>Mit dem erweiterten Formalismus der <a data-tooltip-position="top" aria-label="Kovariante Ableitung" data-href="Kovariante Ableitung" href="the-guide/mathematics/differential-geometry/riemannian-geometry/metriken-und-zusammenhänge/kovariante-ableitung.html" class="internal-link" target="_self" rel="noopener nofollow">kovarianten Ableitung</a> erhalten wir die Bedingung  für eine Kurve  in . Um eine lokale Darstellung zu erhalten betrachten wir den Hauptteil  bezüglich der <a data-tooltip-position="top" aria-label="Topology and Topological Space" data-href="Topology and Topological Space" href="the-guide/mathematics/topology/topology-and-topological-space.html" class="internal-link" target="_self" rel="noopener nofollow">Karte</a>  und erhalten <br>Satz - DGL für Geodätische
Damit ist  Geodätische genau dann, wenn  das nicht-lineare System <a data-tooltip-position="top" aria-label="ODEs - Ordinary Differential Equations" data-href="ODEs - Ordinary Differential Equations" href="the-guide/mathematics/differential-equations/odes-ordinary-differential-equations.html" class="internal-link" target="_self" rel="noopener nofollow">gewöhnlicher Differentialgleichungen</a> erfüllt. Wir schreiben dann .
<br>Intuition
Die Bilinearform kann folgendermaßen interpretiert werden:

<br>Wir berechnen mit den <a data-tooltip-position="top" aria-label="Christoffel-Symbol" data-href="Christoffel-Symbol" href="the-guide/mathematics/differential-geometry/elementary-differential-geometry/intrinsische-geometrie-von-hyperflächen/christoffel-symbol.html" class="internal-link" target="_self" rel="noopener nofollow">Christoffel-Symbolen</a> den Änderungsvektor von  wenn wir in einen benachbarten Tangentialraum übergehen.
<br>Wir messen mittels Skalarprodukt wie sehr diese Änderung mit unserer Richtung übereinstimmt 
<br>Wir adjustieren unsere Richtung so, dass Sie sich genau entgegen dieser Richtung ändert, damit  tangential immer gleich bleibt.

<br>Satz - Geodätische als geometrische Größe
Geodätische sing <a data-tooltip-position="top" aria-label="Geometrische Größen" data-href="Geometrische Größen" href="the-guide/mathematics/differential-geometry/elementary-differential-geometry/lokale-kurventheorie/geometrische-größen.html" class="internal-link" target="_self" rel="noopener nofollow">geometrische Größen</a>/Objekte: Fpr  geodätisch und  <a data-tooltip-position="top" aria-label="Isometrische Mannigfaltigkeiten" data-href="Isometrische Mannigfaltigkeiten" href="the-guide/mathematics/differential-geometry/riemannian-geometry/metriken-und-zusammenhänge/isometrische-mannigfaltigkeiten.html" class="internal-link" target="_self" rel="noopener nofollow">lokale Isometrie</a> ist auch  geodätisch.
<br>Satz - lineare Umparametrisierung von Geodätischen
Die lineare Umparametrisierung  einer Geodätischen  ist auf  geodätisch mit Anfangswerten  und . 
<br><br><br>Exponentialabbildung
Sei  <a data-tooltip-position="top" aria-label="(Semi-) Riemannsche Metriken" data-href="(Semi-) Riemannsche Metriken" href="the-guide/mathematics/differential-geometry/riemannian-geometry/metriken-und-zusammenhänge/(semi-)-riemannsche-metriken.html" class="internal-link" target="_self" rel="noopener nofollow">semi-Riemannsche</a> <a data-tooltip-position="top" aria-label="Mannigfaltigkeiten" data-href="Mannigfaltigkeiten" href="the-guide/mathematics/differential-geometry/riemannian-geometry/mannigfaltigkeiten-und-differenzierbare-abbildungen/mannigfaltigkeiten.html" class="internal-link" target="_self" rel="noopener nofollow">Mannigfaltigkeit</a>,  und . Die Abbildung heißt Exponentialabbildung und ist alleine durch die Lösung der obigen DGL definiert.
<br>
<br>Existenz - Es gibt <a data-tooltip-position="top" aria-label="Set" data-href="Set" href="the-guide/mathematics/general-stuff/set.html" class="internal-link" target="_self" rel="noopener nofollow">offene Umgebung</a>  des Nullschnitts , auf der die Exponentialabbildung definiert ist. Folgt anschaulich daraus, das Mannigfaltigkeiten ohne Rand, also durch offene Gebiete gebildet werden und wir damit immer eine solche Umgebung definieren können.
<br>Differenzierbarkeit - Die obige Abbildung hängt differenzierbar von den Anfangswerten ab und ist damit differenzierbar.
<br>Korollar - lokale Existenz
Für jedes  besitzt der Tangentialvektor  eine Umgebung , sodass  <a data-tooltip-position="top" aria-label="Mathematical Relations" data-href="Mathematical Relations" href="the-guide/mathematics/general-stuff/mathematical-relations.html" class="internal-link" target="_self" rel="noopener nofollow">Diffeomorphismus</a> auf sein Bild ist.
<br>Injektivitätsradius
Man kann für eine gegebene <a data-tooltip-position="top" aria-label="Mannigfaltigkeiten" data-href="Mannigfaltigkeiten" href="the-guide/mathematics/differential-geometry/riemannian-geometry/mannigfaltigkeiten-und-differenzierbare-abbildungen/mannigfaltigkeiten.html" class="internal-link" target="_self" rel="noopener nofollow">Mannigfaltigkeit</a> die untere Schranke der <a data-tooltip-position="top" aria-label="Mathematical Relations" data-href="Mathematical Relations" href="the-guide/mathematics/general-stuff/mathematical-relations.html" class="internal-link" target="_self" rel="noopener nofollow">injektiven</a> Radien (im <a data-tooltip-position="top" aria-label="Set" data-href="Set" href="the-guide/mathematics/general-stuff/set.html" class="internal-link" target="_self" rel="noopener nofollow">Supremum</a>) aller Punkte betrachten und damit den Injektivitätsradius definieren:Beispielsweise erhält man , ,  (Zylinder),  (flacher Torus). Der Radius verschwindet für Mannigfaltigkeiten mit kleiner werdenden Lächern oder Spitzen.
<br><br><br>Differential der Exponentialabbildung
Das <a data-href="Pushforward Operator on Manifolds" href="the-guide/mathematics/differential-geometry/riemannian-geometry/mannigfaltigkeiten-und-differenzierbare-abbildungen/pushforward-operator-on-manifolds.html" class="internal-link" target="_self" rel="noopener nofollow">Pushforward Operator on Manifolds</a> der Exponentialabbildung ist die <a data-tooltip-position="top" aria-label="Mathematical Relations" data-href="Mathematical Relations" href="the-guide/mathematics/general-stuff/mathematical-relations.html" class="internal-link" target="_self" rel="noopener nofollow">Abbildung</a> es beschreibt also  die Änderungsrichtung in  , wenn man  in Richtung  ändert.
<br><img alt="center" src="lib/media/pasted-image-20240520144519.png" style="width: 450px; max-width: 100%;"><br>
<br>Identifikation - Wie so oft wird der <a data-href="Tangentialraum" href="the-guide/mathematics/differential-geometry/riemannian-geometry/mannigfaltigkeiten-und-differenzierbare-abbildungen/tangentialraum.html" class="internal-link" target="_self" rel="noopener nofollow">Tangentialraum</a> an einen <a data-tooltip-position="top" aria-label="Vector Space" data-href="Vector Space" href="the-guide/mathematics/general-stuff/vector-space.html" class="internal-link" target="_self" rel="noopener nofollow">Vektorraum</a> mit dem Vektorraum selbst identifiziert. Gedanklich verschieben wir einfach durch Vergessen des Fußpunktes und identifizieren so Erst darüber können wir schreiben , ansonsten hätten wir das Problem verschiedener Räume.
<br>Identität - Mit Argumenten der Variationsrechnung erhält man die Gleichung 
]]></description><link>the-guide/mathematics/differential-geometry/riemannian-geometry/metriken-und-zusammenhänge/exponentialabbildung.html</link><guid isPermaLink="false">The Guide/Mathematics/Differential Geometry/Riemannian Geometry/Metriken und Zusammenhänge/Exponentialabbildung.md</guid><pubDate>Wed, 23 Apr 2025 08:34:35 GMT</pubDate><enclosure url="lib/media/pasted-image-20240520144519.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src=&quot;lib/media/pasted-image-20240520144519.png&quot;&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[Gauß Lemma]]></title><description><![CDATA[ 
 <br>Lemma
Sei  <a data-tooltip-position="top" aria-label="(Semi-) Riemannsche Metriken" data-href="(Semi-) Riemannsche Metriken" href="the-guide/mathematics/differential-geometry/riemannian-geometry/metriken-und-zusammenhänge/(semi-)-riemannsche-metriken.html" class="internal-link" target="_self" rel="noopener nofollow">semi-riemannsche</a> <a data-tooltip-position="top" aria-label="Mannigfaltigkeiten" data-href="Mannigfaltigkeiten" href="the-guide/mathematics/differential-geometry/riemannian-geometry/mannigfaltigkeiten-und-differenzierbare-abbildungen/mannigfaltigkeiten.html" class="internal-link" target="_self" rel="noopener nofollow">Mannigfaltigkeit</a> und  sowie , so dass die <a data-href="Exponentialabbildung" href="the-guide/mathematics/differential-geometry/riemannian-geometry/metriken-und-zusammenhänge/exponentialabbildung.html" class="internal-link" target="_self" rel="noopener nofollow">Exponentialabbildung</a>  durch die Identifikation definiert ist. Dann gilt für jedes  die Gleichung 
<br>Intuitiv stehen die Bilder von Abstands-Sphären damit senkrecht aufeinander.<img alt="center" src="lib/media/pasted-image-20240610141848.png" style="width: 500px; max-width: 100%;"><br>Beweisskizze

<br>Man betrachtet die Linke Seite als Variation der kurve . Dazu drückt man die <a data-tooltip-position="top" aria-label="Variation der Energie" data-href="Variation der Energie" href="the-guide/mathematics/differential-geometry/riemannian-geometry/variation-der-energie.html" class="internal-link" target="_self" rel="noopener nofollow">Variation</a> aus als womit  gilt.

<br>Folgt mit Kettenregel, e.g. und analog .
<br>Damit lässt sich die rechte Seite schreiben als wobei wir  benutzen.


<br>Wir formulieren den Integranden um

<br>Erste Umformung beruht auf Eigenschaft der <a data-tooltip-position="top" aria-label="Kovariante Ableitung" data-href="Kovariante Ableitung" href="the-guide/mathematics/differential-geometry/riemannian-geometry/metriken-und-zusammenhänge/kovariante-ableitung.html" class="internal-link" target="_self" rel="noopener nofollow">kovarianten Ableitung</a>
<br>Zweite Umformung ist <a data-tooltip-position="top" aria-label="Kovariante Ableitung" data-href="Kovariante Ableitung" href="the-guide/mathematics/differential-geometry/riemannian-geometry/metriken-und-zusammenhänge/kovariante-ableitung.html" class="internal-link" target="_self" rel="noopener nofollow">Kovarianter Satz von Schwartz</a>
<br>Dritte Umformung folgt aus erster in Kombination mit partieller Integration


<br>Da die radialen kurven entlang  geodätische sind ist ihre Geschwindigkeit konstant, also die Anfangsgeschwindigkeit .
<br>Damit folgt für die Terme im IntegralDamit ist der Integrand konstant und es bleibt bei den gegeben Grenzen nur  übrig.

]]></description><link>the-guide/mathematics/differential-geometry/riemannian-geometry/metriken-und-zusammenhänge/gauß-lemma.html</link><guid isPermaLink="false">The Guide/Mathematics/Differential Geometry/Riemannian Geometry/Metriken und Zusammenhänge/Gauß Lemma.md</guid><pubDate>Mon, 12 Aug 2024 17:05:43 GMT</pubDate><enclosure url="lib/media/pasted-image-20240610141848.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src=&quot;lib/media/pasted-image-20240610141848.png&quot;&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[Isometrische Mannigfaltigkeiten]]></title><description><![CDATA[ 
 <br>In a Nutshell
Natürliche <a data-tooltip-position="top" aria-label="Equivalence Relation and Class" data-href="Equivalence Relation and Class" href="the-guide/mathematics/general-stuff/equivalence-relation-and-class.html" class="internal-link" target="_self" rel="noopener nofollow">Äquivalenzrelation</a> der mathematischen Struktur <a data-tooltip-position="top" aria-label="(Semi-) Riemannsche Metriken" data-href="(Semi-) Riemannsche Metriken" href="the-guide/mathematics/differential-geometry/riemannian-geometry/metriken-und-zusammenhänge/(semi-)-riemannsche-metriken.html" class="internal-link" target="_self" rel="noopener nofollow">(semi-) Riemann'sche</a> <a data-tooltip-position="top" aria-label="Mannigfaltigkeiten" data-href="Mannigfaltigkeiten" href="the-guide/mathematics/differential-geometry/riemannian-geometry/mannigfaltigkeiten-und-differenzierbare-abbildungen/mannigfaltigkeiten.html" class="internal-link" target="_self" rel="noopener nofollow">Mannigfaltigkeit</a>. Entspricht <a data-tooltip-position="top" aria-label="Differenzierbare Abbildung zwischen Mannigfaltigkeiten" data-href="Differenzierbare Abbildung zwischen Mannigfaltigkeiten" href="the-guide/mathematics/differential-geometry/riemannian-geometry/mannigfaltigkeiten-und-differenzierbare-abbildungen/differenzierbare-abbildung-zwischen-mannigfaltigkeiten.html" class="internal-link" target="_self" rel="noopener nofollow">Diffeomorphismus</a>, der zusätzlich die Metrik erhält.
<br><br>Ein lokaler <a data-tooltip-position="top" aria-label="Mathematical Relations" data-href="Mathematical Relations" href="the-guide/mathematics/general-stuff/mathematical-relations.html" class="internal-link" target="_self" rel="noopener nofollow">Diffeomorphismus</a> ist eine Abbildung , so dass für alle  eine Umgebung  existiert, für die  Diffeomorphismus ist.<br>Isometrische Mannigfaltigkeiten
Seien  <a data-tooltip-position="top" aria-label="(Semi-) Riemannsche Metriken" data-href="(Semi-) Riemannsche Metriken" href="the-guide/mathematics/differential-geometry/riemannian-geometry/metriken-und-zusammenhänge/(semi-)-riemannsche-metriken.html" class="internal-link" target="_self" rel="noopener nofollow">semi-Riemannsche</a> <a data-href="Mannigfaltigkeiten" href="the-guide/mathematics/differential-geometry/riemannian-geometry/mannigfaltigkeiten-und-differenzierbare-abbildungen/mannigfaltigkeiten.html" class="internal-link" target="_self" rel="noopener nofollow">Mannigfaltigkeiten</a>. Ein (lokaler) Diffeomorphismus  heißt (lokale) Isometrie, wenn der <a data-tooltip-position="top" aria-label="Mathematical Relations" data-href="Mathematical Relations" href="the-guide/mathematics/general-stuff/mathematical-relations.html" class="internal-link" target="_self" rel="noopener nofollow">pullback</a> der Metrik  durch  genau  enstpricht, also  gilt. Konkret berechnet man also der Punkt transformiert mit der Abbildung und die Tangentialvektoren mit dem <a data-tooltip-position="top" aria-label="Pushforward Operator on Manifolds" data-href="Pushforward Operator on Manifolds" href="the-guide/mathematics/differential-geometry/riemannian-geometry/mannigfaltigkeiten-und-differenzierbare-abbildungen/pushforward-operator-on-manifolds.html" class="internal-link" target="_self" rel="noopener nofollow">Differential</a>, wodurch die geometrische Struktur (Winkel und Länge) erhalten bleiben. In diesem Fall sind  und  isometrische Mannigfaltigkeiten.
<br>
<br>Induziert eine <a data-tooltip-position="top" aria-label="Equivalence Relation and Class" data-href="Equivalence Relation and Class" href="the-guide/mathematics/general-stuff/equivalence-relation-and-class.html" class="internal-link" target="_self" rel="noopener nofollow">Äquivalenzrelation</a> semi-Riemannscher Mannigfaltigkeiten
<br>Die Menge der Isometrien  (Abbildungen unter denen Isometrie) auf einer Mannigfaltigkeit bildet eine <a data-tooltip-position="top" aria-label="Group" data-href="Group" href="the-guide/mathematics/group-theory/group.html" class="internal-link" target="_self" rel="noopener nofollow">Gruppe</a>, die Isometriegruppe

<br> besteht aus allen Bewegungen
<br>, Rotationen und Spiegelung um Ursprung
<br>, orientierungserhaltende Rotationen um Ursprung
<br><a data-tooltip-position="top" aria-label="Hyperbolischer Raum" data-href="Hyperbolischer Raum" href="the-guide/mathematics/differential-geometry/riemannian-geometry/hyperbolischer-raum.html" class="internal-link" target="_self" rel="noopener nofollow">Isometriegruppe des hyperbolischen Raumes ist die Lorentz-Gruppe</a>


<br>Intuition
Anschaulich sind geometrische Mannigfaltigkeiten im geometrischen Sinne identisch, die transformation von  zu  ist so geartet, dass Winkel und Längen zwischen Punkten unverändert bleiben. Man kann sich auch gut an <a data-tooltip-position="top" aria-label="Flächen" data-href="Flächen" href="the-guide/mathematics/differential-geometry/elementary-differential-geometry/flächen.html" class="internal-link" target="_self" rel="noopener nofollow">Isometrie</a> im klassischen Fall von Flächen erinnern. Anschauliche Beispiele sind ...

<br>Zylinder und Ebene sind lokal isometrisch, global gibt es Probleme beim "zusammenkleben"
<br>Sphäre und Ebene sind ebenfalls lokal isometrisch über Projektionen, global aber nicht

<br>Sehen zwei Mannigfaltigkeiten von Innen global, also in jedem Punkt und jeder Richtung gleich aus, so übersetzt sich dies in die folgenden zwei Eigenschaften:<br>Eigenschaften

<br>Homogenität - eine semi-Riemannsche Mannigfaltigkeit ist homogen, wenn es eine Isometrie  gibt, sodass für zwei Punkte  .
<br>Isotropie - eine Riemannsche Mannigfaltigkeit ist isotrop, wenn für jeden Punkt  eine Isometrie  existiert, die nach der jeweiligen Metrik normierten Tangentialvektoren beliebig aufeinander abbildet, also  und  mit .

<br>
<br>Zylinder ist homogen (Translation in  und Rotationen), aber nicht isotrop, da es keine Isometrien gibt die die -Richtung mit der Kreisrichtung vertauscht
<br>Nur Flächen vom <a data-tooltip-position="top" aria-label="Genus" data-href="Genus" href="the-guide/mathematics/general-stuff/genus.html" class="internal-link" target="_self" rel="noopener nofollow">Geschlecht</a>  sind isotrop
<br><br>Satz
Sei  eine Isometrie einer <a data-tooltip-position="top" aria-label="(Semi-) Riemannsche Metriken" data-href="(Semi-) Riemannsche Metriken" href="the-guide/mathematics/differential-geometry/riemannian-geometry/metriken-und-zusammenhänge/(semi-)-riemannsche-metriken.html" class="internal-link" target="_self" rel="noopener nofollow">semi-Riemannschen</a> <a data-tooltip-position="top" aria-label="Mannigfaltigkeiten" data-href="Mannigfaltigkeiten" href="the-guide/mathematics/differential-geometry/riemannian-geometry/mannigfaltigkeiten-und-differenzierbare-abbildungen/mannigfaltigkeiten.html" class="internal-link" target="_self" rel="noopener nofollow">Mannigfaltigkeit</a> . Dann sind die Zusammenhangskomponenten der Fixpunktmenge <a data-tooltip-position="top" aria-label="Mannigfaltigkeiten" data-href="Mannigfaltigkeiten" href="the-guide/mathematics/differential-geometry/riemannian-geometry/mannigfaltigkeiten-und-differenzierbare-abbildungen/mannigfaltigkeiten.html" class="internal-link" target="_self" rel="noopener nofollow">total geodätische Untermannigfaltigkeiten</a> von .
<br>
<br>Menge aller zusammenhängenden Punkten die unter einer Isometrie unbeeinflusst bleiben 
<br>Für  je nach Isometrie die leere Menge, zwei antipodale Punkte (bei Drehungen um feste Achse), ein Großkreis bei Spiegelung an einer halbierenden Ebene oder ganz 
]]></description><link>the-guide/mathematics/differential-geometry/riemannian-geometry/metriken-und-zusammenhänge/isometrische-mannigfaltigkeiten.html</link><guid isPermaLink="false">The Guide/Mathematics/Differential Geometry/Riemannian Geometry/Metriken und Zusammenhänge/Isometrische Mannigfaltigkeiten.md</guid><pubDate>Sat, 05 Apr 2025 16:23:52 GMT</pubDate></item><item><title><![CDATA[Kovariante Ableitung]]></title><description><![CDATA[ 
 <br>In a Nutshell
Ableitungsbegriff für Vektorfelder längs Kurven auf Mannigfaltigkeiten.
<br>Für Funktionen könnten wir einen intrinsischen Ableitungsbegriff mittels der <a data-href="Lie Ableitung" href="the-guide/mathematics/differential-geometry/riemannian-geometry/mannigfaltigkeiten-und-differenzierbare-abbildungen/lie-ableitung.html" class="internal-link" target="_self" rel="noopener nofollow">Lie Ableitung</a> definieren, da dieser über die Funktion auf der Mannigfaltigkeit punktweise definiert war.<br>
Wollen wir Ableitungen für Vektorfelder intrinsisch definieren, so benötigen wir einen Vergleichsmaßstab, da sich der <a data-href="Tangentialraum" href="the-guide/mathematics/differential-geometry/riemannian-geometry/mannigfaltigkeiten-und-differenzierbare-abbildungen/tangentialraum.html" class="internal-link" target="_self" rel="noopener nofollow">Tangentialraum</a> in jedem Punkt unterscheidet. Im Differenzenquotienten müssten wir daher Vektoren aus verschiedenen Vektorräumen voneinander abziehen. Hierfür können wir <a data-tooltip-position="top" aria-label="Parallele Vektorfelder und Parallelverschiebung" data-href="Parallele Vektorfelder und Parallelverschiebung" href="the-guide/mathematics/differential-geometry/riemannian-geometry/metriken-und-zusammenhänge/parallele-vektorfelder-und-parallelverschiebung.html" class="internal-link" target="_self" rel="noopener nofollow">parallele Vektorfelder</a> verwenden, da diese im gewünschten Maße konstant entlang einer Kurve sind.<br><br><br>Betrachte eine <a data-tooltip-position="top" aria-label="Flächen" data-href="Flächen" href="the-guide/mathematics/differential-geometry/elementary-differential-geometry/flächen.html" class="internal-link" target="_self" rel="noopener nofollow">Fläche</a>  mit einer <a data-href="Flächenkurve" href="the-guide/mathematics/differential-geometry/elementary-differential-geometry/flächenkurve.html" class="internal-link" target="_self" rel="noopener nofollow">Flächenkurve</a>  mit . Das Vektorfeld ordnet jedem Punkt auf der Kurve in  einen Vektor in  zu. Das entsprechende Bild im <a data-href="Tangentialraum" href="the-guide/mathematics/differential-geometry/riemannian-geometry/mannigfaltigkeiten-und-differenzierbare-abbildungen/tangentialraum.html" class="internal-link" target="_self" rel="noopener nofollow">Tangentialraum</a> der Fläche in  bezeichnen wir mit .<br><img alt="center" src="lib/media/pasted-image-20240224170641.png" style="width: 500px; max-width: 100%;"><br>Ableiten von  führt auf  mit tangentialem Anteil Im Fall von <a data-href="Hyperflächen" href="the-guide/mathematics/differential-geometry/elementary-differential-geometry/hyperflächen.html" class="internal-link" target="_self" rel="noopener nofollow">Hyperflächen</a> lässt sich zudem die Projektion auf den Normalenvektor schreiben als .<br>Kovariante Ableitung
Die kovariante Ableitung  des Vektorfeldes  längs der Kurve  ist (mit <a data-href="Stack - Formalismus" href="the-guide/mathematics/differential-geometry/elementary-differential-geometry/intrinsische-geometrie-von-hyperflächen/stack-formalismus.html" class="internal-link" target="_self" rel="noopener nofollow">Stack - Formalismus</a>) definiert durch das VektorfeldSie entspricht einer Richtungsableitung von  in Richtung . Analog ist die kovariante Ableitung in Richtung des Vektorfeldes  (variable Richtung statt immer in Richtung der Kurve ) gegeben durch  
<br>
<br>Darstellung mit Summenschreibweise

<br>Vektorfeld  im Tangentialraum, kovariante Ableitung von  längs  kovariante Ableitung in Richtung des Vektorfeldes  


<br>Intuition

<br>Vektorfeld  ist tangential, so wie man es wahrnimmt wenn man die normalen Dimensionen nicht sieht. Beschreibt wahrnehmbaren Teil der Änderung dieses Vektorfeldes (Tangentialraum). 


<br><br>
<br>
<br>Ist  <a data-tooltip-position="top" aria-label="Parallele Vektorfelder und Parallelverschiebung" data-href="Parallele Vektorfelder und Parallelverschiebung" href="the-guide/mathematics/differential-geometry/riemannian-geometry/metriken-und-zusammenhänge/parallele-vektorfelder-und-parallelverschiebung.html" class="internal-link" target="_self" rel="noopener nofollow">parallel</a>, so gilt 
<br>Ist die <a data-href="Flächenkurve" href="the-guide/mathematics/differential-geometry/elementary-differential-geometry/flächenkurve.html" class="internal-link" target="_self" rel="noopener nofollow">Flächenkurve</a> selbst eine <a data-href="Geodätische" href="the-guide/mathematics/differential-geometry/elementary-differential-geometry/intrinsische-geometrie-von-hyperflächen/geodätische.html" class="internal-link" target="_self" rel="noopener nofollow">Geodätische</a>, so gilt da sich die Kurve dann nicht tangential ändern darf (Hier ist das Vektorfeld  einfach die Richtung der Kurve selbst, also ). 
<br>Ist  eine <a data-href="BL-Kurve" href="the-guide/mathematics/differential-geometry/elementary-differential-geometry/lokale-kurventheorie/bl-kurve.html" class="internal-link" target="_self" rel="noopener nofollow">BL-Kurve</a>, so gilt 
<br><br><br> Sei  ein Vektorfeld längs der Kurve . Die kovariante Ableitung von  entlang von  ist das eindeutig bestimmte Vektorfeld  mit <br>
<br>Sinvoll, da parallele Felder sich nur soviel Ändern wie nötig, damit sie aus der <a data-tooltip-position="top" aria-label="Mannigfaltigkeiten" data-href="Mannigfaltigkeiten" href="the-guide/mathematics/differential-geometry/riemannian-geometry/mannigfaltigkeiten-und-differenzierbare-abbildungen/mannigfaltigkeiten.html" class="internal-link" target="_self" rel="noopener nofollow">Mannigfaltigkeit</a> heraus konstant sind.<br>
Mit gegebener <a data-tooltip-position="top" aria-label="Topology and Topological Space" data-href="Topology and Topological Space" href="the-guide/mathematics/topology/topology-and-topological-space.html" class="internal-link" target="_self" rel="noopener nofollow">Karte</a>  und lokaler Darstellung  folgt die lokale Darstellung der kovarianten Ableitung mit Die kovariante Ableitung eines <a data-tooltip-position="top" aria-label="Vektorfelder auf Mannigfaltigkeiten" data-href="Vektorfelder auf Mannigfaltigkeiten" href="the-guide/mathematics/differential-geometry/riemannian-geometry/mannigfaltigkeiten-und-differenzierbare-abbildungen/vektorfelder-auf-mannigfaltigkeiten.html" class="internal-link" target="_self" rel="noopener nofollow">Vektorfeldes</a>  in Richtung eines anderen Feldes  ist im Punkt  gegeben durch wobei  eine "künstliche" Kurve durch  mit  ist. 
<br>Definition
Die kovariante Ableitung bezeichnet also die Änderung eines Vektorfeldes  bezüglich seiner gedachten <a data-tooltip-position="top" aria-label="Parallele Vektorfelder und Parallelverschiebung" data-href="Parallele Vektorfelder und Parallelverschiebung" href="the-guide/mathematics/differential-geometry/riemannian-geometry/metriken-und-zusammenhänge/parallele-vektorfelder-und-parallelverschiebung.html" class="internal-link" target="_self" rel="noopener nofollow">Parallelverschiebung</a> entlang . Lokal erhält man mit den Darstellungen  und  wieder mit den <a data-tooltip-position="top" aria-label="Christoffel-Symbol" data-href="Christoffel-Symbol" href="the-guide/mathematics/differential-geometry/elementary-differential-geometry/intrinsische-geometrie-von-hyperflächen/christoffel-symbol.html" class="internal-link" target="_self" rel="noopener nofollow">Christoffel-Symbolen</a>wobei der zweite Term oft als Korrekturterm bezeichnet wird, da er dafür sorgt, dass die Ableitung relativ zu paralleln Feldern ist. So verschwindet er im Falle .
<br>Als Deutung dieser Ableitung sei angemerkt, dass  gilt, die kovariante Ableitung ist also Tangentialprojektion der doppelten <a data-tooltip-position="top" aria-label="Lie Ableitung" data-href="Lie Ableitung" href="the-guide/mathematics/differential-geometry/riemannian-geometry/mannigfaltigkeiten-und-differenzierbare-abbildungen/lie-ableitung.html" class="internal-link" target="_self" rel="noopener nofollow">Lie Ableitung</a>.<br>Satz 10 Axiomatisch
Sei  eine Immersion, die eine <a data-tooltip-position="top" aria-label="(Semi-) Riemannsche Metriken" data-href="(Semi-) Riemannsche Metriken" href="the-guide/mathematics/differential-geometry/riemannian-geometry/metriken-und-zusammenhänge/(semi-)-riemannsche-metriken.html" class="internal-link" target="_self" rel="noopener nofollow">Riemannsche Metrik</a>  auf  induziert. Die kovariante Ableitung  auf  erfüllt für alle <a data-tooltip-position="top" aria-label="Vektorfelder auf Mannigfaltigkeiten" data-href="Vektorfelder auf Mannigfaltigkeiten" href="the-guide/mathematics/differential-geometry/riemannian-geometry/mannigfaltigkeiten-und-differenzierbare-abbildungen/vektorfelder-auf-mannigfaltigkeiten.html" class="internal-link" target="_self" rel="noopener nofollow">Vektorfelder</a>  und 

<br>In der Richtung  ist sie -linear, also 
<br>Im abzuleitenden Feld ist sie -linear, also und derivativ 
<br>Es gilt mit <a data-href="Lie-Klammer" href="the-guide/mathematics/differential-geometry/riemannian-geometry/mannigfaltigkeiten-und-differenzierbare-abbildungen/lie-klammer.html" class="internal-link" target="_self" rel="noopener nofollow">Lie-Klammer</a>.
<br>Es gilt die Produktregel 

<br>Additional Information
Erweitert sich nur auf semi-Riemannschen, wenn der pullback der Metrik  auf  eine semi-Riemannsche Metrik auf  induziert.
<br><br>Kovarianter Satz von Schwartz
Sei  offen und . Dann gilt 
<br>
<br><a data-href="Lie Ableitung" href="the-guide/mathematics/differential-geometry/riemannian-geometry/mannigfaltigkeiten-und-differenzierbare-abbildungen/lie-ableitung.html" class="internal-link" target="_self" rel="noopener nofollow">Lie Ableitung</a> der Kurve nach einem Parameter gibt 
]]></description><link>the-guide/mathematics/differential-geometry/riemannian-geometry/metriken-und-zusammenhänge/kovariante-ableitung.html</link><guid isPermaLink="false">The Guide/Mathematics/Differential Geometry/Riemannian Geometry/Metriken und Zusammenhänge/Kovariante Ableitung.md</guid><pubDate>Wed, 23 Apr 2025 08:34:35 GMT</pubDate><enclosure url="lib/media/pasted-image-20240224170641.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src=&quot;lib/media/pasted-image-20240224170641.png&quot;&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[Kürzeste]]></title><description><![CDATA[ 
 <br>Definition
Sei  <a data-tooltip-position="top" aria-label="(Semi-) Riemannsche Metriken" data-href="(Semi-) Riemannsche Metriken" href="the-guide/mathematics/differential-geometry/riemannian-geometry/metriken-und-zusammenhänge/(semi-)-riemannsche-metriken.html" class="internal-link" target="_self" rel="noopener nofollow">Riemannsche</a> <a data-tooltip-position="top" aria-label="Mannigfaltigkeiten" data-href="Mannigfaltigkeiten" href="the-guide/mathematics/differential-geometry/riemannian-geometry/mannigfaltigkeiten-und-differenzierbare-abbildungen/mannigfaltigkeiten.html" class="internal-link" target="_self" rel="noopener nofollow">Mannigfaltigkeit</a>. Eine <a data-tooltip-position="top" aria-label="Stückweise differenzierbare Kurven" data-href="Stückweise differenzierbare Kurven" href="the-guide/mathematics/differential-geometry/elementary-differential-geometry/globale-kurven-und-flächentheorie/stückweise-differenzierbare-kurven.html" class="internal-link" target="_self" rel="noopener nofollow">stückweise differenzierbare Kurve</a>  heißt Kürzeste, wenn für jedes Intervall  und jede Kurve  mit festen Endpunkten  für die <a data-tooltip-position="top" aria-label="Variation der Energie" data-href="Variation der Energie" href="the-guide/mathematics/differential-geometry/riemannian-geometry/variation-der-energie.html" class="internal-link" target="_self" rel="noopener nofollow">Energie</a> gilt Dabei kann  nicht kompakt sein.
<br>Satz
Ist ein  stückweise differenzierbare Kürzeste, so ist sie <a data-tooltip-position="top" aria-label="Exponentialabbildung" data-href="Exponentialabbildung" href="the-guide/mathematics/differential-geometry/riemannian-geometry/metriken-und-zusammenhänge/exponentialabbildung.html" class="internal-link" target="_self" rel="noopener nofollow">Geodätische</a> und damit insbesondere differenzierbar.
<br>Proof
Folgt  unmittelbar daraus, dass eine Kürzeste kritische Energie haben muss (erste Variation verschwindet).
<br><br>Mithilfe <a data-tooltip-position="top" aria-label="Normale Bälle" data-href="Normale Bälle" href="the-guide/mathematics/differential-geometry/riemannian-geometry/metriken-und-zusammenhänge/normale-bälle.html" class="internal-link" target="_self" rel="noopener nofollow">normaler Bälle</a> lässt sich zudem zeigen ....<br>Theorem - Radiale Geodätische sind Kürzeste
Sei  Riemannsch,  und  ein normaler Ball mit . Unter allen stückweise differenzierbaren Kurven  von  nach  ist die radiale Geodätische die eindeutig bestimmte Kurve mit minimaler <a data-tooltip-position="top" aria-label="Variation der Energie" data-href="Variation der Energie" href="the-guide/mathematics/differential-geometry/riemannian-geometry/variation-der-energie.html" class="internal-link" target="_self" rel="noopener nofollow">Energie</a>
<br><img alt="center" src="lib/media/pasted-image-20240730083944.png" style="width: 500px; max-width: 100%;"><br>Proof
Wir zeigen, dass für eine beliebige Kurve  gelten muss . Dazu stellen wir die Kurve im Tangentialraum dar mittelsmit 

<br>Einem Tangentialvektor  mit 
<br>Einer Skalierung ,

was Polarkoordinaten in  entspricht. Damit können wir jede Kurve innerhalb des normalen Balles auf der Mannigfaltigkeit beschreiben. Ableiten nach  ergibt Bildung des Betrages mithilfe der Metrik  liefert drei Faktoren, wobei wir benutzen können

<br>, wobei der letzte Schritt mit dem <a data-href="Gauß Lemma" href="the-guide/mathematics/differential-geometry/riemannian-geometry/metriken-und-zusammenhänge/gauß-lemma.html" class="internal-link" target="_self" rel="noopener nofollow">Gauß Lemma</a> folgt.
<br>
<br>Der gemischte Term ist damit 

Insgesamt erhalten wir Im Integral für die Energie können wir Hölders Ungleichung verwenden, womit die Behauptung aus  folgt.
<br>
<br>Kettenregel nochmal checken 
<br>Intuition
Ich muss bis an den Punkt  kommen, der Teil der Energie, der durch die radiale Komponente kommt ist also unvermeidbar. Der tangentiale ist immer zusätzlich, insgesamt ist die Energie also minimal, wenn ich nur eine radiale Komponente habe.
]]></description><link>the-guide/mathematics/differential-geometry/riemannian-geometry/metriken-und-zusammenhänge/kürzeste.html</link><guid isPermaLink="false">The Guide/Mathematics/Differential Geometry/Riemannian Geometry/Metriken und Zusammenhänge/Kürzeste.md</guid><pubDate>Mon, 12 Aug 2024 17:05:43 GMT</pubDate><enclosure url="lib/media/pasted-image-20240730083944.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src=&quot;lib/media/pasted-image-20240730083944.png&quot;&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[Levi-Civita-Zusammenhang und Koszul-Formel]]></title><description><![CDATA[ 
 <br>In a Nutshell
Verbindet axiomatische Einführung des  <a data-tooltip-position="top" aria-label="Zusammenhänge" data-href="Zusammenhänge" href="the-guide/mathematics/differential-geometry/riemannian-geometry/metriken-und-zusammenhänge/zusammenhänge.html" class="internal-link" target="_self" rel="noopener nofollow">Zusammenhangs</a> mit der Riemannschen Geometrie.
<br><br>Levi-Civita Zusammenhang
Auf jeder <a data-tooltip-position="top" aria-label="(Semi-) Riemannsche Metriken" data-href="(Semi-) Riemannsche Metriken" href="the-guide/mathematics/differential-geometry/riemannian-geometry/metriken-und-zusammenhänge/(semi-)-riemannsche-metriken.html" class="internal-link" target="_self" rel="noopener nofollow">semi-Riemannschen</a> <a data-tooltip-position="top" aria-label="Mannigfaltigkeiten" data-href="Mannigfaltigkeiten" href="the-guide/mathematics/differential-geometry/riemannian-geometry/mannigfaltigkeiten-und-differenzierbare-abbildungen/mannigfaltigkeiten.html" class="internal-link" target="_self" rel="noopener nofollow">Mannigfaltigkeit</a>  gibt es genau einen <a data-tooltip-position="top" aria-label="Zusammenhänge" data-href="Zusammenhänge" href="the-guide/mathematics/differential-geometry/riemannian-geometry/metriken-und-zusammenhänge/zusammenhänge.html" class="internal-link" target="_self" rel="noopener nofollow">Zusammenhang</a> , der torsionsfrei und mit der Metrik verträglich ist. Er ist definiert durch die Koszul-Formel
<br>Da auf jeder <a data-tooltip-position="top" aria-label="Mannigfaltigkeiten" data-href="Mannigfaltigkeiten" href="the-guide/mathematics/differential-geometry/riemannian-geometry/mannigfaltigkeiten-und-differenzierbare-abbildungen/mannigfaltigkeiten.html" class="internal-link" target="_self" rel="noopener nofollow">differenzierbaren Mannigfaltigkeit</a> eine Riemannsche Metrik existiert, existiert auch stets ein Riemannscher Zusammenhang.<br>Proof
Man wende die Verträglichkeitsbedingung auf , und  an, addiere alle Gleichungen und forme unter Ausnutzen der Torsionfreiheit um.
]]></description><link>the-guide/mathematics/differential-geometry/riemannian-geometry/metriken-und-zusammenhänge/levi-civita-zusammenhang-und-koszul-formel.html</link><guid isPermaLink="false">The Guide/Mathematics/Differential Geometry/Riemannian Geometry/Metriken und Zusammenhänge/Levi-Civita-Zusammenhang und Koszul-Formel.md</guid><pubDate>Mon, 12 Aug 2024 17:05:43 GMT</pubDate></item><item><title><![CDATA[Normale Bälle]]></title><description><![CDATA[ 
 <br>In a Nutshell
Verallgemeinerung des <a data-tooltip-position="top" aria-label="Geodätische" data-href="Geodätische" href="the-guide/mathematics/differential-geometry/elementary-differential-geometry/intrinsische-geometrie-von-hyperflächen/geodätische.html" class="internal-link" target="_self" rel="noopener nofollow">geodätischen Radius</a> und dessen Bild von der elementaren Differentialgeometrie auf die Riemman'sche Geometrie. Zentrale Idee für die Formulierung der <a data-tooltip-position="top" aria-label="Mannigfaltigkeiten" data-href="Mannigfaltigkeiten" href="the-guide/mathematics/differential-geometry/riemannian-geometry/mannigfaltigkeiten-und-differenzierbare-abbildungen/mannigfaltigkeiten.html" class="internal-link" target="_self" rel="noopener nofollow">Riemannschen Mannigfaltigkeit</a> als metrischer Raum.
<br><br>Definition
Für <a data-href="Geodätische" href="the-guide/mathematics/differential-geometry/elementary-differential-geometry/intrinsische-geometrie-von-hyperflächen/geodätische.html" class="internal-link" target="_self" rel="noopener nofollow">Geodätische</a> vom Radius  um den Punkt  ist das Bild der <a data-href="Exponentialabbildung" href="the-guide/mathematics/differential-geometry/riemannian-geometry/metriken-und-zusammenhänge/exponentialabbildung.html" class="internal-link" target="_self" rel="noopener nofollow">Exponentialabbildung</a> für einen Kreis wobei Definiertheit von  und Existenz des Diffeomorphismus gefordert sind. Wir bezeichnen das Bild als normalen Ball  vom Radius .
<br>
<br>Für  und Einheitszylinder ist  maximaler normaler Ball
<br>Für  ist es 
<br>Lemma 36
Für  Riemannsch gilt ...

<br>für jedes  existiert ein , sodass  normaler Ball ist
<br>ist  kompakt, so ist für alle  ein einziger Radius wählbar, sodass  normal ist.

]]></description><link>the-guide/mathematics/differential-geometry/riemannian-geometry/metriken-und-zusammenhänge/normale-bälle.html</link><guid isPermaLink="false">The Guide/Mathematics/Differential Geometry/Riemannian Geometry/Metriken und Zusammenhänge/Normale Bälle.md</guid><pubDate>Mon, 12 Aug 2024 17:05:43 GMT</pubDate></item><item><title><![CDATA[Parallele Vektorfelder und Parallelverschiebung]]></title><description><![CDATA[ 
 <br>In a Nutshell
Funktion, die jedem Punkt entlang einer <a data-href="Flächenkurve" href="the-guide/mathematics/differential-geometry/elementary-differential-geometry/flächenkurve.html" class="internal-link" target="_self" rel="noopener nofollow">Flächenkurve</a> einen Vektor zuordnet, dessen Änderung im <a data-href="Tangentialraum" href="the-guide/mathematics/differential-geometry/riemannian-geometry/mannigfaltigkeiten-und-differenzierbare-abbildungen/tangentialraum.html" class="internal-link" target="_self" rel="noopener nofollow">Tangentialraum</a> / intrinsisch unsichtbar ist.
<br><br><br>Man betrachte eine <a data-tooltip-position="top" aria-label="Flächen" data-href="Flächen" href="the-guide/mathematics/differential-geometry/elementary-differential-geometry/flächen.html" class="internal-link" target="_self" rel="noopener nofollow">Fläche</a>  mit einer <a data-href="Flächenkurve" href="the-guide/mathematics/differential-geometry/elementary-differential-geometry/flächenkurve.html" class="internal-link" target="_self" rel="noopener nofollow">Flächenkurve</a>  mit . Das Vektorfeld ordnet jedem Punkt auf der Kurve in  einen Vektor in  zu. Das entsprechende Bild im <a data-href="Tangentialraum" href="the-guide/mathematics/differential-geometry/riemannian-geometry/mannigfaltigkeiten-und-differenzierbare-abbildungen/tangentialraum.html" class="internal-link" target="_self" rel="noopener nofollow">Tangentialraum</a> der Fläche in  bezeichnen wir mit . Ableiten von  führt auf  mit tangentialem Anteil Im Fall von <a data-href="Hyperflächen" href="the-guide/mathematics/differential-geometry/elementary-differential-geometry/hyperflächen.html" class="internal-link" target="_self" rel="noopener nofollow">Hyperflächen</a> lässt sich zudem die Projektion auf den Normalenvektor schreiben als .<br>Paralleles Vektorfeld und Parallelverschiebung
Die Projektion  eines Vektorfeldes  heißt paralleles Vektorfeld längs der <a data-href="Flächenkurve" href="the-guide/mathematics/differential-geometry/elementary-differential-geometry/flächenkurve.html" class="internal-link" target="_self" rel="noopener nofollow">Flächenkurve</a>  bzw. , wennIn diesem Fall bezeichnet man  als Parallelverschiebung (entlang ) von . 
<br><img alt="center" src="lib/media/pasted-image-20240114115454.png" style="width: 250px; max-width: 100%;">Tangentialraum "wandert" mit der Kurve, Vektor sieht darin aber immer gleich aus.<br><img alt="center" src="lib/media/pasted-image-20240224170641.png" style="width: 500px; max-width: 100%;"><br><br>
<br>Ist  parallel, so ändert es sich nur in Normalenrichtung. Damit ist  parallel, genau dann wenn gilt 
<br>Für ein geschlossenes  () gilt im Falle von Parallelität (siehe oben Beispiel) im Allgemeinen
<br>Satz
Sind  und  beide parallel längs , so gilt 
<br>
<br>Begründung - Im <a data-href="Tangentialraum" href="the-guide/mathematics/differential-geometry/riemannian-geometry/mannigfaltigkeiten-und-differenzierbare-abbildungen/tangentialraum.html" class="internal-link" target="_self" rel="noopener nofollow">Tangentialraum</a> entspricht die obige Aussage , was nach Ableiten auf führt. Die Umformung folgt darauf, dass die Vekoren im Tangentialraum definiert sind, womit nur die tangentialen Komponenten der Ableitungen das Skalarprodukt beeinflussen, die aber durch die Parallelität  sind. 
<br><br><br>Wir betrachten eine Immersion , wobei die <a data-tooltip-position="top" aria-label="(Semi-) Riemannsche Metriken" data-href="(Semi-) Riemannsche Metriken" href="the-guide/mathematics/differential-geometry/riemannian-geometry/metriken-und-zusammenhänge/(semi-)-riemannsche-metriken.html" class="internal-link" target="_self" rel="noopener nofollow">Metrik</a> auf  über die pullback Metrik definiert ist. Sei nun  eine Kurve mit Vektorfeld  (differenzierbar) längs . Dieses heißt parallel, wenn seine Tangentialprojektion verschwindet  is dann eine Parallelverschiebung von  längs .<br>Zum Berechnen einer Parallelverschiebung nehmen wir an, wir haben eine <a data-tooltip-position="top" aria-label="Topology and Topological Space" data-href="Topology and Topological Space" href="the-guide/mathematics/topology/topology-and-topological-space.html" class="internal-link" target="_self" rel="noopener nofollow">Karte</a>  von . Ein Flächenstück im Sinnne der elementaren Differentialgeometrie ist dann über die Abbildung  gegeben und die pullback Metrik ist die <a data-tooltip-position="top" aria-label="1. Fundamentalform" data-href="1. Fundamentalform" href="the-guide/mathematics/differential-geometry/elementary-differential-geometry/1.-fundamentalform.html" class="internal-link" target="_self" rel="noopener nofollow">erste Fundamentalform</a>.<br>
Bezüglich der Hauptteile lässt sich das Vektorfeld darstellen als Mithilfe der <a data-tooltip-position="top" aria-label="Christoffel-Symbol" data-href="Christoffel-Symbol" href="the-guide/mathematics/differential-geometry/elementary-differential-geometry/intrinsische-geometrie-von-hyperflächen/christoffel-symbol.html" class="internal-link" target="_self" rel="noopener nofollow">Christoffel-Symbole</a> erhalten wir die Projektion der Zeit-Ableitung des <a data-tooltip-position="top" aria-label="Pushforward Operator on Manifolds" data-href="Pushforward Operator on Manifolds" href="the-guide/mathematics/differential-geometry/riemannian-geometry/mannigfaltigkeiten-und-differenzierbare-abbildungen/pushforward-operator-on-manifolds.html" class="internal-link" target="_self" rel="noopener nofollow">Differentials</a>  mit wobei  der Hauptteil von  ist.<br>Satz 7 - Differentialgleichungen für Parallelfelder
Das <a data-tooltip-position="top" aria-label="Vektorfelder auf Mannigfaltigkeiten" data-href="Vektorfelder auf Mannigfaltigkeiten" href="the-guide/mathematics/differential-geometry/riemannian-geometry/mannigfaltigkeiten-und-differenzierbare-abbildungen/vektorfelder-auf-mannigfaltigkeiten.html" class="internal-link" target="_self" rel="noopener nofollow">Vektorfeld</a>  längs  auf  mit lokaler Darstellung  bzgl.  ist genau dann parallel, wenn  folgendes System linearer <a data-tooltip-position="top" aria-label="ODEs - Ordinary Differential Equations" data-href="ODEs - Ordinary Differential Equations" href="the-guide/mathematics/differential-equations/odes-ordinary-differential-equations.html" class="internal-link" target="_self" rel="noopener nofollow">ODEs</a> für alle  erfüllt: 
<br>Satz 8 - Eindeutigkeit von Parallelverschiebungen
Für eine <a data-tooltip-position="top" aria-label="Pushforward Operator on Manifolds" data-href="Pushforward Operator on Manifolds" href="the-guide/mathematics/differential-geometry/riemannian-geometry/mannigfaltigkeiten-und-differenzierbare-abbildungen/pushforward-operator-on-manifolds.html" class="internal-link" target="_self" rel="noopener nofollow">Immersion</a>  und eine Kurce  mit  exisitert für jedes  eine eindeutige Parallelverschiebung  längs  mit Anfangswert .
<br>
<br><a data-href="Picard-Lindelöf Theorem" href="the-guide/mathematics/differential-equations/picard-lindelöf-theorem.html" class="internal-link" target="_self" rel="noopener nofollow">Picard-Lindelöf Theorem</a> und <a data-href="Peano Existence Theorem" href="the-guide/mathematics/differential-equations/peano-existence-theorem.html" class="internal-link" target="_self" rel="noopener nofollow">Peano Existence Theorem</a> 
]]></description><link>the-guide/mathematics/differential-geometry/riemannian-geometry/metriken-und-zusammenhänge/parallele-vektorfelder-und-parallelverschiebung.html</link><guid isPermaLink="false">The Guide/Mathematics/Differential Geometry/Riemannian Geometry/Metriken und Zusammenhänge/Parallele Vektorfelder und Parallelverschiebung.md</guid><pubDate>Wed, 23 Apr 2025 08:34:35 GMT</pubDate><enclosure url="lib/media/pasted-image-20240114115454.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src=&quot;lib/media/pasted-image-20240114115454.png&quot;&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[Riemannsche Mannigfaltigkeiten als Metrische Räume]]></title><description><![CDATA[ 
 <br>Da die <a data-tooltip-position="top" aria-label="(Semi-) Riemannsche Metriken" data-href="(Semi-) Riemannsche Metriken" href="the-guide/mathematics/differential-geometry/riemannian-geometry/metriken-und-zusammenhänge/(semi-)-riemannsche-metriken.html" class="internal-link" target="_self" rel="noopener nofollow">Riemannsche Metrik</a>  der <a data-tooltip-position="top" aria-label="Mannigfaltigkeiten" data-href="Mannigfaltigkeiten" href="the-guide/mathematics/differential-geometry/riemannian-geometry/mannigfaltigkeiten-und-differenzierbare-abbildungen/mannigfaltigkeiten.html" class="internal-link" target="_self" rel="noopener nofollow">Riemannschen Mannigfaltigkeit</a>  Distanzen infinitesimal misst, ergibt sich der Abstandsbegriff für : <br>
<br>Wir betrachten nur Riemannschen Fall, da für semi-Riemannsch das <a data-tooltip-position="top" aria-label="Set" data-href="Set" href="the-guide/mathematics/general-stuff/set.html" class="internal-link" target="_self" rel="noopener nofollow">Infimum</a> immer  ist (integral über Elemente der infin. Länge 0).
<br><br><br>Satz
Sei  zusammenhängende Riemman'sche Mannigfaltigkeit. 

<br>Der oben definierte Abstand definiert eine <a data-tooltip-position="top" aria-label="Metric Space and Completeness" data-href="Metric Space and Completeness" href="the-guide/mathematics/general-stuff/metric-space-and-completeness.html" class="internal-link" target="_self" rel="noopener nofollow">Metrik</a>  auf 
<br>Die von  induzierte <a data-tooltip-position="top" aria-label="Topology and Topological Space" data-href="Topology and Topological Space" href="the-guide/mathematics/topology/topology-and-topological-space.html" class="internal-link" target="_self" rel="noopener nofollow">Topologie</a> stimmt mit der von  überein

<br>
<br>Wir erzeugen die <a data-tooltip-position="top" aria-label="Topology and Topological Space" data-href="Topology and Topological Space" href="the-guide/mathematics/topology/topology-and-topological-space.html" class="internal-link" target="_self" rel="noopener nofollow">Topologie</a> aus den offenen Umgebungen der normalen Bälle. Startet man z.B. von  mit einer Umgebung , so gibt es für jeden Punkt einen normalen Ball, der im Tangentialraum  eine Umgebung um den Ursprung definiert. Diese enthält wieder einen Ball, dessen Bild über die Exponentialabbildung eine Umgebung um  definiert. Umgekehrt funktioniert die Argumentation genauso.
<br>Korollar
Für jedes  ist die auf  definierte <a data-tooltip-position="top" aria-label="Mathematical Relations" data-href="Mathematical Relations" href="the-guide/mathematics/general-stuff/mathematical-relations.html" class="internal-link" target="_self" rel="noopener nofollow">Funktion</a>  stetig. Damit ist insbesondere noch auf dem Rand eines normalen Balles  die Länge radialer geodätischer gegeben.
<br><br><br>Vollständigkeit

<br>Eine Riemann'sche Mannigfaltigkeit  ist metrisch vollständig, wenn <a data-tooltip-position="top" aria-label="Cauchy Sequence" data-href="Cauchy Sequence" href="the-guide/mathematics/general-stuff/cauchy-sequence.html" class="internal-link" target="_self" rel="noopener nofollow">Cauchy-Folgen</a> bzgl.  konvergieren.
<br>Kann jede Geodätische auf  zu einer auf ganz  definierten Geodätischen fortgesetzt werden, so heißt  geodätisch vollständig.

<br>
<br> sind vollständig
<br>Satz - Metrisch Vollständig  Geodätisch Vollständig 
Ist  metrisch vollständig, so ist  auch geodätisch vollständig. Aus der Konvergenz von Cauchy-Folgen folgt also, das geodätische fortgesetzt werden können.
<br>Proof
Nach dem lokalen Existenzsatz wissen wir, das Geodätische lokal zunächst in einem offenen Intervall existieren. Wir nennen dieses Intervall  und betrachten darauf eine nach <a data-tooltip-position="top" aria-label="BL-Kurve" data-href="BL-Kurve" href="the-guide/mathematics/differential-geometry/elementary-differential-geometry/lokale-kurventheorie/bl-kurve.html" class="internal-link" target="_self" rel="noopener nofollow">Bogenlänge parametrisierte</a> Geodätische. 
Wir nutzen die metrische Vollständigkeit, indem wir eine Cauchy-Folge  konstruieren mit , womit ich mit dem Grenzwert  beliebig nahe an  heran komme. 
Um jeden Punkt auf der geodätischen gibt es einen <a data-tooltip-position="top" aria-label="Normale Bälle" data-href="Normale Bälle" href="the-guide/mathematics/differential-geometry/riemannian-geometry/metriken-und-zusammenhänge/normale-bälle.html" class="internal-link" target="_self" rel="noopener nofollow">normalen Ball</a>, ich finde also ein , sodass der Ball  immer normal ist. 
Aufgrund der Cauchy-Folgen Eigenschaft gibt es ein  mit , ich finde immer Folgeglieder, die innerhalb des Balles des Grenzwertes liegen.
Da in diesem Ball immer radiale geodätische existieren, kann ich immer eine Verlängerung über  hinaus erhalten.
<br>Mit dem <a data-href="Satz von Hopf-Rinow" href="the-guide/mathematics/differential-geometry/riemannian-geometry/metriken-und-zusammenhänge/satz-von-hopf-rinow.html" class="internal-link" target="_self" rel="noopener nofollow">Satz von Hopf-Rinow</a> (geodätisch Vollständig  Kürzeste zwischen beliebigen Punkten exisitert) können wir auch für den umgekehrten Fall ergänzen:<br>Satz
Ist für eine zusammenhängende Riemann'sche Mannigfaltigkeit  die <a data-href="Exponentialabbildung" href="the-guide/mathematics/differential-geometry/riemannian-geometry/metriken-und-zusammenhänge/exponentialabbildung.html" class="internal-link" target="_self" rel="noopener nofollow">Exponentialabbildung</a> in einem Punkt  global definiert, so folgt

<br> ist metrisch vollständig
<br>Die <a data-href="Exponentialabbildung" href="the-guide/mathematics/differential-geometry/riemannian-geometry/metriken-und-zusammenhänge/exponentialabbildung.html" class="internal-link" target="_self" rel="noopener nofollow">Exponentialabbildung</a> ist sogar für alle übrigen Punkte  definiert

<br>
<br>Wir müssen zeigen, dass eine beliebige <a data-tooltip-position="top" aria-label="Cauchy Sequence" data-href="Cauchy Sequence" href="the-guide/mathematics/general-stuff/cauchy-sequence.html" class="internal-link" target="_self" rel="noopener nofollow">Cauchy-Folge</a> konvergiert. Wir können per Voraussetzung eine solche Folge  im Tangentialraum  beschreiben, wo durch <a data-tooltip-position="top" aria-label="Satz von Hopf-Rinow" data-href="Satz von Hopf-Rinow" href="the-guide/mathematics/differential-geometry/riemannian-geometry/metriken-und-zusammenhänge/satz-von-hopf-rinow.html" class="internal-link" target="_self" rel="noopener nofollow">Hopf-Rinow</a> zu jedem Folgeglied eine <a data-href="Kürzeste" href="the-guide/mathematics/differential-geometry/riemannian-geometry/metriken-und-zusammenhänge/kürzeste.html" class="internal-link" target="_self" rel="noopener nofollow">Kürzeste</a> existiert. 
<br>Folgt direkt aus metrischer Vollständigkeit.
<br>Damit folgt, dass metrische und geodätische Vollständigkeit Äquivalent sind.]]></description><link>the-guide/mathematics/differential-geometry/riemannian-geometry/metriken-und-zusammenhänge/riemannsche-mannigfaltigkeiten-als-metrische-räume.html</link><guid isPermaLink="false">The Guide/Mathematics/Differential Geometry/Riemannian Geometry/Metriken und Zusammenhänge/Riemannsche Mannigfaltigkeiten als Metrische Räume.md</guid><pubDate>Wed, 23 Apr 2025 08:34:36 GMT</pubDate></item><item><title><![CDATA[Satz von Hopf-Rinow]]></title><description><![CDATA[ 
 <br>In a Nutshell
Wichtiges Ergebnis der Riemannschen Geometrie, welches eine Verbindung zwischen metrischer und geodätischer Vollständigkeit liefert. Wenn meine Mannigfaltigkeit geodätisch vollständig ist, ich also jede Geodätische immer weiter fortsetzen kann, so kann ich auch beliebige Punkte mit Kürzesten verbinden.
<br><br>Satz von Hopf-Rinow
Für eine <a data-tooltip-position="top" aria-label="Mannigfaltigkeiten" data-href="Mannigfaltigkeiten" href="the-guide/mathematics/differential-geometry/riemannian-geometry/mannigfaltigkeiten-und-differenzierbare-abbildungen/mannigfaltigkeiten.html" class="internal-link" target="_self" rel="noopener nofollow">zusammenhängende Riemann'sche Mannigfaltigkeit</a>  gilt 

<br>Ist  für ein  auf ganz  definiert, so exisitert eine <a data-href="Kürzeste" href="the-guide/mathematics/differential-geometry/riemannian-geometry/metriken-und-zusammenhänge/kürzeste.html" class="internal-link" target="_self" rel="noopener nofollow">Kürzeste</a>  von  nach ,  ist damit insbesondere <a data-tooltip-position="top" aria-label="Exponentialabbildung" data-href="Exponentialabbildung" href="the-guide/mathematics/differential-geometry/riemannian-geometry/metriken-und-zusammenhänge/exponentialabbildung.html" class="internal-link" target="_self" rel="noopener nofollow">Geodätische</a>.
<br>Ist  <a data-tooltip-position="top" aria-label="Mannigfaltigkeiten" data-href="Mannigfaltigkeiten" href="the-guide/mathematics/differential-geometry/riemannian-geometry/mannigfaltigkeiten-und-differenzierbare-abbildungen/mannigfaltigkeiten.html" class="internal-link" target="_self" rel="noopener nofollow">geodätisch vollständig</a>, so lässt sich jedes Paar von Punkten  durch eine Kürzeste verbinden

<br>Da der zweite Punkt aus dem ersten folgt beweisen wir nur diesen. Liegt der Punkt innerhalb eines normalen Balles  um , so folgt der Satz direkt aus dem Beweis, dass <a data-tooltip-position="top" aria-label="Kürzeste" data-href="Kürzeste" href="the-guide/mathematics/differential-geometry/riemannian-geometry/metriken-und-zusammenhänge/kürzeste.html" class="internal-link" target="_self" rel="noopener nofollow">radiale Geodätische Kürzeste sind</a>. Liegt  außerhalb, wird es involvierter.<br>Wir benötigen das folgende Lemma:<br>Lemma
Sei  (Ball mit Rand) enthalten in einem <a data-tooltip-position="top" aria-label="Normale Bälle" data-href="Normale Bälle" href="the-guide/mathematics/differential-geometry/riemannian-geometry/metriken-und-zusammenhänge/normale-bälle.html" class="internal-link" target="_self" rel="noopener nofollow">normalen Ball</a> um  und . Dann existiert ein  mit .
<br>In diesem Fall liegt  auf dem Abschluss oder außerhalb des Balles, also . Wir finden mit dem Lemma ein , welches den Abstand zu  minimiert. Die Geodätische zu diesem  existiert und ist radial. Wir wollen nun zeigen dass  auf liegt, obwohl der Punkt außerhalb des Balles ist. <br>Dazu betrachten wir zunächst diejenigen Punkte mitdie also den für eine Geodätische zu erwartenden Abstand haben. Wenn wir zeigen können, dass  auf  definiert ist, so folgt  und damit , da wir geodätische Vollständigkeit voraussetzen und damit immer eine Geodätische für jedes  existiert.<br>(Es bleibt zu zeigen, dass das Intervall wirklich  entspricht, damit die geodätische Vollständigkeit greift. Wir wissen, dass der Startpunkt Teil des Intervalls ist . Zudem ist das Intervall abgeschlossen, da wir jedem  stetig einen Abstand zuordnen können -damit gilt der obige Zusammenhang für den Abstand com Endpunkt auch für den Grenzwert, wenn er für eine konvergente Folge gilt.)<br>Um zu zeigen, dass  der Endpunkt des Intervalles ist verwenden wir einen Widerspruch für  mit , indem wir ein  konstruieren, für das  gilt. Dazu nehmen wir einen normalen Ball um , der  nicht enthält und damit .<br>
Mit dem Lemma finden wir wieder eine Minimalstelle  auf dem Abschluss des Balles mit  Mit Dreiecksungleichung erhalten wir und somit Die Verkettung dieser stückweise differenzierbare Kurve muss Kürzeste von  nach  sein, mit dem <a data-tooltip-position="top" aria-label="Kürzeste" data-href="Kürzeste" href="the-guide/mathematics/differential-geometry/riemannian-geometry/metriken-und-zusammenhänge/kürzeste.html" class="internal-link" target="_self" rel="noopener nofollow">Satz zur Differenzierbarkeit von Kürzesten</a> sogar global differenzierbar. Damit liegt  auf der Kurve,  und die Verkettung liefert die Kürzeste für den Punkt .]]></description><link>the-guide/mathematics/differential-geometry/riemannian-geometry/metriken-und-zusammenhänge/satz-von-hopf-rinow.html</link><guid isPermaLink="false">The Guide/Mathematics/Differential Geometry/Riemannian Geometry/Metriken und Zusammenhänge/Satz von Hopf-Rinow.md</guid><pubDate>Mon, 12 Aug 2024 17:05:43 GMT</pubDate></item><item><title><![CDATA[Zusammenhänge]]></title><description><![CDATA[ 
 <br>hDie <a data-tooltip-position="top" aria-label="Kovariante Ableitung" data-href="Kovariante Ableitung" href="the-guide/mathematics/differential-geometry/riemannian-geometry/metriken-und-zusammenhänge/kovariante-ableitung.html" class="internal-link" target="_self" rel="noopener nofollow">kovariante Ableitung</a> wird über eine <a data-tooltip-position="top" aria-label="Differenzierbare Abbildung zwischen Mannigfaltigkeiten" data-href="Differenzierbare Abbildung zwischen Mannigfaltigkeiten" href="the-guide/mathematics/differential-geometry/riemannian-geometry/mannigfaltigkeiten-und-differenzierbare-abbildungen/differenzierbare-abbildung-zwischen-mannigfaltigkeiten.html" class="internal-link" target="_self" rel="noopener nofollow">Immersion</a>  einer <a data-tooltip-position="top" aria-label="Mannigfaltigkeiten" data-href="Mannigfaltigkeiten" href="the-guide/mathematics/differential-geometry/riemannian-geometry/mannigfaltigkeiten-und-differenzierbare-abbildungen/mannigfaltigkeiten.html" class="internal-link" target="_self" rel="noopener nofollow">Mannigfaltigkeit</a>  induziert und relativ zu einer <a data-tooltip-position="top" aria-label="Parallele Vektorfelder und Parallelverschiebung" data-href="Parallele Vektorfelder und Parallelverschiebung" href="the-guide/mathematics/differential-geometry/riemannian-geometry/metriken-und-zusammenhänge/parallele-vektorfelder-und-parallelverschiebung.html" class="internal-link" target="_self" rel="noopener nofollow">Parallelverschiebung</a> längs einer Kurve auf dieser beschrieben. In der Riemannschen Geometrie sind wir an einer rein intrinsischen Definition interessiert ...<br>In a Nutshell
Zusammenhänge beschreiben Parallelverschiebungen von <a data-tooltip-position="top" aria-label="Tangentialraum" data-href="Tangentialraum" href="the-guide/mathematics/differential-geometry/riemannian-geometry/mannigfaltigkeiten-und-differenzierbare-abbildungen/tangentialraum.html" class="internal-link" target="_self" rel="noopener nofollow">Tangentialräumen</a> rein axiomatisch, ohne eine Immersion zu benötigen.
<br>Sie haben keine Verbindung zu Zusammenhängen in der Topologie.<br><br>Alle weiteren Resultate basieren auf folgender initialer Definition:<br>Definition Zusammenhang
Ein (linearer) Zusammenhang  auf einer <a data-tooltip-position="top" aria-label="Mannigfaltigkeiten" data-href="Mannigfaltigkeiten" href="the-guide/mathematics/differential-geometry/riemannian-geometry/mannigfaltigkeiten-und-differenzierbare-abbildungen/mannigfaltigkeiten.html" class="internal-link" target="_self" rel="noopener nofollow">differenzierbaren Mannigfaltigkeit</a>  ist eine <a data-tooltip-position="top" aria-label="Mathematical Relations" data-href="Mathematical Relations" href="the-guide/mathematics/general-stuff/mathematical-relations.html" class="internal-link" target="_self" rel="noopener nofollow">Abbildung</a> die für alle ,  und  erfüllt:

<br>
<br>
<br>

<br>
<br>Zusammenhang hängt nur von  und  in einer Umgebung von  ab
<br><br>Christoffel-Symbole über Zusammenhänge
Die Christoffelsymbole eines Zusammenhanges sind für jede <a data-tooltip-position="top" aria-label="Topology and Topological Space" data-href="Topology and Topological Space" href="the-guide/mathematics/topology/topology-and-topological-space.html" class="internal-link" target="_self" rel="noopener nofollow">Karte</a>  die Funktionen  mit Sie messen den Unterschied zwischen der <a data-tooltip-position="top" aria-label="Lie Ableitung" data-href="Lie Ableitung" href="the-guide/mathematics/differential-geometry/riemannian-geometry/mannigfaltigkeiten-und-differenzierbare-abbildungen/lie-ableitung.html" class="internal-link" target="_self" rel="noopener nofollow">Richtungsableitung</a> der Karte und dem Zusammenhang.
<br>Intuition
Christoffel-Symbole kodieren die Änderung der Basisvektoren des Tangentialraumes, wenn wir von einem zum anderen wandern. Oben messen wir wie sich der Basisvektor  ändert, wenn wir in Richtung  auf der Mannigfaltigkeit gehen.
<br>Theorem
Jede Kurve  bestimmt genau eine <a data-tooltip-position="top" aria-label="Kovariante Ableitung" data-href="Kovariante Ableitung" href="the-guide/mathematics/differential-geometry/riemannian-geometry/metriken-und-zusammenhänge/kovariante-ableitung.html" class="internal-link" target="_self" rel="noopener nofollow">kovariante Ableitung</a> längs  mit den Eigenschaften

<br>-linear 
<br>Derivativ  für alle 
<br>Ist  eine Umgebung von  und , so gilt für alle :

<br>Parallelität für Zusammenhänge 
Das <a data-tooltip-position="top" aria-label="Vektorfelder auf Mannigfaltigkeiten" data-href="Vektorfelder auf Mannigfaltigkeiten" href="the-guide/mathematics/differential-geometry/riemannian-geometry/mannigfaltigkeiten-und-differenzierbare-abbildungen/vektorfelder-auf-mannigfaltigkeiten.html" class="internal-link" target="_self" rel="noopener nofollow">Vektorfeld</a>  heißt parallel längs , wenn  für alle  gilt. Wie gewohnt erhält man daher mit Resultaten über die Lösbarkeit von <a data-tooltip-position="top" aria-label="ODEs - Ordinary Differential Equations" data-href="ODEs - Ordinary Differential Equations" href="the-guide/mathematics/differential-equations/odes-ordinary-differential-equations.html" class="internal-link" target="_self" rel="noopener nofollow">Differentialgleichungen</a> zu jedem Anfangswert eine Parallelverschiebung.
<br> Im allgemeinen lässt sich ein gegebenes Vektorfeld  entlang einer Kurve durch Doppelabbildung etc. nicht auf ein Feld  auf einer Mannigfaltigkeit  fortsetzen.<br> Wir schränken uns daher auf lokale Fortsetzungen in einer Umgebung  von  ein. <br>Lemma
Ist  Vektorfeld längs einer Kurve  mit , so gibt es eine lokale Fortsetzung  von  um .
<br><br><br>Torsionsfreiheit
Ein Zusammenhang  auf einer <a data-tooltip-position="top" aria-label="Mannigfaltigkeiten" data-href="Mannigfaltigkeiten" href="the-guide/mathematics/differential-geometry/riemannian-geometry/mannigfaltigkeiten-und-differenzierbare-abbildungen/mannigfaltigkeiten.html" class="internal-link" target="_self" rel="noopener nofollow">Mannigfaltigkeit</a> ist torsionsfrei oder symmetrisch, wenn die Torsion für alle  verschwindet.
<br>
<br>In diesem Fall sind die Christoffel-Symbole wieder symmetrisch
<br>Die obige Formel erzwingt, dass die Parallelverschiebung entlang eines infinitesimalen Parallelograms definiert ist und nicht von dieser Abweicht. Dadurch kann keine Rotation durch die Verschiebungs-Operation selbst entstehen, Sie muss aus der Krümmung resultieren.
<br>Verträglichkeit
Ein Zusammenhang auf einer <a data-tooltip-position="top" aria-label="(Semi-) Riemannsche Metriken" data-href="(Semi-) Riemannsche Metriken" href="the-guide/mathematics/differential-geometry/riemannian-geometry/metriken-und-zusammenhänge/(semi-)-riemannsche-metriken.html" class="internal-link" target="_self" rel="noopener nofollow">semi-Riemannschen</a> Mannigfaltigkeit  heißt verträglich mit der Metrik, wenn erfür alle  erfüllt.
<br>
<br>Änderung der Winkel / Längen nur durch Änderung der Vektorfelder 
<br>Führt zu den konstanten Längen und Winkeln zwischen <a data-tooltip-position="top" aria-label="Parallele Vektorfelder und Parallelverschiebung" data-href="Parallele Vektorfelder und Parallelverschiebung" href="the-guide/mathematics/differential-geometry/riemannian-geometry/metriken-und-zusammenhänge/parallele-vektorfelder-und-parallelverschiebung.html" class="internal-link" target="_self" rel="noopener nofollow">parallel verschobenen Vektorfeldern</a>
<br>Theorem 
Ein Zusammenhang ist genau dann verträglich mit , wenn längs jeder Kurve für je zwei parallele <a data-tooltip-position="top" aria-label="Vektorfelder auf Mannigfaltigkeiten" data-href="Vektorfelder auf Mannigfaltigkeiten" href="the-guide/mathematics/differential-geometry/riemannian-geometry/mannigfaltigkeiten-und-differenzierbare-abbildungen/vektorfelder-auf-mannigfaltigkeiten.html" class="internal-link" target="_self" rel="noopener nofollow">Vektorfelder</a>  der Wert  konstant bleibt.
]]></description><link>the-guide/mathematics/differential-geometry/riemannian-geometry/metriken-und-zusammenhänge/zusammenhänge.html</link><guid isPermaLink="false">The Guide/Mathematics/Differential Geometry/Riemannian Geometry/Metriken und Zusammenhänge/Zusammenhänge.md</guid><pubDate>Wed, 23 Apr 2025 08:34:36 GMT</pubDate></item><item><title><![CDATA[Gaussians on Riemannian Manifolds]]></title><description><![CDATA[ 
 <br>The absence of euclidean structure on a <a data-tooltip-position="top" aria-label="(Semi-) Riemannsche Metriken" data-href="(Semi-) Riemannsche Metriken" href="the-guide/mathematics/differential-geometry/riemannian-geometry/metriken-und-zusammenhänge/(semi-)-riemannsche-metriken.html" class="internal-link" target="_self" rel="noopener nofollow">Riemannian</a> <a data-tooltip-position="top" aria-label="Mannigfaltigkeiten" data-href="Mannigfaltigkeiten" href="the-guide/mathematics/differential-geometry/riemannian-geometry/mannigfaltigkeiten-und-differenzierbare-abbildungen/mannigfaltigkeiten.html" class="internal-link" target="_self" rel="noopener nofollow">manifold</a> prohibits the transfer of the concept of a <a data-tooltip-position="top" aria-label="Expectations" data-href="Expectations" href="the-guide/mathematics/statistics/expectations.html" class="internal-link" target="_self" rel="noopener nofollow">mean value</a>. However, the variance is based on a <a data-tooltip-position="top" aria-label="Metric Space and Completeness" data-href="Metric Space and Completeness" href="the-guide/mathematics/general-stuff/metric-space-and-completeness.html" class="internal-link" target="_self" rel="noopener nofollow">distance</a> and can thereby be defined using <a data-tooltip-position="top" aria-label="Exponentialabbildung" data-href="Exponentialabbildung" href="the-guide/mathematics/differential-geometry/riemannian-geometry/metriken-und-zusammenhänge/exponentialabbildung.html" class="internal-link" target="_self" rel="noopener nofollow">geodesics</a>.<br>We can, however try to find a mean value based on the samples by minimizing the variancewhich is denoted the Riemannian center of mass. Existence and uniqueness are not guaranteed and require<br>
<br>That the whole support is included in a <a data-tooltip-position="top" aria-label="Normale Bälle" data-href="Normale Bälle" href="the-guide/mathematics/differential-geometry/riemannian-geometry/metriken-und-zusammenhänge/normale-bälle.html" class="internal-link" target="_self" rel="noopener nofollow">geodesic ball</a> (no intersections)
<br>...
<br><br><br>
<br>
Heat Kernel on  via infinite serieswhere  is a concentration parameter and  is the angle of the <a data-tooltip-position="top" aria-label="A Hitchhiker's Guide to Rotation Representations" data-href="A Hitchhiker's Guide to Rotation Representations" href="the-guide/robotics,-dynamics-and-control/a-hitchhiker's-guide-to-rotation-representations.html" class="internal-link" target="_self" rel="noopener nofollow">axis-angle</a> representation in the <a data-href="Lie Algebra" href="the-guide/mathematics/lie-theory/lie-algebra.html" class="internal-link" target="_self" rel="noopener nofollow">Lie Algebra</a> . This series converges very quickly for .

<br>
There exists a very good approximation if  in the form of 

<br>
Given a mean rotation  and a scale , the <a data-tooltip-position="top" aria-label="Probability Density Function" data-href="Probability Density Function" href="the-guide/mathematics/probability-theory/probability-density-function.html" class="internal-link" target="_self" rel="noopener nofollow">probability density</a> of a rotation  under the isotropic Gaussian is given by where  is the <a data-tooltip-position="top" aria-label="The Lie Group SO(n) and its Lie Algebra" data-href="The Lie Group SO(n) and its Lie Algebra" href="the-guide/mathematics/lie-theory/special-orthogonal-group-so(3)/the-lie-group-so(n)-and-its-lie-algebra.html" class="internal-link" target="_self" rel="noopener nofollow">log-map of SO(3)</a> (for ).

<br>
To sample a random rotation, we can use <a data-href="Inverse Transform Sampling" href="the-guide/computational-statistics/inverse-transform-sampling.html" class="internal-link" target="_self" rel="noopener nofollow">Inverse Transform Sampling</a> by computing the <a data-tooltip-position="top" aria-label="Cumulative Distribution Function" data-href="Cumulative Distribution Function" href="the-guide/mathematics/probability-theory/cumulative-distribution-function.html" class="internal-link" target="_self" rel="noopener nofollow">cdf</a>. For this we need to integrate out the pdf, for whcih we use the Haar measure on topological groups. This yields 

<br>We do this by first sampling from a uniform distribution and then select the respective angle. 
<br>Afterwards, we sample a random rotation axis uniformly from . We can do this because our Gaussian is isotropic.


<br>
Intuitively, we have ...

<br>For larger and larger , the distribution converges to 
<br>For smaller and smaller , the distribution can be locally approximated by a normal distribution on the tangent plane with 


<br>
<a rel="noopener nofollow" class="external-link" href="https://arxiv.org/html/2312.11707v1" target="_blank">https://arxiv.org/html/2312.11707v1</a>

<br><br>]]></description><link>the-guide/mathematics/differential-geometry/riemannian-geometry/gaussians-on-riemannian-manifolds.html</link><guid isPermaLink="false">The Guide/Mathematics/Differential Geometry/Riemannian Geometry/Gaussians on Riemannian Manifolds.md</guid><pubDate>Wed, 26 Feb 2025 10:33:30 GMT</pubDate></item><item><title><![CDATA[Hyperbolischer Raum]]></title><description><![CDATA[ 
 <br>In a Nutshell
Raum mit konstant negativer Krümmung , notiert mit . Revolutionierte die moderne Sicht auf die Geometrie, da für ihn alle euklidischen Axiome außer dem Parallelenaxiom gelten.
<br><br><br>Es existieren viele analoge Modelle für den hyperbolischen <a data-tooltip-position="top" aria-label="Space" data-href="Space" href="the-guide/mathematics/general-stuff/space.html" class="internal-link" target="_self" rel="noopener nofollow">Raum</a>. Die gängigsten sind ...<br>Poincaré-Halbraum Modell
Allgemein definiert mit Dichtefunktion , die für  explodiert. Man erhält Hierbei ist jeder Punkt unendlich weit vom idealen Rand  entfernt.
<br>
<br>Winkel und Längen werden für größeres  immer kleiner, Raum ist dichter je näher man der -Achse kommt. 
<br>Parallelen der -Achse haben konstante Krümmung <br>
<img alt="center" src="lib/media/pasted-image-20240612172755.png" style="width: 300px; max-width: 100%;">
<br>Poincaré-Ball Modell
Allgemein definiert auf einem <a data-tooltip-position="top" aria-label="Set" data-href="Set" href="the-guide/mathematics/general-stuff/set.html" class="internal-link" target="_self" rel="noopener nofollow">offenen Ball</a> mit Dichtefunktion, die gegen den Rand explodiert. Man erhält Hierbei ist jeder Punkt unendlich weit vom idealen Rand  entfernt.
<br>
<br>Raum wird dichter, je näher man dem idealen Rand kommt
<br><a data-tooltip-position="top" aria-label="(Semi-) Riemannsche Metriken" data-href="(Semi-) Riemannsche Metriken" href="the-guide/mathematics/differential-geometry/riemannian-geometry/metriken-und-zusammenhänge/(semi-)-riemannsche-metriken.html" class="internal-link" target="_self" rel="noopener nofollow">Konform</a> zur Standardmetrik, Vorfaktor entspricht genau Funktion 
<br><img alt="center" src="lib/media/pasted-image-20240612172501.png" style="width: 300px; max-width: 100%;"><br>Hyperboloid Modell
Die obere Schale des zweischaligen Hyperboloids  ist im Lorentz-Raum die Rotationshyperfläche mit der Lorentz-Metrik .
<br><img alt="center" src="lib/media/pasted-image-20240612172944.png" style="width: 200px; max-width: 100%;"><br>Es existieren zudem <a data-tooltip-position="top" aria-label="Isometrische Mannigfaltigkeiten" data-href="Isometrische Mannigfaltigkeiten" href="the-guide/mathematics/differential-geometry/riemannian-geometry/metriken-und-zusammenhänge/isometrische-mannigfaltigkeiten.html" class="internal-link" target="_self" rel="noopener nofollow">Isometrien</a> zwischen den Darstellungsformen:<br>
<br>Projektion des Hyperboloids auf Scheibe in der -Ebene auf Punkt .<img alt="center" src="lib/media/pasted-image-20240612170005.png" style="width: 200px; max-width: 100%;">
<br>Cayley Abbildung von Poincare Ball auf Poincare Halbraum
<br>Intuition
Die Modelle muss man sich immer in Verbindung mit der Metrik vorstellen, der Tangentialraum sieht in jedem Punkt gleich aus !<br>
Man kann sich zum Beispiel für das Halbschalen-Modell immer den gefragten Punkt via Isometrie auf die Spitze schieben.
<br><br><br>Im Falle des Hyperboloidmodells können wir gut <a data-tooltip-position="top" aria-label="Isometrische Mannigfaltigkeiten" data-href="Isometrische Mannigfaltigkeiten" href="the-guide/mathematics/differential-geometry/riemannian-geometry/metriken-und-zusammenhänge/isometrische-mannigfaltigkeiten.html" class="internal-link" target="_self" rel="noopener nofollow">Isometriegruppen</a> beschreiben, da wir sie als Transformationen des umgebenden Raumes darstellen können. Wir müssen Sie einzig dadurch Einschränken, dass sie die beiden Schalen nicht vertauschen können.<br>
Wir definieren die <a data-tooltip-position="top" aria-label="Group" data-href="Group" href="the-guide/mathematics/group-theory/group.html" class="internal-link" target="_self" rel="noopener nofollow">Gruppe</a> . Deren Einschänkung der Lorentz-Gruppe, die  erhält ist die gesuchte Isometriegruppe.<br>
Beispiele<br>
<br>Drehungen um die -Achse
<br>Boost aus Skript
<br><br><br>Die Geodätischen von  sind Parametrisierungen proportional zur Bogenlänge von den Mengen ...<br>
<br>Hyperboloid - Schnitte von Ursprungsebenen mit 
<br>Poincaré-Ball - Strecken und Kreisbögen, den den idealen Rand senkrecht treffen
<br>Poincaré-Halbraum -  Halbgerade und Halbkreise, die den Rand  senkrecht treffen
<br>Satz
 ist <a data-tooltip-position="top" aria-label="Mannigfaltigkeiten" data-href="Mannigfaltigkeiten" href="the-guide/mathematics/differential-geometry/riemannian-geometry/mannigfaltigkeiten-und-differenzierbare-abbildungen/mannigfaltigkeiten.html" class="internal-link" target="_self" rel="noopener nofollow">vollständige Mannigfaltigkeit</a>, man kann also global Geodätische bestimmen. 
<br>Satz
Geodätische von  sind auf jedem Teilstück <a data-tooltip-position="top" aria-label="Kürzeste" data-href="Kürzeste" href="the-guide/mathematics/differential-geometry/riemannian-geometry/metriken-und-zusammenhänge/kürzeste.html" class="internal-link" target="_self" rel="noopener nofollow">Kürzeste</a> und jeder metrische Ball  ist normal.
<br><br><br>Das Parallelenaxiom besagt, dass für jede Gerade  und einen Punkt, der nicht auf ihr liegt  genau eine Gerade  existiert, die  nicht schneidet. Stellt man sich dies jedoch auf z.B. der Poincare-Scheibe vor, so lassen sich zu einer Geodätischen unendlich viele Parallele Geodätische finden, die senkrecht den Rand treffen, aber  nicht schneiden.<br><br><br>Satz
Für jedes <a data-tooltip-position="top" aria-label="Genus" data-href="Genus" href="the-guide/mathematics/general-stuff/genus.html" class="internal-link" target="_self" rel="noopener nofollow">Geschlecht</a>  gibt es eine zweidimensionale kompakte orientierbare <a data-tooltip-position="top" aria-label="Mannigfaltigkeiten" data-href="Mannigfaltigkeiten" href="the-guide/mathematics/differential-geometry/riemannian-geometry/mannigfaltigkeiten-und-differenzierbare-abbildungen/mannigfaltigkeiten.html" class="internal-link" target="_self" rel="noopener nofollow">Riemann'sche Mannigfaltigkeit</a> , deren <a data-tooltip-position="top" aria-label="(Semi-) Riemannsche Metriken" data-href="(Semi-) Riemannsche Metriken" href="the-guide/mathematics/differential-geometry/riemannian-geometry/metriken-und-zusammenhänge/(semi-)-riemannsche-metriken.html" class="internal-link" target="_self" rel="noopener nofollow">Metrik</a> lokal <a data-tooltip-position="top" aria-label="Isometrische Mannigfaltigkeiten" data-href="Isometrische Mannigfaltigkeiten" href="the-guide/mathematics/differential-geometry/riemannian-geometry/metriken-und-zusammenhänge/isometrische-mannigfaltigkeiten.html" class="internal-link" target="_self" rel="noopener nofollow">isometrisch</a> zu  ist.
<br>
<br>Konstruieren Rechtwinklige Sechsecke aus Poincare-Scheibe, verklebe je 2 zu Y-Stück<img alt="center" src="lib/media/pasted-image-20240730113724.png" style="width: 300px; max-width: 100%;">
<br>Verklebe  Y-Stücke für Fläche mit <a data-tooltip-position="top" aria-label="Genus" data-href="Genus" href="the-guide/mathematics/general-stuff/genus.html" class="internal-link" target="_self" rel="noopener nofollow">Geschlecht</a> <img alt="center" src="lib/media/pasted-image-20240730113829.png" style="width: 250px; max-width: 100%;">
]]></description><link>the-guide/mathematics/differential-geometry/riemannian-geometry/hyperbolischer-raum.html</link><guid isPermaLink="false">The Guide/Mathematics/Differential Geometry/Riemannian Geometry/Hyperbolischer Raum.md</guid><pubDate>Mon, 12 Aug 2024 17:05:43 GMT</pubDate><enclosure url="lib/media/pasted-image-20240612172755.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src=&quot;lib/media/pasted-image-20240612172755.png&quot;&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[Projective Space]]></title><description><![CDATA[ 
 <br> ...<br><br><br>
<br>Projektive Räume  - kompakte Mannigfaltigkeit 

<br>Raum bestehend aus der Menge aller Ursprungsgeraden in 
<br>Zum Beispiel , jeder Winkel ist ein Punkt auf Achse, gibt eine Dimension
<br>In  habe ich zwei Winkel, gibt Punkt in Ebene
<br>Identifikation von Geraden in entgegengesetzte Richtugn beachten ! 


]]></description><link>the-guide/mathematics/differential-geometry/riemannian-geometry/projective-space.html</link><guid isPermaLink="false">The Guide/Mathematics/Differential Geometry/Riemannian Geometry/Projective Space.md</guid><pubDate>Mon, 12 Aug 2024 17:05:43 GMT</pubDate></item><item><title><![CDATA[Relativitätstheorie]]></title><description><![CDATA[ 
 <br>Das Tupel aus <a data-tooltip-position="top" aria-label="Vector Space" data-href="Vector Space" href="the-guide/mathematics/general-stuff/vector-space.html" class="internal-link" target="_self" rel="noopener nofollow">Vektorraum</a> und <a data-tooltip-position="top" aria-label="(Semi-) Riemannsche Metriken" data-href="(Semi-) Riemannsche Metriken" href="the-guide/mathematics/differential-geometry/riemannian-geometry/metriken-und-zusammenhänge/(semi-)-riemannsche-metriken.html" class="internal-link" target="_self" rel="noopener nofollow">Metrik</a> ist eine <a data-tooltip-position="top" aria-label="(Semi-) Riemannsche Metriken" data-href="(Semi-) Riemannsche Metriken" href="the-guide/mathematics/differential-geometry/riemannian-geometry/metriken-und-zusammenhänge/(semi-)-riemannsche-metriken.html" class="internal-link" target="_self" rel="noopener nofollow">semi-Riemannsche</a> <a data-tooltip-position="top" aria-label="Mannigfaltigkeiten" data-href="Mannigfaltigkeiten" href="the-guide/mathematics/differential-geometry/riemannian-geometry/mannigfaltigkeiten-und-differenzierbare-abbildungen/mannigfaltigkeiten.html" class="internal-link" target="_self" rel="noopener nofollow">Mannigfaltigkeit</a>. Speziell nennt man  den Minkowskiraum. <br>Man schreibt Punkte in diesem Raum oft über den Vektor , um auf die Zeitkoordinate zu verweisen. Mit Wahl der Einheiten, sodass  erhält man damit den Lichtkegel dessen Punkte Licht in der Raumzeit vom Ursprung aus erreichen kann.<br>Die Punkte im inneren des Kegels, also sind diejenigen, die vom Ursprung aus mit einer Geschwindigkeit kleiner der Lichtgeschwindigkeit erreichbar sind.<br>Ein <a data-tooltip-position="top" aria-label="Tangentialraum" data-href="Tangentialraum" href="the-guide/mathematics/differential-geometry/riemannian-geometry/mannigfaltigkeiten-und-differenzierbare-abbildungen/tangentialraum.html" class="internal-link" target="_self" rel="noopener nofollow">Tangentialvektor</a> an eine semi-Riemannsche Mannigfaltigkeit heißt ...<br>
<br>raumartig, wenn  oder 
<br>zeitartig, wenn 
<br>lichtartig, wenn 
]]></description><link>the-guide/mathematics/differential-geometry/riemannian-geometry/relativitätstheorie.html</link><guid isPermaLink="false">The Guide/Mathematics/Differential Geometry/Riemannian Geometry/Relativitätstheorie.md</guid><pubDate>Mon, 24 Feb 2025 23:32:25 GMT</pubDate></item><item><title><![CDATA[Variation der Energie]]></title><description><![CDATA[ 
 <br>In a Nutshell
Um die Frage zu klären, wann geodätische Kürzeste sind, betrachtet man in der Differentialgeometrie natürlicherweise meist <a data-tooltip-position="top" aria-label="Calculus of Variations" data-href="Calculus of Variations" href="the-guide/mathematics/functional-analysis-and-calculus-of-variations/calculus-of-variations.html" class="internal-link" target="_self" rel="noopener nofollow">Variationen</a> der Bogenlänge. Da dies aber nur im Riemannschen Fall sinnvoll ist, haben wir in der Vorlesung den Weg über die Energie gewählt.
<br><br><br>Um in der Riemannschen Geometrie über die Frage von <a data-tooltip-position="top" aria-label="Exponentialabbildung" data-href="Exponentialabbildung" href="the-guide/mathematics/differential-geometry/riemannian-geometry/metriken-und-zusammenhänge/exponentialabbildung.html" class="internal-link" target="_self" rel="noopener nofollow">Geodätischen</a> als Kürzeste sprechen zu können, benötigen wir eine Größe die bei Variation die Durchlaufgeschwindigkeit mit einbezieht. Dies ist durch die <a data-tooltip-position="top" aria-label="(Semi-) Riemannsche Metriken" data-href="(Semi-) Riemannsche Metriken" href="the-guide/mathematics/differential-geometry/riemannian-geometry/metriken-und-zusammenhänge/(semi-)-riemannsche-metriken.html" class="internal-link" target="_self" rel="noopener nofollow">Riemannsche Länge</a> zum Beispiel nicht gegeben. Wir definieren daher für eine differenzierbare Kurve  auf einer <a data-tooltip-position="top" aria-label="Mannigfaltigkeiten" data-href="Mannigfaltigkeiten" href="the-guide/mathematics/differential-geometry/riemannian-geometry/mannigfaltigkeiten-und-differenzierbare-abbildungen/mannigfaltigkeiten.html" class="internal-link" target="_self" rel="noopener nofollow">Mannigfaltigkeit</a>  die (mathematische Energie) <br>Variation und Variationsvektorfeld
Eine Variation von obigem  ist eine differenzierbare <a data-tooltip-position="top" aria-label="Mathematical Relations" data-href="Mathematical Relations" href="the-guide/mathematics/general-stuff/mathematical-relations.html" class="internal-link" target="_self" rel="noopener nofollow">Abbildung</a> Das dadurch definierte <a data-tooltip-position="top" aria-label="Vektorfelder auf Mannigfaltigkeiten" data-href="Vektorfelder auf Mannigfaltigkeiten" href="the-guide/mathematics/differential-geometry/riemannian-geometry/mannigfaltigkeiten-und-differenzierbare-abbildungen/vektorfelder-auf-mannigfaltigkeiten.html" class="internal-link" target="_self" rel="noopener nofollow">Vektorfeld</a> längs  heißt Variationsvektorfeld von .  bzw.  heißen eigentlich oder proper, wenn , die Endpunkte bleiben also unverändert.
<br>Mithilfe der <a data-href="Exponentialabbildung" href="the-guide/mathematics/differential-geometry/riemannian-geometry/metriken-und-zusammenhänge/exponentialabbildung.html" class="internal-link" target="_self" rel="noopener nofollow">Exponentialabbildung</a> können wir andersrum zu einem gegebenen Variationsvektorfeld eine Variation erhalten:<br>Lemma 27 - Existenz
Sei  und . Dann existiert , sodass die Variation für alle  definiert ist. Damit hat  das Variationsfeld .
<br>
<br>Folgt aus lokaler Existenz der <a data-href="Exponentialabbildung" href="the-guide/mathematics/differential-geometry/riemannian-geometry/metriken-und-zusammenhänge/exponentialabbildung.html" class="internal-link" target="_self" rel="noopener nofollow">Exponentialabbildung</a> und der Kompaktheit der Spur von .
<br><img alt="center" src="lib/media/pasted-image-20240610133541.png" style="width: 200px; max-width: 100%;"><br>Variiert man nun die oben definierte Energie, so erhält man<br>Erste Variation
Für eine Kurve  auf einer <a data-tooltip-position="top" aria-label="(Semi-) Riemannsche Metriken" data-href="(Semi-) Riemannsche Metriken" href="the-guide/mathematics/differential-geometry/riemannian-geometry/metriken-und-zusammenhänge/(semi-)-riemannsche-metriken.html" class="internal-link" target="_self" rel="noopener nofollow">semi-Riemannschen</a> <a data-tooltip-position="top" aria-label="Mannigfaltigkeiten" data-href="Mannigfaltigkeiten" href="the-guide/mathematics/differential-geometry/riemannian-geometry/mannigfaltigkeiten-und-differenzierbare-abbildungen/mannigfaltigkeiten.html" class="internal-link" target="_self" rel="noopener nofollow">Mannigfaltigkeit</a>  ist die erste Variation der Energie entlang der Richtung  gegeben durch Wir geben Tangenten- und Variationsfeld einen Namen: Eine Kurve heißt kritisch für eine gegebene Energie, wenn  für jedes eigentliche Variationsfeld .
<br>
<br>Folgt aus <a data-tooltip-position="top" aria-label="Kovariante Ableitung" data-href="Kovariante Ableitung" href="the-guide/mathematics/differential-geometry/riemannian-geometry/metriken-und-zusammenhänge/kovariante-ableitung.html" class="internal-link" target="_self" rel="noopener nofollow">kovariantem Satz von Schwarz</a> und Produktregel der <a data-tooltip-position="top" aria-label="Kovariante Ableitung" data-href="Kovariante Ableitung" href="the-guide/mathematics/differential-geometry/riemannian-geometry/metriken-und-zusammenhänge/kovariante-ableitung.html" class="internal-link" target="_self" rel="noopener nofollow">kovarianten Ableitung</a>/ Verträglichkeit von .
<br>Intuition

<br>Erster Term ist Randterm, Veränderung der Endpunkte verursacht Änderung der Kurvenlänge
<br>Zweiter Term ist für Kurve an sich. Man stelle sich vor man fahre entlang der Kurve auf einer Mannigfaltigkeit. Wenn man Kurven darauf fährt, so beötigt man (intrinsische) Beschleunigungsvektoren in die Kurve hinein. Zeigt das Variationsfeld  in die selbe Richtung die diese intrinsische Beschleunigung , so wird die Energie kleiner.

<br>Satz - Geodätische sind kritische Kurven
Eine Kurve  ist <a data-tooltip-position="top" aria-label="Exponentialabbildung" data-href="Exponentialabbildung" href="the-guide/mathematics/differential-geometry/riemannian-geometry/metriken-und-zusammenhänge/exponentialabbildung.html" class="internal-link" target="_self" rel="noopener nofollow">Geodätische</a> genau dann, wenn sie kritisch für die Energie ist (  für eigentliches Variationsfeld). 
<br>Proof
"" Folgt direkt mit der Definition der Geodätischen via .
"" Man nehme ein beliebiges Vektorfeld  entlang  und Skaliere dieses mit einer Hutfunktion  mit , ,  und . Damit erzeugt man eine eigentliche Variation (siehe Definition kritisch), wodurch der erste Term in der Variation der Energie verschwindet. Verschwindet nun die Variation der Energie ganz, so muss dies für alle  und alle  geschehen. Da die Metrik nicht ausgeartet ist, kann dies nur für  und damit Geodätische passieren. 
<br><br><br>Für <a data-tooltip-position="top" aria-label="Stückweise differenzierbare Kurven" data-href="Stückweise differenzierbare Kurven" href="the-guide/mathematics/differential-geometry/elementary-differential-geometry/globale-kurven-und-flächentheorie/stückweise-differenzierbare-kurven.html" class="internal-link" target="_self" rel="noopener nofollow">stückweise differenzierbare Kurven</a>  definiert man die gebrochene Variation , indem man fordert, dass jede Einschränkung der Variationdifferenzierbare Variation der jeweiligen Teilkurve  ist. Die Energie der gesamten Kurve  ist dann Die Formel der ersten Variation erweitert sich um Terme für die Sprungwinkel an den Knickstellen zuHierbei sind  die Differenzen der Tangentenwinkel.<br>Stückweise Kurven als Geodätische über Energie
Sei  <a data-tooltip-position="top" aria-label="Stückweise differenzierbare Kurven" data-href="Stückweise differenzierbare Kurven" href="the-guide/mathematics/differential-geometry/elementary-differential-geometry/globale-kurven-und-flächentheorie/stückweise-differenzierbare-kurven.html" class="internal-link" target="_self" rel="noopener nofollow">stückweise differenzierbare Kurve</a>.  ist eine differenzierbare <a data-tooltip-position="top" aria-label="Exponentialabbildung" data-href="Exponentialabbildung" href="the-guide/mathematics/differential-geometry/riemannian-geometry/metriken-und-zusammenhänge/exponentialabbildung.html" class="internal-link" target="_self" rel="noopener nofollow">Geodätische</a> (die <a data-tooltip-position="top" aria-label="Kovariante Ableitung" data-href="Kovariante Ableitung" href="the-guide/mathematics/differential-geometry/riemannian-geometry/metriken-und-zusammenhänge/kovariante-ableitung.html" class="internal-link" target="_self" rel="noopener nofollow">kovariante Ableitung</a> verschwindet auf dem gesamten Intervall ), wenn  für jede eigentliche gebrochene Variation  von  mit Variationsfeld  gilt.
<br>
<br>Gebrochene Variation verschiebt Knickpunkte, sodass die Verschiebung an Anfangs und Endpunkten jedes Intervalls passt
<br><br><br>Zusätzlich zu den Extrema oben interessiert uns logischerweise, ob es sich um ein lokales Minimum der Energie handelt. Dazu betrachten wir die zweite Variation, wenn wir jeden Punkt der Kurve durch <a data-tooltip-position="top" aria-label="Exponentialabbildung" data-href="Exponentialabbildung" href="the-guide/mathematics/differential-geometry/riemannian-geometry/metriken-und-zusammenhänge/exponentialabbildung.html" class="internal-link" target="_self" rel="noopener nofollow">Geodätische variieren</a>.<br>Zweite Variation der Energie
Sei  <a data-tooltip-position="top" aria-label="(Semi-) Riemannsche Metriken" data-href="(Semi-) Riemannsche Metriken" href="the-guide/mathematics/differential-geometry/riemannian-geometry/metriken-und-zusammenhänge/(semi-)-riemannsche-metriken.html" class="internal-link" target="_self" rel="noopener nofollow">semi-Riemansch</a>,  Kurve,  <a data-tooltip-position="top" aria-label="Vektorfelder auf Mannigfaltigkeiten" data-href="Vektorfelder auf Mannigfaltigkeiten" href="the-guide/mathematics/differential-geometry/riemannian-geometry/mannigfaltigkeiten-und-differenzierbare-abbildungen/vektorfelder-auf-mannigfaltigkeiten.html" class="internal-link" target="_self" rel="noopener nofollow">Vektorfeld</a> entlang  und Dann gilt für alle solche  und mit  und  der Zusammenhang wobei  die <a data-tooltip-position="top" aria-label="Sectional Curvature" data-href="Sectional Curvature" href="the-guide/mathematics/differential-geometry/riemannian-geometry/curvature/sectional-curvature.html" class="internal-link" target="_self" rel="noopener nofollow">Schnittkrümmung</a> bezeichnet.
<br>
<br>Ist eigentlich nicht wie im Skript behauptet Spezialfall, da ich immer so Umparametrisieren kann, dass  orthogonal zur Tangente an die Kurve ist.
<br>Intuition
Das Minuszeichen in der Formel hat im Riemannschen Fall folgende Konsequenz: Da der erste Term immer positiv ist, kann nur bei positiver Krümmung der zweite Term zu einer negativen zweiten Variation führen. In diesem Fall ist die Minimaleigenschaft von Geodätischen also in Gefahr.
<br>Beweis
Mit dem <a data-tooltip-position="top" aria-label="Kovariante Ableitung" data-href="Kovariante Ableitung" href="the-guide/mathematics/differential-geometry/riemannian-geometry/metriken-und-zusammenhänge/kovariante-ableitung.html" class="internal-link" target="_self" rel="noopener nofollow">kovarianten Satz von Schwarz</a> gilt . Ziehen wir die zweite Ableitung in das Integral der Energie und beschränken uns auf diese, so berechnen wir Wir wissen durch die Eigenschaft als Geodätische, dass . Mit der <a data-tooltip-position="top" aria-label="Riemann Curvature Tensor" data-href="Riemann Curvature Tensor" href="the-guide/mathematics/differential-geometry/riemannian-geometry/curvature/riemann-curvature-tensor.html" class="internal-link" target="_self" rel="noopener nofollow">Definition des Krümmungstensors</a> folgt für den doppelten kovarianten Ableitungstermund damit<br>
was genau dem Integranden entspricht.
<br>Korollar
Für eine Riemannsche Mannigfaltigkeit  mit nicht-positiver Schnittkrümmung ist jede Geodätische ein lokales Minimum der Energie, es gilt 
<br>Proof
Da die Variation in den Endpunkten verschwindet (eigentlich), muss sie sich irgendwann ändern, um nicht ganz zu verschwinden, also , womit der erste Term positiv ist. Der zweite Term ist für nicht-positive Krümmungen negativ, das Integral also immer positiv, womit die kritische Variation ein Minimum sein muss.
]]></description><link>the-guide/mathematics/differential-geometry/riemannian-geometry/variation-der-energie.html</link><guid isPermaLink="false">The Guide/Mathematics/Differential Geometry/Riemannian Geometry/Variation der Energie.md</guid><pubDate>Mon, 12 Aug 2024 17:05:43 GMT</pubDate><enclosure url="lib/media/pasted-image-20240610133541.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src=&quot;lib/media/pasted-image-20240610133541.png&quot;&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[Calculus of Variations]]></title><description><![CDATA[ 
 <br>In a Nutshell
Field of mathematical analysis that uses small changes in functions and more importantly functionals (<a data-tooltip-position="top" aria-label="Mathematical Relations" data-href="Mathematical Relations" href="the-guide/mathematics/general-stuff/mathematical-relations.html" class="internal-link" target="_self" rel="noopener nofollow">mapping</a> from <a data-tooltip-position="top" aria-label="Set" data-href="Set" href="the-guide/mathematics/general-stuff/set.html" class="internal-link" target="_self" rel="noopener nofollow">set</a> of functions to real numbers) to find their extrema.
<br><br>Similar to regular functions, the extrema may be found by finding the points where the <a data-tooltip-position="top" aria-label="Calculus of Variations" data-href="Calculus of Variations" href="the-guide/mathematics/functional-analysis-and-calculus-of-variations/calculus-of-variations.html" class="internal-link" target="_self" rel="noopener nofollow">functional derivative</a> vanishes. This leads to the problem of solving the so-called Euler-Lagrange-equation:<br>Consider a functional with constant ,  and  twice differentiable w.r.t all of its arguments.  attains a local minimum at ...<br><br>
<br>Strong Formulation exists, but doesn't exist in many cases
<br>First Variation
In many practical cases, the first variation is defined as the directional derivative 
]]></description><link>the-guide/mathematics/functional-analysis-and-calculus-of-variations/calculus-of-variations.html</link><guid isPermaLink="false">The Guide/Mathematics/Functional Analysis and Calculus of Variations/Calculus of Variations.md</guid><pubDate>Thu, 15 Aug 2024 12:51:15 GMT</pubDate></item><item><title><![CDATA[Euler-Lagrange Equations]]></title><description><![CDATA[ 
 <br>In a Nutshell
System of <a data-tooltip-position="top" aria-label="ODEs - Ordinary Differential Equations" data-href="ODEs - Ordinary Differential Equations" href="the-guide/mathematics/differential-equations/odes-ordinary-differential-equations.html" class="internal-link" target="_self" rel="noopener nofollow">second-order ODEs</a> whose solutions are <a data-tooltip-position="top" aria-label="Stationary Points" data-href="Stationary Points" href="the-guide/mathematics/optimization/convex-optimization-lecture/stationary-points.html" class="internal-link" target="_self" rel="noopener nofollow">stationary points</a> of the given <a data-tooltip-position="top" aria-label="Lagrangian Mechanics" data-href="Lagrangian Mechanics" href="the-guide/robotics,-dynamics-and-control/dynamics/lagrangian-mechanics.html" class="internal-link" target="_self" rel="noopener nofollow">action functional</a>.
<br><br>Let the tuple  of a <a data-tooltip-position="top" aria-label="Mannigfaltigkeiten" data-href="Mannigfaltigkeiten" href="the-guide/mathematics/differential-geometry/riemannian-geometry/mannigfaltigkeiten-und-differenzierbare-abbildungen/mannigfaltigkeiten.html" class="internal-link" target="_self" rel="noopener nofollow">smooth manifold</a>  and the <a data-tooltip-position="top" aria-label="Lagrangian Mechanics" data-href="Lagrangian Mechanics" href="the-guide/robotics,-dynamics-and-control/dynamics/lagrangian-mechanics.html" class="internal-link" target="_self" rel="noopener nofollow">Lagrangian</a>  (see <a data-tooltip-position="top" aria-label="Tangentialbündel" data-href="Tangentialbündel" href="the-guide/mathematics/differential-geometry/riemannian-geometry/mannigfaltigkeiten-und-differenzierbare-abbildungen/tangentialbündel.html" class="internal-link" target="_self" rel="noopener nofollow">tangent bundle</a>) be a real <a data-tooltip-position="top" aria-label="Static and Dynamic Systems" data-href="Static and Dynamic Systems" href="the-guide/robotics,-dynamics-and-control/dynamics/static-and-dynamic-systems.html" class="internal-link" target="_self" rel="noopener nofollow">dynamic system</a> with  degrees of freedom.<br>Theorem
A path  is a <a data-tooltip-position="top" aria-label="Stationary Points" data-href="Stationary Points" href="the-guide/mathematics/optimization/convex-optimization-lecture/stationary-points.html" class="internal-link" target="_self" rel="noopener nofollow">stationary</a> point of the <a data-tooltip-position="top" aria-label="Lagrangian Mechanics" data-href="Lagrangian Mechanics" href="the-guide/robotics,-dynamics-and-control/dynamics/lagrangian-mechanics.html" class="internal-link" target="_self" rel="noopener nofollow">action functional</a> if and only if
]]></description><link>the-guide/mathematics/functional-analysis-and-calculus-of-variations/euler-lagrange-equations.html</link><guid isPermaLink="false">The Guide/Mathematics/Functional Analysis and Calculus of Variations/Euler-Lagrange Equations.md</guid><pubDate>Sun, 16 Mar 2025 16:27:59 GMT</pubDate></item><item><title><![CDATA[Fundamental Lemma of the Calculus of Variations]]></title><description><![CDATA[ 
 <br>Theorem - Basic Version
If a continuous <a data-tooltip-position="top" aria-label="Mathematical Relations" data-href="Mathematical Relations" href="the-guide/mathematics/general-stuff/mathematical-relations.html" class="internal-link" target="_self" rel="noopener nofollow">function</a>  on an open <a data-tooltip-position="top" aria-label="Set" data-href="Set" href="the-guide/mathematics/general-stuff/set.html" class="internal-link" target="_self" rel="noopener nofollow">intervall</a>  satisfies the equality for all compactly supported smooth functions  on , then  is identically .
]]></description><link>the-guide/mathematics/functional-analysis-and-calculus-of-variations/fundamental-lemma-of-the-calculus-of-variations.html</link><guid isPermaLink="false">The Guide/Mathematics/Functional Analysis and Calculus of Variations/Fundamental Lemma of the Calculus of Variations.md</guid><pubDate>Thu, 15 Aug 2024 12:51:15 GMT</pubDate></item><item><title><![CDATA[Inner Product Space and Hilbert Space]]></title><description><![CDATA[ 
 <br>Inner Product Space
Also called pre-<a data-tooltip-position="top" aria-label="Inner Product Space and Hilbert Space" data-href="Inner Product Space and Hilbert Space" href="the-guide/mathematics/functional-analysis-and-calculus-of-variations/inner-product-space-and-hilbert-space.html" class="internal-link" target="_self" rel="noopener nofollow">Hilbert space</a>, denotes a real or complex <a data-tooltip-position="top" aria-label="Vector Space" data-href="Vector Space" href="the-guide/mathematics/general-stuff/vector-space.html" class="internal-link" target="_self" rel="noopener nofollow">vector space</a> with an additional operation, a <a data-tooltip-position="top" aria-label="Bilinear Form" data-href="Bilinear Form" href="the-guide/mathematics/linear-algebra/bilinear-form.html" class="internal-link" target="_self" rel="noopener nofollow">bilinear form</a> called inner product. 
<br>Definition - Inner Product
Let  be a vector space and  a field that is either the real or complex numbers. An inner product is a <a data-tooltip-position="top" aria-label="Mathematical Relations" data-href="Mathematical Relations" href="the-guide/mathematics/general-stuff/mathematical-relations.html" class="internal-link" target="_self" rel="noopener nofollow">map</a>  that satisfies the following properties for all vectors  and all scalars :

<br>Conjugate Symmetry 

<br>In real case reduces to symmetry


<br>Linearity in first argument 

<br>In real case reduces to bilinearity (linear in both arguments)


<br>Positive-Definiteness  

<br>Only  if  is 0



<br><br>Real- or complex valued <a data-tooltip-position="top" aria-label="Vector Space" data-href="Vector Space" href="the-guide/mathematics/general-stuff/vector-space.html" class="internal-link" target="_self" rel="noopener nofollow">inner product space</a> that is also a complete (every <a data-tooltip-position="top" aria-label="Cauchy Sequence" data-href="Cauchy Sequence" href="the-guide/mathematics/general-stuff/cauchy-sequence.html" class="internal-link" target="_self" rel="noopener nofollow">Cauchy sequence</a> of points in space lies in space) <a data-tooltip-position="top" aria-label="Metric Space and Completeness" data-href="Metric Space and Completeness" href="the-guide/mathematics/general-stuff/metric-space-and-completeness.html" class="internal-link" target="_self" rel="noopener nofollow">metric space</a> with respect to the distance <a data-tooltip-position="top" aria-label="Mathematical Relations" data-href="Mathematical Relations" href="the-guide/mathematics/general-stuff/mathematical-relations.html" class="internal-link" target="_self" rel="noopener nofollow">function</a> induced by the inner product. By definition every Hilbert Space is also a <a data-tooltip-position="top" aria-label="Normed Vector Space and Banach Space" data-href="Normed Vector Space and Banach Space" href="the-guide/mathematics/functional-analysis-and-calculus-of-variations/normed-vector-space-and-banach-space.html" class="internal-link" target="_self" rel="noopener nofollow">Banach Space</a>.<br><a rel="noopener nofollow" class="external-link" href="https://en.wikipedia.org/wiki/Hilbert_space" target="_blank">https://en.wikipedia.org/wiki/Hilbert_space</a><br>
<br>generalized euclidean space
]]></description><link>the-guide/mathematics/functional-analysis-and-calculus-of-variations/inner-product-space-and-hilbert-space.html</link><guid isPermaLink="false">The Guide/Mathematics/Functional Analysis and Calculus of Variations/Inner Product Space and Hilbert Space.md</guid><pubDate>Thu, 27 Feb 2025 15:32:58 GMT</pubDate></item><item><title><![CDATA[Normed Vector Space and Banach Space]]></title><description><![CDATA[ 
 <br>In a Nutshell
Adds abstract notion of length to <a data-tooltip-position="top" aria-label="Vector Space" data-href="Vector Space" href="the-guide/mathematics/general-stuff/vector-space.html" class="internal-link" target="_self" rel="noopener nofollow">vector spaces</a>.
<br><br><br>Normed Vector Space
A normed vector space is a pair , where  is a <a data-tooltip-position="top" aria-label="Vector Space" data-href="Vector Space" href="the-guide/mathematics/general-stuff/vector-space.html" class="internal-link" target="_self" rel="noopener nofollow">vector space</a> over the <a data-tooltip-position="top" aria-label="Field" data-href="Field" href="the-guide/mathematics/general-stuff/field.html" class="internal-link" target="_self" rel="noopener nofollow">field</a>  or , and  is  a <a data-tooltip-position="top" aria-label="Norms" data-href="Norms" href="the-guide/mathematics/linear-algebra/norms.html" class="internal-link" target="_self" rel="noopener nofollow">norm</a>.
<br>Difference to Metric Spaces
A <a data-tooltip-position="top" aria-label="Metric Space and Completeness" data-href="Metric Space and Completeness" href="the-guide/mathematics/general-stuff/metric-space-and-completeness.html" class="internal-link" target="_self" rel="noopener nofollow">metric space</a> defines distances on an arbitrary <a data-tooltip-position="top" aria-label="Set" data-href="Set" href="the-guide/mathematics/general-stuff/set.html" class="internal-link" target="_self" rel="noopener nofollow">set</a>, not asking for any algebraic structure (which vector spaces do in form of linear algebra). 
<br>Examples<br>
<br>Euclidean space  with any <a data-tooltip-position="top" aria-label="Norms" data-href="Norms" href="the-guide/mathematics/linear-algebra/norms.html" class="internal-link" target="_self" rel="noopener nofollow">norm</a>
<br><br><br>Banach Space
A Banach <a data-tooltip-position="top" aria-label="Space" data-href="Space" href="the-guide/mathematics/general-stuff/space.html" class="internal-link" target="_self" rel="noopener nofollow">space</a> is a normed vector space  that is <a data-tooltip-position="top" aria-label="Metric Space and Completeness" data-href="Metric Space and Completeness" href="the-guide/mathematics/general-stuff/metric-space-and-completeness.html" class="internal-link" target="_self" rel="noopener nofollow">complete</a> under the metric  induced by the norm.
<br>Why is this interesting
Important in many proofs. In finite dimensions, all norms are equivalent and induce completeness, but in infinite dimensions this has to be verified. 
]]></description><link>the-guide/mathematics/functional-analysis-and-calculus-of-variations/normed-vector-space-and-banach-space.html</link><guid isPermaLink="false">The Guide/Mathematics/Functional Analysis and Calculus of Variations/Normed Vector Space and Banach Space.md</guid><pubDate>Thu, 27 Feb 2025 15:27:30 GMT</pubDate></item><item><title><![CDATA[Sobolev Spaces]]></title><description><![CDATA[ 
 <br>TODO<br>
<a data-href="Weak Derivatives" href="the-guide/mathematics/functional-analysis-and-calculus-of-variations/weak-derivatives.html" class="internal-link" target="_self" rel="noopener nofollow">Weak Derivatives</a>]]></description><link>the-guide/mathematics/functional-analysis-and-calculus-of-variations/sobolev-spaces.html</link><guid isPermaLink="false">The Guide/Mathematics/Functional Analysis and Calculus of Variations/Sobolev Spaces.md</guid><pubDate>Tue, 11 Mar 2025 09:30:55 GMT</pubDate></item><item><title><![CDATA[Weak Derivatives]]></title><description><![CDATA[ 
 <br>In a Nutshell
Generalizes the concept of a <a data-tooltip-position="top" aria-label="Derivative, Gradient, Jacobian and Hessian" data-href="Derivative, Gradient, Jacobian and Hessian" href="the-guide/mathematics/analysis-and-calculus/derivative,-gradient,-jacobian-and-hessian.html" class="internal-link" target="_self" rel="noopener nofollow">derivative</a> for <a data-tooltip-position="top" aria-label="Mathematical Relations" data-href="Mathematical Relations" href="the-guide/mathematics/general-stuff/mathematical-relations.html" class="internal-link" target="_self" rel="noopener nofollow">function</a> that are not differentiable in the strong sense,  but integrable, i.e. in the <a data-tooltip-position="top" aria-label="Lebesgue Space" data-href="Lebesgue Space" href="Lebesgue Space" class="internal-link" target="_self" rel="noopener nofollow">Lebesgue</a> <a data-tooltip-position="top" aria-label="Space" data-href="Space" href="the-guide/mathematics/general-stuff/space.html" class="internal-link" target="_self" rel="noopener nofollow">space</a> .
<br><br>Weak Derivatives
Let  be a function in the <a data-href="Lebesgue Space" href="Lebesgue Space" class="internal-link" target="_self" rel="noopener nofollow">Lebesgue Space</a> . We say that  is a weak derivative of  iffor all infinitely differentiable functions with compact support . 
For higher dimensions and the space of locally integrable functions  for some open <a data-tooltip-position="top" aria-label="Set" data-href="Set" href="the-guide/mathematics/general-stuff/set.html" class="internal-link" target="_self" rel="noopener nofollow">subset</a> of , we use a multi-index  and the notationto state that  is the -th weak derivative of , if
]]></description><link>the-guide/mathematics/functional-analysis-and-calculus-of-variations/weak-derivatives.html</link><guid isPermaLink="false">The Guide/Mathematics/Functional Analysis and Calculus of Variations/Weak Derivatives.md</guid><pubDate>Sun, 09 Feb 2025 21:58:55 GMT</pubDate></item><item><title><![CDATA[Accuracy of Numerical Methods]]></title><description><![CDATA[ 
 <br>In a Nutshell
Accuracy in numerics quantifies how closely the computed solution approximates the true solution.
<br><br><br>
<br>forward, backward and mixed stability
<br><br><br>We follow our notation for <a data-tooltip-position="top" aria-label="ODEs - Ordinary Differential Equations" data-href="ODEs - Ordinary Differential Equations" href="the-guide/mathematics/differential-equations/odes-ordinary-differential-equations.html" class="internal-link" target="_self" rel="noopener nofollow">ODEs</a> that is based on a <a data-tooltip-position="top" aria-label="Mathematical Relations" data-href="Mathematical Relations" href="the-guide/mathematics/general-stuff/mathematical-relations.html" class="internal-link" target="_self" rel="noopener nofollow">function</a>  with  and its <a data-tooltip-position="top" aria-label="Derivative, Gradient, Jacobian and Hessian" data-href="Derivative, Gradient, Jacobian and Hessian" href="the-guide/mathematics/analysis-and-calculus/derivative,-gradient,-jacobian-and-hessian.html" class="internal-link" target="_self" rel="noopener nofollow">derviatives</a>. Additionally, the basic setting for numerical analysis of ODEs usually assumes a <a data-tooltip-position="top" aria-label="ODEs - Ordinary Differential Equations > Reduction to first Order System" data-href="ODEs - Ordinary Differential Equations#Reduction to first Order System" href="the-guide/mathematics/differential-equations/odes-ordinary-differential-equations.html#Reduction_to_first_Order_System" class="internal-link" target="_self" rel="noopener nofollow">reformulation to a first order system</a> without loss of generality. We can thereby consider the initial value problem<br>Local Truncation Error (LTE)
Measures the error made in a single step of the method, assuming that the previous value is exact. Formally, for a one--step method (generalization of <a data-tooltip-position="top" aria-label="Explicit Runge Kutta Methods" data-href="Explicit Runge Kutta Methods" href="the-guide/mathematics/differential-equations/explicit-runge-kutta-methods.html" class="internal-link" target="_self" rel="noopener nofollow">explicit Runge-Kutta</a>), Using a Taylor expansion of the exact solution, a method of order  will satisfy  For example, <a data-tooltip-position="top" aria-label="Explicit Runge Kutta Methods" data-href="Explicit Runge Kutta Methods" href="the-guide/mathematics/differential-equations/explicit-runge-kutta-methods.html" class="internal-link" target="_self" rel="noopener nofollow">Euler's method</a> is first--order (i.e., ), while a fourth--order Runge--Kutta method satisfies . 
<br>Consistency
The method is said to be consistent if the local truncation error (LTE) from above tends to zero as the step size  tends to zero. That is, if the exact solution satisfies  then consistency means  More precisely, if there exists an integer  such that  the method is said to be of order .
<br>Convergence
A numerical method is convergent if the global error, defined by  tends to zero as . Moreover, if there exists a constant  independent of  such that  the method is convergent of order . 
<br>Connetion between Convergence, Consistency and Stability
The three fundamental concepts are related via the <a data-href="Lax-Richtmyer and Dahlquist Equivalence Theorems" href="the-guide/mathematics/differential-equations/lax-richtmyer-and-dahlquist-equivalence-theorems.html" class="internal-link" target="_self" rel="noopener nofollow">Lax-Richtmyer and Dahlquist Equivalence Theorems</a>.
]]></description><link>the-guide/mathematics/general-stuff/accuracy-of-numerical-methods.html</link><guid isPermaLink="false">The Guide/Mathematics/General Stuff/Accuracy of Numerical Methods.md</guid><pubDate>Sun, 09 Feb 2025 14:15:18 GMT</pubDate></item><item><title><![CDATA[Algebra]]></title><description><![CDATA[ 
 <br>In a Nutshell
Study of generalizations of arithmetic operations. Mostly used for an algebraic structure or an algebra over a field.
<br><br><br>A <a data-tooltip-position="top" aria-label="Vector Space" data-href="Vector Space" href="the-guide/mathematics/general-stuff/vector-space.html" class="internal-link" target="_self" rel="noopener nofollow">vector space</a> equipped with a bilinear prodcut. More precisely, a <a data-tooltip-position="top" aria-label="Set" data-href="Set" href="the-guide/mathematics/general-stuff/set.html" class="internal-link" target="_self" rel="noopener nofollow">set</a>  equipped with one or more binary operations  that manipulates any two input elements of the set into a single output element of the set as well as scalar multiplication by elements of a field, both satisfying the axioms of bilinear and vector spaces.  <br>For , :<br>
<br>
<br>
<br><br>
<br>
<br>Polynomials in , 
<br>Complex  <a data-tooltip-position="top" aria-label="Matrices" data-href="Matrices" href="the-guide/mathematics/linear-algebra/matrices.html" class="internal-link" target="_self" rel="noopener nofollow">matrices</a> with matrix multiplication
<br> with cross product
<br><br><br>Nonempty <a data-tooltip-position="top" aria-label="Set" data-href="Set" href="the-guide/mathematics/general-stuff/set.html" class="internal-link" target="_self" rel="noopener nofollow">set</a> (domain), a collection of operation on this set  and identities or axioms that these must satisfy.<br>
<br>Notation

<br>Unary operator takes single element, binary takes two
<br>ANY (not just the usual mutliplication) binary operation is denoted If more than one operation is defined on a set, the first one is denoted  and the second one .


<br>Properties

<br>Commutativity of a binary operation

<br>Input order doesn't matter


<br>Associativity of a binary operation

<br>For more than two inputs, the grouping does not matter


<br>Distributivity of one binary operation  across another one  

<br>Asymmetric




<br>Special Elements

<br>Unit/Identity
<br>Null/Zero


<br>]]></description><link>the-guide/mathematics/general-stuff/algebra.html</link><guid isPermaLink="false">The Guide/Mathematics/General Stuff/Algebra.md</guid><pubDate>Sat, 05 Apr 2025 16:01:02 GMT</pubDate></item><item><title><![CDATA[Cauchy Sequence]]></title><description><![CDATA[ 
 <br>In a Nutshell
Sequence, whose elements become arbitrary "close" to each other. 
<br><br>Definition
Given a <a data-tooltip-position="top" aria-label="Metric Space and Completeness" data-href="Metric Space and Completeness" href="the-guide/mathematics/general-stuff/metric-space-and-completeness.html" class="internal-link" target="_self" rel="noopener nofollow">metric</a> <a data-tooltip-position="top" aria-label="Space" data-href="Space" href="the-guide/mathematics/general-stuff/space.html" class="internal-link" target="_self" rel="noopener nofollow">space</a> , a sequence is Cauchy, if for every positive real number , there is a positive integer , such that for all following integers  the distance is smaller than  considering the given metric: 
<br>A metric space, in which every Cauchy sequence converges to an element in that space  is denoted complete.]]></description><link>the-guide/mathematics/general-stuff/cauchy-sequence.html</link><guid isPermaLink="false">The Guide/Mathematics/General Stuff/Cauchy Sequence.md</guid><pubDate>Mon, 24 Feb 2025 23:49:44 GMT</pubDate></item><item><title><![CDATA[Contraction Mapping]]></title><description><![CDATA[ 
 <br>Definition
<a data-tooltip-position="top" aria-label="Mathematical Relations" data-href="Mathematical Relations" href="the-guide/mathematics/general-stuff/mathematical-relations.html" class="internal-link" target="_self" rel="noopener nofollow">Function</a>  from the <a data-tooltip-position="top" aria-label="Set" data-href="Set" href="the-guide/mathematics/general-stuff/set.html" class="internal-link" target="_self" rel="noopener nofollow">set</a>  of a <a data-tooltip-position="top" aria-label="Metric Space and Completeness" data-href="Metric Space and Completeness" href="the-guide/mathematics/general-stuff/metric-space-and-completeness.html" class="internal-link" target="_self" rel="noopener nofollow">metric space</a>  to itself, for which there is some real number , such that The smallest such  is the Lipschitz constant. If , the mapping is denoted non-expansive.
<br>(Examples)]]></description><link>the-guide/mathematics/general-stuff/contraction-mapping.html</link><guid isPermaLink="false">The Guide/Mathematics/General Stuff/Contraction Mapping.md</guid><pubDate>Mon, 24 Feb 2025 23:49:44 GMT</pubDate></item><item><title><![CDATA[Differential Form]]></title><description><![CDATA[ 
 <br>Mathematical objects constructed via tangent spaces and multilinear forms that behave, in many ways, like differentials in the classical sense.<br><br><br><br>]]></description><link>the-guide/mathematics/general-stuff/differential-form.html</link><guid isPermaLink="false">The Guide/Mathematics/General Stuff/Differential Form.md</guid><pubDate>Tue, 25 Feb 2025 08:31:47 GMT</pubDate></item><item><title><![CDATA[Dual Vector Space]]></title><description><![CDATA[ 
 <br>In a Nutshell
If  is a vector space over a field , the set of all <a data-tooltip-position="top" aria-label="Differential Form" data-href="Differential Form" href="the-guide/mathematics/general-stuff/differential-form.html" class="internal-link" target="_self" rel="noopener nofollow">linear functionals</a> from  to  itself form a vector space with addition and scalar multiplication defined pointwise. This <a data-tooltip-position="top" aria-label="Space" data-href="Space" href="the-guide/mathematics/general-stuff/space.html" class="internal-link" target="_self" rel="noopener nofollow">space</a> is denoted the dual space .
<br><br><br>Definition
If  is finite-dimensional, then  has the same dimensions as . Given a basis  of , we can construct a basis of linear <a data-tooltip-position="top" aria-label="Differential Form" data-href="Differential Form" href="the-guide/mathematics/general-stuff/differential-form.html" class="internal-link" target="_self" rel="noopener nofollow">linear functionals</a> , such that In the standard basis (orthonormal), it holds that To transform one into the other, we use the isomorphism , such that The inverse isomorphism is . Both operators can be defined on <a data-tooltip-position="top" aria-label="Mannigfaltigkeiten" data-href="Mannigfaltigkeiten" href="the-guide/mathematics/differential-geometry/riemannian-geometry/mannigfaltigkeiten-und-differenzierbare-abbildungen/mannigfaltigkeiten.html" class="internal-link" target="_self" rel="noopener nofollow">manifolds</a>, see <a data-tooltip-position="top" aria-label="Musikalischer Isomorphismus" data-href="Musikalischer Isomorphismus" href="the-guide/mathematics/differential-geometry/riemannian-geometry/curvature/musikalischer-isomorphismus.html" class="internal-link" target="_self" rel="noopener nofollow">musical isomorphisms</a>.
Not assuming an orthonormal basis, a covector is a linear function that takes a vector to produce a number, a - <a data-tooltip-position="top" aria-label="Tensors" data-href="Tensors" href="the-guide/mathematics/general-stuff/tensors.html" class="internal-link" target="_self" rel="noopener nofollow">tensor</a>.
<br>Covectors can be visualized similar to elevation in maps using lines, where each position vector on that line yields the same result when combined with the covector (depends on inner product).<img alt="center" src="lib/media/pasted-image-20240802101245.png" style="width: 200px; max-width: 100%;"><br>Intuition
Can be thought of as the space of coordinates. Given an abstract vector that is floating around in space, we can work with it only after applying a picked set of dual vector to it to "measure its components", turning it into the usual array of numbers.<br>
When using orthonormal coordinates, a covector is just a transposed vector.
<br><br>
<br>On , a row vector  can be considered a linear functional acting on  via 
<br>The <a data-tooltip-position="top" aria-label="Matrices" data-href="Matrices" href="the-guide/mathematics/linear-algebra/matrices.html" class="internal-link" target="_self" rel="noopener nofollow">trace</a> of a <a data-tooltip-position="top" aria-label="Matrices" data-href="Matrices" href="the-guide/mathematics/linear-algebra/matrices.html" class="internal-link" target="_self" rel="noopener nofollow">matrix</a> is a linear functional on the space of  matrices, since  and 
<br>The linear transformation defined by the Riemannian integral is a linear functional
]]></description><link>the-guide/mathematics/general-stuff/dual-vector-space.html</link><guid isPermaLink="false">The Guide/Mathematics/General Stuff/Dual Vector Space.md</guid><pubDate>Sun, 26 Jan 2025 18:56:53 GMT</pubDate><enclosure url="lib/media/pasted-image-20240802101245.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src=&quot;lib/media/pasted-image-20240802101245.png&quot;&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[Equivalence Relation and Class]]></title><description><![CDATA[ 
 <br>Definition
A binary relation  on a <a data-tooltip-position="top" aria-label="Set" data-href="Set" href="the-guide/mathematics/general-stuff/set.html" class="internal-link" target="_self" rel="noopener nofollow">set</a>  is an equivalence relation iff for all  the properties 

<br>Reflexivity 
<br>Symmetry
<br>Transitivityhold.

<br>Equivalence Classes
Given any element , the set of all elements in  for which a given equivalence relation holds is denoted an equivalence class
<br>Intuition
Breaks elements of a <a data-tooltip-position="top" aria-label="Set" data-href="Set" href="the-guide/mathematics/general-stuff/set.html" class="internal-link" target="_self" rel="noopener nofollow">set</a> into classes that are indistinguishable by the <a data-tooltip-position="top" aria-label="Space" data-href="Space" href="the-guide/mathematics/general-stuff/space.html" class="internal-link" target="_self" rel="noopener nofollow">strucutre of the studied space</a> alone.
]]></description><link>the-guide/mathematics/general-stuff/equivalence-relation-and-class.html</link><guid isPermaLink="false">The Guide/Mathematics/General Stuff/Equivalence Relation and Class.md</guid><pubDate>Mon, 14 Apr 2025 15:09:46 GMT</pubDate></item><item><title><![CDATA[Field]]></title><description><![CDATA[ 
 <br>In a Nutshell
<a data-tooltip-position="top" aria-label="Algebra" data-href="Algebra" href="the-guide/mathematics/general-stuff/algebra.html" class="internal-link" target="_self" rel="noopener nofollow">Algebraic</a> structure where you can add, subtract, multiply, and divide (except by zero) in a way that mirrors familiar arithmetic. Genralizes number systems like  or .
<br>]]></description><link>the-guide/mathematics/general-stuff/field.html</link><guid isPermaLink="false">The Guide/Mathematics/General Stuff/Field.md</guid><pubDate>Thu, 27 Feb 2025 00:01:48 GMT</pubDate></item><item><title><![CDATA[Genus]]></title><description><![CDATA[ 
 <br>Can have different meaning in different fields of science and even in different disciplines of mathematics ...<br><br><br>Definition for orientable Surfaces
The genus of a connected, orientable surface is the maximum number of cuttings along non-intersecting closed curves without rendering the resulting <a data-tooltip-position="top" aria-label="Mannigfaltigkeiten" data-href="Mannigfaltigkeiten" href="the-guide/mathematics/differential-geometry/riemannian-geometry/mannigfaltigkeiten-und-differenzierbare-abbildungen/mannigfaltigkeiten.html" class="internal-link" target="_self" rel="noopener nofollow">manifold</a> disconnected. 
<br>It can also be defined via the <a data-tooltip-position="top" aria-label="Euler-Charakteristik" data-href="Euler-Charakteristik" href="the-guide/mathematics/differential-geometry/elementary-differential-geometry/globale-kurven-und-flächentheorie/euler-charakteristik.html" class="internal-link" target="_self" rel="noopener nofollow">Euler characteristic</a>  for <a data-tooltip-position="top" aria-label="Geschlossene Flächen" data-href="Geschlossene Flächen" href="the-guide/mathematics/differential-geometry/elementary-differential-geometry/globale-kurven-und-flächentheorie/geschlossene-flächen.html" class="internal-link" target="_self" rel="noopener nofollow">closed surfaces</a>. <br>
<br>Example

<br>Sphere has genus 
<br>Torus has genus 


<br>]]></description><link>the-guide/mathematics/general-stuff/genus.html</link><guid isPermaLink="false">The Guide/Mathematics/General Stuff/Genus.md</guid><pubDate>Wed, 23 Apr 2025 08:34:36 GMT</pubDate></item><item><title><![CDATA[Gronwall Lemma]]></title><description><![CDATA[ 
 <br>In a Nutshell
Allows to bound a function that is known to satisfy differential of integral inequalities by the solution of the corresponding <a data-tooltip-position="top" aria-label="ODEs - Ordinary Differential Equations" data-href="ODEs - Ordinary Differential Equations" href="the-guide/mathematics/differential-equations/odes-ordinary-differential-equations.html" class="internal-link" target="_self" rel="noopener nofollow">ODE</a> / integral equation.
<br><br>Differential Form - Continuous Time
Let  satisfy for some . Then 
<br>Discrete Time
If a sequence  satisfies  with constants  and , then  where . This lemma is crucial for showing that the error does not grow uncontrollably.
]]></description><link>the-guide/mathematics/general-stuff/gronwall-lemma.html</link><guid isPermaLink="false">The Guide/Mathematics/General Stuff/Gronwall Lemma.md</guid><pubDate>Sun, 09 Feb 2025 13:40:14 GMT</pubDate></item><item><title><![CDATA[Logarithm and Exponential]]></title><description><![CDATA[ 
 <br>In a Nutshell
Collection of identities and tricks I encountered regularly to e.g. reformulate objectives or to in derivations. For integration, see <a data-href="Integration Tricks and Theorems" href="the-guide/mathematics/analysis-and-calculus/integration-tricks-and-theorems.html" class="internal-link" target="_self" rel="noopener nofollow">Integration Tricks and Theorems</a>.
<br><br><br><br>Log-Ratio Trick
Based on the derivative of the logarithm, we can write
<br>
<br>Used extensively in deriving cost functions in <a data-tooltip-position="top" aria-label="- Machine Learning -" data-href="- Machine Learning -" href="the-guide/machine-learning/-machine-learning-.html" class="internal-link" target="_self" rel="noopener nofollow">ML</a> and statistics
<br>Log-Sum Inequality

<br><br><br>]]></description><link>the-guide/mathematics/general-stuff/logarithm-and-exponential.html</link><guid isPermaLink="false">The Guide/Mathematics/General Stuff/Logarithm and Exponential.md</guid><pubDate>Thu, 27 Feb 2025 12:05:03 GMT</pubDate></item><item><title><![CDATA[Mathematical Relations]]></title><description><![CDATA[ 
 <br><br>Also called mapping from <a data-tooltip-position="top" aria-label="Set" data-href="Set" href="the-guide/mathematics/general-stuff/set.html" class="internal-link" target="_self" rel="noopener nofollow">set</a>  (denoted domain) to <a data-tooltip-position="top" aria-label="Set" data-href="Set" href="the-guide/mathematics/general-stuff/set.html" class="internal-link" target="_self" rel="noopener nofollow">set</a>  (denoted codomain). Assigns each input element of set  an output that is element of .<img alt="center" src="lib/media/pasted-image-20231129214300.png" style="width: 400px; max-width: 100%;">In terms of <a data-tooltip-position="top" aria-label="Category" data-href="Category" href="the-guide/mathematics/category-theory/category.html" class="internal-link" target="_self" rel="noopener nofollow">categories</a>, functions are the <a data-tooltip-position="top" aria-label="Morphisms" data-href="Morphisms" href="the-guide/mathematics/category-theory/morphisms.html" class="internal-link" target="_self" rel="noopener nofollow">morphisms</a> of the class <a data-tooltip-position="top" aria-label="Set" data-href="Set" href="the-guide/mathematics/general-stuff/set.html" class="internal-link" target="_self" rel="noopener nofollow">set</a>.<br>Image vs. Codomain
The codomain is the set of all possible outputs promised by the definition, while the image is the (sub-) set that is actually mapped to.
<br>Injectivity  one-to-one mapping
Theres at most one element in  for every element in , maps any two different elements of  to different elements of .
<br><img alt="center" src="lib/media/pasted-image-20231129214357.png" style="width: 450px; max-width: 100%;"><br>Surjectivity (onto)  possible to hit every output
For every element in  there is at least one element of  that is mapped to it (top). We say that the image equals the codomain.
<br><img alt="center" src="lib/media/pasted-image-20231129214456.png" style="width: 400px; max-width: 100%;"><br>Bijectivity  unique back and forth
No unpaired elements, for every element in , there is a unique  vice versa.
<br><br><br><br>While the functions above define how elements of one set are transformed into elements of another, but how the underlying structure of the space transforms can be of interest too. If we want to map an entire <a data-tooltip-position="top" aria-label="Space" data-href="Space" href="the-guide/mathematics/general-stuff/space.html" class="internal-link" target="_self" rel="noopener nofollow">space</a>  with structure  (topological, algebraic, metric ,...) to another space , we need to define the structure  of that output space as well.<br>
In many cases, a map between sets will induce a map between structures.<br>Pushforward and Pullback of Functions
Consider the set  of all possible structures that we can endow onto our input space  and equivalently all possible structures  on . The pushforward function along  is defined via where in our initial case we could write . It is important to differentiate between the pushforward function  and the particular pushforward structure, in our case . Similarly, the pullback function against  denoted pulls output structures back onto the input set.
<br>It is important to note that for a general function, there could be only one of pushforward / pullback or none of them available. Bijective functions on the other hand enable us to move back and forth between input and output space without loosing information about elements or structures.<br>In Category Theory
Pushforward and pullback are essentially tools to move <a data-tooltip-position="top" aria-label="Category" data-href="Category" href="the-guide/mathematics/category-theory/category.html" class="internal-link" target="_self" rel="noopener nofollow">objects</a> and structures along <a data-tooltip-position="top" aria-label="Morphisms" data-href="Morphisms" href="the-guide/mathematics/category-theory/morphisms.html" class="internal-link" target="_self" rel="noopener nofollow">morphisms</a> while preserving categorial properties.
<br><br>Homomorphism
Relating <a data-tooltip-position="top" aria-label="Algebra" data-href="Algebra" href="the-guide/mathematics/general-stuff/algebra.html" class="internal-link" target="_self" rel="noopener nofollow">Algebras</a> - Function  that preserves <a data-tooltip-position="top" aria-label="Algebra" data-href="Algebra" href="the-guide/mathematics/general-stuff/algebra.html" class="internal-link" target="_self" rel="noopener nofollow">algebraic structure</a>, meaning that e.g. <a data-tooltip-position="top" aria-label="Group" data-href="Group" href="the-guide/mathematics/group-theory/group.html" class="internal-link" target="_self" rel="noopener nofollow">groups</a>, <a data-tooltip-position="top" aria-label="Vector Space" data-href="Vector Space" href="the-guide/mathematics/general-stuff/vector-space.html" class="internal-link" target="_self" rel="noopener nofollow">vector spaces</a>, ... . We can map in one direction without loosing structural information.
<br>Isomorphism
Homomorphism that is bijective, we can map back and forth without loosing information about structure and state. Result is a structure-preserving mapping between two structures of the same type.
<br>
<br>Example: Logarithm and Exponential 
<br>See <a data-href="Morphisms" href="the-guide/mathematics/category-theory/morphisms.html" class="internal-link" target="_self" rel="noopener nofollow">Morphisms</a> for a category-theory perspective
<br>Depending on the type of the underlying structure that is preserved, there exist many specialized names for isomorphisms, namely :<br>
<br>Isomorphism of two <a data-tooltip-position="top" aria-label="Mannigfaltigkeiten" data-href="Mannigfaltigkeiten" href="the-guide/mathematics/differential-geometry/riemannian-geometry/mannigfaltigkeiten-und-differenzierbare-abbildungen/mannigfaltigkeiten.html" class="internal-link" target="_self" rel="noopener nofollow">differentiable manifolds</a> is diffeomorphism
<br>Isomorphism of a set is a permutation
<br>Others are ...<br>Automorphism
An isomorphism from a structure to itself.
<br>Isometry
Relating <a data-tooltip-position="top" aria-label="Metric Space and Completeness" data-href="Metric Space and Completeness" href="the-guide/mathematics/general-stuff/metric-space-and-completeness.html" class="internal-link" target="_self" rel="noopener nofollow">Metrics</a> - Injective transformation that preserves metric structure grid of equally distant input lines maps to a grid of equally distant lines on the output.
<br>Homeomorphismus
Topological Isomorphism - Relating <a data-tooltip-position="top" aria-label="Topology and Topological Space" data-href="Topology and Topological Space" href="the-guide/mathematics/topology/topology-and-topological-space.html" class="internal-link" target="_self" rel="noopener nofollow">Topologies</a> - Bijective functions (one-to one map) that preserve the <a data-tooltip-position="top" aria-label="Topology and Topological Space" data-href="Topology and Topological Space" href="the-guide/mathematics/topology/topology-and-topological-space.html" class="internal-link" target="_self" rel="noopener nofollow">topology</a>, meaning every subset in the input pushes forward to a subset in the output, preserving the abstract neighborhoods. 
<br>
<br>Order-Preserving Relations

<br>Monotonically increasing / decreasing functions
<br>Monotonically non-increasing / non-decreasing functions


]]></description><link>the-guide/mathematics/general-stuff/mathematical-relations.html</link><guid isPermaLink="false">The Guide/Mathematics/General Stuff/Mathematical Relations.md</guid><pubDate>Wed, 23 Apr 2025 08:34:36 GMT</pubDate><enclosure url="lib/media/pasted-image-20231129214300.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src=&quot;lib/media/pasted-image-20231129214300.png&quot;&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[Metric Space and Completeness]]></title><description><![CDATA[ 
 <br>A <a data-tooltip-position="top" aria-label="Set" data-href="Set" href="the-guide/mathematics/general-stuff/set.html" class="internal-link" target="_self" rel="noopener nofollow">set</a>  equipped with a distance function or metric . In many cases, the tuple  is used to notate the metric space. <br>Metric
A metric is a binary operation on a set  with This operation has to satisfy the three axioms for all inputs 

<br>When the result is zero the two inputs are equal 
<br>Positivity
<br>Symmetric operation
<br>Triangle Inequality

<br>In general metrics are symmetric and generalize linear distances. <a data-tooltip-position="top" aria-label="Divergence" data-href="Divergence" href="the-guide/information-theory/information-geometry/divergence.html" class="internal-link" target="_self" rel="noopener nofollow">Divergences</a> on the other hand are asymmetric and generalize squared distances.<br><br><br>Complete Space
A metric space  is complete, if every <a data-href="Cauchy Sequence" href="the-guide/mathematics/general-stuff/cauchy-sequence.html" class="internal-link" target="_self" rel="noopener nofollow">Cauchy Sequence</a> in  has a limit in .
<br>Intuition
Ensures that there are no "missing" limits, we cannot escape the space via limits. This makes a lot of arguments more robust, enabling us to take limits without invalidating initial assumptions.
<br><br><br>Theorem
For metric spaces, compactness and sequential compactness are equivalent. The easier notion is sequential compactness: Every sequence of points has to have a converging subsequence for the space to be compact. 
]]></description><link>the-guide/mathematics/general-stuff/metric-space-and-completeness.html</link><guid isPermaLink="false">The Guide/Mathematics/General Stuff/Metric Space and Completeness.md</guid><pubDate>Wed, 26 Feb 2025 23:53:03 GMT</pubDate></item><item><title><![CDATA[Partition of Unity]]></title><description><![CDATA[ 
 <br>In a Nutshell
Concept in mathematics that is often used in proofs to extend local constructions to the whole <a data-tooltip-position="top" aria-label="Space" data-href="Space" href="the-guide/mathematics/general-stuff/space.html" class="internal-link" target="_self" rel="noopener nofollow">space</a> of interest.
<br><br><br><br>Definition
Eine differenzierbare Zerlegung der Eins relativ zu einer Überdeckung  ist eine Familie  von <a data-tooltip-position="top" aria-label="Mathematical Relations" data-href="Mathematical Relations" href="the-guide/mathematics/general-stuff/mathematical-relations.html" class="internal-link" target="_self" rel="noopener nofollow">Funktionen</a>  mit 

<br> und 
<br> gibt es eine Umgebung, die nur endlich viele  trifft
<br>

<br>Satz
Auf jeder <a data-tooltip-position="top" aria-label="Mannigfaltigkeiten" data-href="Mannigfaltigkeiten" href="the-guide/mathematics/differential-geometry/riemannian-geometry/mannigfaltigkeiten-und-differenzierbare-abbildungen/mannigfaltigkeiten.html" class="internal-link" target="_self" rel="noopener nofollow">differenzierbaren Mannigfaltigkeit</a> gibt es eine Zerlegung der eins relativ zu einem <a data-tooltip-position="top" aria-label="Charts and Atlas" data-href="Charts and Atlas" href="the-guide/mathematics/differential-geometry/riemannian-geometry/mannigfaltigkeiten-und-differenzierbare-abbildungen/charts-and-atlas.html" class="internal-link" target="_self" rel="noopener nofollow">Atlas</a>.
]]></description><link>the-guide/mathematics/general-stuff/partition-of-unity.html</link><guid isPermaLink="false">The Guide/Mathematics/General Stuff/Partition of Unity.md</guid><pubDate>Fri, 26 Jul 2024 07:32:28 GMT</pubDate></item><item><title><![CDATA[Quaternions]]></title><description><![CDATA[ 
 <br>In a Nutshell
Four-dimensional <a data-tooltip-position="top" aria-label="Algebra" data-href="Algebra" href="the-guide/mathematics/general-stuff/algebra.html" class="internal-link" target="_self" rel="noopener nofollow">algebra</a> over the real numbers, extending complex numbers to three dimensions. Intimately connected to <a data-href="Geometric Algebra" href="the-guide/mathematics/geometric-algebra/geometric-algebra.html" class="internal-link" target="_self" rel="noopener nofollow">Geometric Algebra</a>.
<br><br><a data-href="A Hitchhiker's Guide to Rotation Representations" href="the-guide/robotics,-dynamics-and-control/a-hitchhiker's-guide-to-rotation-representations.html" class="internal-link" target="_self" rel="noopener nofollow">A Hitchhiker's Guide to Rotation Representations</a><br>For a special treatment of unit Quaternions and their relationship with rotations, see <a data-href="Unit Quaternions and SU(2)" href="the-guide/mathematics/lie-theory/unit-quaternions-and-su(2).html" class="internal-link" target="_self" rel="noopener nofollow">Unit Quaternions and SU(2)</a>.]]></description><link>the-guide/mathematics/general-stuff/quaternions.html</link><guid isPermaLink="false">The Guide/Mathematics/General Stuff/Quaternions.md</guid><pubDate>Sun, 30 Mar 2025 13:50:10 GMT</pubDate></item><item><title><![CDATA[Quotient Space]]></title><description><![CDATA[ 
 <br>Definition
The quotient of a <a data-tooltip-position="top" aria-label="Vector Space" data-href="Vector Space" href="the-guide/mathematics/general-stuff/vector-space.html" class="internal-link" target="_self" rel="noopener nofollow">vector space</a>  by a <a data-tooltip-position="top" aria-label="Space" data-href="Space" href="the-guide/mathematics/general-stuff/space.html" class="internal-link" target="_self" rel="noopener nofollow">subspace</a>  is another vector space, obtained by collapsing all dimensions of  to zero. We write 
<br>
<br>Examples

<br>...


]]></description><link>the-guide/mathematics/general-stuff/quotient-space.html</link><guid isPermaLink="false">The Guide/Mathematics/General Stuff/Quotient Space.md</guid><pubDate>Mon, 24 Feb 2025 23:32:25 GMT</pubDate></item><item><title><![CDATA[Set]]></title><description><![CDATA[ 
 <br>Info
A set  is the mathematical model for a collection of different things; a set contains elements / members, which can be mathematical objects of any kind (e.g. numbers, symbols, points in spaces, lines or even other sets).<br>
A set with no elements is the empty set , a set with one element is a singleton or an atomic set and a set containing all elements of interest is the ambient set. A set may contain infinite elements.
<br><img alt="center" src="lib/media/pasted-image-20231129163559.png" style="width: 400px; max-width: 100%;"><br>The combination of all elements into all possible subsets of a set , including the empty set and the full set  is denoted the power set<br><br>Open and Closed Set
An open set is the generalization of open interval , a set without a boundary or given a metric, the collection of all points with a distance strictly smaller than some threshold.<br>
A closed set is a set whose complement is an open set.
<br>Subset and Superset
A set  is a subset of a set  (), if all elements of  are also elements of , which in turn is a superset. 
<br><img alt="center" src="lib/media/pasted-image-20231129163907.png" style="width: 250px; max-width: 100%;"><br>Complement
If  is a set, then the absolute complement of  (or simply the complement of ) is the set of elements not in  (within a larger set that is implicitly defined). We use the notation 
<br><img alt="center" src="lib/media/pasted-image-20231129164025.png" style="width: 350px; max-width: 100%;"><br><br><br>
<br>Union <img alt="center" src="lib/media/pasted-image-20231129164202.png" style="width: 300px; max-width: 100%;">
<br>Intersection <img alt="center" src="lib/media/pasted-image-20231129164250.png" style="width: 300px; max-width: 100%;">
<br>Set Difference <img alt="center" src="lib/media/pasted-image-20230617092730.png" style="width: 150px; max-width: 100%;">
<br>Symmetric Difference <img alt="center" src="lib/media/pasted-image-20230617092807.png" style="width: 150px; max-width: 100%;">
<br>Cartesian Product 

<br>All ordered pairs


<br><br>Using a <a data-tooltip-position="top" aria-label="Metric Space and Completeness" data-href="Metric Space and Completeness" href="the-guide/mathematics/general-stuff/metric-space-and-completeness.html" class="internal-link" target="_self" rel="noopener nofollow">metric</a> , one can define ...<br>
<br>Open Ball, all elements within a distance  from an element  via 
<br>Closed Ball, if elements on with exact distance  are also included 
<br>Boundary, elements with exact distance  
<br><br><br>Infimum - Greatest Lower Bound
The Infimum of subset  of a partially ordered set  is the greatest element in P that is less than or equal to each element in 
<br><img alt="center" src="lib/media/pasted-image-20240225140626.png" style="width: 200px; max-width: 100%;"><br>Supremum - Smallest Upper Bound
The Supremum of s subset  of a partially ordered set  is the least element in  that is greater than or equal to the greatest element in .
<br><img alt="center" src="lib/media/pasted-image-20240225140649.png" style="width: 200px; max-width: 100%;">]]></description><link>the-guide/mathematics/general-stuff/set.html</link><guid isPermaLink="false">The Guide/Mathematics/General Stuff/Set.md</guid><pubDate>Mon, 24 Feb 2025 23:49:44 GMT</pubDate><enclosure url="lib/media/pasted-image-20231129163559.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src=&quot;lib/media/pasted-image-20231129163559.png&quot;&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[Space]]></title><description><![CDATA[ 
 <br>In a Nutshell
A <a data-tooltip-position="top" aria-label="Set" data-href="Set" href="the-guide/mathematics/general-stuff/set.html" class="internal-link" target="_self" rel="noopener nofollow">set</a> endowed with additional structure. Allows for additional interpretation and theorems.
<br><br>To Add: Every algebra implies topoligal structure ?<br>
<br>Ordering Structures

<br>Total ordering , yielding an ordered space
<br>Partial ordering , yielding a partially-ordered space


<br>Algebraic Structure

<br>Endowing a set with an <a data-tooltip-position="top" aria-label="Algebra" data-href="Algebra" href="the-guide/mathematics/general-stuff/algebra.html" class="internal-link" target="_self" rel="noopener nofollow">algebraic structure</a> results in an algebraic space, for example fields () or <a data-tooltip-position="top" aria-label="Group" data-href="Group" href="the-guide/mathematics/group-theory/group.html" class="internal-link" target="_self" rel="noopener nofollow">groups</a> () or vector spaces (linear algebra)


<br><br>The spaces defined above are very general and mostly studied in pure mathematics contexts. In most applications, the underlying spaces are the following.<br><img alt="center" src="lib/media/pasted-image-20240723184431.png" style="width: 300px; max-width: 100%;"><br>
<br><a data-tooltip-position="top" aria-label="Topology and Topological Space" data-href="Topology and Topological Space" href="the-guide/mathematics/topology/topology-and-topological-space.html" class="internal-link" target="_self" rel="noopener nofollow">Topological Space</a>

<br>Most fundamental, avoids metric structure by instead distinguishing certain <a data-tooltip-position="top" aria-label="Set" data-href="Set" href="the-guide/mathematics/general-stuff/set.html" class="internal-link" target="_self" rel="noopener nofollow">subsets</a>, resulting in a <a data-href="Topology and Topological Space" href="the-guide/mathematics/topology/topology-and-topological-space.html" class="internal-link" target="_self" rel="noopener nofollow">Topology and Topological Space</a>.


<br><a data-href="Metric Space and Completeness" href="the-guide/mathematics/general-stuff/metric-space-and-completeness.html" class="internal-link" target="_self" rel="noopener nofollow">Metric Space and Completeness</a>

<br>Endow an arbitrary  set with a notion of distance between its elements, resulting in a <a data-tooltip-position="top" aria-label="Metric Space and Completeness" data-href="Metric Space and Completeness" href="the-guide/mathematics/general-stuff/metric-space-and-completeness.html" class="internal-link" target="_self" rel="noopener nofollow">metric space</a>.


<br><a data-href="Vector Space" href="the-guide/mathematics/general-stuff/vector-space.html" class="internal-link" target="_self" rel="noopener nofollow">Vector Space</a>

<br>Endows set with linear algebraic structure by equipping it with the operations addition and multiplication.


<br><a data-href="Normed Vector Space and Banach Space" href="the-guide/mathematics/functional-analysis-and-calculus-of-variations/normed-vector-space-and-banach-space.html" class="internal-link" target="_self" rel="noopener nofollow">Normed Vector Space and Banach Space</a>

<br>Adds norm, abstract notion of length for vector space. Banach spaces additionally require completeness, eliminating holes.


<br><a data-href="Inner Product Space and Hilbert Space" href="the-guide/mathematics/functional-analysis-and-calculus-of-variations/inner-product-space-and-hilbert-space.html" class="internal-link" target="_self" rel="noopener nofollow">Inner Product Space and Hilbert Space</a>

<br>Adds bilinear form, abstract notion of distance for vector space. Hilbert spaces also require Banach space, eliminating holes.


<br>To add more detail, the above concepts are related via the following diagram.<br><br>Take-Home Message

<br>Every inner product space is a normed space via 
<br>Every normed space is a <a data-tooltip-position="top" aria-label="Metric Space and Completeness" data-href="Metric Space and Completeness" href="the-guide/mathematics/general-stuff/metric-space-and-completeness.html" class="internal-link" target="_self" rel="noopener nofollow">metric space</a> via 
<br>Every metric space is topological (via open balls), but not all topologies are metrizable
<br>Banach/Hilbert spaces add completeness to normed/inner product spaces

]]></description><link>the-guide/mathematics/general-stuff/space.html</link><guid isPermaLink="false">The Guide/Mathematics/General Stuff/Space.md</guid><pubDate>Wed, 23 Apr 2025 08:34:36 GMT</pubDate><enclosure url="lib/media/pasted-image-20240723184431.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src=&quot;lib/media/pasted-image-20240723184431.png&quot;&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[Stability of Numerical Methods]]></title><description><![CDATA[ 
 <br>In a Nutshell
Stability in numerics refers to the property that small perturbations in the data or intermediate computations (whether from initial data, round-off, or truncation) do not grow uncontrollably as the method advances, lyielding controllable changes in the final solution.
<br>A fundamental tool for proving stability is the <a data-href="Gronwall Lemma" href="the-guide/mathematics/general-stuff/gronwall-lemma.html" class="internal-link" target="_self" rel="noopener nofollow">Gronwall Lemma</a>.<br><br><br>
<br>forward, backward and mixed stability
<br><br><br>Stability Functions of One-Step Methods
F&gt;or the linear test equation  a one--step method can be written as  The method is (absolutely) stable for a given  if  For problems with , one typically desires  so that errors decay. 
<br>Zero-Stability for Multistep Methods 
Consider a linear multistep method with recurrence  Its characteristic polynomial is  The method is zero-stable if all roots  of  satisfy  and any root with  is simple.
<br>Connetion between Convergence, Consistency and Stability
The three fundamental concepts are related via the <a data-href="Lax-Richtmyer and Dahlquist Equivalence Theorems" href="the-guide/mathematics/differential-equations/lax-richtmyer-and-dahlquist-equivalence-theorems.html" class="internal-link" target="_self" rel="noopener nofollow">Lax-Richtmyer and Dahlquist Equivalence Theorems</a>.
]]></description><link>the-guide/mathematics/general-stuff/stability-of-numerical-methods.html</link><guid isPermaLink="false">The Guide/Mathematics/General Stuff/Stability of Numerical Methods.md</guid><pubDate>Sat, 12 Apr 2025 11:02:31 GMT</pubDate></item><item><title><![CDATA[Tensors]]></title><description><![CDATA[ 
 <br>In a Nutshell
<a data-tooltip-position="top" aria-label="Algebra" data-href="Algebra" href="the-guide/mathematics/general-stuff/algebra.html" class="internal-link" target="_self" rel="noopener nofollow">Algebraic</a> object that describes multi-linear relationships between <a data-tooltip-position="top" aria-label="Set" data-href="Set" href="the-guide/mathematics/general-stuff/set.html" class="internal-link" target="_self" rel="noopener nofollow">sets</a> of algebraic objects related to a <a data-tooltip-position="top" aria-label="Vector Space" data-href="Vector Space" href="the-guide/mathematics/general-stuff/vector-space.html" class="internal-link" target="_self" rel="noopener nofollow">vector space</a>. Simplest case are scalars, (co-) vectors and <a data-tooltip-position="top" aria-label="Matrices" data-href="Matrices" href="the-guide/mathematics/linear-algebra/matrices.html" class="internal-link" target="_self" rel="noopener nofollow">matrices</a>, extends to bilinear forms, maps between vector spaces and <a data-tooltip-position="top" aria-label="Vector Space" data-href="Vector Space" href="the-guide/mathematics/general-stuff/vector-space.html" class="internal-link" target="_self" rel="noopener nofollow">inner products</a>. Tensors are defined independently of any basis.
<br>In order to actually perform computations with tensors, we'd have to specify vector and dual vector space with bases, inner product operations, tensor components and how the tensor acts. <br><br>Definition
Given an -dimensional <a data-tooltip-position="top" aria-label="Vector Space" data-href="Vector Space" href="the-guide/mathematics/general-stuff/vector-space.html" class="internal-link" target="_self" rel="noopener nofollow">vector space</a>  with <a data-tooltip-position="top" aria-label="Dual Vector Space" data-href="Dual Vector Space" href="the-guide/mathematics/general-stuff/dual-vector-space.html" class="internal-link" target="_self" rel="noopener nofollow">dual vector space</a> , a map that is linear in every argument is denoted a -Tensor. The <a data-tooltip-position="top" aria-label="Set" data-href="Set" href="the-guide/mathematics/general-stuff/set.html" class="internal-link" target="_self" rel="noopener nofollow">set</a> of all these <a data-tooltip-position="top" aria-label="Mathematical Relations" data-href="Mathematical Relations" href="the-guide/mathematics/general-stuff/mathematical-relations.html" class="internal-link" target="_self" rel="noopener nofollow">mappings</a> is denoted . Given a basis  of  and  of , a tensor combines all input vectors and co-vectors and multiplies the resulting scalars. We can write 
<br>
<br>We can show that  is a vector space itself
<br><br>Additional Information
We have to define how a vector and covectors interact with each other to specify the tensor, e.g. using the <a data-tooltip-position="top" aria-label="Vector Space" data-href="Vector Space" href="the-guide/mathematics/general-stuff/vector-space.html" class="internal-link" target="_self" rel="noopener nofollow">standard inner product</a> or the <a data-tooltip-position="top" aria-label="(Semi-) Riemannsche Metriken" data-href="(Semi-) Riemannsche Metriken" href="the-guide/mathematics/differential-geometry/riemannian-geometry/metriken-und-zusammenhänge/(semi-)-riemannsche-metriken.html" class="internal-link" target="_self" rel="noopener nofollow">(semi-) Riemannian</a> metric.
<br><br><br>Contravariant
Both concepts relate to transformation properties. Considering the <a data-tooltip-position="top" aria-label="Vector Space" data-href="Vector Space" href="the-guide/mathematics/general-stuff/vector-space.html" class="internal-link" target="_self" rel="noopener nofollow">vector space</a>  and two bases , , we can write a vector as . Transforming linearly from  to  via the transformation  for , we get and we denote elements that transform via  as contravariant. 
<br>Intuition
The elements contra-vary, because they have to compensate the change of basis on order to keep their information intact.
<br>Covariant
Doing the same vor <a data-tooltip-position="top" aria-label="Dual Vector Space" data-href="Dual Vector Space" href="the-guide/mathematics/general-stuff/dual-vector-space.html" class="internal-link" target="_self" rel="noopener nofollow">covectors</a>  with basis representation  yields leading to covariant elements.
<br>Intuition
Elements vary with the transformation with the space (dual space = space of coordinates).
<br><img alt="center" src="lib/media/pasted-image-20240806175616.png" style="width: 300px; max-width: 100%;"><br>
<br>Vectors are contravariant, covectors covariant. 
<br>A <a data-tooltip-position="top" aria-label="Matrices" data-href="Matrices" href="the-guide/mathematics/linear-algebra/matrices.html" class="internal-link" target="_self" rel="noopener nofollow">matrix</a> is a representation of a type- tensor, but a single matrix can represent many tensors. Which tensor it actually represents is only revealed by the transformation properties.
<br><br><br>Tensor-Product
The tensor-product  is an operationthat simply combines two tensors by assigning the input vectors and co-vectors to the respective tensors and multiplying the resulting scalars of both tensors
<br>
<br> means one of the two inputs has to be zero
<br>Product is basis-independent
<br>Does not commute in general
<br>Contraction
The contraction turns a  tensor into a  tensor by inserting a basis  for the vector and  for the covector at the respective positions  and  and summing over all components, i.e.
<br>
<br>Compare to building the <a data-tooltip-position="top" aria-label="Matrices" data-href="Matrices" href="the-guide/mathematics/linear-algebra/matrices.html" class="internal-link" target="_self" rel="noopener nofollow">trace of a matrix</a>, basis invariant
<br><br>We can easily extend the above to vector-valued outputs. For this to be consistent, we identify vectors with contravariant -tensors and covectors as covariant -tensors. They can be regarded as linear functionals from . Then, a tensor that puts out a vector can be constructed by contractions.  <br>(TODO) - ich lasse iwi einen dualen offen ...]]></description><link>the-guide/mathematics/general-stuff/tensors.html</link><guid isPermaLink="false">The Guide/Mathematics/General Stuff/Tensors.md</guid><pubDate>Sun, 16 Mar 2025 16:12:34 GMT</pubDate><enclosure url="lib/media/pasted-image-20240806175616.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src=&quot;lib/media/pasted-image-20240806175616.png&quot;&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[Vector Space]]></title><description><![CDATA[ 
 <br>In a Nutshell
Also denotes linear <a data-href="Space" href="the-guide/mathematics/general-stuff/space.html" class="internal-link" target="_self" rel="noopener nofollow">Space</a>, a <a data-tooltip-position="top" aria-label="Set" data-href="Set" href="the-guide/mathematics/general-stuff/set.html" class="internal-link" target="_self" rel="noopener nofollow">set</a> of abstract elements equipped with a linear <a data-href="Algebra" href="the-guide/mathematics/general-stuff/algebra.html" class="internal-link" target="_self" rel="noopener nofollow">Algebra</a> in the form of scalar multiplication  and an <a data-tooltip-position="top" aria-label="Algebra" data-href="Algebra" href="the-guide/mathematics/general-stuff/algebra.html" class="internal-link" target="_self" rel="noopener nofollow">associative, commutative and unital binary operator</a> . 
<br><br>Axiomatic Definition
A vector space  over a <a data-href="Field" href="the-guide/mathematics/general-stuff/field.html" class="internal-link" target="_self" rel="noopener nofollow">Field</a>  is a non-empty <a data-tooltip-position="top" aria-label="Set" data-href="Set" href="the-guide/mathematics/general-stuff/set.html" class="internal-link" target="_self" rel="noopener nofollow">set</a>  together with binary operations  and  to endow it with <a data-tooltip-position="top" aria-label="Space" data-href="Space" href="the-guide/mathematics/general-stuff/space.html" class="internal-link" target="_self" rel="noopener nofollow">algebraic structure</a>. They satisfy the following eight axioms for all  and :

<br><a data-tooltip-position="top" aria-label="Algebra" data-href="Algebra" href="the-guide/mathematics/general-stuff/algebra.html" class="internal-link" target="_self" rel="noopener nofollow">Associativity</a> of Addition
<br><a data-tooltip-position="top" aria-label="Algebra" data-href="Algebra" href="the-guide/mathematics/general-stuff/algebra.html" class="internal-link" target="_self" rel="noopener nofollow">Commutativity</a> of Addition
<br>Additive Identity Element
<br>Additive Inverse Element
<br>Compatibility of scalar multiplication and field multiplication
<br>Identity element of scalar multiplication
<br>Distributivity of scalar multiplication with respect to vector addition
<br>Distributivity of scalar multiplication with respect to field addition

<br>Dual Vector Space and Quotient Spaces

<br>The space of coordinates of a vector space is also a vector space, the <a data-tooltip-position="top" aria-label="Dual Vector Space" data-href="Dual Vector Space" href="the-guide/mathematics/general-stuff/dual-vector-space.html" class="internal-link" target="_self" rel="noopener nofollow">dual vector space</a> .
<br>Collapsing all dimensions of a subspace results in a <a data-tooltip-position="top" aria-label="Quotient Space" data-href="Quotient Space" href="the-guide/mathematics/general-stuff/quotient-space.html" class="internal-link" target="_self" rel="noopener nofollow">quotient space</a>.

]]></description><link>the-guide/mathematics/general-stuff/vector-space.html</link><guid isPermaLink="false">The Guide/Mathematics/General Stuff/Vector Space.md</guid><pubDate>Wed, 23 Apr 2025 21:55:35 GMT</pubDate></item><item><title><![CDATA[Young's Inequality for Products]]></title><description><![CDATA[ 
 <br>Widely-used inequality to estimate a product of two terms by a sum of scaled exponentials.<br>Elementary Case
For exponent , we can use the substitution  and  to obtain which can be used to control the tightness of second term's bound. However, this is always connected to losing some control of the first term (Peter-Paul inequality).
]]></description><link>the-guide/mathematics/general-stuff/young&apos;s-inequality-for-products.html</link><guid isPermaLink="false">The Guide/Mathematics/General Stuff/Young&apos;s Inequality for Products.md</guid><pubDate>Fri, 13 Sep 2024 06:58:58 GMT</pubDate></item><item><title><![CDATA[Duality in Geometric Algebra]]></title><description><![CDATA[ 
 <br>Info
In <a data-tooltip-position="top" aria-label="Geometric Algebra" data-href="Geometric Algebra" href="the-guide/mathematics/geometric-algebra/geometric-algebra.html" class="internal-link" target="_self" rel="noopener nofollow">Geometric Algebra</a>, the dual of a <a data-tooltip-position="top" aria-label="Multivector" data-href="Multivector" href="the-guide/mathematics/geometric-algebra/multivector.html" class="internal-link" target="_self" rel="noopener nofollow">multivector</a> is the orthogonal complement, denoted and obtained by applying the <a data-tooltip-position="top" aria-label="Geometric Product" data-href="Geometric Product" href="the-guide/mathematics/geometric-algebra/operations/geometric-product.html" class="internal-link" target="_self" rel="noopener nofollow">geometric product</a> between each <a data-tooltip-position="top" aria-label="Multivector" data-href="Multivector" href="the-guide/mathematics/geometric-algebra/multivector.html" class="internal-link" target="_self" rel="noopener nofollow">multivector</a>  component and the <a data-tooltip-position="top" aria-label="Basis Blades" data-href="Basis Blades" href="the-guide/mathematics/geometric-algebra/basis-blades.html" class="internal-link" target="_self" rel="noopener nofollow">inverse</a> pseudoscalarFor the pseudoscalar itself, it holds that .
<br>
<br>Intuitively swaps empty and full dimensions. 
]]></description><link>the-guide/mathematics/geometric-algebra/operations/duality-in-geometric-algebra.html</link><guid isPermaLink="false">The Guide/Mathematics/Geometric Algebra/Operations/Duality in Geometric Algebra.md</guid><pubDate>Mon, 09 Sep 2024 15:35:53 GMT</pubDate></item><item><title><![CDATA[Generalized Exterior Product]]></title><description><![CDATA[ 
 <br>Initially, the exterior product of <a data-tooltip-position="top" aria-label="Geometric Algebra" data-href="Geometric Algebra" href="the-guide/mathematics/geometric-algebra/geometric-algebra.html" class="internal-link" target="_self" rel="noopener nofollow">geometric algebra</a> is only defined for vectors, but can be extended to the entire algebra.<br>Definition
Also called the "Join", the outer product can be written using the <a data-tooltip-position="top" aria-label="Geometric Product" data-href="Geometric Product" href="the-guide/mathematics/geometric-algebra/operations/geometric-product.html" class="internal-link" target="_self" rel="noopener nofollow">geometric product</a> and the <a data-tooltip-position="top" aria-label="Grade Projection Operator" data-href="Grade Projection Operator" href="the-guide/mathematics/geometric-algebra/operations/grade-projection-operator.html" class="internal-link" target="_self" rel="noopener nofollow">grade projection operator</a> as which therefore joins perpendicular components and thereby constructs a weighted and oriented subspace spanned by these components. The outer / exterior product of a j-vector and a k-vector is  or a -vector 
<br>
<br>Properties

<br><a data-tooltip-position="top" aria-label="Algebra" data-href="Algebra" href="the-guide/mathematics/general-stuff/algebra.html" class="internal-link" target="_self" rel="noopener nofollow">Anticommutativity</a> 
<br><a data-tooltip-position="top" aria-label="Algebra" data-href="Algebra" href="the-guide/mathematics/general-stuff/algebra.html" class="internal-link" target="_self" rel="noopener nofollow">Distributivity</a> 
<br><a data-tooltip-position="top" aria-label="Algebra" data-href="Algebra" href="the-guide/mathematics/general-stuff/algebra.html" class="internal-link" target="_self" rel="noopener nofollow">Associativity</a> 
<br>Alternating 
<br>For vectors , the exterior product is 


<br><br><br>The outer product constructs oriented and weighted subspaces from its components. The magnitude of a <a data-tooltip-position="top" aria-label="Basis Blades" data-href="Basis Blades" href="the-guide/mathematics/geometric-algebra/basis-blades.html" class="internal-link" target="_self" rel="noopener nofollow">blade</a>  formed out of two vectors  is the area of the parallelogram spanned by these vectors, given by  with  being the angel between them. <img alt="center" src="lib/media/pasted-image-20240209131133.png" style="width: 200px; max-width: 100%;"><br><br><br>
<br>To compute the outer product start by distributingAll terms that have repeated factors collapse to 0, leading toBecause these terms do not have repeating factors, their outer product is equivalent to the <a data-tooltip-position="top" aria-label="Geometric Product" data-href="Geometric Product" href="the-guide/mathematics/geometric-algebra/operations/geometric-product.html" class="internal-link" target="_self" rel="noopener nofollow">geometric product</a>
]]></description><link>the-guide/mathematics/geometric-algebra/operations/generalized-exterior-product.html</link><guid isPermaLink="false">The Guide/Mathematics/Geometric Algebra/Operations/Generalized Exterior Product.md</guid><pubDate>Mon, 09 Sep 2024 15:35:53 GMT</pubDate><enclosure url="lib/media/pasted-image-20240209131133.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src=&quot;lib/media/pasted-image-20240209131133.png&quot;&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[Generalized Inner Product]]></title><description><![CDATA[ 
 <br>Definition
The different generalizations of the <a data-tooltip-position="top" aria-label="Vector Space" data-href="Vector Space" href="the-guide/mathematics/general-stuff/vector-space.html" class="internal-link" target="_self" rel="noopener nofollow">inner product</a>  used as the bilinear form of <a data-tooltip-position="top" aria-label="Geometric Algebra" data-href="Geometric Algebra" href="the-guide/mathematics/geometric-algebra/geometric-algebra.html" class="internal-link" target="_self" rel="noopener nofollow">geometric algebra</a> can be written using the <a data-tooltip-position="top" aria-label="Geometric Product" data-href="Geometric Product" href="the-guide/mathematics/geometric-algebra/operations/geometric-product.html" class="internal-link" target="_self" rel="noopener nofollow">geometric product</a> and the <a data-tooltip-position="top" aria-label="Grade Projection Operator" data-href="Grade Projection Operator" href="the-guide/mathematics/geometric-algebra/operations/grade-projection-operator.html" class="internal-link" target="_self" rel="noopener nofollow">grade projection operator</a>

<br>The Left Contraction, project first argument on second. If the <a data-tooltip-position="top" aria-label="Multivector" data-href="Multivector" href="the-guide/mathematics/geometric-algebra/multivector.html" class="internal-link" target="_self" rel="noopener nofollow">grade</a> of  is larger than that of  the result is . 
<br>The Right Contraction, project second argument on first. If the <a data-tooltip-position="top" aria-label="Multivector" data-href="Multivector" href="the-guide/mathematics/geometric-algebra/multivector.html" class="internal-link" target="_self" rel="noopener nofollow">grade</a> of  is larger than that of  the result is .
<br>Scalar Product, if grade of both arguments is equal (also denoted *).Generally, the inner product contracts the parallel components and thereby computes similarities. 

<br>
<br>Properties

<br><a data-tooltip-position="top" aria-label="Algebra" data-href="Algebra" href="the-guide/mathematics/general-stuff/algebra.html" class="internal-link" target="_self" rel="noopener nofollow">Distributivity</a> on addition 
<br>Identities between the different versions 
<br>For vectors , the outer product is 


<br><br><br>
<br>
]]></description><link>the-guide/mathematics/geometric-algebra/operations/generalized-inner-product.html</link><guid isPermaLink="false">The Guide/Mathematics/Geometric Algebra/Operations/Generalized Inner Product.md</guid><pubDate>Mon, 24 Feb 2025 23:32:25 GMT</pubDate></item><item><title><![CDATA[Geometric Product]]></title><description><![CDATA[ 
 <br>Based on the fundamental equation  resulting from the symmetric bilinear form of the <a data-tooltip-position="top" aria-label="Geometric Algebra" data-href="Geometric Algebra" href="the-guide/mathematics/geometric-algebra/geometric-algebra.html" class="internal-link" target="_self" rel="noopener nofollow">geometric algebra</a> we can define its fundamental product.<br>Special Case for Vectors
If both <a data-tooltip-position="top" aria-label="Multivector" data-href="Multivector" href="the-guide/mathematics/geometric-algebra/multivector.html" class="internal-link" target="_self" rel="noopener nofollow">multivectors</a> only have one-vector components , we denote them as vectors  and their geometric product reduces to a combination of the <a data-tooltip-position="top" aria-label="Generalized Inner Product" data-href="Generalized Inner Product" href="the-guide/mathematics/geometric-algebra/operations/generalized-inner-product.html" class="internal-link" target="_self" rel="noopener nofollow">inner product</a> (symmetric) and the <a data-tooltip-position="top" aria-label="Generalized Exterior Product" data-href="Generalized Exterior Product" href="the-guide/mathematics/geometric-algebra/operations/generalized-exterior-product.html" class="internal-link" target="_self" rel="noopener nofollow">outer/exterior product</a> (antisymmetric) 
<br>Using the fundamental equations from the vector basis and the properties below, the geometric product is naturally extended to an operation between arbitrary <a data-tooltip-position="top" aria-label="Multivector" data-href="Multivector" href="the-guide/mathematics/geometric-algebra/multivector.html" class="internal-link" target="_self" rel="noopener nofollow">multivectors</a> (higher order <a data-tooltip-position="top" aria-label="Basis Blades" data-href="Basis Blades" href="the-guide/mathematics/geometric-algebra/basis-blades.html" class="internal-link" target="_self" rel="noopener nofollow">blades</a> are constructed via geometric product).<br>
<br>Properties

<br>Closure
<br>Non-commutative 

<br>For basis vectors, the geometric product is anticommutative


<br><a data-tooltip-position="top" aria-label="Algebra" data-href="Algebra" href="the-guide/mathematics/general-stuff/algebra.html" class="internal-link" target="_self" rel="noopener nofollow">Associative</a> 
<br><a data-tooltip-position="top" aria-label="Algebra" data-href="Algebra" href="the-guide/mathematics/general-stuff/algebra.html" class="internal-link" target="_self" rel="noopener nofollow">Commutative</a> Scalar multiplication 
<br>Distributive over addition 


<br><br>
<br>Geometric Understanding

<br>The geometric product contracts parallel directions and joins perpendicular directions.


<br>Algebraic Understanding

<br>The geometric product is an <a data-tooltip-position="top" aria-label="Algebra" data-href="Algebra" href="the-guide/mathematics/general-stuff/algebra.html" class="internal-link" target="_self" rel="noopener nofollow">algebra</a> using basis vectors that square to  or  (see definition of <a data-tooltip-position="top" aria-label="Geometric Algebra" data-href="Geometric Algebra" href="the-guide/mathematics/geometric-algebra/geometric-algebra.html" class="internal-link" target="_self" rel="noopener nofollow">geometric algebra</a>). 


<br>Transformation Understanding
]]></description><link>the-guide/mathematics/geometric-algebra/operations/geometric-product.html</link><guid isPermaLink="false">The Guide/Mathematics/Geometric Algebra/Operations/Geometric Product.md</guid><pubDate>Mon, 09 Sep 2024 15:35:53 GMT</pubDate></item><item><title><![CDATA[Grade Projection Operator]]></title><description><![CDATA[ 
 <br>Definition
Operator in <a data-href="Geometric Algebra" href="the-guide/mathematics/geometric-algebra/geometric-algebra.html" class="internal-link" target="_self" rel="noopener nofollow">Geometric Algebra</a> that , if applied to an arbitrary <a data-tooltip-position="top" aria-label="Multivector" data-href="Multivector" href="the-guide/mathematics/geometric-algebra/multivector.html" class="internal-link" target="_self" rel="noopener nofollow">multivector</a> , simply extracts the -vector part of that multivector The zero for the zero-grade operator is often left out.
]]></description><link>the-guide/mathematics/geometric-algebra/operations/grade-projection-operator.html</link><guid isPermaLink="false">The Guide/Mathematics/Geometric Algebra/Operations/Grade Projection Operator.md</guid><pubDate>Mon, 09 Sep 2024 15:35:53 GMT</pubDate></item><item><title><![CDATA[Regressive Product]]></title><description><![CDATA[ 
 <br>Definition
In <a data-href="Geometric Algebra" href="the-guide/mathematics/geometric-algebra/geometric-algebra.html" class="internal-link" target="_self" rel="noopener nofollow">Geometric Algebra</a> usually referred to as the "meet", is the <a data-tooltip-position="top" aria-label="Duality in Geometric Algebra" data-href="Duality in Geometric Algebra" href="the-guide/mathematics/geometric-algebra/operations/duality-in-geometric-algebra.html" class="internal-link" target="_self" rel="noopener nofollow">dual</a> () of the <a data-tooltip-position="top" aria-label="Generalized Exterior Product" data-href="Generalized Exterior Product" href="the-guide/mathematics/geometric-algebra/operations/generalized-exterior-product.html" class="internal-link" target="_self" rel="noopener nofollow">exterior product</a> or "join". For arbitrary <a data-tooltip-position="top" aria-label="Multivector" data-href="Multivector" href="the-guide/mathematics/geometric-algebra/multivector.html" class="internal-link" target="_self" rel="noopener nofollow">multivectors</a>  with pseudoscalar , it is computed via 
<br>
<br>Properties

<br><a data-tooltip-position="top" aria-label="Algebra" data-href="Algebra" href="the-guide/mathematics/general-stuff/algebra.html" class="internal-link" target="_self" rel="noopener nofollow">Associativity</a> 


]]></description><link>the-guide/mathematics/geometric-algebra/operations/regressive-product.html</link><guid isPermaLink="false">The Guide/Mathematics/Geometric Algebra/Operations/Regressive Product.md</guid><pubDate>Mon, 09 Sep 2024 15:35:53 GMT</pubDate></item><item><title><![CDATA[Reverse]]></title><description><![CDATA[ 
 <br>Reverse of a Multivector
The reverse  or  of a <a data-tooltip-position="top" aria-label="Multivector" data-href="Multivector" href="the-guide/mathematics/geometric-algebra/multivector.html" class="internal-link" target="_self" rel="noopener nofollow">multivector</a>  is the <a data-tooltip-position="top" aria-label="Multivector" data-href="Multivector" href="the-guide/mathematics/geometric-algebra/multivector.html" class="internal-link" target="_self" rel="noopener nofollow">multivector</a> with reversed order of the <a data-tooltip-position="top" aria-label="Geometric Product" data-href="Geometric Product" href="the-guide/mathematics/geometric-algebra/operations/geometric-product.html" class="internal-link" target="_self" rel="noopener nofollow">geometric</a> / <a data-tooltip-position="top" aria-label="Generalized Exterior Product" data-href="Generalized Exterior Product" href="the-guide/mathematics/geometric-algebra/operations/generalized-exterior-product.html" class="internal-link" target="_self" rel="noopener nofollow">exterior product</a> components. 
<br>Resulting from the anti-commutativity of the <a data-tooltip-position="top" aria-label="Geometric Product" data-href="Geometric Product" href="the-guide/mathematics/geometric-algebra/operations/geometric-product.html" class="internal-link" target="_self" rel="noopener nofollow">geometric product</a>, the reverse can be computed by applying (! not switching) the following signs to the components<br>
<br>
0-Vector +

<br>
1-Vector +

<br>
2-Vector -

<br>
3-Vector -

<br>
4-Vector +

<br>
...<br>
A similar operation called grade involution changes this scheme to 

<br>
Properties

<br>Linear 
<br><a data-tooltip-position="top" aria-label="Algebra" data-href="Algebra" href="the-guide/mathematics/general-stuff/algebra.html" class="internal-link" target="_self" rel="noopener nofollow">Distributive</a> 
<br>
<br>TODO

<br>Applied to a <a data-tooltip-position="top" aria-label="Geometric Product" data-href="Geometric Product" href="the-guide/mathematics/geometric-algebra/operations/geometric-product.html" class="internal-link" target="_self" rel="noopener nofollow">geometric product</a>, the reverse simply reverses the operation order
<br>




<br>
Example 

]]></description><link>the-guide/mathematics/geometric-algebra/operations/reverse.html</link><guid isPermaLink="false">The Guide/Mathematics/Geometric Algebra/Operations/Reverse.md</guid><pubDate>Mon, 09 Sep 2024 15:35:53 GMT</pubDate></item><item><title><![CDATA[Cl(2,0,1) - 2D Projective Geometric Algebra]]></title><description><![CDATA[ 
 <br>Definition
Based on the linear space of lines, 2D <a data-tooltip-position="top" aria-label="Projective Geometric Algebra (PGA)" data-href="Projective Geometric Algebra (PGA)" href="the-guide/mathematics/geometric-algebra/relevant-flavours/projective-geometric-algebra-(pga).html" class="internal-link" target="_self" rel="noopener nofollow">PGA</a> is a <a data-href="Geometric Algebra" href="the-guide/mathematics/geometric-algebra/geometric-algebra.html" class="internal-link" target="_self" rel="noopener nofollow">Geometric Algebra</a> with signature meaning 2 basis vectors square to  (, one squares to  () and none squares to . This results in an 8-dimensional <a data-tooltip-position="top" aria-label="Space" data-href="Space" href="the-guide/mathematics/general-stuff/space.html" class="internal-link" target="_self" rel="noopener nofollow">space</a> with (algebra?)
<br><img alt="center" src="lib/media/pasted-image-20240204093956.png" style="width: 300px; max-width: 100%;"><br><br><br><img alt="center" src="lib/media/pasted-image-20240204094148.png" style="width: 400px; max-width: 100%;"><br><br><br><img alt="center" src="lib/media/pasted-image-20240212125331.png" style="width: 400px; max-width: 100%;"><br><br>
<br>Vectors as Lines - PGA is based on the linear space of hyperplanes, in this case D planes or lines. A line has an orientation specified by its (normalized) euclidean normal vector  and a distance from the origin encoded by the factor  of the  base vector. A general plane idenoted via 

<br>The euclidean basis vectors  represent lines on the axis (switched from subscript!), as they are normalized and the  component and thereby the distance to the origin is zero.
<br>The non-euclidean basis vector  is a line at infinity, because normalization of the zero normal vector leads to an infinite .


<br>Bivectors as Points - Based on the notion of a line as a vector, we can meet two lines at their intersection point via the outer product. Intuitively, a point represents all planes that pass through that line. Mathematically, the intersection point of  and  is computed via To explicitly compute the points coordinates, it needs to be normalized by the  component, yielding the representation of an D point as There is also a dual representation that can be used to create a line from two points , where the points are -vectors. 
<br><br><img alt="center" src="lib/media/pasted-image-20240204094527.png" style="width: 350px; max-width: 100%;"><br>
<br>Meet  <a data-tooltip-position="top" aria-label="Generalized Exterior Product" data-href="Generalized Exterior Product" href="the-guide/mathematics/geometric-algebra/operations/generalized-exterior-product.html" class="internal-link" target="_self" rel="noopener nofollow">Exterior Poduct</a> to produce reduced elements

<br>Meet of two lines is a point


<br>Join  <a data-href="Regressive Product" href="the-guide/mathematics/geometric-algebra/operations/regressive-product.html" class="internal-link" target="_self" rel="noopener nofollow">Regressive Product</a> to produce extended elements 

<br>Join of two points is a line


<br>...]]></description><link>the-guide/mathematics/geometric-algebra/relevant-flavours/cl(2,0,1)-2d-projective-geometric-algebra.html</link><guid isPermaLink="false">The Guide/Mathematics/Geometric Algebra/Relevant Flavours/Cl(2,0,1) - 2D Projective Geometric Algebra.md</guid><pubDate>Mon, 09 Sep 2024 15:35:53 GMT</pubDate><enclosure url="lib/media/pasted-image-20240204093956.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src=&quot;lib/media/pasted-image-20240204093956.png&quot;&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[Cl(3,0,1) - 3D Projective Geometric Algebra]]></title><description><![CDATA[ 
 <br>Definition
Based on the linear space of planes, 3D <a data-tooltip-position="top" aria-label="Projective Geometric Algebra (PGA)" data-href="Projective Geometric Algebra (PGA)" href="the-guide/mathematics/geometric-algebra/relevant-flavours/projective-geometric-algebra-(pga).html" class="internal-link" target="_self" rel="noopener nofollow">PGA</a> is a <a data-href="Geometric Algebra" href="the-guide/mathematics/geometric-algebra/geometric-algebra.html" class="internal-link" target="_self" rel="noopener nofollow">Geometric Algebra</a> with signature meaning 3 basis vectors square to  (, one squares to  () and none squares to .
<br><img alt="center" src="lib/media/3dpga.png" style="width: 600px; max-width: 100%;"><br><br><br><img alt="center" src="lib/media/pasted-image-20240204094312.png" style="width: 600px; max-width: 100%;"><br><br><br><img alt="center" src="lib/media/pasted-image-20240212123752.png"><br><br>
<br>Vectors as Planes - PGA is based on the linear space of hyperplanes, in this case D planes. A plane has an orientation specified by its (normalized) normal vector  and a distance from the origin encoded by the factor  of the  base vector. A general plane is denoted via 

<br>The euclidean basis vectors  represent planes on the euclidean, as they are normalized and the  component and thereby the distance to the origin is zero.
<br>The non-euclidean basis vector  is a plane at infinity, because normalization of the zero normal vector leads to an infinite .


<br>Bivectors as Lines - Based on the notion of a plane as a vector, we can meet two planes to their intersection line via the outer product. Intuitively, the line represents all planes that pass through that line TODO
<br>Trivectors as Points - Following the intuition from above, the intersection of three planes is a point that represents all the planes passing through it. ...To explicitly compute the points coordinates, it needs to be normalized by the  component, yielding the representation of a D point as 
<br><br><img alt="center" src="lib/media/pasted-image-20240204094500.png" style="width: 350px; max-width: 100%;"><br>
<br>Meet  <a data-tooltip-position="top" aria-label="Generalized Exterior Product" data-href="Generalized Exterior Product" href="the-guide/mathematics/geometric-algebra/operations/generalized-exterior-product.html" class="internal-link" target="_self" rel="noopener nofollow">Exterior Poduct</a> to produce reduced elements

<br>Meet of three planes is a point
<br>Meet of two planes is a line
<br>Meet of a plane and a line is a point (line is intersection of two planes, outer product of three planes is again a point)


<br>Join  <a data-href="Regressive Product" href="the-guide/mathematics/geometric-algebra/operations/regressive-product.html" class="internal-link" target="_self" rel="noopener nofollow">Regressive Product</a> to produce extended elements 

<br>Join of two points is a line
<br>Join of a line and point is plane (plane is intersection of three planes, line of two planes, common plane is join)
<br>Join of three points is a plane


<br>Projection 

<br>Combination of <a data-tooltip-position="top" aria-label="Generalized Inner Product" data-href="Generalized Inner Product" href="the-guide/mathematics/geometric-algebra/operations/generalized-inner-product.html" class="internal-link" target="_self" rel="noopener nofollow">geometric inner product</a> and <a data-tooltip-position="top" aria-label="Geometric Product" data-href="Geometric Product" href="the-guide/mathematics/geometric-algebra/operations/geometric-product.html" class="internal-link" target="_self" rel="noopener nofollow">geometric product</a>, project  onto  by 
<br>Because result of inner product is perpendicular the geometric product is the meet


<br>Rigid Transformations - translations, rotations, ...

<br><a data-href="Geometric Product" href="the-guide/mathematics/geometric-algebra/operations/geometric-product.html" class="internal-link" target="_self" rel="noopener nofollow">Geometric Product</a>

<br>Rotation as composition of two reflections on intersecting planes
<br>Translation as composition of two reflections on parallel planes




]]></description><link>the-guide/mathematics/geometric-algebra/relevant-flavours/cl(3,0,1)-3d-projective-geometric-algebra.html</link><guid isPermaLink="false">The Guide/Mathematics/Geometric Algebra/Relevant Flavours/Cl(3,0,1) - 3D Projective Geometric Algebra.md</guid><pubDate>Mon, 09 Sep 2024 15:35:53 GMT</pubDate><enclosure url="lib/media/3dpga.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src=&quot;lib/media/3dpga.png&quot;&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[Cl(3,1,0) - Compass Ruler Algebra]]></title><description><![CDATA[ 
 <br>2D CGA
<a data-href="Geometric Algebra" href="the-guide/mathematics/geometric-algebra/geometric-algebra.html" class="internal-link" target="_self" rel="noopener nofollow">Geometric Algebra</a> to perform calculations in  space that uses 4 basis vectors, namely the two known euclidean ones  and additionally two orthogonal basis vectors  with  and  with . This yields an algebra with the signature In practice, interpretability based on the basis objects point and circle is reached via the transformations to  and . 
<br>The resulting D conformal <a data-href="Geometric Algebra" href="the-guide/mathematics/geometric-algebra/geometric-algebra.html" class="internal-link" target="_self" rel="noopener nofollow">Geometric Algebra</a> has a total of  <a data-tooltip-position="top" aria-label="Basis Blades" data-href="Basis Blades" href="the-guide/mathematics/geometric-algebra/basis-blades.html" class="internal-link" target="_self" rel="noopener nofollow">basis blades</a>, namely<br>
<br>1 grade-0 scalar

<br>1


<br>4 grade-1 vectors

<br>


<br>6 grade-2 bivectors

<br>all pairs wedged


<br>4 grade-3 trivectors

<br>all triplets wedged


<br>1 grade-5 pseudoscalar<br>
- <br>
Because of the transformations, one has to be careful when performing computations. Some things to consider are 
<br>, since both are no base vectors of the algebra
<br>
<br>
<br><br>In any <a data-tooltip-position="top" aria-label="Conformal Geometric Algebra (CGA)" data-href="Conformal Geometric Algebra (CGA)" href="the-guide/mathematics/geometric-algebra/relevant-flavours/conformal-geometric-algebra-(cga).html" class="internal-link" target="_self" rel="noopener nofollow">Conformal Geometric Algebra</a>, the geometric interpretation has two dual versions: IPNS and OPNS.<br>
The basis elements that are the hyperplanes in <a data-tooltip-position="top" aria-label="Projective Geometric Algebra (PGA)" data-href="Projective Geometric Algebra (PGA)" href="the-guide/mathematics/geometric-algebra/relevant-flavours/projective-geometric-algebra-(pga).html" class="internal-link" target="_self" rel="noopener nofollow">PGA</a> are circles and points (circles with zero radius).<br>
In practive, you can use the duality to switch between the representations depending on the available information.<br><br><br>
<br>Point (Grade 1) - A simple point in the IPNS representation of the compass ruler algebra is expressed via For interpretation as a euclidean point , we have to normalize with regard to the basis vector  first.
<br>Circle (Grade 1) - A circle is expressed by subtracting a term of its radius from the center point  via 

<br>Intuition - by subtracting the radius of the circle from the  term, all points with distance  from the center fulfill the inner product condition of the IPNS.


<br>Line (Grade 2) - A line is represented by taking its euclidean normal vector  and adding the  component scaled by the lines distance to the origin via 

<br>Intuition - ...


<br>Point Pair (Grade 3) - The special CGA element of the point pair is computed by the <a data-tooltip-position="top" aria-label="Generalized Exterior Product" data-href="Generalized Exterior Product" href="the-guide/mathematics/geometric-algebra/operations/generalized-exterior-product.html" class="internal-link" target="_self" rel="noopener nofollow">outer product</a> of two circles 
<br><br>
<br><a data-tooltip-position="top" aria-label="Generalized Exterior Product" data-href="Generalized Exterior Product" href="the-guide/mathematics/geometric-algebra/operations/generalized-exterior-product.html" class="internal-link" target="_self" rel="noopener nofollow">Outer Product</a> = Intersections

<br>Intersecting three circles yields a point 
<br>Intersection of line  with a circle  is a point pair (one at infinity if through middle point)
<br>Intersection of two circles results in point pair
<br>Intersection of two lines  and  is point pair of intersection and point at infinity (lines are circles with infinite radii)

<br>If the lines are parallel, this results in a free translation vector from one line to the other. The above yields zero if the lines coincide.




<br><a data-tooltip-position="top" aria-label="Generalized Inner Product" data-href="Generalized Inner Product" href="the-guide/mathematics/geometric-algebra/operations/generalized-inner-product.html" class="internal-link" target="_self" rel="noopener nofollow">Inner Product</a> = Distances and Angles<img alt="center" src="lib/media/pasted-image-20240210125424.png">

<br>Point -Point - The inner product of two points  and  yields which is a distance measure.
<br>Point - Line -  The inner product of a Point  and a Line  yields which represents the euclidean distance between them with the sign encoding if the point lies in direction of the normal (+) or on the opposite side (-).
<br>Line - Line - The inner product of two lines results in the vector spaces inner product of the normal vectors, representing the angle between the lines.  
<br>Line - Circle - The inner product of a line and a circle results in the distance of the middle point of that circle to the line 
<br>Point - Circle - Here, assuming the euclidean coordinates  for the points and  for the circle, the inner product yields the expression (use special rules for  and ) which can be linked to the length of the blue segment via the Pythagorean theorem<img alt="center" src="lib/media/pasted-image-20240210152905.png">This can be used for the the categorization of points, as if

<br>, the point is outside the circle,
<br>, the point is inside the circle,
<br>, the point is on the circle.


<br>Circle - Circle - Points in CRA are just circles with zero radius, which is why the above also holds for two circles. However, the geometric argument now includes an additional step.<img alt="center" src="lib/media/pasted-image-20240211103049.png" style="width: 300px; max-width: 100%;">After computing the distances like above, the blue line results from a second Pythagorean theorem  which yields .


<br><a data-href="Geometric Product" href="the-guide/mathematics/geometric-algebra/operations/geometric-product.html" class="internal-link" target="_self" rel="noopener nofollow">Geometric Product</a> = Rotation, Translation, Reflection, Inversion,...<img alt="center" src="lib/media/pasted-image-20240210182917.png">

<br>Rotation around origin with angle  via 
<br>Translation along vector  via 
<br>Circle Inversion - Special case of reflection, where a circle is used instead of a line. This can be used to compute a circles center by inverting infinity 


<br><br><br><br>
<br>Point 
<br>Circle (Grade 3) - Three points uniquely define a circle through them 
<br>Line (Grade 3) - Because a circle is obtained by combining three points, one of these points can be chosen at infinity to obtain a straight line through the other two points 

<br>Intuition - Line is circle with infinite radius


<br>Point Pair (Grade 2) - Is obtained by intersecting two points 

<br>...


<br><br><br>
<br>In CGA, the square (<a data-tooltip-position="top" aria-label="Geometric Product" data-href="Geometric Product" href="the-guide/mathematics/geometric-algebra/operations/geometric-product.html" class="internal-link" target="_self" rel="noopener nofollow">geometric product</a>) of a circle will yield the squared radius 
<br>Point Pair from circle and line<br>
- <a rel="noopener nofollow" class="external-link" href="https://gaalopweb.fme.vutbr.cz/gaalopweb/res/python/result?id=3c54b40d615c73d782f92a9f0c2f1ccc43d778c2c3acd7830015728" target="_blank">https://gaalopweb.fme.vutbr.cz/gaalopweb/res/python/result?id=3c54b40d615c73d782f92a9f0c2f1ccc43d778c2c3acd7830015728</a>
<br>Intersection of three circles is middle point

<br><a rel="noopener nofollow" class="external-link" href="https://gaalopweb.fme.vutbr.cz/gaalopweb/res/python/result?id=1d17201591704aad96be902bbf0c92f5ac9d3f058f876cbabcfc30e" target="_blank">https://gaalopweb.fme.vutbr.cz/gaalopweb/res/python/result?id=1d17201591704aad96be902bbf0c92f5ac9d3f058f876cbabcfc30e</a>


<br>Reflection of circle 

<br><a rel="noopener nofollow" class="external-link" href="https://gaalopweb.fme.vutbr.cz/gaalopweb/res/python/result?id=1b687b45e498506427bb199e5385d5b4525bf4542122d24b633493a" target="_blank">https://gaalopweb.fme.vutbr.cz/gaalopweb/res/python/result?id=1b687b45e498506427bb199e5385d5b4525bf4542122d24b633493a</a>


<br>Meeting two circles, computing the middle point of the resulting point pair

<br><a rel="noopener nofollow" class="external-link" href="https://gaalopweb.fme.vutbr.cz/gaalopweb/res/python/result?id=3275f7cdf06fbaf16b99565bce7a5182264375864f8fc1438b8b673" target="_blank">https://gaalopweb.fme.vutbr.cz/gaalopweb/res/python/result?id=3275f7cdf06fbaf16b99565bce7a5182264375864f8fc1438b8b673</a>


<br>Subtracting points / circles yields a line

<br><a rel="noopener nofollow" class="external-link" href="https://gaalopweb.fme.vutbr.cz/gaalopweb/res/python/result?id=a8fc3dfcadee4fe0ce76b47d2ea8f65d331ecf18aeba1a1ce036f41" target="_blank">https://gaalopweb.fme.vutbr.cz/gaalopweb/res/python/result?id=a8fc3dfcadee4fe0ce76b47d2ea8f65d331ecf18aeba1a1ce036f41</a>
<br>This can also be computed rather efficiently by hand, as the  component simply vanishes while  and  result in the difference-vector of the (middle)points as the normal vector. 


<br>Rotation of circle around arbitrary point using motor

<br><a rel="noopener nofollow" class="external-link" href="https://gaalopweb.fme.vutbr.cz/gaalopweb/res/python/result?id=dc82ae3717fa57baa22aa31db79b6c3dddffeba353caf37fc0e3eac" target="_blank">https://gaalopweb.fme.vutbr.cz/gaalopweb/res/python/result?id=dc82ae3717fa57baa22aa31db79b6c3dddffeba353caf37fc0e3eac</a>


]]></description><link>the-guide/mathematics/geometric-algebra/relevant-flavours/cl(3,1,0)-compass-ruler-algebra.html</link><guid isPermaLink="false">The Guide/Mathematics/Geometric Algebra/Relevant Flavours/Cl(3,1,0) - Compass Ruler Algebra.md</guid><pubDate>Mon, 09 Sep 2024 15:35:53 GMT</pubDate><enclosure url="lib/media/pasted-image-20240210125424.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src=&quot;lib/media/pasted-image-20240210125424.png&quot;&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[Cl(4,1,0) - 3D Conformal Geometric Algebra]]></title><description><![CDATA[ 
 <br>3D CGA
<a data-href="Geometric Algebra" href="the-guide/mathematics/geometric-algebra/geometric-algebra.html" class="internal-link" target="_self" rel="noopener nofollow">Geometric Algebra</a> to perform calculations in  space that uses 5 basis vectors, namely the three known euclidean ones and additionally two orthogonal basis vectors  and  with and These can be transformed to two other basis vectors with geometric meaning via to represent the D origin and to represent a point at infinity
<br>The resulting D conformal <a data-href="Geometric Algebra" href="the-guide/mathematics/geometric-algebra/geometric-algebra.html" class="internal-link" target="_self" rel="noopener nofollow">Geometric Algebra</a> has a total of  <a data-tooltip-position="top" aria-label="Basis Blades" data-href="Basis Blades" href="the-guide/mathematics/geometric-algebra/basis-blades.html" class="internal-link" target="_self" rel="noopener nofollow">basis blades</a>, namely<br>
<br>1 grade-0 scalar

<br>1


<br>5 grade-1 vectors

<br>


<br>10 grade-2 bivectors

<br>all pairs wedged


<br>10 grade-3 trivectors

<br>all triplets wedged


<br>5 grade-4 quadvectors

<br>all quartetts wedged


<br>1 grade-5 pseudoscalar<br>
- <br>
Because of the transformations, one has to be careful when performing computations. Some things to consider are 
<br>, since both are no base vectors of the algebra
<br>
<br><br><br><br>Point <br>
<br>
Sphere 

<br>
Plane 

<br>
Circle 

<br>Intersection of two spheres


<br>
Line 

<br>...


<br>
Point pair 

<br>Intersection of three spheres


<br>
Point 

<br>Sphere 
<br>Plane 
<br>Circle 

<br>...


<br>Line 

<br>...


<br>Point pair 

<br>...




<br><br>
<br>...
<br>**<a data-tooltip-position="top" aria-label="Generalized Inner Product" data-href="Generalized Inner Product" href="the-guide/mathematics/geometric-algebra/operations/generalized-inner-product.html" class="internal-link" target="_self" rel="noopener nofollow">Inner Product</a> = Distances and Angles

<br>Point -Point - As in <a data-tooltip-position="top" aria-label="Cl(3,1,0) - Compass Ruler Algebra" data-href="Cl(3,1,0) - Compass Ruler Algebra" href="the-guide/mathematics/geometric-algebra/relevant-flavours/cl(3,1,0)-compass-ruler-algebra.html" class="internal-link" target="_self" rel="noopener nofollow">CRA</a>, the inner product of two points is related to the squared euclidean distance 
<br>Point - Sphere - With the same arguments as for the point-circle case in <a data-tooltip-position="top" aria-label="Cl(3,1,0) - Compass Ruler Algebra" data-href="Cl(3,1,0) - Compass Ruler Algebra" href="the-guide/mathematics/geometric-algebra/relevant-flavours/cl(3,1,0)-compass-ruler-algebra.html" class="internal-link" target="_self" rel="noopener nofollow">CRA</a>, we can deriveThis can again be used for the the categorization of points, as if

<br>, the point is outside the sphere,
<br>, the point is inside the sphere,
<br>, the point is on the sphere.




<br><br><br><a data-tooltip-position="top" aria-label="Quaternions" data-href="Quaternions" href="the-guide/mathematics/general-stuff/quaternions.html" class="internal-link" target="_self" rel="noopener nofollow">Quaternions</a> can be described using 3D CGA by identifying the imaginary parts according to the table. <img alt="center" src="lib/media/pasted-image-20240211104747.png">In practice, computing a Quaternion that rotates a point  to another point  can be achieved by ...<br>
<br>Computing the middle plane  through the origin via subtraction 
<br>Computing the plane  in which  lie via 
<br>Computing the middle line via 
<br>Using the normalized line  as the Quaternion  (since we are rotating by )<br>
To generalize this to arbitrary rotations ...
<br><br><br>
<br>Fitting a Sphere to 3D points via This results in conditions on the sphere parameters depending on the points, which can be translated into a linear system. 
<br>Using the constraint , we can also solve a constrained problem. This way, if the solution has a  component, we fitted a sphere, otherwise a plane. 
<br><br><br>
<br>Create a pyramid from a triangle with given lengths  for the new edges by first computing spheres around the triangles vertices  with the desired radii and then computing the intersections (always two because of symmetry of the problem) of the three spheres via Computing the sphere around this pyramid would require , where dualization is required for the IPNS representation.
<br>Reflection of a plane on a sphere 

<br><a rel="noopener nofollow" class="external-link" href="https://gaalopweb.esa.informatik.tu-darmstadt.de/gaalopweb/res/python/result?id=e5e91f9f351fbd492092c2ede1b40d54e7c12cd7b50828d3ce7d116" target="_blank">https://gaalopweb.esa.informatik.tu-darmstadt.de/gaalopweb/res/python/result?id=e5e91f9f351fbd492092c2ede1b40d54e7c12cd7b50828d3ce7d116</a>


]]></description><link>the-guide/mathematics/geometric-algebra/relevant-flavours/cl(4,1,0)-3d-conformal-geometric-algebra.html</link><guid isPermaLink="false">The Guide/Mathematics/Geometric Algebra/Relevant Flavours/Cl(4,1,0) - 3D Conformal Geometric Algebra.md</guid><pubDate>Wed, 15 Jan 2025 17:34:54 GMT</pubDate><enclosure url="lib/media/pasted-image-20240211104747.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src=&quot;lib/media/pasted-image-20240211104747.png&quot;&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[Conformal Geometric Algebra (CGA)]]></title><description><![CDATA[ 
 <br>Conformal Geometric Algebra is a family of - and D point-based flavors of <a data-tooltip-position="top" aria-label="Geometric Algebra" data-href="Geometric Algebra" href="the-guide/mathematics/geometric-algebra/geometric-algebra.html" class="internal-link" target="_self" rel="noopener nofollow">geometric algebra</a> that allows for manipulating points, point-pairs, hyperplanes and hyperspheres.<br>
The <a data-tooltip-position="top" aria-label="Algebra" data-href="Algebra" href="the-guide/mathematics/general-stuff/algebra.html" class="internal-link" target="_self" rel="noopener nofollow">algebra</a> includes an infinite point and has exception-free meet and join operations and supports conformal transformations. <br>Definition
Mathematicalls, CGA adds two extra dimensions to the three known euclidean ones via two orthogonal basis vectors  and  with and These can be transformed to two other basis vectors with geometric meaning via to represent the origin and to represent infinity. For both new vectors it holds that  and .
<br>
<br>Special Flavors

<br><a data-href="Cl(3,1,0) - Compass Ruler Algebra" href="the-guide/mathematics/geometric-algebra/relevant-flavours/cl(3,1,0)-compass-ruler-algebra.html" class="internal-link" target="_self" rel="noopener nofollow">Cl(3,1,0) - Compass Ruler Algebra</a>
<br><a data-href="Cl(4,1,0) - 3D Conformal Geometric Algebra" href="the-guide/mathematics/geometric-algebra/relevant-flavours/cl(4,1,0)-3d-conformal-geometric-algebra.html" class="internal-link" target="_self" rel="noopener nofollow">Cl(4,1,0) - 3D Conformal Geometric Algebra</a>


<br><br><br>CGA provides the basic geometric entities of points, point-pairs, hyperspheres and hyperplanes via two distinct, but <a data-tooltip-position="top" aria-label="Duality in Geometric Algebra" data-href="Duality in Geometric Algebra" href="the-guide/mathematics/geometric-algebra/operations/duality-in-geometric-algebra.html" class="internal-link" target="_self" rel="noopener nofollow">dual</a> algebraic representations. Using  and  as vectors in euclidean space, we obtain ...<br>Inner Product Null Space (IPNS)
Using the <a data-tooltip-position="top" aria-label="Generalized Inner Product" data-href="Generalized Inner Product" href="the-guide/mathematics/geometric-algebra/operations/generalized-inner-product.html" class="internal-link" target="_self" rel="noopener nofollow">inner product</a> of geometric algebra, a <a data-tooltip-position="top" aria-label="Multivector" data-href="Multivector" href="the-guide/mathematics/geometric-algebra/multivector.html" class="internal-link" target="_self" rel="noopener nofollow">multivector</a>  represents the entity for which all points  (-vectors) satisfy
<br>
<br>In general, the <a data-tooltip-position="top" aria-label="Generalized Exterior Product" data-href="Generalized Exterior Product" href="the-guide/mathematics/geometric-algebra/operations/generalized-exterior-product.html" class="internal-link" target="_self" rel="noopener nofollow">outer product</a> in IPNS realizes intersections of geometric primitives, as 
<br>Outer Product Null Space (OPNS)
Alternatively, one can interpret entities via the <a data-tooltip-position="top" aria-label="Generalized Exterior Product" data-href="Generalized Exterior Product" href="the-guide/mathematics/geometric-algebra/operations/generalized-exterior-product.html" class="internal-link" target="_self" rel="noopener nofollow">outer product</a> by letting a <a data-tooltip-position="top" aria-label="Multivector" data-href="Multivector" href="the-guide/mathematics/geometric-algebra/multivector.html" class="internal-link" target="_self" rel="noopener nofollow">multivector</a> represent all points  (-vectors) satisfying 
]]></description><link>the-guide/mathematics/geometric-algebra/relevant-flavours/conformal-geometric-algebra-(cga).html</link><guid isPermaLink="false">The Guide/Mathematics/Geometric Algebra/Relevant Flavours/Conformal Geometric Algebra (CGA).md</guid><pubDate>Mon, 09 Sep 2024 15:35:53 GMT</pubDate></item><item><title><![CDATA[Projective Geometric Algebra (PGA)]]></title><description><![CDATA[ 
 <br>Projective Geometric Algebra is a family of - and D hyperplane-based flavors of <a data-tooltip-position="top" aria-label="Geometric Algebra" data-href="Geometric Algebra" href="the-guide/mathematics/geometric-algebra/geometric-algebra.html" class="internal-link" target="_self" rel="noopener nofollow">geometric algebra</a> that allows for manipulating points and hyperplanes.<br>
The <a data-tooltip-position="top" aria-label="Algebra" data-href="Algebra" href="the-guide/mathematics/general-stuff/algebra.html" class="internal-link" target="_self" rel="noopener nofollow">algebra</a> includes elements at infinity and has exception-free meet and join operations and supports rigid transformations. This is done by treating hyperplanes as their own linear space. <br>Definition
Mathematically, (euclidean) PGA is a <a data-tooltip-position="top" aria-label="Geometric Algebra" data-href="Geometric Algebra" href="the-guide/mathematics/geometric-algebra/geometric-algebra.html" class="internal-link" target="_self" rel="noopener nofollow">geometric algebra</a> that adds one additional basis vector that is used to represent a hyperplane at infinity.
<br>
<br>
One null vector, <a data-tooltip-position="top" aria-label="Pseudoscalar" data-href="Pseudoscalar" href="the-guide/mathematics/geometric-algebra/pseudoscalar.html" class="internal-link" target="_self" rel="noopener nofollow">pseudoscalar</a> is a nullblade and not invertible

<br>
Special Flavors

<br><a data-href="Cl(2,0,1) - 2D Projective Geometric Algebra" href="the-guide/mathematics/geometric-algebra/relevant-flavours/cl(2,0,1)-2d-projective-geometric-algebra.html" class="internal-link" target="_self" rel="noopener nofollow">Cl(2,0,1) - 2D Projective Geometric Algebra</a>
<br><a data-href="Cl(3,0,1) - 3D Projective Geometric Algebra" href="the-guide/mathematics/geometric-algebra/relevant-flavours/cl(3,0,1)-3d-projective-geometric-algebra.html" class="internal-link" target="_self" rel="noopener nofollow">Cl(3,0,1) - 3D Projective Geometric Algebra</a>


<br><br><br>
<br><a rel="noopener nofollow" class="external-link" href="https://www.youtube.com/watch?v=0i3ocLhbxJ4&amp;t=717s" target="_blank">https://www.youtube.com/watch?v=0i3ocLhbxJ4&amp;t=717s</a>
]]></description><link>the-guide/mathematics/geometric-algebra/relevant-flavours/projective-geometric-algebra-(pga).html</link><guid isPermaLink="false">The Guide/Mathematics/Geometric Algebra/Relevant Flavours/Projective Geometric Algebra (PGA).md</guid><pubDate>Mon, 09 Sep 2024 15:35:53 GMT</pubDate></item><item><title><![CDATA[Basis Blades]]></title><description><![CDATA[ 
 <br>Definition
Basis blades are the elementary <a data-tooltip-position="top" aria-label="Algebra" data-href="Algebra" href="the-guide/mathematics/general-stuff/algebra.html" class="internal-link" target="_self" rel="noopener nofollow">algebraic</a> elements of <a data-tooltip-position="top" aria-label="Geometric Algebra" data-href="Geometric Algebra" href="the-guide/mathematics/geometric-algebra/geometric-algebra.html" class="internal-link" target="_self" rel="noopener nofollow">Geometric Algebra</a>. Depending on their grade, they represent the whole subspace spanned by their factors (attitude) along with a weight and an orientation. The linear combination of different -blades produces a -vector. Based on this, the linear combination of -vectors of different grades is a <a data-tooltip-position="top" aria-label="Multivector" data-href="Multivector" href="the-guide/mathematics/geometric-algebra/multivector.html" class="internal-link" target="_self" rel="noopener nofollow">multivector</a>.
<br>A -blade  is the <a data-tooltip-position="top" aria-label="Generalized Exterior Product" data-href="Generalized Exterior Product" href="the-guide/mathematics/geometric-algebra/operations/generalized-exterior-product.html" class="internal-link" target="_self" rel="noopener nofollow">outer product</a> of  different vectors / -vectors , therefore representing the subspace spanned by the vectors along with a weight that also defines orientation by its sign. It thereby generalizes the concept of scalars and vectors to higher dimensional objects. If the used vectors are linearly dependent the blade is zero. Examples are<br>
<br>-Blade

<br>Scalar, always one


<br>-Blade

<br>Basis vectors


<br>2-Blade

<br>E.g. 


<br>...
<br><a data-href="Pseudoscalar" href="the-guide/mathematics/geometric-algebra/pseudoscalar.html" class="internal-link" target="_self" rel="noopener nofollow">Pseudoscalar</a>

<br>Outer product of all basis vectors, denoted 


]]></description><link>the-guide/mathematics/geometric-algebra/basis-blades.html</link><guid isPermaLink="false">The Guide/Mathematics/Geometric Algebra/Basis Blades.md</guid><pubDate>Mon, 09 Sep 2024 15:35:53 GMT</pubDate></item><item><title><![CDATA[Cartan-Dieudonne Theorem]]></title><description><![CDATA[ 
 <br>Cartan-Dieudonne Theorem
Every <a data-tooltip-position="top" aria-label="Matrices" data-href="Matrices" href="the-guide/mathematics/linear-algebra/matrices.html" class="internal-link" target="_self" rel="noopener nofollow">orthogonal transformation</a> of an -dimensional non-degenerate symmetric bilinear <a data-tooltip-position="top" aria-label="Space" data-href="Space" href="the-guide/mathematics/general-stuff/space.html" class="internal-link" target="_self" rel="noopener nofollow">space</a>  over a field with characteristic not equal to  can be decomposed into, at most,  reflections.
<br><img alt="center" src="lib/media/pasted-image-20240812150637.png">]]></description><link>the-guide/mathematics/geometric-algebra/cartan-dieudonne-theorem.html</link><guid isPermaLink="false">The Guide/Mathematics/Geometric Algebra/Cartan-Dieudonne Theorem.md</guid><pubDate>Mon, 09 Sep 2024 15:35:53 GMT</pubDate><enclosure url="lib/media/pasted-image-20240812150637.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src=&quot;lib/media/pasted-image-20240812150637.png&quot;&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[Chasles' Theorem]]></title><description><![CDATA[ 
 <br>Theorem
In <a data-tooltip-position="top" aria-label="Kinematics" data-href="Kinematics" href="the-guide/robotics,-dynamics-and-control/kinematics/kinematics.html" class="internal-link" target="_self" rel="noopener nofollow">kinematics</a>, the most general rigid body displacement can be produced by a translation along a line (screw axis) followed or preceded by a <a data-tooltip-position="top" aria-label="A Hitchhiker's Guide to Rotation Representations" data-href="A Hitchhiker's Guide to Rotation Representations" href="the-guide/robotics,-dynamics-and-control/a-hitchhiker's-guide-to-rotation-representations.html" class="internal-link" target="_self" rel="noopener nofollow">rotation</a> about an axis parallel to the screw axis. the total transformation is a screw displacement.
<br><br><br><br>The calculation of the commuting translation and rotation from a screw motion can be performed using <a data-tooltip-position="top" aria-label="Cl(3,0,1) - 3D Projective Geometric Algebra" data-href="Cl(3,0,1) - 3D Projective Geometric Algebra" href="the-guide/mathematics/geometric-algebra/relevant-flavours/cl(3,0,1)-3d-projective-geometric-algebra.html" class="internal-link" target="_self" rel="noopener nofollow">3DPGA</a> (), the <a data-tooltip-position="top" aria-label="Projective Geometric Algebra (PGA)" data-href="Projective Geometric Algebra (PGA)" href="the-guide/mathematics/geometric-algebra/relevant-flavours/projective-geometric-algebra-(pga).html" class="internal-link" target="_self" rel="noopener nofollow">projective</a> <a data-href="Geometric Algebra" href="the-guide/mathematics/geometric-algebra/geometric-algebra.html" class="internal-link" target="_self" rel="noopener nofollow">Geometric Algebra</a> of 3D Euclidean space.<br>It has three Euclidean basis vectors  satisfying  representing orthogonal planes through the origin, and one Grassmanian basis vector  satisfying  to represent the plane at infinity. Any plane a distance  from the origin can then be formed as a linear combination which is normalized such that . Because reflections can be represented by the plane in which the reflection occurs, the product of two planes  and  is the bireflection . The result is a rotation around their intersection line , which could also lie on the plane at infinity when the two reflections are parallel, in which case the bireflection  is a translation.<br>A screw motion  is the product of four non-collinear reflections (<a data-href="Cartan-Dieudonne Theorem" href="the-guide/mathematics/geometric-algebra/cartan-dieudonne-theorem.html" class="internal-link" target="_self" rel="noopener nofollow">Cartan-Dieudonne Theorem</a>), and thus . But according to the Mozzi-Chasles' theorem a screw motion can be decomposed into a commuting translation  where  is the axis of translation satisfying , and rotationwhere  is the axis of rotation satisfying . The two bivector lines  and  are orthogonal and commuting. To find  and  from , we simply write out  and consider the result grade-by-grade:Because the quadvector part  and ,  is directly found to beand thusThus, for a given screw motion  the commuting translation and rotation can be found using the two formulae above, after which the lines  and  are found to be proportional to  and  respectively.]]></description><link>the-guide/mathematics/geometric-algebra/chasles&apos;-theorem.html</link><guid isPermaLink="false">The Guide/Mathematics/Geometric Algebra/Chasles&apos; Theorem.md</guid><pubDate>Wed, 26 Feb 2025 10:33:30 GMT</pubDate></item><item><title><![CDATA[Exterior Algebra]]></title><description><![CDATA[ 
 <br>Graded associative (addition, multiplication and scalar multiplication) <a data-tooltip-position="top" aria-label="Algebra" data-href="Algebra" href="the-guide/mathematics/general-stuff/algebra.html" class="internal-link" target="_self" rel="noopener nofollow">algebra</a> connected to a <a data-tooltip-position="top" aria-label="Vector Space" data-href="Vector Space" href="the-guide/mathematics/general-stuff/vector-space.html" class="internal-link" target="_self" rel="noopener nofollow">vector space</a> , written as The product  on  is the <a data-tooltip-position="top" aria-label="Generalized Exterior Product" data-href="Generalized Exterior Product" href="the-guide/mathematics/geometric-algebra/operations/generalized-exterior-product.html" class="internal-link" target="_self" rel="noopener nofollow">exterior product</a> and elements in the space are <a data-tooltip-position="top" aria-label="Multivector" data-href="Multivector" href="the-guide/mathematics/geometric-algebra/multivector.html" class="internal-link" target="_self" rel="noopener nofollow">multivectors</a><br><br><a rel="noopener nofollow" class="external-link" href="https://en.wikipedia.org/wiki/Exterior_algebra" target="_blank">https://en.wikipedia.org/wiki/Exterior_algebra</a>]]></description><link>the-guide/mathematics/geometric-algebra/exterior-algebra.html</link><guid isPermaLink="false">The Guide/Mathematics/Geometric Algebra/Exterior Algebra.md</guid><pubDate>Mon, 24 Feb 2025 23:32:25 GMT</pubDate></item><item><title><![CDATA[GAALOP]]></title><description><![CDATA[ 
 <br>Geometric Algebra tool for creating optimized code based on CLUCalc notation.<br><br><img alt="center" src="lib/media/pasted-image-20231212100614.png"><br>
<br>A lot of steps are pre-computed before runtime, yielding fast and robust implementations
<br>Two-Stage Optimization

<br>Product-Level Optimization of products of <a data-tooltip-position="top" aria-label="Multivector" data-href="Multivector" href="the-guide/mathematics/geometric-algebra/multivector.html" class="internal-link" target="_self" rel="noopener nofollow">multivectors</a> 

<br>Only translate non-zero components (multiplication table) to code


<br>Algorithm-Level 

<br>Only the <a data-tooltip-position="top" aria-label="Multivector" data-href="Multivector" href="the-guide/mathematics/geometric-algebra/multivector.html" class="internal-link" target="_self" rel="noopener nofollow">multivectors</a> denoted with "?" are actually computed (Maxima)
<br>Other computations are directly intertwined




]]></description><link>the-guide/mathematics/geometric-algebra/gaalop.html</link><guid isPermaLink="false">The Guide/Mathematics/Geometric Algebra/GAALOP.md</guid><pubDate>Mon, 09 Sep 2024 15:35:53 GMT</pubDate><enclosure url="lib/media/pasted-image-20231212100614.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src=&quot;lib/media/pasted-image-20231212100614.png&quot;&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[Geometric Algebra]]></title><description><![CDATA[ 
 <br>Geometric algebra (also known as a <a data-tooltip-position="top" aria-label="https://en.wikipedia.org/wiki/Clifford_algebra#Examples:_real_and_complex_Clifford_algebras" rel="noopener nofollow" class="external-link" title="Clifford algebra" href="https://en.wikipedia.org/wiki/Clifford_algebra#Examples:_real_and_complex_Clifford_algebras" target="_blank">real Clifford algebra</a>) is an extension of elementary <a data-tooltip-position="top" aria-label="Algebra" data-href="Algebra" href="the-guide/mathematics/general-stuff/algebra.html" class="internal-link" target="_self" rel="noopener nofollow">algebra</a> to work with geometrical objects such as <a data-tooltip-position="top" aria-label="Vector Space" data-href="Vector Space" href="the-guide/mathematics/general-stuff/vector-space.html" class="internal-link" target="_self" rel="noopener nofollow">vectors</a>. A Geometric algebra is built out of two fundamental operations, addition and the <a data-tooltip-position="top" aria-label="Geometric Product" data-href="Geometric Product" href="the-guide/mathematics/geometric-algebra/operations/geometric-product.html" class="internal-link" target="_self" rel="noopener nofollow">geometric product</a> as a form of multiplication that are defined on a <a data-tooltip-position="top" aria-label="Set" data-href="Set" href="the-guide/mathematics/general-stuff/set.html" class="internal-link" target="_self" rel="noopener nofollow">set</a> . Multiplication of vectors results in higher-dimensional objects called <a data-tooltip-position="top" aria-label="Multivector" data-href="Multivector" href="the-guide/mathematics/geometric-algebra/multivector.html" class="internal-link" target="_self" rel="noopener nofollow">multivectors</a>. Compared to other formalism's for manipulating geometric objects, geometric algebra is noteworthy for supporting vector division and addition of objects of different dimensions.<br>The general idea is that in regular vector algebra, scalars, vectors and planes have three features<br>
<br>An attitude - which subspace is represented
<br>A weight - amount, length, area, ...
<br>An orientation - sign, direction (2 in every possible dimension of the space, since you need conventions e.g. clock-wise vs. counter-clockwise)<br>
Geometric algebra extends this concept to higher-dimensional objects. An -dimensional Geometric algebra consists of <a data-tooltip-position="top" aria-label="Basis Blades" data-href="Basis Blades" href="the-guide/mathematics/geometric-algebra/basis-blades.html" class="internal-link" target="_self" rel="noopener nofollow">basis blades</a>, representing the subspaces of grade  (scalar),  (<a data-tooltip-position="top" aria-label="Vector Space" data-href="Vector Space" href="the-guide/mathematics/general-stuff/vector-space.html" class="internal-link" target="_self" rel="noopener nofollow">vector</a>), ... ,  (pseudoscalar, only one). <img alt="center" src="lib/media/pasted-image-20240203083132.png"><br>
The combination of all -blades form a -vector. A combination of -vectors is a <a data-tooltip-position="top" aria-label="Multivector" data-href="Multivector" href="the-guide/mathematics/geometric-algebra/multivector.html" class="internal-link" target="_self" rel="noopener nofollow">multivector</a>. 
<br><br><br>Info
Mathematically, a Geometric Algebra is associated to an underlying (finite-dimensional) <a data-tooltip-position="top" aria-label="Vector Space" data-href="Vector Space" href="the-guide/mathematics/general-stuff/vector-space.html" class="internal-link" target="_self" rel="noopener nofollow">vector space</a> over a field  together with a symmetric bilinear form  (e.g. euclidean inner product*). The chosen base vectors determine the resulting algebra. Therefore, to specify a geometric algebra, we denote it via after Clifford (sometimes also ) with the signature of the bilinear form, encoding that

<br> basis vectors square to 
<br> basis vectors square to 
<br> basis vectors square to 

<br>In general, different signatures lead to various interpretations of the resulting algebras, which are often denoted "flavours"  of geometric algebra. These include, but are not limited to:<br>
<br>Euclidean Geometric Algebra

<br>Cl(2,0,0) naturally describes the complex numbers, because the <a data-tooltip-position="top" aria-label="Pseudoscalar" data-href="Pseudoscalar" href="the-guide/mathematics/geometric-algebra/pseudoscalar.html" class="internal-link" target="_self" rel="noopener nofollow">pseudoscalar</a> squares to describing the complex unit .
<br>Cl(3,0,0), whose even part can be identified with <a data-href="Quaternions" href="the-guide/mathematics/general-stuff/quaternions.html" class="internal-link" target="_self" rel="noopener nofollow">Quaternions</a>.


<br><a data-href="Projective Geometric Algebra (PGA)" href="the-guide/mathematics/geometric-algebra/relevant-flavours/projective-geometric-algebra-(pga).html" class="internal-link" target="_self" rel="noopener nofollow">Projective Geometric Algebra (PGA)</a>

<br><a data-href="Cl(2,0,1) - 2D Projective Geometric Algebra" href="the-guide/mathematics/geometric-algebra/relevant-flavours/cl(2,0,1)-2d-projective-geometric-algebra.html" class="internal-link" target="_self" rel="noopener nofollow">Cl(2,0,1) - 2D Projective Geometric Algebra</a>
<br><a data-href="Cl(3,0,1) - 3D Projective Geometric Algebra" href="the-guide/mathematics/geometric-algebra/relevant-flavours/cl(3,0,1)-3d-projective-geometric-algebra.html" class="internal-link" target="_self" rel="noopener nofollow">Cl(3,0,1) - 3D Projective Geometric Algebra</a>


<br><a data-href="Conformal Geometric Algebra (CGA)" href="the-guide/mathematics/geometric-algebra/relevant-flavours/conformal-geometric-algebra-(cga).html" class="internal-link" target="_self" rel="noopener nofollow">Conformal Geometric Algebra (CGA)</a>

<br><a data-href="Cl(3,1,0) - Compass Ruler Algebra" href="the-guide/mathematics/geometric-algebra/relevant-flavours/cl(3,1,0)-compass-ruler-algebra.html" class="internal-link" target="_self" rel="noopener nofollow">Cl(3,1,0) - Compass Ruler Algebra</a>
<br><a data-href="Cl(4,1,0) - 3D Conformal Geometric Algebra" href="the-guide/mathematics/geometric-algebra/relevant-flavours/cl(4,1,0)-3d-conformal-geometric-algebra.html" class="internal-link" target="_self" rel="noopener nofollow">Cl(4,1,0) - 3D Conformal Geometric Algebra</a>


<br>Difference to Vector Spaces

<br>Whereas a plain vector space like  allows us to take linear combinations of elements  and  (vectors), a geometric algebra additionally has a bilinear associative operation: the <a data-tooltip-position="top" aria-label="Geometric Product" data-href="Geometric Product" href="the-guide/mathematics/geometric-algebra/operations/geometric-product.html" class="internal-link" target="_self" rel="noopener nofollow">geometric product</a>, denoted simply by xy. By multiplying vectors, one obtains so-called <a data-tooltip-position="top" aria-label="Multivector" data-href="Multivector" href="the-guide/mathematics/geometric-algebra/multivector.html" class="internal-link" target="_self" rel="noopener nofollow">multivectors</a>, which can represent both geometrical objects and operators.
<br>

<br>General <a data-tooltip-position="top" aria-label="Vector Space" data-href="Vector Space" href="the-guide/mathematics/general-stuff/vector-space.html" class="internal-link" target="_self" rel="noopener nofollow">inner product space</a> does not require bilinearity



<br><br><br><br>
<br><a data-tooltip-position="top" aria-label="Generalized Exterior Product" data-href="Generalized Exterior Product" href="the-guide/mathematics/geometric-algebra/operations/generalized-exterior-product.html" class="internal-link" target="_self" rel="noopener nofollow">Outer</a> and <a data-tooltip-position="top" aria-label="Generalized Inner Product" data-href="Generalized Inner Product" href="the-guide/mathematics/geometric-algebra/operations/generalized-inner-product.html" class="internal-link" target="_self" rel="noopener nofollow">Inner</a> products have preference over <a data-tooltip-position="top" aria-label="Geometric Product" data-href="Geometric Product" href="the-guide/mathematics/geometric-algebra/operations/geometric-product.html" class="internal-link" target="_self" rel="noopener nofollow">geometric product</a> 
<br>Outer products have preference over inner products
]]></description><link>the-guide/mathematics/geometric-algebra/geometric-algebra.html</link><guid isPermaLink="false">The Guide/Mathematics/Geometric Algebra/Geometric Algebra.md</guid><pubDate>Mon, 24 Feb 2025 23:32:25 GMT</pubDate><enclosure url="lib/media/pasted-image-20240203083132.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src=&quot;lib/media/pasted-image-20240203083132.png&quot;&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[Multivector]]></title><description><![CDATA[ 
 <br>k-Vector
Linear combination of only -<a data-tooltip-position="top" aria-label="Basis Blades" data-href="Basis Blades" href="the-guide/mathematics/geometric-algebra/basis-blades.html" class="internal-link" target="_self" rel="noopener nofollow">blades</a>.
<br>Multivector
Also called Clifford Number, element of the <a data-tooltip-position="top" aria-label="Exterior Algebra" data-href="Exterior Algebra" href="the-guide/mathematics/geometric-algebra/exterior-algebra.html" class="internal-link" target="_self" rel="noopener nofollow">exterior algebra</a> of a <a data-tooltip-position="top" aria-label="Vector Space" data-href="Vector Space" href="the-guide/mathematics/general-stuff/vector-space.html" class="internal-link" target="_self" rel="noopener nofollow">vector space</a> that consists of linear combinations of arbitrary k-Vectors or <a data-tooltip-position="top" aria-label="Basis Blades" data-href="Basis Blades" href="the-guide/mathematics/geometric-algebra/basis-blades.html" class="internal-link" target="_self" rel="noopener nofollow">blades</a>.
<br>
<br>Examples

<br><a data-tooltip-position="top" aria-label="Vector Space" data-href="Vector Space" href="the-guide/mathematics/general-stuff/vector-space.html" class="internal-link" target="_self" rel="noopener nofollow">Vector</a> is one-dimensional subspace with orientation and magnitude
<br>Bivector is two-dimensional subspace with orientation and magnitude
<br>...


<br><br><br>Inverse
The inverse of a multivector  is defined via Therefore, it is only defined for <a data-tooltip-position="top" aria-label="Basis Blades" data-href="Basis Blades" href="the-guide/mathematics/geometric-algebra/basis-blades.html" class="internal-link" target="_self" rel="noopener nofollow">blades</a> that have euclidean components of the <a data-tooltip-position="top" aria-label="Geometric Algebra" data-href="Geometric Algebra" href="the-guide/mathematics/geometric-algebra/geometric-algebra.html" class="internal-link" target="_self" rel="noopener nofollow">signature</a> (). 
<br>
<br>Example

<br>Inverse of a vector  is 
<br>For higher order <a data-tooltip-position="top" aria-label="Multivector" data-href="Multivector" href="the-guide/mathematics/geometric-algebra/multivector.html" class="internal-link" target="_self" rel="noopener nofollow">multivectors</a>, you can express the inverse using the <a data-tooltip-position="top" aria-label="Reverse" data-href="Reverse" href="the-guide/mathematics/geometric-algebra/operations/reverse.html" class="internal-link" target="_self" rel="noopener nofollow">reverse</a> via 


<br><br><br>Magnitude of a Multivector
The magnitude of a multivector is defined using the <a data-href="Grade Projection Operator" href="the-guide/mathematics/geometric-algebra/operations/grade-projection-operator.html" class="internal-link" target="_self" rel="noopener nofollow">Grade Projection Operator</a> via 
]]></description><link>the-guide/mathematics/geometric-algebra/multivector.html</link><guid isPermaLink="false">The Guide/Mathematics/Geometric Algebra/Multivector.md</guid><pubDate>Mon, 24 Feb 2025 23:32:25 GMT</pubDate></item><item><title><![CDATA[Outermorphism]]></title><description><![CDATA[ 
 <br>In <a data-href="Geometric Algebra" href="the-guide/mathematics/geometric-algebra/geometric-algebra.html" class="internal-link" target="_self" rel="noopener nofollow">Geometric Algebra</a>, an outermorphism is the extension of a linear <a data-tooltip-position="top" aria-label="Mathematical Relations" data-href="Mathematical Relations" href="the-guide/mathematics/general-stuff/mathematical-relations.html" class="internal-link" target="_self" rel="noopener nofollow">function</a>  between <a data-tooltip-position="top" aria-label="Vector Space" data-href="Vector Space" href="the-guide/mathematics/general-stuff/vector-space.html" class="internal-link" target="_self" rel="noopener nofollow">vector spaces</a>  to a linear map  between <a data-tooltip-position="top" aria-label="Exterior Algebra" data-href="Exterior Algebra" href="the-guide/mathematics/geometric-algebra/exterior-algebra.html" class="internal-link" target="_self" rel="noopener nofollow">exterior algebras</a> on these spaces. Therefore, it has to fulfill<br>
<br>
<br>
<br>
<br>It is therefore grade-preserving.<br><br><a rel="noopener nofollow" class="external-link" href="https://en.wikipedia.org/wiki/Outermorphism" target="_blank">https://en.wikipedia.org/wiki/Outermorphism</a>]]></description><link>the-guide/mathematics/geometric-algebra/outermorphism.html</link><guid isPermaLink="false">The Guide/Mathematics/Geometric Algebra/Outermorphism.md</guid><pubDate>Mon, 24 Feb 2025 23:32:25 GMT</pubDate></item><item><title><![CDATA[Pseudoscalar]]></title><description><![CDATA[ 
 <br>Definition
Oriented volume element of a <a data-tooltip-position="top" aria-label="Geometric Algebra" data-href="Geometric Algebra" href="the-guide/mathematics/geometric-algebra/geometric-algebra.html" class="internal-link" target="_self" rel="noopener nofollow">Clifford Algebra</a>, formed by the <a data-tooltip-position="top" aria-label="Geometric Product" data-href="Geometric Product" href="the-guide/mathematics/geometric-algebra/operations/geometric-product.html" class="internal-link" target="_self" rel="noopener nofollow">product</a> of all  elements of the basis gives information about the orientation of the whole space described by the algebra. 
<br> The square of the pseudoscalar is given by and the dimension of  (underlying <a data-tooltip-position="top" aria-label="Vector Space" data-href="Vector Space" href="the-guide/mathematics/general-stuff/vector-space.html" class="internal-link" target="_self" rel="noopener nofollow">vector space</a> with sym. bilinear form). For  we denote , we have that If the squared pseudoscalar is zero, the underlying linear form is degenerate (non-zero elements can lead to zero) and we also call the pseudoscalar degenerate.<br><br><br>
<br>The pseudoscalar of the two-dimensional euclidean geometric algebra is  and its square is .
]]></description><link>the-guide/mathematics/geometric-algebra/pseudoscalar.html</link><guid isPermaLink="false">The Guide/Mathematics/Geometric Algebra/Pseudoscalar.md</guid><pubDate>Mon, 24 Feb 2025 23:32:25 GMT</pubDate></item><item><title><![CDATA[Versor]]></title><description><![CDATA[ 
 <br>Definition
A -versor is a <a data-tooltip-position="top" aria-label="Multivector" data-href="Multivector" href="the-guide/mathematics/geometric-algebra/multivector.html" class="internal-link" target="_self" rel="noopener nofollow">multivector</a> that can be expressed as the <a data-tooltip-position="top" aria-label="Geometric Product" data-href="Geometric Product" href="the-guide/mathematics/geometric-algebra/operations/geometric-product.html" class="internal-link" target="_self" rel="noopener nofollow">geometric product</a> of  invertible vectors.
<br>
<br>Versors with length one are called unit versors
<br><br><br>In -dimensional space, compositions of reflections construct a <a data-tooltip-position="top" aria-label="Group" data-href="Group" href="the-guide/mathematics/group-theory/group.html" class="internal-link" target="_self" rel="noopener nofollow">group</a>. Any element of the group can be written as a combination of  linearly independent reflections realized by unit <a data-tooltip-position="top" aria-label="Versor" data-href="Versor" href="the-guide/mathematics/geometric-algebra/versor.html" class="internal-link" target="_self" rel="noopener nofollow">versors</a> . For two group elements  and , where  is a -reflection and  is an -reflection, the group action of  on  is defined as <br>
<br>E.g. ,  is a simple reflection, reversing orientation
<br><a data-tooltip-position="top" aria-label="Geometric Product" data-href="Geometric Product" href="the-guide/mathematics/geometric-algebra/operations/geometric-product.html" class="internal-link" target="_self" rel="noopener nofollow">Geometric product</a> of two reflections / <a data-tooltip-position="top" aria-label="Basis Blades" data-href="Basis Blades" href="the-guide/mathematics/geometric-algebra/basis-blades.html" class="internal-link" target="_self" rel="noopener nofollow">1-Blades</a> yields a <a data-tooltip-position="top" aria-label="Multivector" data-href="Multivector" href="the-guide/mathematics/geometric-algebra/multivector.html" class="internal-link" target="_self" rel="noopener nofollow">bivector</a>, encoding rotations.
<br> and  parametrize the same isometries (distance preserving transformations), because signs in group action cancel. Therefore, Pin() is the double-cover of  (2-to-1 map) 
<br><br><br>Excluding improper <a data-tooltip-position="top" aria-label="Mathematical Relations" data-href="Mathematical Relations" href="the-guide/mathematics/general-stuff/mathematical-relations.html" class="internal-link" target="_self" rel="noopener nofollow">isometries</a> (those who change orientation, only even number of unit vectors), we get the Spin()  Pin() group, the double cover of the special orthogonal group . <br>
<br>All elements fulfill .
<br>In this case the <a data-tooltip-position="top" aria-label="Multivector" data-href="Multivector" href="the-guide/mathematics/geometric-algebra/multivector.html" class="internal-link" target="_self" rel="noopener nofollow">inverse of a multivector</a>  can be written via the <a data-tooltip-position="top" aria-label="Reverse" data-href="Reverse" href="the-guide/mathematics/geometric-algebra/operations/reverse.html" class="internal-link" target="_self" rel="noopener nofollow">reverse</a>  and the elements are denoted Rotors, which denote rotations around the origin
]]></description><link>the-guide/mathematics/geometric-algebra/versor.html</link><guid isPermaLink="false">The Guide/Mathematics/Geometric Algebra/Versor.md</guid><pubDate>Mon, 09 Sep 2024 15:35:53 GMT</pubDate></item><item><title><![CDATA[Adjacency Matrix]]></title><description><![CDATA[ 
 <br>Defines a diffusion operation, where the -th entry of the product  is a weighted sum of the entries in x corresponding to neighbors on the <a data-tooltip-position="top" aria-label="Graph" data-href="Graph" href="the-guide/mathematics/graph-theory/graph.html" class="internal-link" target="_self" rel="noopener nofollow">graph</a>.<br>Definition
<a data-tooltip-position="top" aria-label="Matrices" data-href="Matrices" href="the-guide/mathematics/linear-algebra/matrices.html" class="internal-link" target="_self" rel="noopener nofollow">Matrix</a> , where entry  is equal to the weight of edge from node  to node .

<br>Row corresponds to incoming edges for each <a data-tooltip-position="top" aria-label="Node or Vertex Set" data-href="Node or Vertex Set" href="the-guide/mathematics/graph-theory/node-or-vertex-set.html" class="internal-link" target="_self" rel="noopener nofollow">node</a>.
<br>Column corresponds to outgoing edge for each <a data-tooltip-position="top" aria-label="Node or Vertex Set" data-href="Node or Vertex Set" href="the-guide/mathematics/graph-theory/node-or-vertex-set.html" class="internal-link" target="_self" rel="noopener nofollow">node</a>.

<br><br><br>
<br>In a weighted graph the entries can be interpreted as "From column to row"
<br>In an <a data-tooltip-position="top" aria-label="Graph" data-href="Graph" href="the-guide/mathematics/graph-theory/graph.html" class="internal-link" target="_self" rel="noopener nofollow">unweighted graph</a>   represents the number of paths of length  from  to . 

<br>Graph has no self-loops if .
<br>Direction of path only matters for directed graph.


<br>For an <a data-tooltip-position="top" aria-label="Graph" data-href="Graph" href="the-guide/mathematics/graph-theory/graph.html" class="internal-link" target="_self" rel="noopener nofollow">undirected graph</a>  is 

<br>Symmetric
<br>All eigenvalues are real
<br>Diagonalizable
<br>Eigenvectors are orthogonal


<br>The number of distinct eigenvalues  of the matrix  upper bounds the <a data-tooltip-position="top" aria-label="Graph" data-href="Graph" href="the-guide/mathematics/graph-theory/graph.html" class="internal-link" target="_self" rel="noopener nofollow">diameter</a> of the graph

<br>


<br>Change in node labeling can be written as a product with permutation matrices switching the rows and columns

<br>


<br><a data-tooltip-position="top" aria-label="Graph" data-href="Graph" href="the-guide/mathematics/graph-theory/graph.html" class="internal-link" target="_self" rel="noopener nofollow">Chromatic Number</a> of graph is bounded by  with  being the largest (positive) eigenvalue of .
<br>If graph is connected all entries of <a data-tooltip-position="top" aria-label="Eigenvalue Problem" data-href="Eigenvalue Problem" href="the-guide/mathematics/linear-algebra/eigenvalue-problem.html" class="internal-link" target="_self" rel="noopener nofollow">eigenvector</a> to largest eigenvalue of  will be positive (<a data-href="Perron-Frobenius Theorem" href="the-guide/mathematics/linear-algebra/perron-frobenius-theorem.html" class="internal-link" target="_self" rel="noopener nofollow">Perron-Frobenius Theorem</a>)
<br><br><br>This form is maximized for very smooth signals, which implies reverse ordering when compared to the quadratic forms using the <a data-tooltip-position="top" aria-label="Graph Laplacians" data-href="Graph Laplacians" href="the-guide/mathematics/graph-theory/graph-laplacians.html" class="internal-link" target="_self" rel="noopener nofollow">normalized Graph Laplacian</a> or the <a data-tooltip-position="top" aria-label="Graph Laplacians" data-href="Graph Laplacians" href="the-guide/mathematics/graph-theory/graph-laplacians.html" class="internal-link" target="_self" rel="noopener nofollow">Combinatorial Graph Laplacian</a> as operators. <br><br><br>--- start-multi-column: Graph1<br><br>Undirected Graph <br>--- end-column ---<br><img alt="200|center" src="lib/media/pasted-image-20230320113311.png"><br>--- end-multi-column<br>
<br><br>
--- start-multi-column: Graph2<br><br>Directed Graph <br>--- end-column ---<br><img alt="200|center" src="lib/media/pasted-image-20230320114012.png"><br>--- end-multi-column<br>
<br><br>
--- start-multi-column: Graph3<br><br>Block Structure for disconnected graphs<br>--- end-column ---<br><img alt="Pasted image 20221223162903.png" src="lib/media/pasted-image-20221223162903.png"><br>--- end-multi-column<br>
<br><br>
--- start-multi-column: Graph4<br><br>Zero-Block Structure for <a data-tooltip-position="top" aria-label="Graph Coloring" data-href="Graph Coloring" href="the-guide/mathematics/graph-theory/graph-coloring.html" class="internal-link" target="_self" rel="noopener nofollow">colorable graphs</a><br>--- end-column ---<br><img alt="Pasted image 20221223163854.png" src="lib/media/pasted-image-20221223163854.png"><br>--- end-multi-column]]></description><link>the-guide/mathematics/graph-theory/adjacency-matrix.html</link><guid isPermaLink="false">The Guide/Mathematics/Graph Theory/Adjacency Matrix.md</guid><pubDate>Sat, 29 Mar 2025 16:19:05 GMT</pubDate><enclosure url="lib/media/pasted-image-20230320113311.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src=&quot;lib/media/pasted-image-20230320113311.png&quot;&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[Clustering and Graph Cuts]]></title><description><![CDATA[ 
 <br>Question
How to optimally choose clusters / partitions  of all the <a data-tooltip-position="top" aria-label="Node or Vertex Set" data-href="Node or Vertex Set" href="the-guide/mathematics/graph-theory/node-or-vertex-set.html" class="internal-link" target="_self" rel="noopener nofollow">nodes</a> of a <a data-tooltip-position="top" aria-label="Graph" data-href="Graph" href="the-guide/mathematics/graph-theory/graph.html" class="internal-link" target="_self" rel="noopener nofollow">graph</a>?
<br>
<br>Assign labels  to each node to indicate to which cluster  they belong.
<br><br><br>
<br>Cost of labeling between two nodes with indicator function 	- Therefore, cost is high if similiar nodes get assigned different labels.
<br><br><br>
<br>For , the -class clustering problem consists of selecting labels  for each node  in a <a data-tooltip-position="top" aria-label="Graph" data-href="Graph" href="the-guide/mathematics/graph-theory/graph.html" class="internal-link" target="_self" rel="noopener nofollow">graph</a> in such a way as to minimize the costwhere the summation is over all pairs of connected nodes (denoted by ).<br>
<br><br>
<img alt="center" src="lib/media/pasted-image-20230216090633.png" style="width: 300px; max-width: 100%;">
<br><br><br>
<br>A graph cut is a set of edges  in a connected graph  such that:

<br> is disconneted if all edges in  are removed.
<br>After removing all edges of any set  such that  ,  remains connected.


<br>A cut is considered minimal/maximal, if<br>
- The weights of the edges between the nodes of the two resulting sets are minimal/maximal.<br>
<img alt="center" src="lib/media/pasted-image-20230216090754.png" style="width: 300px; max-width: 100%;">
]]></description><link>the-guide/mathematics/graph-theory/clustering-and-graph-cuts.html</link><guid isPermaLink="false">The Guide/Mathematics/Graph Theory/Clustering and Graph Cuts.md</guid><pubDate>Sat, 29 Mar 2025 16:19:05 GMT</pubDate><enclosure url="lib/media/pasted-image-20230216090633.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src=&quot;lib/media/pasted-image-20230216090633.png&quot;&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[Complement Graph]]></title><description><![CDATA[ 
 <br>The complement of an <a data-tooltip-position="top" aria-label="Graph" data-href="Graph" href="the-guide/mathematics/graph-theory/graph.html" class="internal-link" target="_self" rel="noopener nofollow">unweighted graph</a>  with-out self-loops is a graph  with same <a data-tooltip-position="top" aria-label="Node or Vertex Set" data-href="Node or Vertex Set" href="the-guide/mathematics/graph-theory/node-or-vertex-set.html" class="internal-link" target="_self" rel="noopener nofollow">node set</a>  but with edge set  , the complement of  in the set of possible edges, i.e., an edge  if and only if .]]></description><link>the-guide/mathematics/graph-theory/complement-graph.html</link><guid isPermaLink="false">The Guide/Mathematics/Graph Theory/Complement Graph.md</guid><pubDate>Sat, 29 Mar 2025 16:19:05 GMT</pubDate></item><item><title><![CDATA[Degree Matrix]]></title><description><![CDATA[<a class="tag" href="?query=tag:GSP-Course" style="background-color: rgb(4, 108, 116); color: white; font-weight: 700; border: none; border-radius: 1em; padding: 0.2em 0.5em;">#GSP-Course</a> 
 <br>Definition
Diagonal <a data-tooltip-position="top" aria-label="Matrices" data-href="Matrices" href="the-guide/mathematics/linear-algebra/matrices.html" class="internal-link" target="_self" rel="noopener nofollow">matrix</a> corresponding to a <a data-tooltip-position="top" aria-label="Graph" data-href="Graph" href="the-guide/mathematics/graph-theory/graph.html" class="internal-link" target="_self" rel="noopener nofollow">graph</a>, where each diagonal entry represents the [[Vertex Set|degree]] of the corresponding node. For undirected graphs, it is related to the <a data-tooltip-position="top" aria-label="Adjacency Matrix" data-href="Adjacency Matrix" href="the-guide/mathematics/graph-theory/adjacency-matrix.html" class="internal-link" target="_self" rel="noopener nofollow">adjacency matrix</a> via In the directed case, we differentiate between

<br>Out-Degree Matrix
<br>In-Degree Matrix

<br>
<br> defines a scaling of a <a data-tooltip-position="top" aria-label="Graph Signal" data-href="Graph Signal" href="the-guide/integral-transforms-and-signals/graph-signal.html" class="internal-link" target="_self" rel="noopener nofollow">graph signal's</a> values at each <a data-tooltip-position="top" aria-label="Node or Vertex Set" data-href="Node or Vertex Set" href="the-guide/mathematics/graph-theory/node-or-vertex-set.html" class="internal-link" target="_self" rel="noopener nofollow">node</a>.
<br><a href=".?query=tag:GSP-Course" class="tag" target="_blank" rel="noopener nofollow">#GSP-Course</a>]]></description><link>the-guide/mathematics/graph-theory/degree-matrix.html</link><guid isPermaLink="false">The Guide/Mathematics/Graph Theory/Degree Matrix.md</guid><pubDate>Sat, 29 Mar 2025 16:19:05 GMT</pubDate></item><item><title><![CDATA[Geodesic Distance]]></title><description><![CDATA[<a class="tag" href="?query=tag:GSP-Course" style="background-color: rgb(4, 108, 116); color: white; font-weight: 700; border: none; border-radius: 1em; padding: 0.2em 0.5em;">#GSP-Course</a> 
 <br>Definition
Let  be a path connecting the <a data-tooltip-position="top" aria-label="Node or Vertex Set" data-href="Node or Vertex Set" href="the-guide/mathematics/graph-theory/node-or-vertex-set.html" class="internal-link" target="_self" rel="noopener nofollow">nodes</a>  and  of a <a data-tooltip-position="top" aria-label="Graph" data-href="Graph" href="the-guide/mathematics/graph-theory/graph.html" class="internal-link" target="_self" rel="noopener nofollow">graph</a>. Then the geodesic distance between  and  is the minimum over all possible paths:
<br><br>
<br>In an unweighted graph, the geodesic distance is the shortest path in number of hops between the two nodes
<br>In <a data-tooltip-position="top" aria-label="Graph" data-href="Graph" href="the-guide/mathematics/graph-theory/graph.html" class="internal-link" target="_self" rel="noopener nofollow">directed graphs</a> the geodesic distance is not reciprocal, as the weights do not have to be equal<br>
<a href=".?query=tag:GSP-Course" class="tag" target="_blank" rel="noopener nofollow">#GSP-Course</a>
]]></description><link>the-guide/mathematics/graph-theory/geodesic-distance.html</link><guid isPermaLink="false">The Guide/Mathematics/Graph Theory/Geodesic Distance.md</guid><pubDate>Sat, 29 Mar 2025 16:19:05 GMT</pubDate></item><item><title><![CDATA[Graph]]></title><description><![CDATA[ 
 <br>Definition 
Mathematical representation of a network. A graph  is defined by

<br>A set of <a data-tooltip-position="top" aria-label="Node or Vertex Set" data-href="Node or Vertex Set" href="the-guide/mathematics/graph-theory/node-or-vertex-set.html" class="internal-link" target="_self" rel="noopener nofollow">nodes</a> or vertices 
<br>A set of edges or links 
<br>Edge weights 

<br><img alt="center" src="lib/media/pasted-image-20240222191016.png" style="width: 200px; max-width: 100%;"><br><br><br>The following covers global properties of the graph, for a nodes see <a data-href="Node or Vertex Set" href="the-guide/mathematics/graph-theory/node-or-vertex-set.html" class="internal-link" target="_self" rel="noopener nofollow">Node or Vertex Set</a>.<br>
<br>Edge Weigths

<br>Weighted if 

<br>Distance Graph - Larger weigths corrresponds to nodes that are farther apart. 
<br>Similiarity Graph - Larger weigths correspond to nodes that exhibit greater similiarity or smaller distances.


<br>Unweighted if 


<br>Direction of the edges

<br>Undirected Graph - Connections are always mutual.
<br>Directed Graph - Connections have start and end node


<br>Density

<br>Dense, if <a data-tooltip-position="top" aria-label="Node or Vertex Set" data-href="Node or Vertex Set" href="the-guide/mathematics/graph-theory/node-or-vertex-set.html" class="internal-link" target="_self" rel="noopener nofollow">degree</a> of most nodes is close to . 
<br>Sparse, if <a data-tooltip-position="top" aria-label="Node or Vertex Set" data-href="Node or Vertex Set" href="the-guide/mathematics/graph-theory/node-or-vertex-set.html" class="internal-link" target="_self" rel="noopener nofollow">degree</a> of most nodes is much smaller than .


<br>Connectivity

<br>Connected - Any node in an undirected graph can be reached from every other node. 
<br>Strongly connected - Any node in a directed graph can be reached from every other node.
<br>Weakly connected - Corresponding undirected graph is connected.
<br>Regular - Every <a data-tooltip-position="top" aria-label="Node or Vertex Set" data-href="Node or Vertex Set" href="the-guide/mathematics/graph-theory/node-or-vertex-set.html" class="internal-link" target="_self" rel="noopener nofollow">node</a> has same number of edges.


<br>Distances ( is <a data-tooltip-position="top" aria-label="Geodesic Distance" data-href="Geodesic Distance" href="the-guide/mathematics/graph-theory/geodesic-distance.html" class="internal-link" target="_self" rel="noopener nofollow">distance</a> from node i to node farthest from it)

<br>Radius 

<br>Smallest largest distance in a network.


<br>Diameter 

<br>Largest distance of any two nodes in the network.




<br>Chromatic Number

<br>Smallest number of colors needed to <a data-tooltip-position="top" aria-label="Graph Coloring" data-href="Graph Coloring" href="the-guide/mathematics/graph-theory/graph-coloring.html" class="internal-link" target="_self" rel="noopener nofollow">color the graph</a>


<br><br><br>
<br>Radius and Diameter are equal in a regular graph.
<br>...
<br><br><br>Tree Graphs<br>
Graphs that do not have any cycles.<br>Path Graph <img alt="center" src="lib/media/pasted-image-20221214100626.png" style="width: 300px; max-width: 100%;"><br>
Cycle Graph<img alt="center" src="lib/media/pasted-image-20221214100644.png" style="width: 150px; max-width: 100%;"><br>
Molecules as Graphs<br>
<img alt="Screenshot 2025-04-06 at 21-22-52 A Gentle Introduction to Graph Neural Networks.png" src="lib/media/screenshot-2025-04-06-at-21-22-52-a-gentle-introduction-to-graph-neural-networks.png">]]></description><link>the-guide/mathematics/graph-theory/graph.html</link><guid isPermaLink="false">The Guide/Mathematics/Graph Theory/Graph.md</guid><pubDate>Sun, 06 Apr 2025 20:02:57 GMT</pubDate><enclosure url="lib/media/pasted-image-20240222191016.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src=&quot;lib/media/pasted-image-20240222191016.png&quot;&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[Graph Coloring]]></title><description><![CDATA[<a class="tag" href="?query=tag:GSP-Course" style="background-color: rgb(4, 108, 116); color: white; font-weight: 700; border: none; border-radius: 1em; padding: 0.2em 0.5em;">#GSP-Course</a> 
 <br>k-Colorable Graph
A <a data-tooltip-position="top" aria-label="Graph" data-href="Graph" href="the-guide/mathematics/graph-theory/graph.html" class="internal-link" target="_self" rel="noopener nofollow">graph</a> that is -colorable or -partite can be described using  <a data-tooltip-position="top" aria-label="Set" data-href="Set" href="the-guide/mathematics/general-stuff/set.html" class="internal-link" target="_self" rel="noopener nofollow">sets</a> of <a data-tooltip-position="top" aria-label="Node or Vertex Set" data-href="Node or Vertex Set" href="the-guide/mathematics/graph-theory/node-or-vertex-set.html" class="internal-link" target="_self" rel="noopener nofollow">nodes</a> , where  and .<br>
All edges connect nodes of different colors.
<br><img alt="center" src="lib/media/pasted-image-20221215090221.png" style="width: 300px; max-width: 100%;"><br>
<br>Special Case Bipartite Graph

<br>2-colorable, e.g. path graph or cycle graph with even number of nodes.


<br><br><br>
<br>After removing edges from a graph, the chromatic number (min number of colours) of the modified graph can never increase.
<br>If a k-colorable graph for some pre-specified k is needed, the graph can be modified (keeping all nodes, but removing some edges).
<br>Removing edges can lead to a graph that is very different from the original one.
<br>Goal: Find the best approximation, while requiring the removal of the least number of edges or the removal of edges with the least total weight.
<br><br><br>
<br>Relation to <a data-tooltip-position="top" aria-label="Total Variation" data-href="Total Variation" href="the-guide/integral-transforms-and-signals/total-variation.html" class="internal-link" target="_self" rel="noopener nofollow">Signal Variation</a>

<br>Signal variations within a coloring group do not increase the total variation.


<br>Relation to <a data-href="Graph Sampling" href="the-guide/mathematics/graph-theory/graph-sampling.html" class="internal-link" target="_self" rel="noopener nofollow">Graph Sampling</a>

<br>After coloring a graph, one should sample all nodes with one color, as they are less likely to provide redundant information


<br><a href=".?query=tag:GSP-Course" class="tag" target="_blank" rel="noopener nofollow">#GSP-Course</a>]]></description><link>the-guide/mathematics/graph-theory/graph-coloring.html</link><guid isPermaLink="false">The Guide/Mathematics/Graph Theory/Graph Coloring.md</guid><pubDate>Sat, 29 Mar 2025 16:19:05 GMT</pubDate><enclosure url="lib/media/pasted-image-20221215090221.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src=&quot;lib/media/pasted-image-20221215090221.png&quot;&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[Graph Filters]]></title><description><![CDATA[<a class="tag" href="?query=tag:GSP-Course" style="background-color: rgb(4, 108, 116); color: white; font-weight: 700; border: none; border-radius: 1em; padding: 0.2em 0.5em;">#GSP-Course</a> 
 <br>Similar to filters in signal processing, we can recompute values at graph nodes by damping frequencies.<br><br><br>
<br><a data-tooltip-position="top" aria-label="Matrices" data-href="Matrices" href="the-guide/mathematics/linear-algebra/matrices.html" class="internal-link" target="_self" rel="noopener nofollow">Linear operator</a> , which produces output  from input vector . For a given one-hop <a data-tooltip-position="top" aria-label="Graph Operator" data-href="Graph Operator" href="the-guide/mathematics/graph-theory/graph-operator.html" class="internal-link" target="_self" rel="noopener nofollow">operator</a> , a graph filter can be written as a polynomial 
<br>Properties

<br>Commutativity

<br>


<br>In contrast to delay of z-Transform,  is usually not invertible.


<br><br><br>
<br>Any graph filter (polynomial of <a data-tooltip-position="top" aria-label="Graph Operator" data-href="Graph Operator" href="the-guide/mathematics/graph-theory/graph-operator.html" class="internal-link" target="_self" rel="noopener nofollow">operator</a> ) with <a data-tooltip-position="top" aria-label="Graph Fourier Transform" data-href="Graph Fourier Transform" href="the-guide/mathematics/graph-theory/graph-fourier-transform.html" class="internal-link" target="_self" rel="noopener nofollow">GFT</a>  can be written as 

<br>Frequency representation of input via 
<br>Frequency representation of output via 
<br>Node domain output via 
<br>If an eigenvalue has a multiplicity greater than 1, the diagonal entry is copied.


<br><br><br>
<br>Filter  with desired eigenvalues  viawhere 
<br>Interpolation of polynomial with  and .

<br>Initial eigenvalues are those of the chosen <a data-tooltip-position="top" aria-label="Graph Operator" data-href="Graph Operator" href="the-guide/mathematics/graph-theory/graph-operator.html" class="internal-link" target="_self" rel="noopener nofollow">operator</a>.
<br>Aim is to find polynomial that transforms them to desired values.

<br>Here polynomial degree is 
<br>If min polynomial of <a data-tooltip-position="top" aria-label="Graph Operator" data-href="Graph Operator" href="the-guide/mathematics/graph-theory/graph-operator.html" class="internal-link" target="_self" rel="noopener nofollow">operator</a> is given as , filter can be expressed as polynomial of degree .




<br><br><br>
<br>An ideal bandpass filter  selects a range of frequencies  to  via 
<br> is a lowpass filter that selects the first  frequencies.
<br> is a highpass filter that selects the last  frequencies.
<br><a href=".?query=tag:GSP-Course" class="tag" target="_blank" rel="noopener nofollow">#GSP-Course</a>]]></description><link>the-guide/mathematics/graph-theory/graph-filters.html</link><guid isPermaLink="false">The Guide/Mathematics/Graph Theory/Graph Filters.md</guid><pubDate>Mon, 09 Sep 2024 15:39:16 GMT</pubDate></item><item><title><![CDATA[Graph Fourier Transform]]></title><description><![CDATA[<a class="tag" href="?query=tag:GSP-Course" style="background-color: rgb(4, 108, 116); color: white; font-weight: 700; border: none; border-radius: 1em; padding: 0.2em 0.5em;">#GSP-Course</a> 
 <br>Info
The GFT  of the <a data-tooltip-position="top" aria-label="Graph Signal" data-href="Graph Signal" href="the-guide/integral-transforms-and-signals/graph-signal.html" class="internal-link" target="_self" rel="noopener nofollow">signal</a>  is defined as where the <a data-tooltip-position="top" aria-label="Matrices" data-href="Matrices" href="the-guide/mathematics/linear-algebra/matrices.html" class="internal-link" target="_self" rel="noopener nofollow">matrix</a>  is computed from the <a data-tooltip-position="top" aria-label="Eigenvalue Problem" data-href="Eigenvalue Problem" href="the-guide/mathematics/linear-algebra/eigenvalue-problem.html" class="internal-link" target="_self" rel="noopener nofollow">eigenvalue decomposition</a>   of the used <a data-tooltip-position="top" aria-label="Graph Operator" data-href="Graph Operator" href="the-guide/mathematics/graph-theory/graph-operator.html" class="internal-link" target="_self" rel="noopener nofollow">graph operator</a> . This basic definition therefore assumes the operator to be diagonalizable.
<br>For defective operators there are two possible choices<br>
<br><a data-tooltip-position="top" aria-label="Schur Normal Form" data-href="Schur Normal Form" href="the-guide/mathematics/linear-algebra/schur-normal-form.html" class="internal-link" target="_self" rel="noopener nofollow">Schur Decomposition</a>

<br>The GFT  of the <a data-tooltip-position="top" aria-label="Graph Signal" data-href="Graph Signal" href="the-guide/integral-transforms-and-signals/graph-signal.html" class="internal-link" target="_self" rel="noopener nofollow">signal</a>  is computed via where  ( is upper triangular matrix). 


<br>Jordan Canonical Form

<br>The GFT  of the <a data-tooltip-position="top" aria-label="Graph Signal" data-href="Graph Signal" href="the-guide/integral-transforms-and-signals/graph-signal.html" class="internal-link" target="_self" rel="noopener nofollow">signal</a>  is computed via where  ( is block diagonal matrix). 


<br><br><br>
<br>GFT depends on  and therefore the <a data-tooltip-position="top" aria-label="Graph" data-href="Graph" href="the-guide/mathematics/graph-theory/graph.html" class="internal-link" target="_self" rel="noopener nofollow">graphs</a> topology.
<br>Only unique if all eigenvalues have multiplicity 1.
<br>In an <a data-tooltip-position="top" aria-label="Graph" data-href="Graph" href="the-guide/mathematics/graph-theory/graph.html" class="internal-link" target="_self" rel="noopener nofollow">undirected graph</a>,  is orthogonal (), which means the inversion can be replaced by a transposition.
<br>Computational cost can be reduced by using <a data-tooltip-position="top" aria-label="Graph Learning" data-href="Graph Learning" href="the-guide/mathematics/graph-theory/graph-learning.html" class="internal-link" target="_self" rel="noopener nofollow">graph sparsification techniques</a> or by using <a data-tooltip-position="top" aria-label="Givens Rotation" data-href="Givens Rotation" href="the-guide/mathematics/linear-algebra/givens-rotation.html" class="internal-link" target="_self" rel="noopener nofollow">givens rotations</a> to approximate .
<br><a href=".?query=tag:GSP-Course" class="tag" target="_blank" rel="noopener nofollow">#GSP-Course</a>]]></description><link>the-guide/mathematics/graph-theory/graph-fourier-transform.html</link><guid isPermaLink="false">The Guide/Mathematics/Graph Theory/Graph Fourier Transform.md</guid><pubDate>Mon, 09 Sep 2024 15:39:16 GMT</pubDate></item><item><title><![CDATA[Graph Laplacians]]></title><description><![CDATA[<a class="tag" href="?query=tag:GSP-Course" style="background-color: rgb(4, 108, 116); color: white; font-weight: 700; border: none; border-radius: 1em; padding: 0.2em 0.5em;">#GSP-Course</a> 
 <br><a data-tooltip-position="top" aria-label="Matrices" data-href="Matrices" href="the-guide/mathematics/linear-algebra/matrices.html" class="internal-link" target="_self" rel="noopener nofollow">Matrix</a> representation of a graph that can be viewed as a discrete Laplace operator acting on a graph that approximates the continuous Laplacian via finite difference method. Depending on the desired properties, there exist multiple similar variants:<br><br>Combinatorial Laplacian Matrix 
Defined for an <a data-tooltip-position="top" aria-label="Graph" data-href="Graph" href="the-guide/mathematics/graph-theory/graph.html" class="internal-link" target="_self" rel="noopener nofollow">undirected graph</a>. Can be computed via <a data-tooltip-position="top" aria-label="Incidence Matrix" data-href="Incidence Matrix" href="the-guide/mathematics/graph-theory/incidence-matrix.html" class="internal-link" target="_self" rel="noopener nofollow">incidence matrix</a>  or via <a data-tooltip-position="top" aria-label="Degree Matrix" data-href="Degree Matrix" href="the-guide/mathematics/graph-theory/degree-matrix.html" class="internal-link" target="_self" rel="noopener nofollow">degree matrix</a>  and <a data-tooltip-position="top" aria-label="Adjacency Matrix" data-href="Adjacency Matrix" href="the-guide/mathematics/graph-theory/adjacency-matrix.html" class="internal-link" target="_self" rel="noopener nofollow">adjacency matrix</a>  as 
<br>
<br>
Properties

<br>Positive semi-definite matrix.
<br>According to <a data-href="Gerschgorin Circle Theorem" href="the-guide/mathematics/linear-algebra/gerschgorin-circle-theorem.html" class="internal-link" target="_self" rel="noopener nofollow">Gerschgorin Circle Theorem</a>, it holds that , where  is the maximum degree of any node in the graph.
<br><a data-tooltip-position="top" aria-label="Graph Spectrum" data-href="Graph Spectrum" href="the-guide/mathematics/graph-theory/graph-spectrum.html" class="internal-link" target="_self" rel="noopener nofollow">Removing an edge</a> can be modelled by a difference Laplacian , where .

<br> is maximum degree of 

<br> is edge weight set to zero


<br>Deviation of any eigenvalue of  and  is bounded by 


<br>Multiplicity of zero eigenvalue corresponds to number of connected components

<br><a data-href="Perron-Frobenius Theorem" href="the-guide/mathematics/linear-algebra/perron-frobenius-theorem.html" class="internal-link" target="_self" rel="noopener nofollow">Perron-Frobenius Theorem</a>


<br>Eigenvector of second smallest eigenvalue (if connected nonzero) can be used for partitioning into connected <a data-tooltip-position="top" aria-label="Subgraph" data-href="Subgraph" href="the-guide/mathematics/graph-theory/subgraph.html" class="internal-link" target="_self" rel="noopener nofollow">subgraphs</a>

<br>Use e.g. sign of entries in eigenvector


<br> is always an eigenvector for eigenvalue 0.
<br>For undirected graphs

<br>Symmetric
<br>Their <a data-tooltip-position="top" aria-label="Eigenvalue Problem" data-href="Eigenvalue Problem" href="the-guide/mathematics/linear-algebra/eigenvalue-problem.html" class="internal-link" target="_self" rel="noopener nofollow">eigenvalues</a> are real (they can always be diagonalized)
<br>Eigenvectors are orthogognal.




<br>
Weighted Prediction Error

<br>Computed via .
<br>Extends the <a data-tooltip-position="top" aria-label="Graph Laplacians" data-href="Graph Laplacians" href="the-guide/mathematics/graph-theory/graph-laplacians.html" class="internal-link" target="_self" rel="noopener nofollow">prediction error</a> by giving more weight to nodes with higher <a data-tooltip-position="top" aria-label="Node or Vertex Set" data-href="Node or Vertex Set" href="the-guide/mathematics/graph-theory/node-or-vertex-set.html" class="internal-link" target="_self" rel="noopener nofollow">degree</a>.


<br>
Combinatorial Laplacian <a data-tooltip-position="top" aria-label="Graph Operator" data-href="Graph Operator" href="the-guide/mathematics/graph-theory/graph-operator.html" class="internal-link" target="_self" rel="noopener nofollow">Quadratic Form</a><br>
Quantifies the total <a data-tooltip-position="top" aria-label="Graph Signal" data-href="Graph Signal" href="the-guide/integral-transforms-and-signals/graph-signal.html" class="internal-link" target="_self" rel="noopener nofollow">graph signal</a> variationEach edge contributes to this variation according to their weight. Eigenvectors are sorted such that <a data-tooltip-position="top" aria-label="Total Variation" data-href="Total Variation" href="the-guide/integral-transforms-and-signals/total-variation.html" class="internal-link" target="_self" rel="noopener nofollow">variation</a> increases. The quadratic form can also be written in the <a data-tooltip-position="top" aria-label="Graph Fourier Transform" data-href="Graph Fourier Transform" href="the-guide/mathematics/graph-theory/graph-fourier-transform.html" class="internal-link" target="_self" rel="noopener nofollow">frequency domain</a> using  via

<br>Generalized form  with matrix  of self loops yields 


<br><br>Symmetric Normalized Graph Laplacian Matrix 
Can be computed using <a data-href="Degree Matrix" href="the-guide/mathematics/graph-theory/degree-matrix.html" class="internal-link" target="_self" rel="noopener nofollow">Degree Matrix</a> and <a data-href="Adjacency Matrix" href="the-guide/mathematics/graph-theory/adjacency-matrix.html" class="internal-link" target="_self" rel="noopener nofollow">Adjacency Matrix</a> via 
<br>
<br>
Properties

<br>Multiplicity of zero eigenvalue corresponds to number of connected components

<br><a data-href="Perron-Frobenius Theorem" href="the-guide/mathematics/linear-algebra/perron-frobenius-theorem.html" class="internal-link" target="_self" rel="noopener nofollow">Perron-Frobenius Theorem</a>


<br>Eigenvector of second smallest eigenvalue can be used for partitioning into connected <a data-tooltip-position="top" aria-label="Subgraph" data-href="Subgraph" href="the-guide/mathematics/graph-theory/subgraph.html" class="internal-link" target="_self" rel="noopener nofollow">subgraphs</a>
<br> is always an eigenvector for eigenvalue 0.
<br>Generally 
<br>For undirected graphs

<br>Symmetric
<br>Their <a data-tooltip-position="top" aria-label="Eigenvalue Problem" data-href="Eigenvalue Problem" href="the-guide/mathematics/linear-algebra/eigenvalue-problem.html" class="internal-link" target="_self" rel="noopener nofollow">eigenvalues</a> are real (they can always be diagonalized)
<br>Eigenvectors are orthogognal.




<br>
Prediction Error with normalized Path Weigths

<br>Does not depend on size of neighborhood, but path weigths have been normalized

<br>




<br>
Normalized Laplacian <a data-tooltip-position="top" aria-label="Graph Operator" data-href="Graph Operator" href="the-guide/mathematics/graph-theory/graph-operator.html" class="internal-link" target="_self" rel="noopener nofollow">Quadratic Form</a><br>


<br><br>Random Walk Laplacian Matrix 
Can be computeed from <a data-href="Random Walk Matrix" href="the-guide/mathematics/graph-theory/random-walk-matrix.html" class="internal-link" target="_self" rel="noopener nofollow">Random Walk Matrix</a>  via 
<br>
<br>
Properties

<br>Not symmetric
<br>Guaranteed to have real eigenvalues
<br> is an eigenvector


<br>
Prediction Error

<br> can be interpreted as an error that is small, if the value at a node is close to the average of its neighbors.
<br>Can be extended to the <a data-tooltip-position="top" aria-label="Graph Laplacians" data-href="Graph Laplacians" href="the-guide/mathematics/graph-theory/graph-laplacians.html" class="internal-link" target="_self" rel="noopener nofollow">weighted prediction error</a> by granting nodes with a larger k-hop neighborhood more importance.


<br><a href=".?query=tag:GSP-Course" class="tag" target="_blank" rel="noopener nofollow">#GSP-Course</a>]]></description><link>the-guide/mathematics/graph-theory/graph-laplacians.html</link><guid isPermaLink="false">The Guide/Mathematics/Graph Theory/Graph Laplacians.md</guid><pubDate>Sat, 29 Mar 2025 16:19:05 GMT</pubDate></item><item><title><![CDATA[Graph Learning]]></title><description><![CDATA[<a class="tag" href="?query=tag:GSP-Course" style="background-color: rgb(4, 108, 116); color: white; font-weight: 700; border: none; border-radius: 1em; padding: 0.2em 0.5em;">#GSP-Course</a> 
 <br><img alt="center" src="lib/media/screenshot-from-2023-01-29-16-03-45.png" style="width: 500px; max-width: 100%;"><br>
<br> 

<br>Modify given <a data-tooltip-position="top" aria-label="Graph" data-href="Graph" href="the-guide/mathematics/graph-theory/graph.html" class="internal-link" target="_self" rel="noopener nofollow">graph</a> in order to get a "similiar" graph with more desirable properties.
<br>

<br>Reduce number of edges to have less computational cost while preserving desired properties as much as possible.

<br>E.g. to reduce cost of <a data-tooltip-position="top" aria-label="Graph Fourier Transform" data-href="Graph Fourier Transform" href="the-guide/mathematics/graph-theory/graph-fourier-transform.html" class="internal-link" target="_self" rel="noopener nofollow">GFT</a>


<br>Increases <a data-tooltip-position="top" aria-label="Graph Filters" data-href="Graph Filters" href="the-guide/mathematics/graph-theory/graph-filters.html" class="internal-link" target="_self" rel="noopener nofollow">filter locality</a> and <a data-tooltip-position="top" aria-label="Geodesic Distance" data-href="Geodesic Distance" href="the-guide/mathematics/graph-theory/geodesic-distance.html" class="internal-link" target="_self" rel="noopener nofollow">geodesic distance</a>

<br>Leads to less computational intensity for <a data-tooltip-position="top" aria-label="Graph Filters" data-href="Graph Filters" href="the-guide/mathematics/graph-theory/graph-filters.html" class="internal-link" target="_self" rel="noopener nofollow">filtering</a>.


<br>K-Nearest-Neighbour (K-NN)

<br>Decision made per node
<br>For each node keep  neighbour connections with highest weight.

<br>Preserves local topology
<br>Avoids disconnected graph


<br>Does not aim to preserve <a data-tooltip-position="top" aria-label="Graph Spectrum" data-href="Graph Spectrum" href="the-guide/mathematics/graph-theory/graph-spectrum.html" class="internal-link" target="_self" rel="noopener nofollow">spectral properties</a>
<br>Does not take into account if chosen nodes are redundant
<br>Local Linear Embedding (LLE)

<br>After K-NN search compute matrix  for each node  

<br>Columns are attribute vectors of  closest nodes


<br>Compute vector  of new edge weights satisfying

<br>


<br>If two nodes are very similiar one of their weights might be chosen 0 or weight is split 




<br>Spectral Sparsification

<br>Decision made per edge
<br>Keeping edge according to some probability, e.g. 

<br>Effective Resistance  between two nodes

<br>E.g. direct path and one path through node  leads to 
<br>Decreases if there are more and more paths between to nodes.
<br>If there are multiple paths same logic as electric circuits
<br>Lower resistance if higher similiarity








<br>**

<br>Select subset of nodes and create smaller graph, taking connectivity of original graph into account.

<br>Kron Reduction

<br>Sort Laplacian according to selected subset and its complement and create new Laplacian using the Schur complement 

<br>Graph needs to be connected in order for  to be invertible


<br>Consider any two nodes  and , where node  has been removed

<br>
<br>
<br>If  and  were connected and both were connected to removed node, the weight of their edge willl change (not if only one was connected).
<br>If both were connected to , but not with each other , a new connection will be created.


<br>The more nodes are removed, the denser becomes the graph

<br>Often sparsification applied afterwards










<br><br>
- From node attributes<br>
- <br>
- Create a topology in order to reflect the relation between <a data-tooltip-position="top" aria-label="Node or Vertex Set" data-href="Node or Vertex Set" href="the-guide/mathematics/graph-theory/node-or-vertex-set.html" class="internal-link" target="_self" rel="noopener nofollow">nodes</a> regarding a certain attribute.<br>
- Pairwise distance <br>
- Selecting a large weight for nearby nodes<br>
- Gaussian Kernel<br>
- Define edge weights as <br>
- If  close to zero, edge weight is close to 1<br>
- If  increases, weight decreases<br>
- All nodes are connected<br>
- Parameter Selection<br>
- If  is much larger than  many edges will be close to 1<br>
- If  is much smaller than  most weights will be vanishingly small<br>
- Use statistics of local distances, e.g. average distance<br>
- Might also lead to redundancy<br>
- Non-Negative Kernel Regression (NNK)<br>
- Hyperplane passing through attribute vector of close nodes and orthogonal to attribute vector of inspected node <br>
- Additional nodes are only added if inside the subspace containing , further enclosing this space<br>
- Cosine Similiarity<br>
- Often for data with discrete attributes, e.g. <br>
- Define edge weight <br>
- Nodes closer connected if more entries in  of both nodes are similiar<br>
- Rarely leads to sparse graph<br>
- From <a data-tooltip-position="top" aria-label="Graph Signal" data-href="Graph Signal" href="the-guide/integral-transforms-and-signals/graph-signal.html" class="internal-link" target="_self" rel="noopener nofollow">graph signal</a> observations<br>
- Given  graph signals  in training set <br>
- Goal<br>
- Select  node graph , such that signals have desirable properties, e.g. smoothness<br>
- Empirical Covariance<br>
- Construct graph (here: find Laplacian), such that signals are similiar to low-frequency eigenvectors<br>
- Matching a signal<br>
- ..<br>
- Empirical <a data-tooltip-position="top" aria-label="Covariance and Variance" data-href="Covariance and Variance" href="the-guide/mathematics/statistics/covariance-and-variance.html" class="internal-link" target="_self" rel="noopener nofollow">Covariance and Variance</a> <a data-tooltip-position="top" aria-label="Matrices" data-href="Matrices" href="the-guide/mathematics/linear-algebra/matrices.html" class="internal-link" target="_self" rel="noopener nofollow">Matrix</a><br>
- , sum of projection matrices<br>
- If  maximizes  it is the eigenvector of maximum eigenvalue<br>
- If  invertible, the same vector then minimizes <br>
- Consider precision matrix  as estimate for <a data-tooltip-position="top" aria-label="Graph Laplacians" data-href="Graph Laplacians" href="the-guide/mathematics/graph-theory/graph-laplacians.html" class="internal-link" target="_self" rel="noopener nofollow">Laplacian</a><br>
- Covariance Selection <br>
- Second term ensures eigenvalues do not become too small, enforces connectivity<br>
- Requires knowledge of edge set  !<br>
- Graphical Lasso** <br>
- Third term enforces sparsity, penalizes non-zero entries<br>
- Large  makes result more sparse<br>
<a href=".?query=tag:GSP-Course" class="tag" target="_blank" rel="noopener nofollow">#GSP-Course</a>
]]></description><link>the-guide/mathematics/graph-theory/graph-learning.html</link><guid isPermaLink="false">The Guide/Mathematics/Graph Theory/Graph Learning.md</guid><pubDate>Sat, 29 Mar 2025 16:19:05 GMT</pubDate><enclosure url="lib/media/screenshot-from-2023-01-29-16-03-45.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src=&quot;lib/media/screenshot-from-2023-01-29-16-03-45.png&quot;&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[Graph Operator]]></title><description><![CDATA[<a class="tag" href="?query=tag:GSP-Course" style="background-color: rgb(4, 108, 116); color: white; font-weight: 700; border: none; border-radius: 1em; padding: 0.2em 0.5em;">#GSP-Course</a> 
 <br><br>Function defined for an <a data-tooltip-position="top" aria-label="Graph Operator" data-href="Graph Operator" href="the-guide/mathematics/graph-theory/graph-operator.html" class="internal-link" target="_self" rel="noopener nofollow">operator</a> and associated with any <a data-tooltip-position="top" aria-label="Graph Signal" data-href="Graph Signal" href="the-guide/integral-transforms-and-signals/graph-signal.html" class="internal-link" target="_self" rel="noopener nofollow">signal</a> . It is destinguished between operator quadratic form and a normalized quadratic form<br>
<br>Non-zero in normalized case.
<br>Interpreted as similiarity between  and  (inner product)<br>
- large  indicate that the operator preserves the signal.<br>
<br><br>
<br><br>In some physical systems, the amount sent through all the links should not deviate from an original quantity, e.g. energy in a closed system.<br>
<br>Achieved by computing consensus or average instead of sum<br>
<br><br>
<br><br><br>
<br>Entry  of output vector yields
<br>Replaces each node value with the sum of its neighbors (those who have an edge connecting them to the respective node).
<br><br>
<br>Entry  of output vector yields
<br>Replaces each node with the weighted average of its neighbors.
<br><br>
<br>Entry  of output vector yields
<br>Subtracts the average of the respective <a data-tooltip-position="top" aria-label="Node or Vertex Set" data-href="Node or Vertex Set" href="the-guide/mathematics/graph-theory/node-or-vertex-set.html" class="internal-link" target="_self" rel="noopener nofollow">one-hop neighborhood</a> from each node.
<br><a data-tooltip-position="top" aria-label="Graph Laplacians" data-href="Graph Laplacians" href="the-guide/mathematics/graph-theory/graph-laplacians.html" class="internal-link" target="_self" rel="noopener nofollow">Prediction Error</a>

<br> can be interpreted as an error that is small, if the value at a node is close to the average of its neighbors.


<br><br>
<br>Entry  of output vector yields
<br><a data-tooltip-position="top" aria-label="Graph Laplacians" data-href="Graph Laplacians" href="the-guide/mathematics/graph-theory/graph-laplacians.html" class="internal-link" target="_self" rel="noopener nofollow">Weighted Prediction Error</a>

<br>Extends the <a data-tooltip-position="top" aria-label="Graph Laplacians" data-href="Graph Laplacians" href="the-guide/mathematics/graph-theory/graph-laplacians.html" class="internal-link" target="_self" rel="noopener nofollow">prediction error</a> by giving more weight to nodes with higher <a data-tooltip-position="top" aria-label="Node or Vertex Set" data-href="Node or Vertex Set" href="the-guide/mathematics/graph-theory/node-or-vertex-set.html" class="internal-link" target="_self" rel="noopener nofollow">degree</a>.


<br><br>
<br>Entry  of output vector yields
<br><a data-tooltip-position="top" aria-label="Graph Laplacians" data-href="Graph Laplacians" href="the-guide/mathematics/graph-theory/graph-laplacians.html" class="internal-link" target="_self" rel="noopener nofollow">Prediction Error with normalized Path Weights</a>

<br>Does not depend on size of neighborhood, but path weigths have been normalized


<br><br>
<br>Given a One-Hop Operator , the corresponding -hop Operator is constructed via .
<br><a href=".?query=tag:GSP-Course" class="tag" target="_blank" rel="noopener nofollow">#GSP-Course</a>]]></description><link>the-guide/mathematics/graph-theory/graph-operator.html</link><guid isPermaLink="false">The Guide/Mathematics/Graph Theory/Graph Operator.md</guid><pubDate>Sat, 29 Mar 2025 16:19:05 GMT</pubDate></item><item><title><![CDATA[Graph Sampling]]></title><description><![CDATA[<a class="tag" href="?query=tag:GSP-Course" style="background-color: rgb(4, 108, 116); color: white; font-weight: 700; border: none; border-radius: 1em; padding: 0.2em 0.5em;">#GSP-Course</a> 
 <br>In <a data-tooltip-position="top" aria-label="Graph" data-href="Graph" href="the-guide/mathematics/graph-theory/graph.html" class="internal-link" target="_self" rel="noopener nofollow">graph</a> <a data-tooltip-position="top" aria-label="Graph Signal" data-href="Graph Signal" href="the-guide/integral-transforms-and-signals/graph-signal.html" class="internal-link" target="_self" rel="noopener nofollow">signal</a> sampling, we select a subset of  out of  <a data-tooltip-position="top" aria-label="Node or Vertex Set" data-href="Node or Vertex Set" href="the-guide/mathematics/graph-theory/node-or-vertex-set.html" class="internal-link" target="_self" rel="noopener nofollow">nodes</a> (the sampling set) such that the signal obtained from these observed nodes can be used to estimate the signal at unobserved nodes.<br>
<a href=".?query=tag:GSP-Course" class="tag" target="_blank" rel="noopener nofollow">#GSP-Course</a>]]></description><link>the-guide/mathematics/graph-theory/graph-sampling.html</link><guid isPermaLink="false">The Guide/Mathematics/Graph Theory/Graph Sampling.md</guid><pubDate>Sat, 29 Mar 2025 16:19:05 GMT</pubDate></item><item><title><![CDATA[Graph Spectrum]]></title><description><![CDATA[<a class="tag" href="?query=tag:GSP-Course" style="background-color: rgb(4, 108, 116); color: white; font-weight: 700; border: none; border-radius: 1em; padding: 0.2em 0.5em;">#GSP-Course</a> 
 <br>Generally defined as the <a data-tooltip-position="top" aria-label="Set" data-href="Set" href="the-guide/mathematics/general-stuff/set.html" class="internal-link" target="_self" rel="noopener nofollow">set</a> of <a data-tooltip-position="top" aria-label="Eigenvalue Problem" data-href="Eigenvalue Problem" href="the-guide/mathematics/linear-algebra/eigenvalue-problem.html" class="internal-link" target="_self" rel="noopener nofollow">eigenvalues</a> of a <a data-tooltip-position="top" aria-label="Graph Laplacians" data-href="Graph Laplacians" href="the-guide/mathematics/graph-theory/graph-laplacians.html" class="internal-link" target="_self" rel="noopener nofollow">Graph Laplacian</a>.<br><br><br>
<br>
Symmetric Perturbation

<br>For a symmetric <a data-tooltip-position="top" aria-label="Matrices" data-href="Matrices" href="the-guide/mathematics/linear-algebra/matrices.html" class="internal-link" target="_self" rel="noopener nofollow">matrix</a>  (e.g. <a data-tooltip-position="top" aria-label="Graph" data-href="Graph" href="the-guide/mathematics/graph-theory/graph.html" class="internal-link" target="_self" rel="noopener nofollow">undirected graph</a>) and a symmetric perturbation, e.g.with  also a graph <a data-tooltip-position="top" aria-label="Graph Laplacians" data-href="Graph Laplacians" href="the-guide/mathematics/graph-theory/graph-laplacians.html" class="internal-link" target="_self" rel="noopener nofollow">Laplacian</a>, the eigenvalues of the resulting matrix  are bounded by where  is the largest singular eigenvalue.

<br>Removing edge with weight  results in degree of node  to reduce to , the smallest weight leads to smallest change.




<br>
Eigenvector Perturbation

<br>Let  and  be the eigenvectors corresponding to an eigenvalue that changed due to perturbation as above. Then the angle between those eigenvectors is 

<br>If minimum eigenvalue spacing is small even smallest perturbation has significant effect.




<br><br><br>Interlacing Theorem
If  is a real and symmetric matrix with eigenvalues  and  is a submatrix (e.g. <a data-href="Adjacency Matrix" href="the-guide/mathematics/graph-theory/adjacency-matrix.html" class="internal-link" target="_self" rel="noopener nofollow">Adjacency Matrix</a> with one node and all its edges removed), the eigenvalues of  are such that andThis can be applied recursively to reach  after  nodes have been removed.
<br><a href=".?query=tag:GSP-Course" class="tag" target="_blank" rel="noopener nofollow">#GSP-Course</a>]]></description><link>the-guide/mathematics/graph-theory/graph-spectrum.html</link><guid isPermaLink="false">The Guide/Mathematics/Graph Theory/Graph Spectrum.md</guid><pubDate>Mon, 09 Sep 2024 15:39:16 GMT</pubDate></item><item><title><![CDATA[Incidence Matrix]]></title><description><![CDATA[<a class="tag" href="?query=tag:GSP-Course" style="background-color: rgb(4, 108, 116); color: white; font-weight: 700; border: none; border-radius: 1em; padding: 0.2em 0.5em;">#GSP-Course</a> 
 <br>Definition
Rectangular  <a data-tooltip-position="top" aria-label="Matrices" data-href="Matrices" href="the-guide/mathematics/linear-algebra/matrices.html" class="internal-link" target="_self" rel="noopener nofollow">matrix</a> representing a <a data-tooltip-position="top" aria-label="Graph" data-href="Graph" href="the-guide/mathematics/graph-theory/graph.html" class="internal-link" target="_self" rel="noopener nofollow">graph</a> with  <a data-tooltip-position="top" aria-label="Node or Vertex Set" data-href="Node or Vertex Set" href="the-guide/mathematics/graph-theory/node-or-vertex-set.html" class="internal-link" target="_self" rel="noopener nofollow">nodes</a> and  directed edges. If the -th edge is  (from node  to ) with weight , then 
<br>
<br>Out of (=negative) node  and into (=positive) node .
<br>Each column of  represents an edge.
<br>Each row of  represents a node.
<br>Defines the effect of a network flow on a <a data-href="Graph Signal" href="the-guide/integral-transforms-and-signals/graph-signal.html" class="internal-link" target="_self" rel="noopener nofollow">Graph Signal</a>.
<br>Each column adds up to zero: .

<br>, 


<br>In an undirected graph, 

<br>The signs can be chosen arbitrarily.
<br>The edges can be seen as a pair of edges in opposite direction.


<br><br><br>A quantity flowing from <a data-tooltip-position="top" aria-label="Node or Vertex Set" data-href="Node or Vertex Set" href="the-guide/mathematics/graph-theory/node-or-vertex-set.html" class="internal-link" target="_self" rel="noopener nofollow">node</a>  (being subtracted) to node  (being added) can be represented by a vector , where the -th entry is a flow along the -th edge. Via the term  is added to node  and  is subtracted from .<br>
<br>Flow applied to a <a data-tooltip-position="top" aria-label="Graph Signal" data-href="Graph Signal" href="the-guide/integral-transforms-and-signals/graph-signal.html" class="internal-link" target="_self" rel="noopener nofollow">graph signal</a>  results in the <a data-tooltip-position="top" aria-label="Graph Signal" data-href="Graph Signal" href="the-guide/integral-transforms-and-signals/graph-signal.html" class="internal-link" target="_self" rel="noopener nofollow">graph signal</a> .
<br>Only flows, nothing added or subtracted from sum of entries: 
<br><br><br>--- start-multi-column  <br>number of columns: 2  
largest column: left  
<br>Directed Graph <br>--- end-column ---<br><img alt="Pasted image 20230320122227.png" src="lib/media/pasted-image-20230320122227.png"><br>--- end-multi-column<br>
<a href=".?query=tag:GSP-Course" class="tag" target="_blank" rel="noopener nofollow">#GSP-Course</a>]]></description><link>the-guide/mathematics/graph-theory/incidence-matrix.html</link><guid isPermaLink="false">The Guide/Mathematics/Graph Theory/Incidence Matrix.md</guid><pubDate>Sat, 29 Mar 2025 16:19:05 GMT</pubDate><enclosure url="lib/media/pasted-image-20230320122227.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src=&quot;lib/media/pasted-image-20230320122227.png&quot;&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[Node or Vertex Set]]></title><description><![CDATA[ 
 <br>Collection of important properties that vertices of a graph can be assigned with.<br><br>Degree
Total weight of the edges connected to the node. For  

<br>An <a data-tooltip-position="top" aria-label="Graph" data-href="Graph" href="the-guide/mathematics/graph-theory/graph.html" class="internal-link" target="_self" rel="noopener nofollow">undirected graph</a> this is 
<br>A <a data-tooltip-position="top" aria-label="Graph" data-href="Graph" href="the-guide/mathematics/graph-theory/graph.html" class="internal-link" target="_self" rel="noopener nofollow">directed graph</a>, we differentiate between the concepts of 

<br>in-degree
<br>out-degreediffering between ingoing and outgoing nodes.



<br>
<br>Source - in-degree zero and non-zero out-degree.
<br>Sink - out-degree zero and non-zero in-degree.
<br>k-hop Neighborhood
Given a node  , its -hop neighborhood , is the set of all nodes that are part of a path with no more than  edges starting or ending at .
<br><img alt="center" src="lib/media/pasted-image-20221215082928.png" style="width: 350px; max-width: 100%;"><br>
<br>Locality

<br>If  then processing in a -hop neighborhood of  can be considered as local.
<br>Alternatively, one can consider the distance between nodes as well, e.g. <a data-href="Geodesic Distance" href="the-guide/mathematics/graph-theory/geodesic-distance.html" class="internal-link" target="_self" rel="noopener nofollow">Geodesic Distance</a>.


]]></description><link>the-guide/mathematics/graph-theory/node-or-vertex-set.html</link><guid isPermaLink="false">The Guide/Mathematics/Graph Theory/Node or Vertex Set.md</guid><pubDate>Wed, 12 Feb 2025 22:31:53 GMT</pubDate><enclosure url="lib/media/pasted-image-20221215082928.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src=&quot;lib/media/pasted-image-20221215082928.png&quot;&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[Random Walk Matrix]]></title><description><![CDATA[<a class="tag" href="?query=tag:GSP-Course" style="background-color: rgb(4, 108, 116); color: white; font-weight: 700; border: none; border-radius: 1em; padding: 0.2em 0.5em;">#GSP-Course</a> 
 <br>Definition
Can be computed with the inverse <a data-href="Degree Matrix" href="the-guide/mathematics/graph-theory/degree-matrix.html" class="internal-link" target="_self" rel="noopener nofollow">Degree Matrix</a> and the <a data-href="Adjacency Matrix" href="the-guide/mathematics/graph-theory/adjacency-matrix.html" class="internal-link" target="_self" rel="noopener nofollow">Adjacency Matrix</a> via 
<br>
<br>Properties<br>
- Rows add up to 1.<br>
- Also called normalized <a data-href="Adjacency Matrix" href="the-guide/mathematics/graph-theory/adjacency-matrix.html" class="internal-link" target="_self" rel="noopener nofollow">Adjacency Matrix</a>.<br>
- Eigenvector to largest eigenvalue  is invariant <a data-tooltip-position="top" aria-label="Graph Signal" data-href="Graph Signal" href="the-guide/integral-transforms-and-signals/graph-signal.html" class="internal-link" target="_self" rel="noopener nofollow">signal of maximal smoothness</a>, value at each node is as close as possible to neighbours (<a data-href="Perron-Frobenius Theorem" href="the-guide/mathematics/linear-algebra/perron-frobenius-theorem.html" class="internal-link" target="_self" rel="noopener nofollow">Perron-Frobenius Theorem</a>).<br>
<a href=".?query=tag:GSP-Course" class="tag" target="_blank" rel="noopener nofollow">#GSP-Course</a>
]]></description><link>the-guide/mathematics/graph-theory/random-walk-matrix.html</link><guid isPermaLink="false">The Guide/Mathematics/Graph Theory/Random Walk Matrix.md</guid><pubDate>Mon, 09 Sep 2024 15:39:16 GMT</pubDate></item><item><title><![CDATA[Subgraph]]></title><description><![CDATA[<a class="tag" href="?query=tag:GSP-Course" style="background-color: rgb(4, 108, 116); color: white; font-weight: 700; border: none; border-radius: 1em; padding: 0.2em 0.5em;">#GSP-Course</a> 
 <br>Given a <a data-tooltip-position="top" aria-label="Graph" data-href="Graph" href="the-guide/mathematics/graph-theory/graph.html" class="internal-link" target="_self" rel="noopener nofollow">graph</a>  a subgraph  of  is such that  and , that is, if  and  then we must have that .<br>
<a href=".?query=tag:GSP-Course" class="tag" target="_blank" rel="noopener nofollow">#GSP-Course</a>]]></description><link>the-guide/mathematics/graph-theory/subgraph.html</link><guid isPermaLink="false">The Guide/Mathematics/Graph Theory/Subgraph.md</guid><pubDate>Mon, 09 Sep 2024 15:39:16 GMT</pubDate></item><item><title><![CDATA[Abelian Group]]></title><description><![CDATA[ 
 <br>Definition
A <a data-tooltip-position="top" aria-label="Group" data-href="Group" href="the-guide/mathematics/group-theory/group.html" class="internal-link" target="_self" rel="noopener nofollow">group</a>  is called abelian or <a data-tooltip-position="top" aria-label="Mathematical Relations" data-href="Mathematical Relations" href="the-guide/mathematics/general-stuff/mathematical-relations.html" class="internal-link" target="_self" rel="noopener nofollow">commutative</a>, if  for all . 
]]></description><link>the-guide/mathematics/group-theory/abelian-group.html</link><guid isPermaLink="false">The Guide/Mathematics/Group Theory/Abelian Group.md</guid><pubDate>Mon, 09 Sep 2024 15:37:47 GMT</pubDate></item><item><title><![CDATA[Cosets]]></title><description><![CDATA[ 
 <br>Definition
Given a <a data-tooltip-position="top" aria-label="Group" data-href="Group" href="the-guide/mathematics/group-theory/group.html" class="internal-link" target="_self" rel="noopener nofollow">subgroup</a>  and an element , the corresponding left coset of  in , written  is the <a data-tooltip-position="top" aria-label="Set" data-href="Set" href="the-guide/mathematics/general-stuff/set.html" class="internal-link" target="_self" rel="noopener nofollow">set</a> while the right coset is 
<br>Since , we always have  and <br>Theorem
An element  belongs to a coset  iff , which happens iff . Therefore, distinct cosets are disjoint. 
]]></description><link>the-guide/mathematics/group-theory/cosets.html</link><guid isPermaLink="false">The Guide/Mathematics/Group Theory/Cosets.md</guid><pubDate>Mon, 09 Sep 2024 15:37:48 GMT</pubDate></item><item><title><![CDATA[General Linear Group and Matrix Groups]]></title><description><![CDATA[ 
 <br>General Linear Group
For an -dimensional <a data-tooltip-position="top" aria-label="Vector Space" data-href="Vector Space" href="the-guide/mathematics/general-stuff/vector-space.html" class="internal-link" target="_self" rel="noopener nofollow">vector space</a>  over a field , the general linear group is the <a data-tooltip-position="top" aria-label="Set" data-href="Set" href="the-guide/mathematics/general-stuff/set.html" class="internal-link" target="_self" rel="noopener nofollow">set</a> of all <a data-tooltip-position="top" aria-label="Mathematical Relations" data-href="Mathematical Relations" href="the-guide/mathematics/general-stuff/mathematical-relations.html" class="internal-link" target="_self" rel="noopener nofollow">bijective linear transformations</a> (invertible  <a data-tooltip-position="top" aria-label="Matrices" data-href="Matrices" href="the-guide/mathematics/linear-algebra/matrices.html" class="internal-link" target="_self" rel="noopener nofollow">matrices</a>)
<br><br><br>General Orthogonal Group
The orthogonal group of order  describes all transformations that preserve ... . It is a <a data-tooltip-position="top" aria-label="Topology and Topological Space" data-href="Topology and Topological Space" href="the-guide/mathematics/topology/topology-and-topological-space.html" class="internal-link" target="_self" rel="noopener nofollow">compact</a> <a data-href="Lie Group" href="the-guide/mathematics/lie-theory/lie-group.html" class="internal-link" target="_self" rel="noopener nofollow">Lie Group</a>.
<br>
<br>Special Orthogonal Group of Order n (gen. rotation) 
<br>Special Euclidean Group of Order n (roto-translations) 

<br>E.g. <a data-tooltip-position="top" aria-label="Kinematics" data-href="Kinematics" href="the-guide/robotics,-dynamics-and-control/kinematics/kinematics.html" class="internal-link" target="_self" rel="noopener nofollow">Homogeneous Coordinates</a> with <a data-tooltip-position="top" aria-label="Matrices" data-href="Matrices" href="the-guide/mathematics/linear-algebra/matrices.html" class="internal-link" target="_self" rel="noopener nofollow">matrix</a>  product and inverse 


]]></description><link>the-guide/mathematics/group-theory/general-linear-group-and-matrix-groups.html</link><guid isPermaLink="false">The Guide/Mathematics/Group Theory/General Linear Group and Matrix Groups.md</guid><pubDate>Wed, 23 Apr 2025 08:34:37 GMT</pubDate></item><item><title><![CDATA[Group]]></title><description><![CDATA[ 
 <br><br>Definition
The pair  of a non-empty <a data-tooltip-position="top" aria-label="Set" data-href="Set" href="the-guide/mathematics/general-stuff/set.html" class="internal-link" target="_self" rel="noopener nofollow">set</a>  with a binary operation on  denoted , that satisfies

<br>Closure

<br>The result of combining any two elements of  via  results in an element in 


<br>Associativity 

<br>


<br>Identity element

<br>There exists an , such that  for every 


<br>Inverse element

<br>For every , there exists a unique element  that fulfills 



<br><img alt="center" src="lib/media/pasted-image-20240122084731.png" style="width: 400px; max-width: 100%;"><br>Order
The order of a group is its size and denoted by When talking about particular group elements, the order of that element  is written as and denoted the smallest natural number , such that .
<br><br><br>Subgroup
If  is a group and  is a <a data-tooltip-position="top" aria-label="Set" data-href="Set" href="the-guide/mathematics/general-stuff/set.html" class="internal-link" target="_self" rel="noopener nofollow">subset</a>, such that  satisfies the group axioms, then we call  a subgroup of , which we write . 
<br>Generated Subgroups
For any <a data-tooltip-position="top" aria-label="Set" data-href="Set" href="the-guide/mathematics/general-stuff/set.html" class="internal-link" target="_self" rel="noopener nofollow">subset</a> of a group , the subgroup generated by  is the smallest subgroup of  containing . This subgroup is denoted  and
<br><br><br><br><br><br>Cyclic Grup
A group is cyclic, if it is generated by a single element, that is  for dome .
<br>
<br>Translation Group <img alt="center" src="lib/media/pasted-image-20231216115052.png" style="width: 400px; max-width: 100%;">

<br>Group product is vector addition 
<br>Inverse is opposite direction 


<br>Scale-Translation Group <img alt="center" src="lib/media/pasted-image-20231216120100.png" style="width: 400px; max-width: 100%;">

<br>Product via 
<br>Inverse for  via 


<br>Orthogonal Group of order  
<br>Special Orthogonal Group of Order n (gen. rotation) 
<br>Special Euclidean Group of Order n (roto-translations) 

<br>E.g. <a data-tooltip-position="top" aria-label="Kinematics" data-href="Kinematics" href="the-guide/robotics,-dynamics-and-control/kinematics/kinematics.html" class="internal-link" target="_self" rel="noopener nofollow">Homogeneous Coordinates</a> with <a data-tooltip-position="top" aria-label="Matrices" data-href="Matrices" href="the-guide/mathematics/linear-algebra/matrices.html" class="internal-link" target="_self" rel="noopener nofollow">matrix</a>  product and inverse 


<br>Integers  under addition
<br>Unit complex numbers under multiplication
]]></description><link>the-guide/mathematics/group-theory/group.html</link><guid isPermaLink="false">The Guide/Mathematics/Group Theory/Group.md</guid><pubDate>Sun, 06 Apr 2025 19:21:12 GMT</pubDate><enclosure url="lib/media/pasted-image-20240122084731.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src=&quot;lib/media/pasted-image-20240122084731.png&quot;&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[Group Action]]></title><description><![CDATA[ 
 <br>In a Nutshell
In mathematics, many <a data-tooltip-position="top" aria-label="Set" data-href="Set" href="the-guide/mathematics/general-stuff/set.html" class="internal-link" target="_self" rel="noopener nofollow">sets</a> of transformations form a <a data-tooltip-position="top" aria-label="Group" data-href="Group" href="the-guide/mathematics/group-theory/group.html" class="internal-link" target="_self" rel="noopener nofollow">group</a> under function composition, e.g. <a data-tooltip-position="top" aria-label="A Hitchhiker's Guide to Rotation Representations" data-href="A Hitchhiker's Guide to Rotation Representations" href="the-guide/robotics,-dynamics-and-control/a-hitchhiker's-guide-to-rotation-representations.html" class="internal-link" target="_self" rel="noopener nofollow">rotations</a>. In this sense, it is often useful to consider the group in an abstract way and to define the actual transformation as a separate abstract concept, the group action. Intuitively, if a group acts on any structure, it will usually also act on objects built from that structure, e.g. a triangle in a vector space.
<br><br>Definition - Group Action
A group action of a <a data-tooltip-position="top" aria-label="Group" data-href="Group" href="the-guide/mathematics/group-theory/group.html" class="internal-link" target="_self" rel="noopener nofollow">group</a>  on a <a data-tooltip-position="top" aria-label="Set" data-href="Set" href="the-guide/mathematics/general-stuff/set.html" class="internal-link" target="_self" rel="noopener nofollow">set</a>  is a <a data-tooltip-position="top" aria-label="Mappings between Groups" data-href="Mappings between Groups" href="the-guide/mathematics/group-theory/mappings-between-groups.html" class="internal-link" target="_self" rel="noopener nofollow">group homomorphism</a> from  to some group of functions (with product function composition) on the set . The set  together with an action of the group  is called a G-set.
<br>Additional Information
In this context, the regular <a data-tooltip-position="top" aria-label="Group" data-href="Group" href="the-guide/mathematics/group-theory/group.html" class="internal-link" target="_self" rel="noopener nofollow">group action</a> can be interpreted as a group action on the group itself.
<br><br><br>Depending on in which order a product of group elements acts on the element of a set determines in which of the following categories it falls:<br>Definition - Left Group Action
If  is a <a data-tooltip-position="top" aria-label="Group" data-href="Group" href="the-guide/mathematics/group-theory/group.html" class="internal-link" target="_self" rel="noopener nofollow">group</a> with identity element  and  is a <a data-tooltip-position="top" aria-label="Set" data-href="Set" href="the-guide/mathematics/general-stuff/set.html" class="internal-link" target="_self" rel="noopener nofollow">set</a> with elements , then the left group action  of the group on the set is a <a data-tooltip-position="top" aria-label="Mathematical Relations" data-href="Mathematical Relations" href="the-guide/mathematics/general-stuff/mathematical-relations.html" class="internal-link" target="_self" rel="noopener nofollow">function</a>that satisfies the axioms

<br>Identity
<br>Compatibility

<br>
<br>In some applications, the notation  is used, meaning left action of element, This results in
<br>Definition - Right Group Action
If  is a <a data-tooltip-position="top" aria-label="Group" data-href="Group" href="the-guide/mathematics/group-theory/group.html" class="internal-link" target="_self" rel="noopener nofollow">group</a> with identity element  and  is a <a data-tooltip-position="top" aria-label="Set" data-href="Set" href="the-guide/mathematics/general-stuff/set.html" class="internal-link" target="_self" rel="noopener nofollow">set</a> with elements , then the right group action  of the group on the set is a <a data-tooltip-position="top" aria-label="Mathematical Relations" data-href="Mathematical Relations" href="the-guide/mathematics/general-stuff/mathematical-relations.html" class="internal-link" target="_self" rel="noopener nofollow">function</a>that satisfies the axioms

<br>Identity
<br>Compatibility

<br>A right action can be constructed from a right action and vice versa by composing with the inverse operation . This yields the transformations<br>Additional Information
For both, the notation is simplified in most cases. When the action is clear from the context, e.g. matrix products, the axioms are simply written  and .  
<br><br>Orbit
The orbit of an  is the <a data-tooltip-position="top" aria-label="Set" data-href="Set" href="the-guide/mathematics/general-stuff/set.html" class="internal-link" target="_self" rel="noopener nofollow">subset</a> of elements that can be reached by letting any group element act on  
<br>Stabilizer
The stabilizer / isotry group of an element  is the subset of group elements that map the element on itself
<br>If  for every , we say that the group acts freely. <br>Transitive Group Actions
A (left) group action  of a <a data-tooltip-position="top" aria-label="Group" data-href="Group" href="the-guide/mathematics/group-theory/group.html" class="internal-link" target="_self" rel="noopener nofollow">group</a>  is transitive, if the orbit of any point covers the entire set it can act on.
]]></description><link>the-guide/mathematics/group-theory/group-action.html</link><guid isPermaLink="false">The Guide/Mathematics/Group Theory/Group Action.md</guid><pubDate>Sat, 29 Mar 2025 19:39:05 GMT</pubDate></item><item><title><![CDATA[Group Conjugacy]]></title><description><![CDATA[ 
 <br>In a Nutshell
Concept in group theory that relates <a data-tooltip-position="top" aria-label="Group" data-href="Group" href="the-guide/mathematics/group-theory/group.html" class="internal-link" target="_self" rel="noopener nofollow">group</a> elements through internal symmetries. Groups together elements of a group that are "similar" under the groups own operations. 
<br><br>Definition
For a given group  two elements  are conjugate, if there exists an element , such that The <a data-tooltip-position="top" aria-label="Set" data-href="Set" href="the-guide/mathematics/general-stuff/set.html" class="internal-link" target="_self" rel="noopener nofollow">set</a> of all conjugates to a given element  is called the conjugate class.
<br>
<br>For <a data-tooltip-position="top" aria-label="Abelian Group" data-href="Abelian Group" href="the-guide/mathematics/group-theory/abelian-group.html" class="internal-link" target="_self" rel="noopener nofollow">abelian groups</a> each conjugacy class is a singleton
<br>Members of the same conjugacy class cannot be distinguished using only the group structure.<br>Examples<br>
<br>For the general linear group  of invertible <a data-tooltip-position="top" aria-label="Matrices" data-href="Matrices" href="the-guide/mathematics/linear-algebra/matrices.html" class="internal-link" target="_self" rel="noopener nofollow">matrices</a>, the conjugacy relation is <a data-tooltip-position="top" aria-label="Matrix Similarity" data-href="Matrix Similarity" href="the-guide/mathematics/linear-algebra/matrix-similarity.html" class="internal-link" target="_self" rel="noopener nofollow">matrix similarity</a>.
<br><br><br>Definition
For any two elements  the map defines a <a data-tooltip-position="top" aria-label="Group Action" data-href="Group Action" href="the-guide/mathematics/group-theory/group-action.html" class="internal-link" target="_self" rel="noopener nofollow">group action</a> of  on . The <a data-tooltip-position="top" aria-label="Group" data-href="Group" href="the-guide/mathematics/group-theory/group.html" class="internal-link" target="_self" rel="noopener nofollow">orbits</a> of this action are the conjugacy classes, the <a data-tooltip-position="top" aria-label="Group" data-href="Group" href="the-guide/mathematics/group-theory/group.html" class="internal-link" target="_self" rel="noopener nofollow">stabilizer</a> of any element is the elements centralizer.
<br>Intuition
The conjugate  describes  out of a different perspective, the perspective of . This makes conjugates the natural language of symmetry - conjugate elements are the "same" under the groups internal symmetry, differing only by where they are within the groups framework.
The conjugation can be understood as a change of perspective. Remember that we define the group action from the left, meaning that a conjugate of  as above will first apply , then  and then move back via . This is an unternal action within the group, as we would transform the result into a function via a group homomorphism (group action) before rotating a vector etc.
]]></description><link>the-guide/mathematics/group-theory/group-conjugacy.html</link><guid isPermaLink="false">The Guide/Mathematics/Group Theory/Group Conjugacy.md</guid><pubDate>Sat, 29 Mar 2025 19:37:09 GMT</pubDate></item><item><title><![CDATA[Group Equivariance]]></title><description><![CDATA[ 
 <br>Definition
Given two <a data-tooltip-position="top" aria-label="Group Action" data-href="Group Action" href="the-guide/mathematics/group-theory/group-action.html" class="internal-link" target="_self" rel="noopener nofollow">G-sets</a>  and  with the same group , a <a data-tooltip-position="top" aria-label="Mathematical Relations" data-href="Mathematical Relations" href="the-guide/mathematics/general-stuff/mathematical-relations.html" class="internal-link" target="_self" rel="noopener nofollow">function</a>  is said to be equivariant if If the group acts on an element  of a <a data-tooltip-position="top" aria-label="Vector Space" data-href="Vector Space" href="the-guide/mathematics/general-stuff/vector-space.html" class="internal-link" target="_self" rel="noopener nofollow">vector space</a> via a <a data-tooltip-position="top" aria-label="Group Action" data-href="Group Action" href="the-guide/mathematics/group-theory/group-action.html" class="internal-link" target="_self" rel="noopener nofollow">representation</a> , the formulation changes to    
<br><img alt="center" src="lib/media/pasted-image-20240123095412.png" style="width: 400px; max-width: 100%;">]]></description><link>the-guide/mathematics/group-theory/group-equivariance.html</link><guid isPermaLink="false">The Guide/Mathematics/Group Theory/Group Equivariance.md</guid><pubDate>Sat, 29 Mar 2025 19:37:09 GMT</pubDate><enclosure url="lib/media/pasted-image-20240123095412.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src=&quot;lib/media/pasted-image-20240123095412.png&quot;&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[Group Representation]]></title><description><![CDATA[ 
 <br>In a Nutshell
Special case of a <a data-tooltip-position="top" aria-label="Group Action" data-href="Group Action" href="the-guide/mathematics/group-theory/group-action.html" class="internal-link" target="_self" rel="noopener nofollow">Group Actions</a>, where the <a data-tooltip-position="top" aria-label="Space" data-href="Space" href="the-guide/mathematics/general-stuff/space.html" class="internal-link" target="_self" rel="noopener nofollow">space</a>  being acted on is a <a data-tooltip-position="top" aria-label="Vector Space" data-href="Vector Space" href="the-guide/mathematics/general-stuff/vector-space.html" class="internal-link" target="_self" rel="noopener nofollow">vector space</a>. In this case, we can represent the <a data-tooltip-position="top" aria-label="Group" data-href="Group" href="the-guide/mathematics/group-theory/group.html" class="internal-link" target="_self" rel="noopener nofollow">group</a> elements as <a data-tooltip-position="top" aria-label="Matrices" data-href="Matrices" href="the-guide/mathematics/linear-algebra/matrices.html" class="internal-link" target="_self" rel="noopener nofollow">matrices</a> acting on vectors.
<br><br>Group Representation
If the group  acts on a <a data-tooltip-position="top" aria-label="Vector Space" data-href="Vector Space" href="the-guide/mathematics/general-stuff/vector-space.html" class="internal-link" target="_self" rel="noopener nofollow">vector space</a>  over a <a data-href="Field" href="the-guide/mathematics/general-stuff/field.html" class="internal-link" target="_self" rel="noopener nofollow">Field</a>, here  instead of a <a data-href="Set" href="the-guide/mathematics/general-stuff/set.html" class="internal-link" target="_self" rel="noopener nofollow">Set</a>, it is called a representation of the group. If this <a data-href="Space" href="the-guide/mathematics/general-stuff/space.html" class="internal-link" target="_self" rel="noopener nofollow">Space</a> is for example -dimensional, the representation is a map from a group to a linear map on that <a data-href="Vector Space" href="the-guide/mathematics/general-stuff/vector-space.html" class="internal-link" target="_self" rel="noopener nofollow">Vector Space</a>  that assigns an invertible <a data-tooltip-position="top" aria-label="Matrices" data-href="Matrices" href="the-guide/mathematics/linear-algebra/matrices.html" class="internal-link" target="_self" rel="noopener nofollow">matrix</a> to each group element and satisfies The vector space  is then denoted the representation space of .
<br>Warning
A representation depends on the vector space being considered, they differ if they map into different <a data-tooltip-position="top" aria-label="General Linear Group and Matrix Groups" data-href="General Linear Group and Matrix Groups" href="the-guide/mathematics/group-theory/general-linear-group-and-matrix-groups.html" class="internal-link" target="_self" rel="noopener nofollow">linear groups</a>.<br>
Many authors also distinguish representations by referring to the vector spaces they act on. 
<br>Additional Information
Since the set of  matrices is itself a group, it is often denoted as <a data-tooltip-position="top" aria-label="General Linear Group and Matrix Groups" data-href="General Linear Group and Matrix Groups" href="the-guide/mathematics/group-theory/general-linear-group-and-matrix-groups.html" class="internal-link" target="_self" rel="noopener nofollow">general linear group</a> a <a data-tooltip-position="top" aria-label="Mathematical Relations" data-href="Mathematical Relations" href="the-guide/mathematics/general-stuff/mathematical-relations.html" class="internal-link" target="_self" rel="noopener nofollow">group homomorphism</a> from  to the general linear group on . In the above, we simply chose a specific basis fro  to directly use the invertible matrices.
<br>Definition - Homogeneous Spaces
For transitive group actions, this means that by using the elements of the group on elements of the representation space, I can reach every point in it.<br>
Such representation spaces are denoted homogeneous spaces. In applications, this means we can guarantee that every part of a signal can be seen when the space is probed by a group convolution kernel.
<br>
<br>Examples

<br> acts transitively on points in  (unique)
<br> acts transitively on vectors in  (double-<a data-tooltip-position="top" aria-label="Überlagerung" data-href="Überlagerung" href="the-guide/mathematics/differential-geometry/riemannian-geometry/mannigfaltigkeiten-und-differenzierbare-abbildungen/überlagerung.html" class="internal-link" target="_self" rel="noopener nofollow">cover</a>, not unique)
<br> does not act transitively, because the rotation axis is fixed. Therefore, each point can only reach points on a ring.
<br> acts transitively on 


<br>Theorem
Every homogeneous space is a <a data-tooltip-position="top" aria-label="Quotient Group" data-href="Quotient Group" href="the-guide/mathematics/group-theory/quotient-group.html" class="internal-link" target="_self" rel="noopener nofollow">quotient group</a> and vice versa.
<br>Definition - Kernel of a Representation
The kernel (special case of <a data-tooltip-position="top" aria-label="Mappings between Groups" data-href="Mappings between Groups" href="the-guide/mathematics/group-theory/mappings-between-groups.html" class="internal-link" target="_self" rel="noopener nofollow">kernel</a> of group homomorphism) of a representation  is the subset of group elements whose image under  is the identity element 
<br><br>Equivalent Representations
Two representations  and  of a group are equivalent, if there is a matrix / linear map , such that compare to <a data-tooltip-position="top" aria-label="Matrix Similarity" data-href="Matrix Similarity" href="the-guide/mathematics/linear-algebra/matrix-similarity.html" class="internal-link" target="_self" rel="noopener nofollow">matrix similarity</a>. Intuitively, this means that it does not matter in which basis of  we represent all of the above.
<br><br><br>Definition - Irreducable Representation
A <a data-tooltip-position="top" aria-label="Space" data-href="Space" href="the-guide/mathematics/general-stuff/space.html" class="internal-link" target="_self" rel="noopener nofollow">subspace</a>  of  that is invariant under the group action is called a subrepresentation. If  has exactly two subrepresentations, where one is zero-dimensional, the representation is denoted irreducible.
<br>Consequently, there are reducible representations. If we have two representatons  and  and assuming  and  are defined on the same <a data-href="Field" href="the-guide/mathematics/general-stuff/field.html" class="internal-link" target="_self" rel="noopener nofollow">Field</a>, there is a representation Inversely, for <a data-tooltip-position="top" aria-label="Matrices" data-href="Matrices" href="the-guide/mathematics/linear-algebra/matrices.html" class="internal-link" target="_self" rel="noopener nofollow">matrix</a> <a data-tooltip-position="top" aria-label="General Linear Group and Matrix Groups" data-href="General Linear Group and Matrix Groups" href="the-guide/mathematics/group-theory/general-linear-group-and-matrix-groups.html" class="internal-link" target="_self" rel="noopener nofollow">groups</a>, a matrix representation is reducible, if it can be written as above, even if the diagonal form requires a transformation (remember equivalence) ]]></description><link>the-guide/mathematics/group-theory/group-representation.html</link><guid isPermaLink="false">The Guide/Mathematics/Group Theory/Group Representation.md</guid><pubDate>Wed, 23 Apr 2025 21:55:35 GMT</pubDate></item><item><title><![CDATA[Isomorphism Theorem]]></title><description><![CDATA[ 
 <br>Theorem
If  is a <a data-tooltip-position="top" aria-label="Group" data-href="Group" href="the-guide/mathematics/group-theory/group.html" class="internal-link" target="_self" rel="noopener nofollow">group</a> <a data-tooltip-position="top" aria-label="Mathematical Relations" data-href="Mathematical Relations" href="the-guide/mathematics/general-stuff/mathematical-relations.html" class="internal-link" target="_self" rel="noopener nofollow">homomorphism</a>, then the <a data-tooltip-position="top" aria-label="Quotient Group" data-href="Quotient Group" href="the-guide/mathematics/group-theory/quotient-group.html" class="internal-link" target="_self" rel="noopener nofollow">quotient group</a> of  with the kernel of  is <a data-tooltip-position="top" aria-label="Mappings between Groups" data-href="Mappings between Groups" href="the-guide/mathematics/group-theory/mappings-between-groups.html" class="internal-link" target="_self" rel="noopener nofollow">isomorphic</a> to  
]]></description><link>the-guide/mathematics/group-theory/isomorphism-theorem.html</link><guid isPermaLink="false">The Guide/Mathematics/Group Theory/Isomorphism Theorem.md</guid><pubDate>Wed, 23 Apr 2025 21:55:36 GMT</pubDate></item><item><title><![CDATA[Lagrange's Theorem]]></title><description><![CDATA[ 
 <br>Theorem
If  is a finite <a data-tooltip-position="top" aria-label="Group" data-href="Group" href="the-guide/mathematics/group-theory/group.html" class="internal-link" target="_self" rel="noopener nofollow">group</a> and , then  divides .
]]></description><link>the-guide/mathematics/group-theory/lagrange&apos;s-theorem.html</link><guid isPermaLink="false">The Guide/Mathematics/Group Theory/Lagrange&apos;s Theorem.md</guid><pubDate>Mon, 09 Sep 2024 15:37:48 GMT</pubDate></item><item><title><![CDATA[Mappings between Groups]]></title><description><![CDATA[ 
 <br><br>Homomorphism between Groups
A <a data-tooltip-position="top" aria-label="Mathematical Relations" data-href="Mathematical Relations" href="the-guide/mathematics/general-stuff/mathematical-relations.html" class="internal-link" target="_self" rel="noopener nofollow">mapping</a>  between <a data-tooltip-position="top" aria-label="Group" data-href="Group" href="the-guide/mathematics/group-theory/group.html" class="internal-link" target="_self" rel="noopener nofollow">groups</a>  and  is a <a data-tooltip-position="top" aria-label="Mathematical Relations" data-href="Mathematical Relations" href="the-guide/mathematics/general-stuff/mathematical-relations.html" class="internal-link" target="_self" rel="noopener nofollow">Homomorphism</a> if for every pair of elements  it holds that The mapping respects the group structure.
<br><br>Kernel 
Given a homomorphism , we define its kernel  to be the <a data-tooltip-position="top" aria-label="Set" data-href="Set" href="the-guide/mathematics/general-stuff/set.html" class="internal-link" target="_self" rel="noopener nofollow">set</a> of  that gets mapped to the identity element in  by .
<br>Theorem
If  is a group homomorphism, then  and .
<br><br>Isomorphic Groups
If  is a <a data-tooltip-position="top" aria-label="Mathematical Relations" data-href="Mathematical Relations" href="the-guide/mathematics/general-stuff/mathematical-relations.html" class="internal-link" target="_self" rel="noopener nofollow">bijective homomorphism</a>, we call it an isomorphism and the groups  and  are denoted isomorphic, written as In group theory, isomorphic groups are considered the same group (think of it as relabeling function).
<br>
<br>Example is  via the isomorphism  
<br>Automorphism
A <a data-tooltip-position="top" aria-label="Mathematical Relations" data-href="Mathematical Relations" href="the-guide/mathematics/general-stuff/mathematical-relations.html" class="internal-link" target="_self" rel="noopener nofollow">mapping</a> from a <a data-tooltip-position="top" aria-label="Group" data-href="Group" href="the-guide/mathematics/group-theory/group.html" class="internal-link" target="_self" rel="noopener nofollow">group</a>  to itself is called automorphism and the <a data-tooltip-position="top" aria-label="Set" data-href="Set" href="the-guide/mathematics/general-stuff/set.html" class="internal-link" target="_self" rel="noopener nofollow">set</a> of all such groups is denoted 
<br>
<br>Example is for any , the <a data-tooltip-position="top" aria-label="Group Conjugacy" data-href="Group Conjugacy" href="the-guide/mathematics/group-theory/group-conjugacy.html" class="internal-link" target="_self" rel="noopener nofollow">conjugation</a> 
]]></description><link>the-guide/mathematics/group-theory/mappings-between-groups.html</link><guid isPermaLink="false">The Guide/Mathematics/Group Theory/Mappings between Groups.md</guid><pubDate>Fri, 28 Feb 2025 22:52:00 GMT</pubDate></item><item><title><![CDATA[Normal]]></title><description><![CDATA[ 
 <br>Definition
A <a data-tooltip-position="top" aria-label="Group" data-href="Group" href="the-guide/mathematics/group-theory/group.html" class="internal-link" target="_self" rel="noopener nofollow">subgroup</a>  is called normal, denoted  if for all , we have the equality of <a data-tooltip-position="top" aria-label="Cosets" data-href="Cosets" href="the-guide/mathematics/group-theory/cosets.html" class="internal-link" target="_self" rel="noopener nofollow">cosets</a> , which is often expressed via  
<br>When left and right <a data-tooltip-position="top" aria-label="Cosets" data-href="Cosets" href="the-guide/mathematics/group-theory/cosets.html" class="internal-link" target="_self" rel="noopener nofollow">cosets</a> coincide as above, we can turn the set of cosets of a <a data-tooltip-position="top" aria-label="Group" data-href="Group" href="the-guide/mathematics/group-theory/group.html" class="internal-link" target="_self" rel="noopener nofollow">subgroup</a> into a <a data-tooltip-position="top" aria-label="Group" data-href="Group" href="the-guide/mathematics/group-theory/group.html" class="internal-link" target="_self" rel="noopener nofollow">group</a> itself.]]></description><link>the-guide/mathematics/group-theory/normal.html</link><guid isPermaLink="false">The Guide/Mathematics/Group Theory/Normal.md</guid><pubDate>Mon, 09 Sep 2024 15:37:48 GMT</pubDate></item><item><title><![CDATA[Quotient Group]]></title><description><![CDATA[ 
 <br>Definition
If  is a <a data-tooltip-position="top" aria-label="Normal" data-href="Normal" href="the-guide/mathematics/group-theory/normal.html" class="internal-link" target="_self" rel="noopener nofollow">normal</a> <a data-tooltip-position="top" aria-label="Group" data-href="Group" href="the-guide/mathematics/group-theory/group.html" class="internal-link" target="_self" rel="noopener nofollow">subgroup</a> of , then we define the quotient group  ( mod ) to be the <a data-tooltip-position="top" aria-label="Set" data-href="Set" href="the-guide/mathematics/general-stuff/set.html" class="internal-link" target="_self" rel="noopener nofollow">set</a> of <a data-tooltip-position="top" aria-label="Cosets" data-href="Cosets" href="the-guide/mathematics/group-theory/cosets.html" class="internal-link" target="_self" rel="noopener nofollow">cosets</a>  of  in  with the group law If  is not normal, then  still denotes the set of cosets, but the above operation is no longer well-defined.
<br>Intuition
A group that is obtained by aggregating similar elements of a larger group into a single element using an <a data-tooltip-position="top" aria-label="Equivalence Relation and Class" data-href="Equivalence Relation and Class" href="the-guide/mathematics/general-stuff/equivalence-relation-and-class.html" class="internal-link" target="_self" rel="noopener nofollow">equivalence relation</a> that preserves some of the group structure.- 
<br>
<br>Example

<br>The sphere  is a quotient group and thereby a <a data-tooltip-position="top" aria-label="Group Action" data-href="Group Action" href="the-guide/mathematics/group-theory/group-action.html" class="internal-link" target="_self" rel="noopener nofollow">homogeneous space</a> of : 


]]></description><link>the-guide/mathematics/group-theory/quotient-group.html</link><guid isPermaLink="false">The Guide/Mathematics/Group Theory/Quotient Group.md</guid><pubDate>Sat, 05 Apr 2025 16:23:52 GMT</pubDate></item><item><title><![CDATA[Schur's Lemma]]></title><description><![CDATA[ 
 <br>Lemma
Let  and  be <a data-tooltip-position="top" aria-label="Group Action" data-href="Group Action" href="the-guide/mathematics/group-theory/group-action.html" class="internal-link" target="_self" rel="noopener nofollow">irreducible representations</a> of the <a data-tooltip-position="top" aria-label="Group" data-href="Group" href="the-guide/mathematics/group-theory/group.html" class="internal-link" target="_self" rel="noopener nofollow">group</a>  and  and  be complex <a data-tooltip-position="top" aria-label="Vector Space" data-href="Vector Space" href="the-guide/mathematics/general-stuff/vector-space.html" class="internal-link" target="_self" rel="noopener nofollow">vector spaces</a>Suppose there is a <a data-tooltip-position="top" aria-label="Mathematical Relations" data-href="Mathematical Relations" href="the-guide/mathematics/general-stuff/mathematical-relations.html" class="internal-link" target="_self" rel="noopener nofollow">linear transformation</a>  that realizes an intertwining, i.e. In this case, it holds that 

<br>If  and  are not equivalent, then 
<br>If they are equivalent (implying ), then  with 

<br>We can show that a vector in the <a data-tooltip-position="top" aria-label="Matrices" data-href="Matrices" href="the-guide/mathematics/linear-algebra/matrices.html" class="internal-link" target="_self" rel="noopener nofollow">null space</a> of  is still in this null space after transformation by . That is for , we have Additionally, for a  there is a , such that , so ]]></description><link>the-guide/mathematics/group-theory/schur&apos;s-lemma.html</link><guid isPermaLink="false">The Guide/Mathematics/Group Theory/Schur&apos;s Lemma.md</guid><pubDate>Sat, 29 Mar 2025 19:37:09 GMT</pubDate></item><item><title><![CDATA[Topological Group]]></title><description><![CDATA[ 
 <br>Definition
A topological group, is a <a data-tooltip-position="top" aria-label="Topology and Topological Space" data-href="Topology and Topological Space" href="the-guide/mathematics/topology/topology-and-topological-space.html" class="internal-link" target="_self" rel="noopener nofollow">topological space</a> that is also a group such that the group operation and the inversion map are continuous.<br>
The product  is viewed as a topological space with the product topology.<br>
Such a topology is said to be compatible with the group operations' and is called a group topology.
<br><br><br>Continuous in the definition above means that for any <a data-tooltip-position="top" aria-label="Set" data-href="Set" href="the-guide/mathematics/general-stuff/set.html" class="internal-link" target="_self" rel="noopener nofollow">open set</a> , the map  is open in the domain of .<br>The product map is continuous if and only if for any  and any neighborhood  of  in , there exist neighborhoods  of  and  of  in  such that , where The inversion map is continuous if and only if for any  and any neighborhood  of  in , there exists a neighborhood  of  in , such that , where .<br>To show that a topology is compatible with the group operations, it suffices to check that the mapis continuous. <br>Explicitly, this means that for any  and any neighborhood  in  of , there exist neighborhoods  of g and  of  in  such that .<br><br><br>A '''homomorphism''' of topological groups means a continuous <a data-tooltip-position="top" aria-label="Mappings between Groups" data-href="Mappings between Groups" href="the-guide/mathematics/group-theory/mappings-between-groups.html" class="internal-link" target="_self" rel="noopener nofollow">group homomorphism</a> .<br>
A group homomorphism between topological groups is continuous if and only if it is continuous at ''some'' point.<br>
An '''isomorphism''' of topological groups is a <a data-tooltip-position="top" aria-label="Mappings between Groups" data-href="Mappings between Groups" href="the-guide/mathematics/group-theory/mappings-between-groups.html" class="internal-link" target="_self" rel="noopener nofollow">group isomorphism</a> that is also a <a data-tooltip-position="top" aria-label="Mathematical Relations" data-href="Mathematical Relations" href="the-guide/mathematics/general-stuff/mathematical-relations.html" class="internal-link" target="_self" rel="noopener nofollow">homeomorphism</a> of the underlying topological spaces.<br>
This is stronger than simply requiring a continuous group isomorphism—the inverse must also be continuous. <br>There are examples of topological groups that are isomorphic as ordinary groups but not as topological groups.<br>
Indeed, any non-discrete topological group is also a topological group when considered with the discrete topology.<br>
The underlying groups are the same, but as topological groups there is not an isomorphism.]]></description><link>the-guide/mathematics/group-theory/topological-group.html</link><guid isPermaLink="false">The Guide/Mathematics/Group Theory/Topological Group.md</guid><pubDate>Wed, 23 Apr 2025 08:34:37 GMT</pubDate></item><item><title><![CDATA[Adjoint Representation of SE(3) and its Lie Algebra]]></title><description><![CDATA[ 
 <br><br>The <a data-tooltip-position="top" aria-label="Adjoint Representations of Lie Group and Lie Algebra" data-href="Adjoint Representations of Lie Group and Lie Algebra" href="the-guide/mathematics/lie-theory/adjoint-representations-of-lie-group-and-lie-algebra.html" class="internal-link" target="_self" rel="noopener nofollow">adjoint representation</a> of  is the linear mapWriting this as a left multiplication a matrix with the vector representationyields (obtained by writing out the matrix-matrix product above and writing it as a matrix-vector product).<br>The commutator of  and  is given by <a data-tooltip-position="top" aria-label="Lie-Klammer" data-href="Lie-Klammer" href="the-guide/mathematics/differential-geometry/riemannian-geometry/mannigfaltigkeiten-und-differenzierbare-abbildungen/lie-klammer.html" class="internal-link" target="_self" rel="noopener nofollow">Lie bracket</a>  ]]></description><link>the-guide/mathematics/lie-theory/special-euclidean-group-se(3)/adjoint-representation-of-se(3)-and-its-lie-algebra.html</link><guid isPermaLink="false">The Guide/Mathematics/Lie Theory/Special Euclidean Group SE(3)/Adjoint Representation of SE(3) and its Lie Algebra.md</guid><pubDate>Sat, 29 Mar 2025 19:17:55 GMT</pubDate></item><item><title><![CDATA[The Lie Group SE(n) and its Lie Algebra]]></title><description><![CDATA[ 
 <br>The following works in general dimensions , but is used in applications such as <a data-tooltip-position="top" aria-label="Kinematics" data-href="Kinematics" href="the-guide/robotics,-dynamics-and-control/kinematics/kinematics.html" class="internal-link" target="_self" rel="noopener nofollow">kinematics</a> predominantly in D.<br>
We consider the matrix groupwhere  can also be defined as a group element.<br>Different Conventions
In literature, it is common that the convention of the vector representation of  varies between concatenating the rotational and the translational part or vice versa. We use translation first.
<br><br>Theorem -  as a <a data-href="Lie Group" href="the-guide/mathematics/lie-theory/lie-group.html" class="internal-link" target="_self" rel="noopener nofollow">Lie Group</a>
The roto-translation <a data-href="Group" href="the-guide/mathematics/group-theory/group.html" class="internal-link" target="_self" rel="noopener nofollow">Group</a>  under <a data-tooltip-position="top" aria-label="Matrices" data-href="Matrices" href="the-guide/mathematics/linear-algebra/matrices.html" class="internal-link" target="_self" rel="noopener nofollow">matrix</a> multiplication satisfied the <a data-href="Group" href="the-guide/mathematics/group-theory/group.html" class="internal-link" target="_self" rel="noopener nofollow">Group</a> axioms

<br>
<br>Identity element 
<br>Inverse element via opposite rotation and translation<br>
Additionally, the <a data-href="Group" href="the-guide/mathematics/group-theory/group.html" class="internal-link" target="_self" rel="noopener nofollow">Group</a> operation matrix multiplication and the inverse is continuous, which is why  is also a <a data-tooltip-position="top" aria-label="Mannigfaltigkeiten" data-href="Mannigfaltigkeiten" href="the-guide/mathematics/differential-geometry/riemannian-geometry/mannigfaltigkeiten-und-differenzierbare-abbildungen/mannigfaltigkeiten.html" class="internal-link" target="_self" rel="noopener nofollow">differentiable manifold</a> and therefore a <a data-href="Lie Group" href="the-guide/mathematics/lie-theory/lie-group.html" class="internal-link" target="_self" rel="noopener nofollow">Lie Group</a>.

<br><br><br>Every <a data-tooltip-position="top" aria-label="Matrices" data-href="Matrices" href="the-guide/mathematics/linear-algebra/matrices.html" class="internal-link" target="_self" rel="noopener nofollow">matrix</a> <a data-tooltip-position="top" aria-label="Lie Group" data-href="Lie Group" href="the-guide/mathematics/lie-theory/lie-group.html" class="internal-link" target="_self" rel="noopener nofollow">Lie group</a> is associated with a <a data-tooltip-position="top" aria-label="Lie Algebra" data-href="Lie Algebra" href="the-guide/mathematics/lie-theory/lie-algebra.html" class="internal-link" target="_self" rel="noopener nofollow">Lie algebra</a>, which consists of a vector space, specifically the <a data-tooltip-position="top" aria-label="Tangentialraum" data-href="Tangentialraum" href="the-guide/mathematics/differential-geometry/riemannian-geometry/mannigfaltigkeiten-und-differenzierbare-abbildungen/tangentialraum.html" class="internal-link" target="_self" rel="noopener nofollow">tangent space</a> (at the origin), and a binary operation called via the <a data-tooltip-position="top" aria-label="Lie-Klammer" data-href="Lie-Klammer" href="the-guide/mathematics/differential-geometry/riemannian-geometry/mannigfaltigkeiten-und-differenzierbare-abbildungen/lie-klammer.html" class="internal-link" target="_self" rel="noopener nofollow">Lie Bracket</a>.<br>Lie Algebra of SE(3)
Based on the same notation as <a data-tooltip-position="top" aria-label="The Lie Group SO(n) and its Lie Algebra" data-href="The Lie Group SO(n) and its Lie Algebra" href="the-guide/mathematics/lie-theory/special-orthogonal-group-so(3)/the-lie-group-so(n)-and-its-lie-algebra.html" class="internal-link" target="_self" rel="noopener nofollow">SO(n)</a>, the Lie Algebra  of  iswhich corresponds to the <a data-href="Set" href="the-guide/mathematics/general-stuff/set.html" class="internal-link" target="_self" rel="noopener nofollow">Set</a> of screw <a data-href="Matrices" href="the-guide/mathematics/linear-algebra/matrices.html" class="internal-link" target="_self" rel="noopener nofollow">Matrices</a>. As for , we can identify the elements with vectors of a vector space, this time . As before, we define a short-hand notation for switching between representationswhere each element can be encoded as 
<br><br><br>Mapping back and forth between elements  of the Lie algebra and elements  of the rotation <a data-tooltip-position="top" aria-label="Lie Group" data-href="Lie Group" href="the-guide/mathematics/lie-theory/lie-group.html" class="internal-link" target="_self" rel="noopener nofollow">Lie group</a> elements is achieved via the <a data-tooltip-position="top" aria-label="Exponentialabbildung" data-href="Exponentialabbildung" href="the-guide/mathematics/differential-geometry/riemannian-geometry/metriken-und-zusammenhänge/exponentialabbildung.html" class="internal-link" target="_self" rel="noopener nofollow">exponential</a> mapand the logarithm map <br>Using the infinite series of sine and cosine, we can derive a closed form expression for both exponential and log map. The exponential map is given by withwhere  is the angle of the axis-angle representation of .<br>The log-map based on can be obtained by first computing  using the log-map of <a data-tooltip-position="top" aria-label="The Lie Group SO(n) and its Lie Algebra" data-href="The Lie Group SO(n) and its Lie Algebra" href="the-guide/mathematics/lie-theory/special-orthogonal-group-so(3)/the-lie-group-so(n)-and-its-lie-algebra.html" class="internal-link" target="_self" rel="noopener nofollow">SO(3)</a> and then the translational part via<br><br><br>The Jacobians relate small changes in the Lie Algebra to changes on the manifold. <br>
<br>Left Jacobian 
<br><br><br>As a non-semisimple (abelian components, in this case because the euclidean part commutes) <a data-href="Lie Algebra" href="the-guide/mathematics/lie-theory/lie-algebra.html" class="internal-link" target="_self" rel="noopener nofollow">Lie Algebra</a>,  the associated <a data-href="Cartan-Killing Form" href="the-guide/mathematics/lie-theory/cartan-killing-form.html" class="internal-link" target="_self" rel="noopener nofollow">Cartan-Killing Form</a> is degenerate and does not provide a meaningful <a data-tooltip-position="top" aria-label="Metric Space and Completeness" data-href="Metric Space and Completeness" href="the-guide/mathematics/general-stuff/metric-space-and-completeness.html" class="internal-link" target="_self" rel="noopener nofollow">metric</a> on the entire <a data-tooltip-position="top" aria-label="Algebra" data-href="Algebra" href="the-guide/mathematics/general-stuff/algebra.html" class="internal-link" target="_self" rel="noopener nofollow">algebra</a>. <br>Instead, in applications we often seperate the contributions of the roational and translational part to obtain e.g. the metric<br>The only <a data-tooltip-position="top" aria-label="Left-Invariant and Fundamental Vector Fields" data-href="Left-Invariant and Fundamental Vector Fields" href="the-guide/mathematics/lie-theory/left-invariant-and-fundamental-vector-fields.html" class="internal-link" target="_self" rel="noopener nofollow">left-invariant</a> <a data-tooltip-position="top" aria-label="(Semi-) Riemannsche Metriken" data-href="(Semi-) Riemannsche Metriken" href="the-guide/mathematics/differential-geometry/riemannian-geometry/metriken-und-zusammenhänge/(semi-)-riemannsche-metriken.html" class="internal-link" target="_self" rel="noopener nofollow">metrics</a> (hold under change of inertial frame) on  are which are also the metrics of the product <a data-tooltip-position="top" aria-label="Mannigfaltigkeiten" data-href="Mannigfaltigkeiten" href="the-guide/mathematics/differential-geometry/riemannian-geometry/mannigfaltigkeiten-und-differenzierbare-abbildungen/mannigfaltigkeiten.html" class="internal-link" target="_self" rel="noopener nofollow">manifold</a> .]]></description><link>the-guide/mathematics/lie-theory/special-euclidean-group-se(3)/the-lie-group-se(n)-and-its-lie-algebra.html</link><guid isPermaLink="false">The Guide/Mathematics/Lie Theory/Special Euclidean Group SE(3)/The Lie Group SE(n) and its Lie Algebra.md</guid><pubDate>Wed, 23 Apr 2025 21:55:36 GMT</pubDate></item><item><title><![CDATA[Adjoint Representation of SO(3) and its Lie Algebra]]></title><description><![CDATA[ 
 <br>The <a data-tooltip-position="top" aria-label="Adjoint Representations of Lie Group and Lie Algebra" data-href="Adjoint Representations of Lie Group and Lie Algebra" href="the-guide/mathematics/lie-theory/adjoint-representations-of-lie-group-and-lie-algebra.html" class="internal-link" target="_self" rel="noopener nofollow">adjoint representation</a> of  is the linear mapWriting this as a left multiplication a matrix with the vector representationyields (obtained by writing out the matrix-matrix product above and writing it as a matrix-vector product) the <a data-href="Group" href="the-guide/mathematics/group-theory/group.html" class="internal-link" target="_self" rel="noopener nofollow">Group</a>  is self-adjoint. <br>The commutator is given by <a data-tooltip-position="top" aria-label="Lie-Klammer" data-href="Lie-Klammer" href="the-guide/mathematics/differential-geometry/riemannian-geometry/mannigfaltigkeiten-und-differenzierbare-abbildungen/lie-klammer.html" class="internal-link" target="_self" rel="noopener nofollow">Lie bracket</a> of the generators and has the commutation relation (first column is left element):<br><br>Intuition
Imagine an infinitesimal rotation around  followed by  for a point at . This means we consider a small tilt towards the -axis or positively around y seen from a position reached by a small tilt towards the y-axis. This corresponds to a small counter-clockwise rotation, which is precisely the resulting infinitesimal rotation around .
]]></description><link>the-guide/mathematics/lie-theory/special-orthogonal-group-so(3)/adjoint-representation-of-so(3)-and-its-lie-algebra.html</link><guid isPermaLink="false">The Guide/Mathematics/Lie Theory/Special Orthogonal Group SO(3)/Adjoint Representation of SO(3) and its Lie Algebra.md</guid><pubDate>Wed, 23 Apr 2025 21:55:37 GMT</pubDate></item><item><title><![CDATA[Metrics on SO(3)]]></title><description><![CDATA[ 
 <br>ToDo<br>
<br>work in results from <a rel="noopener nofollow" class="external-link" href="https://rotations.berkeley.edu/geodesics-of-the-rotation-group-so3/" target="_blank">https://rotations.berkeley.edu/geodesics-of-the-rotation-group-so3/</a>
<br>In order to obtain a <a data-tooltip-position="top" aria-label="(Semi-) Riemannsche Metriken" data-href="(Semi-) Riemannsche Metriken" href="the-guide/mathematics/differential-geometry/riemannian-geometry/metriken-und-zusammenhänge/(semi-)-riemannsche-metriken.html" class="internal-link" target="_self" rel="noopener nofollow">Riemannian metric</a> on , we can start by defining an <a data-tooltip-position="top" aria-label="Vector Space" data-href="Vector Space" href="the-guide/mathematics/general-stuff/vector-space.html" class="internal-link" target="_self" rel="noopener nofollow">inner product</a> by using the <a data-tooltip-position="top" aria-label="Cartan-Killing Form" data-href="Cartan-Killing Form" href="the-guide/mathematics/lie-theory/cartan-killing-form.html" class="internal-link" target="_self" rel="noopener nofollow">Killing form</a>Since scaling does not affect validity of a metric, any scaled version is also possible. Based on the above, a <a data-tooltip-position="top" aria-label="Metric Space and Completeness" data-href="Metric Space and Completeness" href="the-guide/mathematics/general-stuff/metric-space-and-completeness.html" class="internal-link" target="_self" rel="noopener nofollow">metric</a> can be defined in two ways as the magnitude of the relative rotation, depending on how we compute a difference betwen two rotations<br><br>Regardless of the chosen metric, we can derive the infinitesimal volume element based on the <a data-href="Haar Measure" href="the-guide/mathematics/lie-theory/haar-measure.html" class="internal-link" target="_self" rel="noopener nofollow">Haar Measure</a> to be where the left-Jacobian can be switched with the right Jacobian and integration is performed over all .]]></description><link>the-guide/mathematics/lie-theory/special-orthogonal-group-so(3)/metrics-on-so(3).html</link><guid isPermaLink="false">The Guide/Mathematics/Lie Theory/Special Orthogonal Group SO(3)/Metrics on SO(3).md</guid><pubDate>Sat, 05 Apr 2025 13:37:32 GMT</pubDate></item><item><title><![CDATA[The Lie Group SO(n) and its Lie Algebra]]></title><description><![CDATA[ 
 <br>The following works in general dimensions , but is used in applications predominantly in D.<br>
We consider the matrix group <br><br>Every <a data-tooltip-position="top" aria-label="Matrices" data-href="Matrices" href="the-guide/mathematics/linear-algebra/matrices.html" class="internal-link" target="_self" rel="noopener nofollow">matrix</a> <a data-tooltip-position="top" aria-label="Lie Group" data-href="Lie Group" href="the-guide/mathematics/lie-theory/lie-group.html" class="internal-link" target="_self" rel="noopener nofollow">Lie group</a> is associated with a <a data-tooltip-position="top" aria-label="Lie Algebra" data-href="Lie Algebra" href="the-guide/mathematics/lie-theory/lie-algebra.html" class="internal-link" target="_self" rel="noopener nofollow">Lie algebra</a>, which consists of a vector space, specifically the <a data-tooltip-position="top" aria-label="Tangentialraum" data-href="Tangentialraum" href="the-guide/mathematics/differential-geometry/riemannian-geometry/mannigfaltigkeiten-und-differenzierbare-abbildungen/tangentialraum.html" class="internal-link" target="_self" rel="noopener nofollow">tangent space</a> (at the origin), and a binary operation via the <a data-tooltip-position="top" aria-label="Lie-Klammer" data-href="Lie-Klammer" href="the-guide/mathematics/differential-geometry/riemannian-geometry/mannigfaltigkeiten-und-differenzierbare-abbildungen/lie-klammer.html" class="internal-link" target="_self" rel="noopener nofollow">Lie Bracket</a>.<br>For elements  of the rotation group , the associated <a data-tooltip-position="top" aria-label="Mannigfaltigkeiten" data-href="Mannigfaltigkeiten" href="the-guide/mathematics/differential-geometry/riemannian-geometry/mannigfaltigkeiten-und-differenzierbare-abbildungen/mannigfaltigkeiten.html" class="internal-link" target="_self" rel="noopener nofollow">manifold</a> structure results from the constraint . Taking the time derivative yields which means this combined expression is a skew-symmetric matrix. To get to the <a data-href="Lie Algebra" href="the-guide/mathematics/lie-theory/lie-algebra.html" class="internal-link" target="_self" rel="noopener nofollow">Lie Algebra</a>, we look at the identity element of rotations (identity matrix) and realize that then the expression says showing that the <a data-tooltip-position="top" aria-label="Set" data-href="Set" href="the-guide/mathematics/general-stuff/set.html" class="internal-link" target="_self" rel="noopener nofollow">set</a> of these skew-symmetric matrices form the <a data-tooltip-position="top" aria-label="Tangentialraum" data-href="Tangentialraum" href="the-guide/mathematics/differential-geometry/riemannian-geometry/mannigfaltigkeiten-und-differenzierbare-abbildungen/tangentialraum.html" class="internal-link" target="_self" rel="noopener nofollow">tangent space</a> at the identity and therefore our <a data-href="Lie Algebra" href="the-guide/mathematics/lie-theory/lie-algebra.html" class="internal-link" target="_self" rel="noopener nofollow">Lie Algebra</a>.<br><br><br>The vector space corresponding to the Lie algebra  of  can be defined via which corresponds to the <a data-tooltip-position="top" aria-label="Set" data-href="Set" href="the-guide/mathematics/general-stuff/set.html" class="internal-link" target="_self" rel="noopener nofollow">set</a> of skew-symmetric matrices. A commonly used basis of infinitesimal <a data-tooltip-position="top" aria-label="Lie Algebra" data-href="Lie Algebra" href="the-guide/mathematics/lie-theory/lie-algebra.html" class="internal-link" target="_self" rel="noopener nofollow">generators</a> (sometimes also called exponential coordinates) is<br>Because the matrices have <a data-tooltip-position="top" aria-label="Vector Space" data-href="Vector Space" href="the-guide/mathematics/general-stuff/vector-space.html" class="internal-link" target="_self" rel="noopener nofollow">vector space</a> structure, we can identify them with the euclidean vectors of their parameters<br>Intuition
This vector / <a data-tooltip-position="top" aria-label="A Hitchhiker's Guide to Rotation Representations" data-href="A Hitchhiker's Guide to Rotation Representations" href="the-guide/robotics,-dynamics-and-control/a-hitchhiker's-guide-to-rotation-representations.html" class="internal-link" target="_self" rel="noopener nofollow">axis-angle</a> representation  has geometric meaning: Let then  is a right-handed rotation of angle  around . The vector  can be interpreted as the vector of angular velocities and the <a data-tooltip-position="top" aria-label="One-Parameter Group" data-href="One-Parameter Group" href="the-guide/mathematics/lie-theory/one-parameter-group.html" class="internal-link" target="_self" rel="noopener nofollow">one-parameter subgroups</a>  maps it to rotations after unit time. The generators above are therefore perturbations around the three axes.
<br>Switching between the matrix and vector representation can be denoted via the hat  and , yielding <br><br><br>Simplified Exponential Map
A key insight for  and matrix groups in general is that the left-action of a group element on another element and also on its tangent vectors is simply matrix multiplication. Therefore, if we start at any element  of the <a data-href="Lie Algebra" href="the-guide/mathematics/lie-theory/lie-algebra.html" class="internal-link" target="_self" rel="noopener nofollow">Lie Algebra</a>, we can extend this to a vector field on the whole manifold by Conversely, integrating over tangent vectors to obtain a curve  requires where  assumes that the curve starts at identity (Lie Algebra). This <a data-tooltip-position="top" aria-label="ODEs - Ordinary Differential Equations" data-href="ODEs - Ordinary Differential Equations" href="the-guide/mathematics/differential-equations/odes-ordinary-differential-equations.html" class="internal-link" target="_self" rel="noopener nofollow">ODE</a> is solved by 
<br>Mapping back and forth between elements  of the Lie algebra and elements  of the rotation <a data-tooltip-position="top" aria-label="Lie Group" data-href="Lie Group" href="the-guide/mathematics/lie-theory/lie-group.html" class="internal-link" target="_self" rel="noopener nofollow">Lie group</a> elements is achieved via the <a data-tooltip-position="top" aria-label="Exponentialabbildung" data-href="Exponentialabbildung" href="the-guide/mathematics/differential-geometry/riemannian-geometry/metriken-und-zusammenhänge/exponentialabbildung.html" class="internal-link" target="_self" rel="noopener nofollow">exponential</a> mapand the logarithm map <br>Exp and Log Map
Using the infinite series of sine and cosine, we can derive a closed-form solution for the maps defined above. Specifically, the exponential map of  is the <a data-tooltip-position="top" aria-label="A Hitchhiker's Guide to Rotation Representations" data-href="A Hitchhiker's Guide to Rotation Representations" href="the-guide/robotics,-dynamics-and-control/a-hitchhiker's-guide-to-rotation-representations.html" class="internal-link" target="_self" rel="noopener nofollow">Rodrigues formula</a>We denote the capitalized version via Conversely, the logarithmic map can be derived by inverting the formula, yieldignIf  the axis is sometimes also just undefined.
<br>
<br>If , we have  and for any  
<br>The exponential map on  is thereby not injective, multiple rotations yield the same vector.
<br><br><br>Jacobians relate small changes in the Lie Algebra to the manifold / Lie Group.<br>
<br>Left Jacobian is given by the matrix seriesand it holds that .
<br>Right Jacobian is given by the matrix series
<br>For both, we have the identites
<br>For small perturbations , there are numerous usefull approximations of the infinite series above.<br>
<br>The Jacobians relate small perturbations in  to small perturbations in  via
<br>The Jacobians have the closed-form approximationsand the closed-from expressions
]]></description><link>the-guide/mathematics/lie-theory/special-orthogonal-group-so(3)/the-lie-group-so(n)-and-its-lie-algebra.html</link><guid isPermaLink="false">The Guide/Mathematics/Lie Theory/Special Orthogonal Group SO(3)/The Lie Group SO(n) and its Lie Algebra.md</guid><pubDate>Sat, 29 Mar 2025 19:29:18 GMT</pubDate></item><item><title><![CDATA[Topology of SO(3)]]></title><description><![CDATA[ 
 <br><a data-tooltip-position="top" aria-label="Topology and Topological Space" data-href="Topology and Topological Space" href="the-guide/mathematics/topology/topology-and-topological-space.html" class="internal-link" target="_self" rel="noopener nofollow">Topologically</a>, the <a data-tooltip-position="top" aria-label="The Lie Group SO(n) and its Lie Algebra" data-href="The Lie Group SO(n) and its Lie Algebra" href="the-guide/mathematics/lie-theory/special-orthogonal-group-so(3)/the-lie-group-so(n)-and-its-lie-algebra.html" class="internal-link" target="_self" rel="noopener nofollow">Lie Group of rotations</a>  is <a data-tooltip-position="top" aria-label="Homeomorphism" data-href="Homeomorphism" href="the-guide/mathematics/topology/homeomorphism.html" class="internal-link" target="_self" rel="noopener nofollow">homeomorphic</a> to the real projective space , which is obtained by identifying antipodal points of a solid 3-dimensional sphere of radius . The identity corresponds to the centre of the ball and the identification is necessary, because rotating by  is equivalent to rotating by .<br>This <a data-tooltip-position="top" aria-label="Mannigfaltigkeiten" data-href="Mannigfaltigkeiten" href="the-guide/mathematics/differential-geometry/riemannian-geometry/mannigfaltigkeiten-und-differenzierbare-abbildungen/mannigfaltigkeiten.html" class="internal-link" target="_self" rel="noopener nofollow">manifold</a> is not simply connected, since the <a data-tooltip-position="top" aria-label="Fundamentalgruppe einer Mannigfaltigkeit" data-href="Fundamentalgruppe einer Mannigfaltigkeit" href="the-guide/mathematics/differential-geometry/riemannian-geometry/mannigfaltigkeiten-und-differenzierbare-abbildungen/fundamentalgruppe-einer-mannigfaltigkeit.html" class="internal-link" target="_self" rel="noopener nofollow">fundamental group</a> is not trivial: There are two <a data-tooltip-position="top" aria-label="Homotope Kurven" data-href="Homotope Kurven" href="the-guide/mathematics/differential-geometry/elementary-differential-geometry/globale-kurven-und-flächentheorie/homotope-kurven.html" class="internal-link" target="_self" rel="noopener nofollow">homotopy classes</a> of curves, a closed curve completely inside the ball and full rotations (lines between antipodal points). <br>A very interesting property is that we do not have additional homotopy classes for multiple loops !<br>Quote
"A wiggle is homotopic to an octopus" - Behiel, 2024
<br>Intuition
Consider the <a data-tooltip-position="top" aria-label="https://en.wikipedia.org/wiki/Plate_trick" rel="noopener nofollow" class="external-link" href="https://en.wikipedia.org/wiki/Plate_trick" target="_blank">belt trick</a>, where rotating an object with strings attached to it by  does not return it into its original state, while a double rotation by  does.
<br>The universal cover of  is <a data-tooltip-position="top" aria-label="Unit Quaternions and SU(2)" data-href="Unit Quaternions and SU(2)" href="the-guide/mathematics/lie-theory/unit-quaternions-and-su(2).html" class="internal-link" target="_self" rel="noopener nofollow">group of unit quaternions</a>  , which is simply connected.]]></description><link>the-guide/mathematics/lie-theory/special-orthogonal-group-so(3)/topology-of-so(3).html</link><guid isPermaLink="false">The Guide/Mathematics/Lie Theory/Special Orthogonal Group SO(3)/Topology of SO(3).md</guid><pubDate>Wed, 23 Apr 2025 08:34:37 GMT</pubDate></item><item><title><![CDATA[Wigner-D Matrices]]></title><description><![CDATA[ 
 <br>In a Nutshell
<a data-tooltip-position="top" aria-label="Group Action" data-href="Group Action" href="the-guide/mathematics/group-theory/group-action.html" class="internal-link" target="_self" rel="noopener nofollow">Irreducible matrix representation</a> of the <a data-tooltip-position="top" aria-label="Group" data-href="Group" href="the-guide/mathematics/group-theory/group.html" class="internal-link" target="_self" rel="noopener nofollow">group</a> . Higher order representations are needed for the transformation of more complex objects. While vectors simply transform with the regular  matrices, e.g. <a data-tooltip-position="top" aria-label="Tensors" data-href="Tensors" href="the-guide/mathematics/general-stuff/tensors.html" class="internal-link" target="_self" rel="noopener nofollow">rank-2 tensors</a> (that are symmetric and traceless) require the  transformation.
<br><br>Definition
The (real) Wigner-D <a data-tooltip-position="top" aria-label="Matrices" data-href="Matrices" href="the-guide/mathematics/linear-algebra/matrices.html" class="internal-link" target="_self" rel="noopener nofollow">matrices</a> of type or frequency  are the <a data-tooltip-position="top" aria-label="Group Action" data-href="Group Action" href="the-guide/mathematics/group-theory/group-action.html" class="internal-link" target="_self" rel="noopener nofollow">irreducible</a> matrix representations of . They consist of - dimensional matrices where the Wigner-D functions  form an orthogonal basis for the  functions on . The matrices naturally act on -dimensional vectors, which we denote steerable vectors of type . naturally, the associated <a data-tooltip-position="top" aria-label="Vector Space" data-href="Vector Space" href="the-guide/mathematics/general-stuff/vector-space.html" class="internal-link" target="_self" rel="noopener nofollow">vector space</a> is denoted steerable vector space of type .  
<br>
<br>Type : A scalar cannot contain directional information, therefore the  matrix is just the identitythey are invariant under all possible rotations.
<br>Intuition
Can be seen as the constant part of a fourier transform, frequency . This denotes a constant function on the sphere.
<br>
<br>Type : A regular three-dimensional vector that transforms directly via the rotation matrix . For a specific choice of <a data-tooltip-position="top" aria-label="A Hitchhiker's Guide to Rotation Representations" data-href="A Hitchhiker's Guide to Rotation Representations" href="the-guide/robotics,-dynamics-and-control/a-hitchhiker's-guide-to-rotation-representations.html" class="internal-link" target="_self" rel="noopener nofollow">euler angles</a>  (there are many forms, also complex ones, because of trig identitites !) they may look likeThese types are invariant under rotations by .
<br>Type  A -dimensional representation is for example given byThese types are invariant under rotations by .
<br>Type : higher order elements represent higher frequencies and are invariant under rotations by 
<br><br>Theorem
The Wigner -D functions  form a complete orthogonal basis for functions on , meaning every function can be represented in such an  <a data-tooltip-position="top" aria-label="Fourier Series and Transform" data-href="Fourier Series and Transform" href="the-guide/integral-transforms-and-signals/fourier-series-and-transform.html" class="internal-link" target="_self" rel="noopener nofollow">Fourier series</a> where the summations are performed by the <a data-tooltip-position="top" aria-label="Matrices" data-href="Matrices" href="the-guide/mathematics/linear-algebra/matrices.html" class="internal-link" target="_self" rel="noopener nofollow">matrix</a> product and trace operator.
<br>
<br>This means that e.g. for , we have thatThe same holds for every row and column.
<br>The central columns  are invariant to rotations around chosen reference axes (compare to examples: they only contain  of the  angles). Mathematically, we sayIt can be shown that the subset of these -invariant basis functions coincide with the <a data-tooltip-position="top" aria-label="Spherical Harmonics" data-href="Spherical Harmonics" href="-daily-/notes-to-process/spherical-harmonics.html" class="internal-link" target="_self" rel="noopener nofollow">spherical harmonics</a>  on the <a data-tooltip-position="top" aria-label="Quotient Group" data-href="Quotient Group" href="the-guide/mathematics/group-theory/quotient-group.html" class="internal-link" target="_self" rel="noopener nofollow">quotient group</a> / <a data-tooltip-position="top" aria-label="Group Action" data-href="Group Action" href="the-guide/mathematics/group-theory/group-action.html" class="internal-link" target="_self" rel="noopener nofollow">homogeneous space</a> : This means that if I want to rotate a function on a sphere, I can transform it via a Fourier transform, shift it with the Wigner-D matrices and transform it back]]></description><link>the-guide/mathematics/lie-theory/special-orthogonal-group-so(3)/wigner-d-matrices.html</link><guid isPermaLink="false">The Guide/Mathematics/Lie Theory/Special Orthogonal Group SO(3)/Wigner-D Matrices.md</guid><pubDate>Sat, 29 Mar 2025 19:37:10 GMT</pubDate></item><item><title><![CDATA[Adjoint Representations of Lie Group and Lie Algebra]]></title><description><![CDATA[ 
 <br>In a Nutshell
Way of representing elements of a <a data-href="Lie Group" href="the-guide/mathematics/lie-theory/lie-group.html" class="internal-link" target="_self" rel="noopener nofollow">Lie Group</a> as linear transformations on the <a data-href="Lie Algebra" href="the-guide/mathematics/lie-theory/lie-algebra.html" class="internal-link" target="_self" rel="noopener nofollow">Lie Algebra</a>.
<br><br><br>Definition - Adjoint Representation of a Lie Group
Given a Lie Group  with Lie Algebra , we consider the mapping given by the <a data-tooltip-position="top" aria-label="Group Conjugacy" data-href="Group Conjugacy" href="the-guide/mathematics/group-theory/group-conjugacy.html" class="internal-link" target="_self" rel="noopener nofollow">conjugation</a> The Adjoint  is the linearization of this mapping at the identity, a <a data-tooltip-position="top" aria-label="Mathematical Relations" data-href="Mathematical Relations" href="the-guide/mathematics/general-stuff/mathematical-relations.html" class="internal-link" target="_self" rel="noopener nofollow">homomorphism</a> from  to the <a data-tooltip-position="top" aria-label="Group" data-href="Group" href="the-guide/mathematics/group-theory/group.html" class="internal-link" target="_self" rel="noopener nofollow">group</a> of <a data-tooltip-position="top" aria-label="Mathematical Relations" data-href="Mathematical Relations" href="the-guide/mathematics/general-stuff/mathematical-relations.html" class="internal-link" target="_self" rel="noopener nofollow">automorphisms</a> of :where  is the <a data-href="Pushforward Operator on Manifolds" href="the-guide/mathematics/differential-geometry/riemannian-geometry/mannigfaltigkeiten-und-differenzierbare-abbildungen/pushforward-operator-on-manifolds.html" class="internal-link" target="_self" rel="noopener nofollow">Pushforward Operator on Manifolds</a>.
<br>For matrix groups, the exponential map is always the matrix exponential and we can write Formally, any infinitesimal element close to the identity can be written as Explicitly using the <a data-tooltip-position="top" aria-label="Group Conjugacy" data-href="Group Conjugacy" href="the-guide/mathematics/group-theory/group-conjugacy.html" class="internal-link" target="_self" rel="noopener nofollow">conjugation</a> on this and using  yields <br>Intuition
Describes how group elements act on the Lie Algebra (the algebra of infinitesimal transformations), twisting the direction of infinitesimal transformations. For rotations, this is the global effect a rotation has on infinitesimal rotations, effectively transforming the infinitesimal rotation.
<br><br><br>The derivative of the above at identity passes from the representation of a Lie Group  to a representation of its Lie AlgebraFormally, we consider the group conjugate action of one element of the lie algebra  on another  via The <a data-tooltip-position="top" aria-label="Exponentialabbildung" data-href="Exponentialabbildung" href="the-guide/mathematics/differential-geometry/riemannian-geometry/metriken-und-zusammenhänge/exponentialabbildung.html" class="internal-link" target="_self" rel="noopener nofollow">exponential map</a> of a zero vector is the identity and the derivative of the exponential maps at the identity are again the elements of the Lie Algebra, which is why we obtain the <a data-tooltip-position="top" aria-label="Lie-Klammer" data-href="Lie-Klammer" href="the-guide/mathematics/differential-geometry/riemannian-geometry/mannigfaltigkeiten-und-differenzierbare-abbildungen/lie-klammer.html" class="internal-link" target="_self" rel="noopener nofollow">commutator</a><br>Definition - Adjoint representation of a Lie Algebra
Let  be a Lie Algebra over some field. For any element , the adjoint action of  on  is the map Any Lie algebra acts on itself via commutators.  
<br>Intuition
Describes how the algebras elements interact with each other. For rotations, this encodes how one infinitesimal rotation affects another. 
<br>Example<br>
<br>Given two infinitesimal rotations as skew-symmetric matrices, one around the , and one around the -axis, the adjoint will give us an infinitesimal rotation around . To grasp this, we have to think on an infinitesimal: tilting an object that is initially centered first around  and then around  yields a rotational part around . However, both rotations are only infinitesimal, which is why only this infinitesimal rotation around  is the result, without any actual tilting.
]]></description><link>the-guide/mathematics/lie-theory/adjoint-representations-of-lie-group-and-lie-algebra.html</link><guid isPermaLink="false">The Guide/Mathematics/Lie Theory/Adjoint Representations of Lie Group and Lie Algebra.md</guid><pubDate>Tue, 17 Sep 2024 15:51:19 GMT</pubDate></item><item><title><![CDATA[Cartan-Killing Form]]></title><description><![CDATA[ 
 <br>In a Nutshell
Bilinear form defined on the <a data-tooltip-position="top" aria-label="Lie Algebra" data-href="Lie Algebra" href="the-guide/mathematics/lie-theory/lie-algebra.html" class="internal-link" target="_self" rel="noopener nofollow">Lie algebra</a> of a <a data-href="Lie Group" href="the-guide/mathematics/lie-theory/lie-group.html" class="internal-link" target="_self" rel="noopener nofollow">Lie Group</a>, giving us an inner product and thereby a notion of distance and angles.
<br><br>Definition
For a finite-dimensional Lie algebra  over  (general field possible) with basis , the Killing form is defined via the trace of compositions of the <a data-tooltip-position="top" aria-label="Adjoint Representations of Lie Group and Lie Algebra" data-href="Adjoint Representations of Lie Group and Lie Algebra" href="the-guide/mathematics/lie-theory/adjoint-representations-of-lie-group-and-lie-algebra.html" class="internal-link" target="_self" rel="noopener nofollow">Adjoint representatios of the Lie Algebra</a>which is a symmetric bilinear form. Remember that the lowercase <a data-tooltip-position="top" aria-label="Adjoint Representations of Lie Group and Lie Algebra" data-href="Adjoint Representations of Lie Group and Lie Algebra" href="the-guide/mathematics/lie-theory/adjoint-representations-of-lie-group-and-lie-algebra.html" class="internal-link" target="_self" rel="noopener nofollow">adjoint</a> is given by the <a data-tooltip-position="top" aria-label="Lie-Klammer" data-href="Lie-Klammer" href="the-guide/mathematics/differential-geometry/riemannian-geometry/mannigfaltigkeiten-und-differenzierbare-abbildungen/lie-klammer.html" class="internal-link" target="_self" rel="noopener nofollow">commutator</a>.
<br><br>Intuition
There is an analogy to the inner product and the <a data-tooltip-position="top" aria-label="Matrices" data-href="Matrices" href="the-guide/mathematics/linear-algebra/matrices.html" class="internal-link" target="_self" rel="noopener nofollow">trace</a> of matrices, which is why the Killing form can be used to measure abstract angles and lengths on the <a data-tooltip-position="top" aria-label="Lie Algebra" data-href="Lie Algebra" href="the-guide/mathematics/lie-theory/lie-algebra.html" class="internal-link" target="_self" rel="noopener nofollow">Lie algebra</a>. Because vectors on the Lie Algebra extend to left-invariant <a data-tooltip-position="top" aria-label="Vektorfelder auf Mannigfaltigkeiten" data-href="Vektorfelder auf Mannigfaltigkeiten" href="the-guide/mathematics/differential-geometry/riemannian-geometry/mannigfaltigkeiten-und-differenzierbare-abbildungen/vektorfelder-auf-mannigfaltigkeiten.html" class="internal-link" target="_self" rel="noopener nofollow">vector fields</a>, the Killing form extends to a metric on the manifold.
<br><br><br>]]></description><link>the-guide/mathematics/lie-theory/cartan-killing-form.html</link><guid isPermaLink="false">The Guide/Mathematics/Lie Theory/Cartan-Killing Form.md</guid><pubDate>Sat, 29 Mar 2025 19:10:15 GMT</pubDate></item><item><title><![CDATA[Haar Measure]]></title><description><![CDATA[ 
 <br><br><br>Let  be a locally compact <a data-tooltip-position="top" aria-label="Topology and Topological Space" data-href="Topology and Topological Space" href="the-guide/mathematics/topology/topology-and-topological-space.html" class="internal-link" target="_self" rel="noopener nofollow">Haussdorf</a> <a data-tooltip-position="top" aria-label="Topological Group" data-href="Topological Group" href="the-guide/mathematics/group-theory/topological-group.html" class="internal-link" target="_self" rel="noopener nofollow">topological group</a>.  The  <a data-tooltip-position="top" aria-label="Sigma-Algebra" data-href="Sigma-Algebra" href="the-guide/mathematics/probability-theory/sigma-algebra.html" class="internal-link" target="_self" rel="noopener nofollow">Sigma-algebra</a> generated by all open subsets of  is called the <a data-tooltip-position="top" aria-label="Borel Set and Borel Sigma Algebra" data-href="Borel Set and Borel Sigma Algebra" href="the-guide/mathematics/probability-theory/borel-set-and-borel-sigma-algebra.html" class="internal-link" target="_self" rel="noopener nofollow">Borel Sigma algebra</a>.  If  is an element of  and  is a subset of , then we define the left and right <a data-tooltip-position="top" aria-label="Coset" data-href="Coset" href="Coset" class="internal-link" target="_self" rel="noopener nofollow">translates</a> of  by ' as follows:<br>
<br>Left translate:
<br>Right translate: 
<br>Left and right translates map Borel sets onto Borel sets.<br>Translation Invariant Measures
A measure  on the Borel subsets of  is called left-translation-invariant if for all Borel subsets  and all  one has:A measure  on the Borel subsets of  is called right-translation-invariant if for all Borel subsets  and all  one has:
<br><br><br>Haar's Theorem
There is, up to a positive multiplicative constant a unique countably additive nontrivial measure  on the <a data-tooltip-position="top" aria-label="Borel Set and Borel Sigma Algebra" data-href="Borel Set and Borel Sigma Algebra" href="the-guide/mathematics/probability-theory/borel-set-and-borel-sigma-algebra.html" class="internal-link" target="_self" rel="noopener nofollow">Borel subsets</a> of  satisfying

<br>The measure  is left-translation-invariant
<br>The measure  is finite on every compact set 
<br>The measure is outer regular
<br>The measure is inner regular

Such a measure is calles a left Haar measure. For every non-empty open set , it holds that . If  is compact, then the measure is finite and positive and can be normalized.
<br>Analogously, the existence of a right Haar measure can be proven. The two need not to coincide.<br><br><br>
<br>For discrete groups, the a left and right invariant Haar measure is the counting measure
<br>On the group  a Haar measure is the <a data-href="Lebesgue Measure" href="the-guide/mathematics/measure-theory/lebesgue-measure.html" class="internal-link" target="_self" rel="noopener nofollow">Lebesgue Measure</a>
<br>On compact groups, the Haar measure can be normalized, yielding a probability measure
]]></description><link>the-guide/mathematics/lie-theory/haar-measure.html</link><guid isPermaLink="false">The Guide/Mathematics/Lie Theory/Haar Measure.md</guid><pubDate>Wed, 23 Apr 2025 08:34:37 GMT</pubDate></item><item><title><![CDATA[Left-Invariant and Fundamental Vector Fields]]></title><description><![CDATA[ 
 <br><br>
<br>TODO

<br><a rel="noopener nofollow" class="external-link" href="https://math.stackexchange.com/questions/1725730/what-is-a-left-invariant-vector-field" target="_blank">https://math.stackexchange.com/questions/1725730/what-is-a-left-invariant-vector-field</a>


<br>For any <a data-tooltip-position="top" aria-label="Tangentialraum" data-href="Tangentialraum" href="the-guide/mathematics/differential-geometry/riemannian-geometry/mannigfaltigkeiten-und-differenzierbare-abbildungen/tangentialraum.html" class="internal-link" target="_self" rel="noopener nofollow">tangent vector</a>  of a <a data-href="Lie Group" href="the-guide/mathematics/lie-theory/lie-group.html" class="internal-link" target="_self" rel="noopener nofollow">Lie Group</a> , we can leverage the group structure to extend this vector to a <a data-tooltip-position="top" aria-label="Vektorfelder auf Mannigfaltigkeiten" data-href="Vektorfelder auf Mannigfaltigkeiten" href="the-guide/mathematics/differential-geometry/riemannian-geometry/mannigfaltigkeiten-und-differenzierbare-abbildungen/vektorfelder-auf-mannigfaltigkeiten.html" class="internal-link" target="_self" rel="noopener nofollow">vector field</a> on the whole <a data-tooltip-position="top" aria-label="Mannigfaltigkeiten" data-href="Mannigfaltigkeiten" href="the-guide/mathematics/differential-geometry/riemannian-geometry/mannigfaltigkeiten-und-differenzierbare-abbildungen/mannigfaltigkeiten.html" class="internal-link" target="_self" rel="noopener nofollow">manifold</a>. Any  <a data-tooltip-position="top" aria-label="Group Action" data-href="Group Action" href="the-guide/mathematics/group-theory/group-action.html" class="internal-link" target="_self" rel="noopener nofollow">acts</a> on elements  by <a data-tooltip-position="top" aria-label="Group Action" data-href="Group Action" href="the-guide/mathematics/group-theory/group-action.html" class="internal-link" target="_self" rel="noopener nofollow">left-action</a> The extension can be denoted using the <a data-tooltip-position="top" aria-label="Mathematical Relations" data-href="Mathematical Relations" href="the-guide/mathematics/general-stuff/mathematical-relations.html" class="internal-link" target="_self" rel="noopener nofollow">pushforward</a>  via pushing the vector field from the tangent space at  to the tangent space at . This operator is given by the <a data-tooltip-position="top" aria-label="Pushforward Operator on Manifolds" data-href="Pushforward Operator on Manifolds" href="the-guide/mathematics/differential-geometry/riemannian-geometry/mannigfaltigkeiten-und-differenzierbare-abbildungen/pushforward-operator-on-manifolds.html" class="internal-link" target="_self" rel="noopener nofollow">differential / pushforward on manifolds</a>. <br>Definition
A vector field  is left-invariant, if for  meaning that the vector field transforms consistent with the group structure.
<br>
<br>Example - For , any  has a left-invariant vector field  given bywhich is simply achieved by matrix multiplication.
<br>Intuition
Central consequence of the additional group structure we have on Lie groups compared to arbitrary manifolds. All the tangent spaces are isomorphic to each other (bijective linear map that preserves lengths and angles).<br>
This allows us to transfer local information at the identity (Lie algebra) to the whole group / manifold. In contrast, a space with varying curvature or non-uniform metric does not provide such an operation that preserves the structure of the tangent spaces.
<br><br><br>We can derive left-invariant vector fields based on the infinitesimal <a data-tooltip-position="top" aria-label="Lie Algebra" data-href="Lie Algebra" href="the-guide/mathematics/lie-theory/lie-algebra.html" class="internal-link" target="_self" rel="noopener nofollow">generators</a> of the <a data-tooltip-position="top" aria-label="Group Action" data-href="Group Action" href="the-guide/mathematics/group-theory/group-action.html" class="internal-link" target="_self" rel="noopener nofollow">group action</a>.<br>
<br>TODO

<br><a rel="noopener nofollow" class="external-link" href="https://en.wikipedia.org/wiki/Fundamental_vector_field" target="_blank">https://en.wikipedia.org/wiki/Fundamental_vector_field</a>
<br><a rel="noopener nofollow" class="external-link" href="https://books.physics.oregonstate.edu/GELG/leftinv.html" target="_blank">https://books.physics.oregonstate.edu/GELG/leftinv.html</a>


]]></description><link>the-guide/mathematics/lie-theory/left-invariant-and-fundamental-vector-fields.html</link><guid isPermaLink="false">The Guide/Mathematics/Lie Theory/Left-Invariant and Fundamental Vector Fields.md</guid><pubDate>Sat, 29 Mar 2025 19:37:09 GMT</pubDate></item><item><title><![CDATA[Lie Algebra]]></title><description><![CDATA[ 
 <br>Definition
A Lie <a data-tooltip-position="top" aria-label="Algebra" data-href="Algebra" href="the-guide/mathematics/general-stuff/algebra.html" class="internal-link" target="_self" rel="noopener nofollow">algebra</a>  is a <a data-tooltip-position="top" aria-label="Vector Space" data-href="Vector Space" href="the-guide/mathematics/general-stuff/vector-space.html" class="internal-link" target="_self" rel="noopener nofollow">vector space</a> together with an operation defined via the <a data-tooltip-position="top" aria-label="Lie-Klammer" data-href="Lie-Klammer" href="the-guide/mathematics/differential-geometry/riemannian-geometry/mannigfaltigkeiten-und-differenzierbare-abbildungen/lie-klammer.html" class="internal-link" target="_self" rel="noopener nofollow">Lie bracket</a>, an alternating bilinear <a data-tooltip-position="top" aria-label="Mathematical Relations" data-href="Mathematical Relations" href="the-guide/mathematics/general-stuff/mathematical-relations.html" class="internal-link" target="_self" rel="noopener nofollow">map</a> . For elements , the Lie bracket satisfies

<br>Anticommutativity 

<br>Equivalent 


<br>The Jacobi identity  (think of cycling through)

In Lie theory, it is also defined as the <a data-tooltip-position="top" aria-label="Tangentialraum" data-href="Tangentialraum" href="the-guide/mathematics/differential-geometry/riemannian-geometry/mannigfaltigkeiten-und-differenzierbare-abbildungen/tangentialraum.html" class="internal-link" target="_self" rel="noopener nofollow">tangent space</a> at the identity  of a <a data-href="Lie Group" href="the-guide/mathematics/lie-theory/lie-group.html" class="internal-link" target="_self" rel="noopener nofollow">Lie Group</a>
<br>Intuition
Because the Lie Algebra is a vector space, every element in it can be identified with a vector in , where  is the number of degrees of freedom of the <a data-href="Lie Group" href="the-guide/mathematics/lie-theory/lie-group.html" class="internal-link" target="_self" rel="noopener nofollow">Lie Group</a> (manifold with group structure) .
<br><br><br>The elements of a Lie Algebra have a non-trivial structure, e.g. skew-symmetric <a data-tooltip-position="top" aria-label="Matrices" data-href="Matrices" href="the-guide/mathematics/linear-algebra/matrices.html" class="internal-link" target="_self" rel="noopener nofollow">matrices</a>, imaginary numbers or pure Quaternions. However, it can be shown that these elements can be expressed by linear combinations of the basis elements  of the <a data-tooltip-position="top" aria-label="Tangentialraum" data-href="Tangentialraum" href="the-guide/mathematics/differential-geometry/riemannian-geometry/mannigfaltigkeiten-und-differenzierbare-abbildungen/tangentialraum.html" class="internal-link" target="_self" rel="noopener nofollow">tangent space</a>. These non-trivial basis elements are denoted generators. Because of this linear structure, the Lie Algebra is <a data-tooltip-position="top" aria-label="Mathematical Relations" data-href="Mathematical Relations" href="the-guide/mathematics/general-stuff/mathematical-relations.html" class="internal-link" target="_self" rel="noopener nofollow">isomorphic</a> to the linear vector space of .<br>Theorem
The Lie Algebra  is isomorphic to the euclidean vector spacewhere we can pass between both representations by inverse linear maps (isomorphisms) given by andwhere  are the elements of the standard basis of  (therefore ).
<br>
<br>Some authors denote the element of the Lie algebra as .
<br><br><br>Because of the additional structure of the manifold that is formalized by group theory, the <a data-tooltip-position="top" aria-label="Exponentialabbildung" data-href="Exponentialabbildung" href="the-guide/mathematics/differential-geometry/riemannian-geometry/metriken-und-zusammenhänge/exponentialabbildung.html" class="internal-link" target="_self" rel="noopener nofollow">exponential map</a> is given in closed form and any tangent space is isomorphic to the Lie Algebra.<br>
More formally, by the results of <a data-href="Left-Invariant and Fundamental Vector Fields" href="the-guide/mathematics/lie-theory/left-invariant-and-fundamental-vector-fields.html" class="internal-link" target="_self" rel="noopener nofollow">Left-Invariant and Fundamental Vector Fields</a>, we can extend any vector field from the Lie Algebra to every point on the manifold. For any tangent vector, we obtain a <a data-tooltip-position="top" aria-label="One-Parameter Group" data-href="One-Parameter Group" href="the-guide/mathematics/lie-theory/one-parameter-group.html" class="internal-link" target="_self" rel="noopener nofollow">one-parameter group</a>, a curve in  that is a <a data-tooltip-position="top" aria-label="Geodätische" data-href="Geodätische" href="the-guide/mathematics/differential-geometry/elementary-differential-geometry/intrinsische-geometrie-von-hyperflächen/geodätische.html" class="internal-link" target="_self" rel="noopener nofollow">geodesic</a>. For Lie groups, the homogeneous structure allows us to derive   <br>In some applications, it is useful to derive the capitalized versions of both maps, who essentially just incorporate the transformation via  back to the Lie Algebrawith  and .<br><img alt="center" src="lib/media/pasted-image-20240720115609.png" style="width: 500px; max-width: 100%;"><br><br><br>The linearization of the inner element of the <a data-tooltip-position="top" aria-label="Group Conjugacy" data-href="Group Conjugacy" href="the-guide/mathematics/group-theory/group-conjugacy.html" class="internal-link" target="_self" rel="noopener nofollow">conjugate</a> group action yields a linear operator on the Lie Algebra that encodes a change in perspective. The same can be extended to additionally linearizing the outer element, which yields the interaction of elements of the Lie Algebra itself, the <a data-tooltip-position="top" aria-label="Lie-Klammer" data-href="Lie-Klammer" href="the-guide/mathematics/differential-geometry/riemannian-geometry/mannigfaltigkeiten-und-differenzierbare-abbildungen/lie-klammer.html" class="internal-link" target="_self" rel="noopener nofollow">commutator</a>. For additional information see <a data-href="Adjoint Representations of Lie Group and Lie Algebra" href="the-guide/mathematics/lie-theory/adjoint-representations-of-lie-group-and-lie-algebra.html" class="internal-link" target="_self" rel="noopener nofollow">Adjoint Representations of Lie Group and Lie Algebra</a><br><br><br>
<br><a data-href="The Lie Group SO(n) and its Lie Algebra" href="the-guide/mathematics/lie-theory/special-orthogonal-group-so(3)/the-lie-group-so(n)-and-its-lie-algebra.html" class="internal-link" target="_self" rel="noopener nofollow">The Lie Group SO(n) and its Lie Algebra</a>
<br><a data-href="The Lie Group SE(n) and its Lie Algebra" href="the-guide/mathematics/lie-theory/special-euclidean-group-se(3)/the-lie-group-se(n)-and-its-lie-algebra.html" class="internal-link" target="_self" rel="noopener nofollow">The Lie Group SE(n) and its Lie Algebra</a>
<br>....
<br>The unit complex numbers 

<br>Group with operation complex multiplication, identity  and inverse of complex conjugate .
<br>Action on complex numbers through complex multiplication, results in rotation
<br><a data-tooltip-position="top" aria-label="Mannigfaltigkeiten" data-href="Mannigfaltigkeiten" href="the-guide/mathematics/differential-geometry/riemannian-geometry/mannigfaltigkeiten-und-differenzierbare-abbildungen/mannigfaltigkeiten.html" class="internal-link" target="_self" rel="noopener nofollow">Manifold</a> - unit norm constraint defines unit circle (1 DoF) with tangent lines that can be identified with , parametrized by .
<br>Exponential and Log Map<img alt="center" src="lib/media/pasted-image-20240720114743.png" style="width: 350px; max-width: 100%;">


<br><br><br>
<br><a rel="noopener nofollow" class="external-link" href="https://www.youtube.com/watch?v=IPzwqAVfce4" target="_blank">https://www.youtube.com/watch?v=IPzwqAVfce4</a>
]]></description><link>the-guide/mathematics/lie-theory/lie-algebra.html</link><guid isPermaLink="false">The Guide/Mathematics/Lie Theory/Lie Algebra.md</guid><pubDate>Sat, 29 Mar 2025 19:10:15 GMT</pubDate><enclosure url="lib/media/pasted-image-20240720115609.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src=&quot;lib/media/pasted-image-20240720115609.png&quot;&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[Lie Group]]></title><description><![CDATA[ 
 <br>In most cases <a data-tooltip-position="top" aria-label="Mannigfaltigkeiten" data-href="Mannigfaltigkeiten" href="the-guide/mathematics/differential-geometry/riemannian-geometry/mannigfaltigkeiten-und-differenzierbare-abbildungen/mannigfaltigkeiten.html" class="internal-link" target="_self" rel="noopener nofollow">manifolds</a> are not constructed via <a data-tooltip-position="top" aria-label="Topology and Topological Space" data-href="Topology and Topological Space" href="the-guide/mathematics/topology/topology-and-topological-space.html" class="internal-link" target="_self" rel="noopener nofollow">charts</a>, but via quotients of manifolds, e.g. constraint manifolds. For <a data-tooltip-position="top" aria-label="Mannigfaltigkeiten" data-href="Mannigfaltigkeiten" href="the-guide/mathematics/differential-geometry/riemannian-geometry/mannigfaltigkeiten-und-differenzierbare-abbildungen/mannigfaltigkeiten.html" class="internal-link" target="_self" rel="noopener nofollow">manifolds</a>, we can find equivalent notions of important notions from group theory. <br>Group acting on a Manifold
Let  be a manifold with a point  and  a <a data-tooltip-position="top" aria-label="Group" data-href="Group" href="the-guide/mathematics/group-theory/group.html" class="internal-link" target="_self" rel="noopener nofollow">group</a> with identity element . The group <a data-tooltip-position="top" aria-label="Group Action" data-href="Group Action" href="the-guide/mathematics/group-theory/group-action.html" class="internal-link" target="_self" rel="noopener nofollow">acts</a> on  (from the left), if there is a <a data-tooltip-position="top" aria-label="Mathematical Relations" data-href="Mathematical Relations" href="the-guide/mathematics/general-stuff/mathematical-relations.html" class="internal-link" target="_self" rel="noopener nofollow">map</a> satisfying
<br>
<br> acts transitive, if we can move between any two points on the manifold via a group element acting on one of them
<br>The orbit of a  is the <a data-tooltip-position="top" aria-label="Set" data-href="Set" href="the-guide/mathematics/general-stuff/set.html" class="internal-link" target="_self" rel="noopener nofollow">set</a> of points that can be reached by letting a group element act on  
<br>The stabilizer / isotry group of a point  is the set of group elements that map the point on itselfIf  for every , we say that the group acts freely. 
<br>Examples <br>
<br> does not act freely on , because the origin is never moved. The orbits are the spheres with the same absolute value as the initial point. Therefore, the group acts transitive on  (we can reach any point on a sphere), but only freely for  because rotations always affect all points (full rotations excluded)
<br> on  acts freely, we can only map an element to itself via identity, but not transitive, because we can only reach the natural numbers.
<br>Orbit Space
By the <a data-tooltip-position="top" aria-label="Equivalence Relation and Class" data-href="Equivalence Relation and Class" href="the-guide/mathematics/general-stuff/equivalence-relation-and-class.html" class="internal-link" target="_self" rel="noopener nofollow">equivalence relation</a> on  given by we define the orbit space  as the <a data-tooltip-position="top" aria-label="Quotient Space" data-href="Quotient Space" href="the-guide/mathematics/general-stuff/quotient-space.html" class="internal-link" target="_self" rel="noopener nofollow">quotient space</a> , the <a data-tooltip-position="top" aria-label="Set" data-href="Set" href="the-guide/mathematics/general-stuff/set.html" class="internal-link" target="_self" rel="noopener nofollow">set</a> of orbits on  under the action of . We write and .
<br>
<br>For  acting on a sphere , the orbit space is a single point, because group acts transitively.
<br>If in addition,  is differentiable, we can define the Lie group. <br><br><br>Lie Group
A -dimensional Lie Group  is a <a data-tooltip-position="top" aria-label="Mannigfaltigkeiten" data-href="Mannigfaltigkeiten" href="the-guide/mathematics/differential-geometry/riemannian-geometry/mannigfaltigkeiten-und-differenzierbare-abbildungen/mannigfaltigkeiten.html" class="internal-link" target="_self" rel="noopener nofollow">smooth manifold</a> whose elements satisfy the <a data-tooltip-position="top" aria-label="Group" data-href="Group" href="the-guide/mathematics/group-theory/group.html" class="internal-link" target="_self" rel="noopener nofollow">group</a> axioms. The following mappings have to be differentiable Every Lie group acts freely and transitive on itself. In addition, every subgroup  acts freely on .
<br>
<br>A Lie group  acts on am -dimensional manifold , if  satisfies the group axioms and is differentiable. 
<br>The operation is proper, if for every compact set , the set is compact as well.
<br>Theorem - Orbit Space inherits Manifold Structure
If a -dimensional Lie Group acts freely and proper on an -dimensional manifold , the space of orbits  is a -dimensional manifold. The <a data-tooltip-position="top" aria-label="Charts and Atlas" data-href="Charts and Atlas" href="the-guide/mathematics/differential-geometry/riemannian-geometry/mannigfaltigkeiten-und-differenzierbare-abbildungen/charts-and-atlas.html" class="internal-link" target="_self" rel="noopener nofollow">differentiable structure</a> follows from the fact that  is a submersion, it is differentiable of maximum rank .
<br>Examples<br>
<br> with lattice 
<br>...
<br>Proof
We'd have to show, that  ...

<br>has a unique differentiable structure
<br>is <a data-tooltip-position="top" aria-label="Topology and Topological Space" data-href="Topology and Topological Space" href="the-guide/mathematics/topology/topology-and-topological-space.html" class="internal-link" target="_self" rel="noopener nofollow">Hausdorff</a>
<br>fulfills the second axiom of countability
<br>locally eucliden<br>
In the lecture, we only showed how to construct <a data-tooltip-position="top" aria-label="Charts and Atlas" data-href="Charts and Atlas" href="the-guide/mathematics/differential-geometry/riemannian-geometry/mannigfaltigkeiten-und-differenzierbare-abbildungen/charts-and-atlas.html" class="internal-link" target="_self" rel="noopener nofollow">charts</a> of . We used

Theorem - Orbits of Lie Groups
Every orbit  of a Lie Group acting on  is a sub-manifold of . 

We define the orbit map This map is <a data-tooltip-position="top" aria-label="Mathematical Relations" data-href="Mathematical Relations" href="the-guide/mathematics/general-stuff/mathematical-relations.html" class="internal-link" target="_self" rel="noopener nofollow">injective</a>, because the group acts freely:Additionally,  acts transitive on itself, and it can be shown that . Thereby  immerses the manifold  to .
<br>Theorem - Orbit Space inherits Riemannian Structure
For  Riemannian and a -dimensional Lie Group  acting freely and proper through <a data-tooltip-position="top" aria-label="Isometrische Mannigfaltigkeiten" data-href="Isometrische Mannigfaltigkeiten" href="the-guide/mathematics/differential-geometry/riemannian-geometry/metriken-und-zusammenhänge/isometrische-mannigfaltigkeiten.html" class="internal-link" target="_self" rel="noopener nofollow">isometries</a>, there is a <a data-tooltip-position="top" aria-label="(Semi-) Riemannsche Metriken" data-href="(Semi-) Riemannsche Metriken" href="the-guide/mathematics/differential-geometry/riemannian-geometry/metriken-und-zusammenhänge/(semi-)-riemannsche-metriken.html" class="internal-link" target="_self" rel="noopener nofollow">Riemannianm metric</a>  on  with regard to which  is local isometry.
<br><br><br>Theorem - Bi-Invariant Metric
Let  be a <a data-tooltip-position="top" aria-label="Topology and Topological Space" data-href="Topology and Topological Space" href="the-guide/mathematics/topology/topology-and-topological-space.html" class="internal-link" target="_self" rel="noopener nofollow">compact</a> Lie Group. Then  admits a bi-invariant Riemannian metric, i.e. all diffeomorphisms resulting from left action  and right-action  are isometries.
<br><br><br>
<br><a rel="noopener nofollow" class="external-link" href="https://www.youtube.com/watch?v=ZRca3Ggpy_g&amp;t=372s" target="_blank">https://www.youtube.com/watch?v=ZRca3Ggpy_g&amp;t=372s</a>
]]></description><link>the-guide/mathematics/lie-theory/lie-group.html</link><guid isPermaLink="false">The Guide/Mathematics/Lie Theory/Lie Group.md</guid><pubDate>Wed, 23 Apr 2025 08:34:37 GMT</pubDate></item><item><title><![CDATA[Lie Group Connection]]></title><description><![CDATA[ 
 <br>Vectors in the <a data-tooltip-position="top" aria-label="Lie Algebra" data-href="Lie Algebra" href="the-guide/mathematics/lie-theory/lie-algebra.html" class="internal-link" target="_self" rel="noopener nofollow">Lie algebra</a> of a <a data-tooltip-position="top" aria-label="Lie Group" data-href="Lie Group" href="the-guide/mathematics/lie-theory/lie-group.html" class="internal-link" target="_self" rel="noopener nofollow">Lie group</a> extend <a data-tooltip-position="top" aria-label="Left-Invariant and Fundamental Vector Fields" data-href="Left-Invariant and Fundamental Vector Fields" href="the-guide/mathematics/lie-theory/left-invariant-and-fundamental-vector-fields.html" class="internal-link" target="_self" rel="noopener nofollow">left-invariant</a> <a data-tooltip-position="top" aria-label="Vektorfelder auf Mannigfaltigkeiten" data-href="Vektorfelder auf Mannigfaltigkeiten" href="the-guide/mathematics/differential-geometry/riemannian-geometry/mannigfaltigkeiten-und-differenzierbare-abbildungen/vektorfelder-auf-mannigfaltigkeiten.html" class="internal-link" target="_self" rel="noopener nofollow">vectorfields</a> on the whole <a data-tooltip-position="top" aria-label="Mannigfaltigkeiten" data-href="Mannigfaltigkeiten" href="the-guide/mathematics/differential-geometry/riemannian-geometry/mannigfaltigkeiten-und-differenzierbare-abbildungen/mannigfaltigkeiten.html" class="internal-link" target="_self" rel="noopener nofollow">manifold</a> via <a data-tooltip-position="top" aria-label="Group Action" data-href="Group Action" href="the-guide/mathematics/group-theory/group-action.html" class="internal-link" target="_self" rel="noopener nofollow">left-action</a>. Thus, the <a data-tooltip-position="top" aria-label="Vector Space" data-href="Vector Space" href="the-guide/mathematics/general-stuff/vector-space.html" class="internal-link" target="_self" rel="noopener nofollow">inner product</a> notion given by the <a data-href="Cartan-Killing Form" href="the-guide/mathematics/lie-theory/cartan-killing-form.html" class="internal-link" target="_self" rel="noopener nofollow">Cartan-Killing Form</a> extends to a <a data-tooltip-position="top" aria-label="(Semi-) Riemannsche Metriken" data-href="(Semi-) Riemannsche Metriken" href="the-guide/mathematics/differential-geometry/riemannian-geometry/metriken-und-zusammenhänge/(semi-)-riemannsche-metriken.html" class="internal-link" target="_self" rel="noopener nofollow">Riemannian metric</a> that is constant on left-invariant vector fields by construction. From Riemannian geometry, we know that given this metric we can construct a <a data-tooltip-position="top" aria-label="Zusammenhänge" data-href="Zusammenhänge" href="the-guide/mathematics/differential-geometry/riemannian-geometry/metriken-und-zusammenhänge/zusammenhänge.html" class="internal-link" target="_self" rel="noopener nofollow">torsion-free and compatible connection</a> in form of the <a data-tooltip-position="top" aria-label="Levi-Civita-Zusammenhang und Koszul-Formel" data-href="Levi-Civita-Zusammenhang und Koszul-Formel" href="the-guide/mathematics/differential-geometry/riemannian-geometry/metriken-und-zusammenhänge/levi-civita-zusammenhang-und-koszul-formel.html" class="internal-link" target="_self" rel="noopener nofollow">Levi-Civita Connection</a> via the <a data-tooltip-position="top" aria-label="Levi-Civita-Zusammenhang und Koszul-Formel" data-href="Levi-Civita-Zusammenhang und Koszul-Formel" href="the-guide/mathematics/differential-geometry/riemannian-geometry/metriken-und-zusammenhänge/levi-civita-zusammenhang-und-koszul-formel.html" class="internal-link" target="_self" rel="noopener nofollow">Koszul-equation</a>.<br><br>Given the <a data-href="Cartan-Killing Form" href="the-guide/mathematics/lie-theory/cartan-killing-form.html" class="internal-link" target="_self" rel="noopener nofollow">Cartan-Killing Form</a> , we know a connection  is metric compatible, if for <a data-tooltip-position="top" aria-label="Vektorfelder auf Mannigfaltigkeiten" data-href="Vektorfelder auf Mannigfaltigkeiten" href="the-guide/mathematics/differential-geometry/riemannian-geometry/mannigfaltigkeiten-und-differenzierbare-abbildungen/vektorfelder-auf-mannigfaltigkeiten.html" class="internal-link" target="_self" rel="noopener nofollow">vector fields</a>  it holds that Because the Cartan-Killing form is constant when acting on left-invariant vector fields, both sides are in fact zero. For the connection to be torsion-free, we need to have that<br>Definition
Inserting this into the Koszul-formula, we getwhere the originally six terms got reduced because of the vanishing condition for metric compatibility and the remaining three terms cancel to one because of the symmetry property of . Since  is arbitrary and  non-degenerate, we conclude that 
]]></description><link>the-guide/mathematics/lie-theory/lie-group-connection.html</link><guid isPermaLink="false">The Guide/Mathematics/Lie Theory/Lie Group Connection.md</guid><pubDate>Sat, 29 Mar 2025 19:37:09 GMT</pubDate></item><item><title><![CDATA[One-Parameter Group]]></title><description><![CDATA[ 
 <br><br>Definition
A continuous <a data-tooltip-position="top" aria-label="Group" data-href="Group" href="the-guide/mathematics/group-theory/group.html" class="internal-link" target="_self" rel="noopener nofollow">group</a> <a data-tooltip-position="top" aria-label="Mathematical Relations" data-href="Mathematical Relations" href="the-guide/mathematics/general-stuff/mathematical-relations.html" class="internal-link" target="_self" rel="noopener nofollow">homomorpishm</a> ( is additive group)from the reals to some topological group . If this map is <a data-tooltip-position="top" aria-label="Mathematical Relations > Relating Sets - Functions" data-href="Mathematical Relations#Relating Sets - Functions" href="the-guide/mathematics/general-stuff/mathematical-relations.html#Relating_Sets_-_Functions" class="internal-link" target="_self" rel="noopener nofollow">injective</a> its image will be a subgroup of  that is isomorphic to .
<br>Intuition
Consider two rotations encoded by two elements of . A path between the two can be constructed by intermediate rotations that are encoded by a single parameter . If the resulting curve does not intersect itself (which would map a rotation to multiple ), this creates an isomorphism between the reals and the subgroup of the rotations along that path. The subgroup structure intuitively means that we can again combine any rotations along that path, the result will always be on the path.
]]></description><link>the-guide/mathematics/lie-theory/one-parameter-group.html</link><guid isPermaLink="false">The Guide/Mathematics/Lie Theory/One-Parameter Group.md</guid><pubDate>Mon, 09 Sep 2024 15:37:48 GMT</pubDate></item><item><title><![CDATA[Unit Quaternions and SU(2)]]></title><description><![CDATA[ 
 <br>For a more intuitive introduction based on <a data-href="Geometric Algebra" href="the-guide/mathematics/geometric-algebra/geometric-algebra.html" class="internal-link" target="_self" rel="noopener nofollow">Geometric Algebra</a>, see <a data-href="Less Weird Quaternions - A Geometric Algebra Perspective" href="the-guide/robotics,-dynamics-and-control/less-weird-quaternions-a-geometric-algebra-perspective.html" class="internal-link" target="_self" rel="noopener nofollow">Less Weird Quaternions - A Geometric Algebra Perspective</a>.<br><br><br>Special case of <a data-href="Quaternions" href="the-guide/mathematics/general-stuff/quaternions.html" class="internal-link" target="_self" rel="noopener nofollow">Quaternions</a><br>The elements of the <a data-href="Lie Algebra" href="the-guide/mathematics/lie-theory/lie-algebra.html" class="internal-link" target="_self" rel="noopener nofollow">Lie Algebra</a> have a direct relation with the velocity vectors  of . For any , the corresponding velocity vector is where the factor is directly related to the double-cover.<br>Isomorphism to the SU(2)
Every unit <a data-tooltip-position="top" aria-label="Quaternions" data-href="Quaternions" href="the-guide/mathematics/general-stuff/quaternions.html" class="internal-link" target="_self" rel="noopener nofollow">Quaternion</a>   can be written as The <a data-tooltip-position="top" aria-label="Set" data-href="Set" href="the-guide/mathematics/general-stuff/set.html" class="internal-link" target="_self" rel="noopener nofollow">subset</a> of unit quaternions is defined as and forms a <a data-tooltip-position="top" aria-label="Group" data-href="Group" href="the-guide/mathematics/group-theory/group.html" class="internal-link" target="_self" rel="noopener nofollow">group</a> that is <a data-tooltip-position="top" aria-label="Mappings between Groups" data-href="Mappings between Groups" href="the-guide/mathematics/group-theory/mappings-between-groups.html" class="internal-link" target="_self" rel="noopener nofollow">isomorphic</a> to the <a data-href="Lie Group" href="the-guide/mathematics/lie-theory/lie-group.html" class="internal-link" target="_self" rel="noopener nofollow">Lie Group</a>  under the isomorphism 
<br><br><br>...<br>]]></description><link>the-guide/mathematics/lie-theory/unit-quaternions-and-su(2).html</link><guid isPermaLink="false">The Guide/Mathematics/Lie Theory/Unit Quaternions and SU(2).md</guid><pubDate>Sun, 30 Mar 2025 14:55:04 GMT</pubDate></item><item><title><![CDATA[Arnoldi Method]]></title><description><![CDATA[ 
 <br>Info
Construct orthonormal basis of <a data-tooltip-position="top" aria-label="Krylov Space Methods" data-href="Krylov Space Methods" href="the-guide/mathematics/linear-algebra/krylov-space-methods.html" class="internal-link" target="_self" rel="noopener nofollow">Krylov space</a> that can be extended on the fly by finding successively ON basis  of Krylov space  and an upper Hessenberg <a data-tooltip-position="top" aria-label="Matrices" data-href="Matrices" href="the-guide/mathematics/linear-algebra/matrices.html" class="internal-link" target="_self" rel="noopener nofollow">matrix</a>  associated to it ( in subspace )
<br><br><br>
<br>Input ,  and number of iterations 
<br>Init 
<br>For  : successively add vector to basis and then orthogonalize w.r.t. other vectors for better separation:

<br>Arnoldi step 

<br>


<br>Modified Gram-Schmidt 

<br>For  : orthogonalisation of  w.r.t. to , mostly done by <a data-href="Gram-Schmidt-Process" href="the-guide/mathematics/linear-algebra/gram-schmidt-process.html" class="internal-link" target="_self" rel="noopener nofollow">Gram-Schmidt-Process</a>

<br>Parameters are stored as 




<br>Compute 
<br>Normalization 

<br>If  stop
<br>Else 




<br>Using the matrices the algorithm above can be compactly written as and it holds that .<br><br><br>
<br>If  is <a data-tooltip-position="top" aria-label="Derivative, Gradient, Jacobian and Hessian" data-href="Derivative, Gradient, Jacobian and Hessian" href="the-guide/mathematics/analysis-and-calculus/derivative,-gradient,-jacobian-and-hessian.html" class="internal-link" target="_self" rel="noopener nofollow">Jacobian</a> of a <a data-tooltip-position="top" aria-label="Mathematical Relations" data-href="Mathematical Relations" href="the-guide/mathematics/general-stuff/mathematical-relations.html" class="internal-link" target="_self" rel="noopener nofollow">function</a> , then the product  can be approximated via . Therefore, the solution can be approximated without computing the product or storing the matrix !
<br>The numerical cost of the loop increases with iterations. Additionally the basis has to be reorthogonalised with increasing iterations (numerical precision ?)
<br>If  is symmetric  is tridiagonal,  has to be orthogonalized w.r.t the last two vectors only.

<br>Leads to very efficient 3-term recursion algorithm
<br>Leads to <a data-tooltip-position="top" aria-label="Lanczos Method" data-href="Lanczos Method" href="the-guide/mathematics/linear-algebra/lanczos-method.html" class="internal-link" target="_self" rel="noopener nofollow">Lanczos method</a>


]]></description><link>the-guide/mathematics/linear-algebra/arnoldi-method.html</link><guid isPermaLink="false">The Guide/Mathematics/Linear Algebra/Arnoldi Method.md</guid><pubDate>Sat, 25 Jan 2025 17:52:28 GMT</pubDate></item><item><title><![CDATA[Bilinear Form]]></title><description><![CDATA[ 
 <br>Bilinear map on a <a data-tooltip-position="top" aria-label="Vector Space" data-href="Vector Space" href="the-guide/mathematics/general-stuff/vector-space.html" class="internal-link" target="_self" rel="noopener nofollow">vector space</a> <br>Ausgeartet [Degenerate]
Eine symmetrisch Bilinearform  auf einem <a data-tooltip-position="top" aria-label="Vector Space" data-href="Vector Space" href="the-guide/mathematics/general-stuff/vector-space.html" class="internal-link" target="_self" rel="noopener nofollow">Vektorraum</a>  heißt ausgeartet, wenn es ein  gibt, sodass  für alle . Der Index  der Bilinearform bezeichnet die maximale Dimension eines Unterraums, auf dem  gilt. Die Form ist positiv definit, wenn  oder äquivalent  für alle  außer dem Nullvektor.
<br>
<br><a rel="noopener nofollow" class="external-link" href="https://en.wikipedia.org/wiki/Bilinear_form" target="_blank">https://en.wikipedia.org/wiki/Bilinear_form</a>
]]></description><link>the-guide/mathematics/linear-algebra/bilinear-form.html</link><guid isPermaLink="false">The Guide/Mathematics/Linear Algebra/Bilinear Form.md</guid><pubDate>Mon, 24 Feb 2025 23:32:25 GMT</pubDate></item><item><title><![CDATA[Condition Number]]></title><description><![CDATA[ 
 <br>In a Nutshell
Quantifies how sensitive a problem is to perturbations of the input data. The higher this number, the more the solution can change from small perturbations of the input.
<br><br>
<br><a data-href="Matrices" href="the-guide/mathematics/linear-algebra/matrices.html" class="internal-link" target="_self" rel="noopener nofollow">Matrices</a>

<br>For  and <a data-tooltip-position="top" aria-label="Norms" data-href="Norms" href="the-guide/mathematics/linear-algebra/norms.html" class="internal-link" target="_self" rel="noopener nofollow">norm</a> , the relative conditional number is defined via 


<br>LGS

<br>For  <a data-tooltip-position="top" aria-label="Matrices" data-href="Matrices" href="the-guide/mathematics/linear-algebra/matrices.html" class="internal-link" target="_self" rel="noopener nofollow">regular</a> and  


<br><br>
<br><a data-tooltip-position="top" aria-label="Eigenvalue Problem" data-href="Eigenvalue Problem" href="the-guide/mathematics/linear-algebra/eigenvalue-problem.html" class="internal-link" target="_self" rel="noopener nofollow">Eigenvalue Problem</a>

<br>,  is singular eigenvalue,  is eigenvector to  and  is adjoint eigenvector. All norms are  2-<a data-tooltip-position="top" aria-label="Norms" data-href="Norms" href="the-guide/mathematics/linear-algebra/norms.html" class="internal-link" target="_self" rel="noopener nofollow">norms</a>.

<br>Absolute condition 

<br>If  symmetric, 
<br>Leads to condition 


<br>Relative condition

<br>leads to condition 






]]></description><link>the-guide/mathematics/linear-algebra/condition-number.html</link><guid isPermaLink="false">The Guide/Mathematics/Linear Algebra/Condition Number.md</guid><pubDate>Sun, 26 Jan 2025 18:55:20 GMT</pubDate></item><item><title><![CDATA[Conjugate Gradient Method]]></title><description><![CDATA[ 
 <br>In a Nutshell
Extension of <a data-tooltip-position="top" aria-label="Lanczos Method" data-href="Lanczos Method" href="the-guide/mathematics/linear-algebra/lanczos-method.html" class="internal-link" target="_self" rel="noopener nofollow">Lanczos method</a> that doesn't need to save the whole <a data-tooltip-position="top" aria-label="Matrices" data-href="Matrices" href="the-guide/mathematics/linear-algebra/matrices.html" class="internal-link" target="_self" rel="noopener nofollow">matrix</a> / to compute approximation , assuming that  is <a data-tooltip-position="top" aria-label="Matrices" data-href="Matrices" href="the-guide/mathematics/linear-algebra/matrices.html" class="internal-link" target="_self" rel="noopener nofollow">symmetric and positive definite</a> (spd). 
<br><br><br> For a spd matrix , it holds that<br>
<br> for  defines a scalar product on  with induced norm . 
<br>The vectors  are denoted -orthogonal / -conjugate, if .
<br>Best-Approximation 	 - The approximation of the method is the best possible in the <a data-tooltip-position="top" aria-label="Krylov Space Methods" data-href="Krylov Space Methods" href="the-guide/mathematics/linear-algebra/krylov-space-methods.html" class="internal-link" target="_self" rel="noopener nofollow">Krylov space</a> under -norm.<br>
<img alt="center" src="lib/media/pasted-image-20230711185649.png" style="width: 500px; max-width: 100%;">
<br>(a) is -orthogonal, because (b) is orthogonal (space in (a) results from linear transformation).
<br><br><br>
<br>From the <a data-tooltip-position="top" aria-label="Lanczos Method" data-href="Lanczos Method" href="the-guide/mathematics/linear-algebra/lanczos-method.html" class="internal-link" target="_self" rel="noopener nofollow">Lanczos method</a> derivation, we can derive via orthogonality, that	- All matrices are spd<br>
-  is tridiagonal and can be decomposed using <a data-tooltip-position="top" aria-label="LU-Decomposition" data-href="LU-Decomposition" href="the-guide/mathematics/linear-algebra/lu-decomposition.html" class="internal-link" target="_self" rel="noopener nofollow">LU-decomposition</a> into two bi-diagonal matrices .
<br>Using the above, we can write the solution to the <a data-tooltip-position="top" aria-label="Lanczos Method" data-href="Lanczos Method" href="the-guide/mathematics/linear-algebra/lanczos-method.html" class="internal-link" target="_self" rel="noopener nofollow">Lanczos method</a> as 

<br> can be solved easily due to the bi-diagonal structure. This yields  and .
<br> leads to 


<br> and  can be computed by extending  and , leading to the formula 

<br>Resulting vectors are -orthogonal: 


<br>One can show that 
<br>Advantages<img alt="center" src="lib/media/pasted-image-20230823133414.png" style="width: 400px; max-width: 100%;">

<br>Each iteration only needs to store  and 
<br>Each iteration consists of 2 dotproducts and 1 matrix-vector product


<br><br><br>Let  be the <a data-tooltip-position="top" aria-label="Condition Number" data-href="Condition Number" href="the-guide/mathematics/linear-algebra/condition-number.html" class="internal-link" target="_self" rel="noopener nofollow">condition number</a> of the <a data-tooltip-position="top" aria-label="Matrices" data-href="Matrices" href="the-guide/mathematics/linear-algebra/matrices.html" class="internal-link" target="_self" rel="noopener nofollow">matrix</a> . Then the error with iteration  behaves as The CG algorithm converges faster for well-conditioned matrices ! If condition number is high<br>
<br>Preconditioning to improve condition: .
]]></description><link>the-guide/mathematics/linear-algebra/conjugate-gradient-method.html</link><guid isPermaLink="false">The Guide/Mathematics/Linear Algebra/Conjugate Gradient Method.md</guid><pubDate>Sun, 26 Jan 2025 18:55:03 GMT</pubDate><enclosure url="lib/media/pasted-image-20230711185649.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src=&quot;lib/media/pasted-image-20230711185649.png&quot;&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[Construction of approximate Solution]]></title><description><![CDATA[ 
 <br>Question
How to choose iterates / approximate solutions  for <a data-tooltip-position="top" aria-label="Krylov Space Methods" data-href="Krylov Space Methods" href="the-guide/mathematics/linear-algebra/krylov-space-methods.html" class="internal-link" target="_self" rel="noopener nofollow">Krylov Space methods</a> to solve  starting from  ? 
<br>
<br>Prerequisites

<br>Matrices of -th <a data-tooltip-position="top" aria-label="Arnoldi Method" data-href="Arnoldi Method" href="the-guide/mathematics/linear-algebra/arnoldi-method.html" class="internal-link" target="_self" rel="noopener nofollow">Arnoldi step</a>, namely orthogonal basis  of  and upper Hessenberg , such that . 


<br>Assumptions

<br>Without loss of generality we set , which leads to .


<br><br>Ritz-Galerkin Ansatz
At iteration , choose approximate solution  such that .
<br>
<br>Because of assumptions, it holds that  and therefore . Using this and the matrices of the -th Arnoldi step , the idea of Ritz-Galerkin can be formalized by  The new iterate is then computed by using 

<br> contains parameters for eigenvectors 


<br>Derived Methods

<br>Full Orthogonalized Method (FOM) 

<br>As above<img alt="center" src="lib/media/pasted-image-20230823131134.png" style="width: 400px; max-width: 100%;">


<br><a data-href="Lanczos Method" href="the-guide/mathematics/linear-algebra/lanczos-method.html" class="internal-link" target="_self" rel="noopener nofollow">Lanczos Method</a>

<br> <a data-tooltip-position="top" aria-label="Matrices" data-href="Matrices" href="the-guide/mathematics/linear-algebra/matrices.html" class="internal-link" target="_self" rel="noopener nofollow">symmetric</a>
<br>More efficient storage and computation of , convergence test doesn't need additional computation.


<br><a data-href="Conjugate Gradient Method" href="the-guide/mathematics/linear-algebra/conjugate-gradient-method.html" class="internal-link" target="_self" rel="noopener nofollow">Conjugate Gradient Method</a> (CG)

<br> <a data-tooltip-position="top" aria-label="Matrices" data-href="Matrices" href="the-guide/mathematics/linear-algebra/matrices.html" class="internal-link" target="_self" rel="noopener nofollow">symmetric and positive definit</a> 




<br><br>Minimal Residuum Ansatz
At iteration , choose  such that  is minimal, i.e. 
<br>
<br>The solution can be computed via 
<br>Derived Methods

<br>General Minimum Residual (GMRES) 

<br>As above


<br>Minimum Residual Method (MINRES)

<br> is symmetric 




<br><br>Petrov-Galerkin Ansatz
At iteration , Choose  such that  for some  and .
<br>
<br>
Motivation - This ansatz searches for a non-orthogonal basis , that is orthogonal to another space in order to reach the very efficient 3-term recursion for both bases.

<br>
We assume a matrix , such that and If all the  are non-zero, the two bases are biorthogonal, meaning  The aim of this ansatz is to find a , such that  is tridiagonal.

<br>
Derived Methods

<br>Biorthogonal Conjugate Gradient (BiCG)
<br>Conjugate Gradient Squared (CGS)
<br>Biorthogonal Conjugate Gradient Stabilized (BiCGStab)


<br><br>Minimal Error-Norm Ansatz
Assuming  symmetric, choose , such that  is minimal.
]]></description><link>the-guide/mathematics/linear-algebra/construction-of-approximate-solution.html</link><guid isPermaLink="false">The Guide/Mathematics/Linear Algebra/Construction of approximate Solution.md</guid><pubDate>Mon, 09 Sep 2024 15:35:46 GMT</pubDate><enclosure url="lib/media/pasted-image-20230823131134.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src=&quot;lib/media/pasted-image-20230823131134.png&quot;&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[Eigenvalue Problem]]></title><description><![CDATA[ 
 <br>In a Nutshell
Eigenvectors are the directions that are only scaled when a given <a data-tooltip-position="top" aria-label="Mathematical Relations" data-href="Mathematical Relations" href="the-guide/mathematics/general-stuff/mathematical-relations.html" class="internal-link" target="_self" rel="noopener nofollow">transformation</a> is applied to it. The scaling is denoted the eigenvalues. Intuitively, computing the eigenvalues is decomposing the complex problem of a transformations into its simpler, linear components. This can be achieved by changing the basis accordingly.
<br><br>Special Eigenvalue Problem
For a given <a data-tooltip-position="top" aria-label="Matrices" data-href="Matrices" href="the-guide/mathematics/linear-algebra/matrices.html" class="internal-link" target="_self" rel="noopener nofollow">matrix</a>  and ,  is an eigenvalue if there is a , for which
<br>
<br> is eigenvector for eigenvalue 
<br>Eigenvectors are not unique
<br>The polynomial  is called characteristic polynomial (degree )
<br>If  is eigenvector for eigenvalue , then  for any polynomial 
<br>For <a data-tooltip-position="top" aria-label="Matrices" data-href="Matrices" href="the-guide/mathematics/linear-algebra/matrices.html" class="internal-link" target="_self" rel="noopener nofollow">symmetric matrices</a> 

<br>All eigenvalues are real, eigenvectors can be chosen as real
<br>Eigenvectors of different eigenvalues are orthogonal
<br>For every eigenvalue of multiplicity  there are  linear independent eigenvectors
<br>There is an orthogonal basis  of  of eigenvectors to the eigenvalues  


<br><br><br>For every real <a data-tooltip-position="top" aria-label="Matrices" data-href="Matrices" href="the-guide/mathematics/linear-algebra/matrices.html" class="internal-link" target="_self" rel="noopener nofollow">symmetric matrix</a> , there exists an orthonormal basis of eigenvectors to the eigenvalues  . Collecting these eigenvectors in a matrix , one obtains a similarity transformation that brings  to diagonal form:<br><br><br>Computing the eigenvalues using the characteristic polynomial is infeasible, because computation of this polynomial is prone to rounding errors and the following computation of roots is bad- <a data-tooltip-position="top" aria-label="Condition Number" data-href="Condition Number" href="the-guide/mathematics/linear-algebra/condition-number.html" class="internal-link" target="_self" rel="noopener nofollow">conditioned</a>. Therefore, use iterative algorithms, depending on the goal:<br>
<br>Compute spectral radius (largest eigenvalue) only

<br>Direct <a data-tooltip-position="top" aria-label="Power Iteration" data-href="Power Iteration" href="the-guide/mathematics/linear-algebra/power-iteration.html" class="internal-link" target="_self" rel="noopener nofollow">power iteration</a>


<br>Compute specific eigenvalue

<br>Inverse <a data-tooltip-position="top" aria-label="Power Iteration" data-href="Power Iteration" href="the-guide/mathematics/linear-algebra/power-iteration.html" class="internal-link" target="_self" rel="noopener nofollow">power iteration</a>


<br>Compute all eigenvalues

<br><a data-tooltip-position="top" aria-label="QR-Algorithm" data-href="QR-Algorithm" href="the-guide/mathematics/linear-algebra/qr-algorithm.html" class="internal-link" target="_self" rel="noopener nofollow">QR-algorithm</a>


]]></description><link>the-guide/mathematics/linear-algebra/eigenvalue-problem.html</link><guid isPermaLink="false">The Guide/Mathematics/Linear Algebra/Eigenvalue Problem.md</guid><pubDate>Mon, 09 Sep 2024 15:35:46 GMT</pubDate></item><item><title><![CDATA[Gerschgorin Circle Theorem]]></title><description><![CDATA[ 
 <br>Theorem
Every eigenvalue of a matrix  lies in at least one of the circles of center  with radius 
<br>
<br>Special Cases

<br>If  circles form a disjoint set, this set contains  <a data-tooltip-position="top" aria-label="Eigenvalue Problem" data-href="Eigenvalue Problem" href="the-guide/mathematics/linear-algebra/eigenvalue-problem.html" class="internal-link" target="_self" rel="noopener nofollow">eigenvalues</a>.
<br>If <a data-tooltip-position="top" aria-label="Matrices" data-href="Matrices" href="the-guide/mathematics/linear-algebra/matrices.html" class="internal-link" target="_self" rel="noopener nofollow">matrix</a> is real, the transpose has the same eigenvalues, intersection of the Gerschgorin circles of both matrices can be used  


]]></description><link>the-guide/mathematics/linear-algebra/gerschgorin-circle-theorem.html</link><guid isPermaLink="false">The Guide/Mathematics/Linear Algebra/Gerschgorin Circle Theorem.md</guid><pubDate>Sun, 26 Jan 2025 18:57:08 GMT</pubDate></item><item><title><![CDATA[Givens Rotation]]></title><description><![CDATA[ 
 <br>Introduce zero in <a data-tooltip-position="top" aria-label="Matrices" data-href="Matrices" href="the-guide/mathematics/linear-algebra/matrices.html" class="internal-link" target="_self" rel="noopener nofollow">matrix</a>  at position . <br>Algorithm<br>
<br>Compute rotation matrix  with 

<br>
<br>
<br>


<br>,     

<br>


<br>Advantages<br>
<br>Can be parallelized
<br>For sparse matrices lower operation count than <a data-href="Householder Transformation" href="the-guide/mathematics/linear-algebra/householder-transformation.html" class="internal-link" target="_self" rel="noopener nofollow">Householder Transformation</a>
<br><a data-href="QR-Decomposition" href="the-guide/mathematics/linear-algebra/qr-decomposition.html" class="internal-link" target="_self" rel="noopener nofollow">QR-Decomposition</a> can be computed via ,   
]]></description><link>the-guide/mathematics/linear-algebra/givens-rotation.html</link><guid isPermaLink="false">The Guide/Mathematics/Linear Algebra/Givens Rotation.md</guid><pubDate>Mon, 09 Sep 2024 15:35:46 GMT</pubDate></item><item><title><![CDATA[Gram-Schmidt-Process]]></title><description><![CDATA[ 
 <br>Info
Method for orthonormalizing a <a data-tooltip-position="top" aria-label="Set" data-href="Set" href="the-guide/mathematics/general-stuff/set.html" class="internal-link" target="_self" rel="noopener nofollow">set</a> of vectors in an <a data-tooltip-position="top" aria-label="Vector Space" data-href="Vector Space" href="the-guide/mathematics/general-stuff/vector-space.html" class="internal-link" target="_self" rel="noopener nofollow">inner product space</a>.
<br><br>Given a finite, linearly independent <a data-tooltip-position="top" aria-label="Set" data-href="Set" href="the-guide/mathematics/general-stuff/set.html" class="internal-link" target="_self" rel="noopener nofollow">set</a> of vectors , generate a orthogonal set  that spans the same space as  via <br>
<br>For each new vector , project it onto all the known vectors and cut away that direction until only the new directions are left.
]]></description><link>the-guide/mathematics/linear-algebra/gram-schmidt-process.html</link><guid isPermaLink="false">The Guide/Mathematics/Linear Algebra/Gram-Schmidt-Process.md</guid><pubDate>Mon, 24 Feb 2025 23:32:25 GMT</pubDate></item><item><title><![CDATA[Householder Transformation]]></title><description><![CDATA[ 
 <br>Info
Algorithm to set entries below the diagonal of a <a data-tooltip-position="top" aria-label="Matrices" data-href="Matrices" href="the-guide/mathematics/linear-algebra/matrices.html" class="internal-link" target="_self" rel="noopener nofollow">matrix</a> to zero. Uses reflection on a hyperplane with normal vector  containing the origin. The resulting <a data-tooltip-position="top" aria-label="Matrices" data-href="Matrices" href="the-guide/mathematics/linear-algebra/matrices.html" class="internal-link" target="_self" rel="noopener nofollow">linear transformation</a> for a single point  can be written as leading to the matrix  (real case).
<br><img alt="center" src="lib/media/pasted-image-20230514111211.png" style="width: 250px; max-width: 100%;"><br>
As for the <a data-href="QR-Algorithm" href="the-guide/mathematics/linear-algebra/qr-algorithm.html" class="internal-link" target="_self" rel="noopener nofollow">QR-Algorithm</a>, we want a transformation that maps a vector onto a unit vector in a coordinate direction, we demand . We therefore must choose .<img alt="center" src="lib/media/pasted-image-20230514114313.png" style="width: 250px; max-width: 100%;"><br>
<br>In practice choose  to avoid the first entry becoming a very small number, which could lead to large relative errors.
<br><br><br>
<br>Take first column vector  of original matrix , compute 
<br>Householder matrix via 
<br>Drop first row and column of original matrix, repeat above with submatrix yielding the matrix . The resulting Householder matrix is  
<br><br><br>
<br>Compute <a data-href="QR-Decomposition" href="the-guide/mathematics/linear-algebra/qr-decomposition.html" class="internal-link" target="_self" rel="noopener nofollow">QR-Decomposition</a>

<br>
<br>


<br>Computing <a data-tooltip-position="top" aria-label="Matrices" data-href="Matrices" href="the-guide/mathematics/linear-algebra/matrices.html" class="internal-link" target="_self" rel="noopener nofollow">upper Hessenberg</a> form via <a data-tooltip-position="top" aria-label="Matrix Similarity" data-href="Matrix Similarity" href="the-guide/mathematics/linear-algebra/matrix-similarity.html" class="internal-link" target="_self" rel="noopener nofollow">similarity transformation</a>

<br>

<br>Normalizing linear  


<br> symmetric 

<br>For iteration , requires one matrix-vector product and multiple vector operations, because : 
<br>


<br> non-symmetric

<br>, two matrix-vector products, introduces factor 2 above




]]></description><link>the-guide/mathematics/linear-algebra/householder-transformation.html</link><guid isPermaLink="false">The Guide/Mathematics/Linear Algebra/Householder Transformation.md</guid><pubDate>Mon, 09 Sep 2024 15:35:46 GMT</pubDate><enclosure url="lib/media/pasted-image-20230514111211.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src=&quot;lib/media/pasted-image-20230514111211.png&quot;&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[Krylov Space Methods]]></title><description><![CDATA[ 
 <br>Info
Class of iterative procedures to solve large-scale, unstructured and sparse linear systems (solving via direct solvers not feasible) of the form  with <a data-tooltip-position="top" aria-label="Matrices" data-href="Matrices" href="the-guide/mathematics/linear-algebra/matrices.html" class="internal-link" target="_self" rel="noopener nofollow">matrix</a>  and .
<br>
<br>Prerequisites

<br>Residuum  of an iterative scheme can be written via polynomial and starting residuum . The scheme converges, if . The residuum can be rewritten in terms of <a data-tooltip-position="top" aria-label="Eigenvalue Problem" data-href="Eigenvalue Problem" href="the-guide/mathematics/linear-algebra/eigenvalue-problem.html" class="internal-link" target="_self" rel="noopener nofollow">eigenvalues and eigenvectors</a> (, ) in the form the reduction of error is determined by how good the polynomial dampens. This leads to the fact that every approximation .


<br>Assumptions

<br>, no restriction of generality


<br><br><br>m-th Krylov Space
With  and an , the subspaceis denoted as the m-th Krylov Space of the <a data-tooltip-position="top" aria-label="Matrices" data-href="Matrices" href="the-guide/mathematics/linear-algebra/matrices.html" class="internal-link" target="_self" rel="noopener nofollow">matrix</a>  to the vector .
<br>
<br>Properties

<br>
<br>
<br>In the case of stagnation, the space is invariant under the transformation : 


<br>Remarks

<br>Intuition - For directions where the approximation is off the residual is large, leads to new direction being mapped, delivers new direction for space
<br>Krylov space can deliver good approximation even if  smaller than , if eigenvalues are close


<br>Generic Basis (above, denoted ) 

<br>It holds that 
<br>Problematic, as vector directions become more and more similar with increasing dimension (see <a data-href="Power Iteration" href="the-guide/mathematics/linear-algebra/power-iteration.html" class="internal-link" target="_self" rel="noopener nofollow">Power Iteration</a>). Can be resolved using:

<br>Orthonormal Basis via <a data-tooltip-position="top" aria-label="QR-Decomposition" data-href="QR-Decomposition" href="the-guide/mathematics/linear-algebra/qr-decomposition.html" class="internal-link" target="_self" rel="noopener nofollow">QR-decomposition</a>

<br>Decompose generic basis in  and extend via which can be computed via  and .
<br> is orthogonal basis of Krylov Space.


<br><a data-tooltip-position="top" aria-label="Arnoldi Method" data-href="Arnoldi Method" href="the-guide/mathematics/linear-algebra/arnoldi-method.html" class="internal-link" target="_self" rel="noopener nofollow">Arnoldi method</a>




<br><br><br>For every , there is a polynomial  with , such that <br><br><br>Since each iteration is costly, we want a good approximation of solution in  with  for . If stagnation occurs () the solution lies in the stagnated Krylov-Space, if<br>
<br> solves  exactly if  solves .
<br>For invertible  the solution lies in the stagnated Krylov space.
]]></description><link>the-guide/mathematics/linear-algebra/krylov-space-methods.html</link><guid isPermaLink="false">The Guide/Mathematics/Linear Algebra/Krylov Space Methods.md</guid><pubDate>Mon, 09 Sep 2024 15:35:46 GMT</pubDate></item><item><title><![CDATA[Lanczos Method]]></title><description><![CDATA[ 
 <br>Info
Assuming  is <a data-tooltip-position="top" aria-label="Matrices" data-href="Matrices" href="the-guide/mathematics/linear-algebra/matrices.html" class="internal-link" target="_self" rel="noopener nofollow">symmetric</a>, construct a more efficient method than <a data-tooltip-position="top" aria-label="Construction of approximate Solution" data-href="Construction of approximate Solution" href="the-guide/mathematics/linear-algebra/construction-of-approximate-solution.html" class="internal-link" target="_self" rel="noopener nofollow">FOM</a> using Ritz-Galerkin ansatz.<br>
Instead of choosing  choose  in a way that  does not need to be computed again / with extra effort. This leads to a basis of the <a data-tooltip-position="top" aria-label="Krylov Space Methods" data-href="Krylov Space Methods" href="the-guide/mathematics/linear-algebra/krylov-space-methods.html" class="internal-link" target="_self" rel="noopener nofollow">Krylov space</a> that is orthogonal rather than orthonormal.
<br><br>If the <a data-tooltip-position="top" aria-label="Matrices" data-href="Matrices" href="the-guide/mathematics/linear-algebra/matrices.html" class="internal-link" target="_self" rel="noopener nofollow">matrix</a>  is symmetric, then the upper-Hessenberg matrix  of the <a data-tooltip-position="top" aria-label="Arnoldi Method" data-href="Arnoldi Method" href="the-guide/mathematics/linear-algebra/arnoldi-method.html" class="internal-link" target="_self" rel="noopener nofollow">Arnoldi method</a> is symmetric and tridiagonal: Therefore,  leads to the 3-term-recursion According to the <a data-tooltip-position="top" aria-label="Construction of approximate Solution" data-href="Construction of approximate Solution" href="the-guide/mathematics/linear-algebra/construction-of-approximate-solution.html" class="internal-link" target="_self" rel="noopener nofollow">Ritz-Galerkin-Ansatz</a>, we choose  orthogonal to , leading to the fact that  has to be perpendicular to . Instead of normalizing the new basis vector  to length , we set . This yields a a orthogonal (not orthonormal) matrix  and instead of  and a tridiagonal Hessenbergmatrix . We obtain Using the fact that  with a polynomial , this then yields a new recursion formula for the sub-diagonal entries of  using only the other entries of the respective column.<br><br><br>
<br>Significantly lowers the needed operations at step  for the orthogonalization from  to , while lowering the needed memory to store the matrix /.
<br>The residuum  is computed automatically with  without explicitly computing . Therefore, the iteration and checking a residuum-bases termination criterion is very efficient. 
<br>To solve , we want a  such that the residuum for  is orthogonal to . This can be achieved by solving However, this means that for explicitly computing  in the end, the whole <a data-tooltip-position="top" aria-label="Matrices" data-href="Matrices" href="the-guide/mathematics/linear-algebra/matrices.html" class="internal-link" target="_self" rel="noopener nofollow">matrix</a>  has to be stored.
]]></description><link>the-guide/mathematics/linear-algebra/lanczos-method.html</link><guid isPermaLink="false">The Guide/Mathematics/Linear Algebra/Lanczos Method.md</guid><pubDate>Mon, 09 Sep 2024 15:35:46 GMT</pubDate></item><item><title><![CDATA[LU-Decomposition]]></title><description><![CDATA[ 
 ]]></description><link>the-guide/mathematics/linear-algebra/lu-decomposition.html</link><guid isPermaLink="false">The Guide/Mathematics/Linear Algebra/LU-Decomposition.md</guid><pubDate>Mon, 09 Sep 2024 15:35:46 GMT</pubDate></item><item><title><![CDATA[Majorization]]></title><description><![CDATA[ 
 <br>Definition
Preorder on vectors of real numbers. Given two vectors , we say that  majorizes , denoted (), iff where  denoted the -th largest entry of the vector. Should the second condition not hold, we say that  weakly majorizes . Geometrically, this means that  is in the <a data-tooltip-position="top" aria-label="Convexity" data-href="Convexity" href="the-guide/mathematics/optimization/convex-optimization-lecture/convexity.html" class="internal-link" target="_self" rel="noopener nofollow">convex</a> hull of all permutations of , or equivalently that  for a doubly-stochastic <a data-tooltip-position="top" aria-label="Matrices" data-href="Matrices" href="the-guide/mathematics/linear-algebra/matrices.html" class="internal-link" target="_self" rel="noopener nofollow">matrix</a>. 
<br><br>
<br>For simplexes (, ) in , the vector  majorizes every other possible, e.g. 
]]></description><link>the-guide/mathematics/linear-algebra/majorization.html</link><guid isPermaLink="false">The Guide/Mathematics/Linear Algebra/Majorization.md</guid><pubDate>Mon, 09 Sep 2024 15:35:46 GMT</pubDate></item><item><title><![CDATA[Matrices]]></title><description><![CDATA[ 
 <br>Throughout the following, we base everything on the matrix and explicitly denote the dimensions. If the definition is valid for an arbitrary <a data-href="Field" href="the-guide/mathematics/general-stuff/field.html" class="internal-link" target="_self" rel="noopener nofollow">Field</a>, we denote it by , otherwise we explicitly use e.g.  or . <br>Note

<br>For derivatives involving matrices, see <a data-href="Matrix Calculus" href="the-guide/mathematics/analysis-and-calculus/matrix-calculus.html" class="internal-link" target="_self" rel="noopener nofollow">Matrix Calculus</a>.
<br>For the <a data-tooltip-position="top" aria-label="Group" data-href="Group" href="the-guide/mathematics/group-theory/group.html" class="internal-link" target="_self" rel="noopener nofollow">group</a> structure resulting from invertible matrices, see <a data-href="General Linear Group and Matrix Groups" href="the-guide/mathematics/group-theory/general-linear-group-and-matrix-groups.html" class="internal-link" target="_self" rel="noopener nofollow">General Linear Group and Matrix Groups</a>.  

<br><br>Rank of 
The given matrix defines an associated linear <a data-tooltip-position="top" aria-label="Mathematical Relations" data-href="Mathematical Relations" href="the-guide/mathematics/general-stuff/mathematical-relations.html" class="internal-link" target="_self" rel="noopener nofollow">mapping</a>  via . The rank of  denotes the number of dimensions of the output <a data-href="Space" href="the-guide/mathematics/general-stuff/space.html" class="internal-link" target="_self" rel="noopener nofollow">Space</a>where we directly see that . The linear map above is 

<br><a data-tooltip-position="top" aria-label="Mathematical Relations" data-href="Mathematical Relations" href="the-guide/mathematics/general-stuff/mathematical-relations.html" class="internal-link" target="_self" rel="noopener nofollow">Injective</a>, iff  has rank  (full column rank, "one-to-one")
<br><a data-tooltip-position="top" aria-label="Mathematical Relations" data-href="Mathematical Relations" href="the-guide/mathematics/general-stuff/mathematical-relations.html" class="internal-link" target="_self" rel="noopener nofollow">Surjective</a>, iff  has ranke  (full row rank, "onto")
<br><a data-tooltip-position="top" aria-label="Mathematical Relations" data-href="Mathematical Relations" href="the-guide/mathematics/general-stuff/mathematical-relations.html" class="internal-link" target="_self" rel="noopener nofollow">Bijective</a>, iff the matrix is square and has rank  (invertible)

<br>
<br>Useful Equalities

<br>For any additional matrix  of rank , it holds that 
<br>For any additional matrix  of rank , it holds that 
<br>For matrices over the reals , the identitieshold.


<br>Useful Inequalities

<br>For any additional matrix , it holds that 
<br>The rank fulfills subadditivity, i.e.  


<br>It is also sometimes useful to define the non-negative rank, which restricts the vectors and coefficients of the linear combination to be non-negativeIt holds that .
<br><br>Trace of a square matrix 
For square matrices, we can define the trace as the sum of its main diagonal resulting in a linear mapping. It can be shown that the trace is equal to the sum of a matrices <a data-tooltip-position="top" aria-label="Eigenvalue Problem" data-href="Eigenvalue Problem" href="the-guide/mathematics/linear-algebra/eigenvalue-problem.html" class="internal-link" target="_self" rel="noopener nofollow">eigenvalues</a> 
<br><br>Intuition
Can be seen as a summary of a linear transformation, especially since it is invariant to <a data-tooltip-position="top" aria-label="Matrix Similarity" data-href="Matrix Similarity" href="the-guide/mathematics/linear-algebra/matrix-similarity.html" class="internal-link" target="_self" rel="noopener nofollow">similarity</a>, which is an <a data-tooltip-position="top" aria-label="Equivalence Relation and Class" data-href="Equivalence Relation and Class" href="the-guide/mathematics/general-stuff/equivalence-relation-and-class.html" class="internal-link" target="_self" rel="noopener nofollow">equivilance relation</a> for transformations. This gives us a notion of an <a data-tooltip-position="top" aria-label="Vector Space" data-href="Vector Space" href="the-guide/mathematics/general-stuff/vector-space.html" class="internal-link" target="_self" rel="noopener nofollow">inner product</a> on the <a data-href="Space" href="the-guide/mathematics/general-stuff/space.html" class="internal-link" target="_self" rel="noopener nofollow">Space</a> of matrices. By transposing one matrix, we are taking inner product of all the columns of the transformation with each other and then summarize the result using the trace.<br>
The inner product / the trace is zero, if the matrices project vectors onto orthogonal sub-spaces, while matrices with similar transformation properties yield large traces. 
<br><br>TODO<br>
<br>
Determinant

<br>

<br> is matrix that results from removing -th row and -th column


<br>Factor by which the <a data-tooltip-position="top" aria-label="Mathematical Relations" data-href="Mathematical Relations" href="the-guide/mathematics/general-stuff/mathematical-relations.html" class="internal-link" target="_self" rel="noopener nofollow">linear map</a> changes the  dimensional unit volume 
<br>Sylvester's Theorem

<br>


<br>
<br>


<br>
Kernel / null <a data-href="Space" href="the-guide/mathematics/general-stuff/space.html" class="internal-link" target="_self" rel="noopener nofollow">Space</a> of a matrix

<br>...


<br>
Spectrum = <a data-href="Set" href="the-guide/mathematics/general-stuff/set.html" class="internal-link" target="_self" rel="noopener nofollow">Set</a> of <a data-tooltip-position="top" aria-label="Eigenvalue Problem" data-href="Eigenvalue Problem" href="the-guide/mathematics/linear-algebra/eigenvalue-problem.html" class="internal-link" target="_self" rel="noopener nofollow">Eigenvalues</a>

<br>If  and eigenvalues  of  and  of , then 


<br><br><br><br><br><br>
<br>
Symmetric 

<br>Sum and difference of symmetric matrices is symmetric, product not
<br>If  symmetric and ,  is symmetric
<br>If  exists, it is symmetric if and only if  is symmetric


<br>
Skew-Symmetric 

<br>Sum and scalar multiples of skew symmetric matrices are skew symmetric
<br>trace, determinant 
<br>


<br>
Hermitian 

<br>Complex conjugate of every element
<br> for any pair of vector
<br>For  it holds that 
<br>


<br>
Unitary 

<br>
<br>Eigenvectors form orthogonal basis of 
<br>
<br>Product of unitary matrices is unitary
<br>Preserve angles between vectors and their lengths


<br>
Orthogonal 

<br>
<br>Determinant is either  or 
<br>Real analog to unitary 


<br>
Upper Hessenberg

<br>Upper triangular matrix with additional element below diagonal


<br><br><br>Let  be a complex hermitian matrix and . <br><br>For real symmetric matrices, the above is true if  is assumed real and the complex conjugate is replaces by a transpose. The notations are only used for the complex case.<br><br>Decomposition of psd Matrices
A  matrix  is positive semi-definite if and only if it can be decomposed into a product where again the conjugate is replaced by a transpose in the real case .   is positive definit if and only if  is invertible (full rank). For any decomposition, it holds that 
]]></description><link>the-guide/mathematics/linear-algebra/matrices.html</link><guid isPermaLink="false">The Guide/Mathematics/Linear Algebra/Matrices.md</guid><pubDate>Wed, 23 Apr 2025 21:55:37 GMT</pubDate></item><item><title><![CDATA[Matrix Similarity]]></title><description><![CDATA[<a class="tag" href="?query=tag:Linear-Algebra" style="background-color: rgb(4, 108, 116); color: white; font-weight: 700; border: none; border-radius: 1em; padding: 0.2em 0.5em;">#Linear-Algebra</a> 
 <br>Info
Two  <a data-tooltip-position="top" aria-label="Matrices" data-href="Matrices" href="the-guide/mathematics/linear-algebra/matrices.html" class="internal-link" target="_self" rel="noopener nofollow">matrices</a>  are called similar, if there exists an invertible  matrix , such that Both matrices represent the same <a data-tooltip-position="top" aria-label="Mathematical Relations" data-href="Mathematical Relations" href="the-guide/mathematics/general-stuff/mathematical-relations.html" class="internal-link" target="_self" rel="noopener nofollow">linear map</a> (equivalent in space of square matrices), but in possibly different bases. The matrix  is the change of basis matrix from the new basis of  to that of . 
<br>
<br>Sometimes also denoted the other way around via , especially in physics (the above is the standard in <a href=".?query=tag:Linear-Algebra" class="tag" target="_blank" rel="noopener nofollow">#Linear-Algebra</a> ). Mathematically, both are equivalent.

<br>The linear algebra conventions is based on the interpretation of  representing a basis transformation. 
<br>The physics convention is based in the interpretation of  being a transformation applied on the whole space.


<br>Connection to Tensors
A matrix as a linear transformation between vector spaces, a -<a data-tooltip-position="top" aria-label="Tensor" data-href="Tensor" href="Tensor" class="internal-link" target="_self" rel="noopener nofollow">tensor</a> that can act on both vectors and <a data-tooltip-position="top" aria-label="Dual Vector Space" data-href="Dual Vector Space" href="the-guide/mathematics/general-stuff/dual-vector-space.html" class="internal-link" target="_self" rel="noopener nofollow">covectors</a>. If both vectors and covectors are in the same (dual) basis, and transformed accordingly, the above is needed to transform both indices.
<br><br><br>Both matrices share<br>
<br><a data-tooltip-position="top" aria-label="Matrices" data-href="Matrices" href="the-guide/mathematics/linear-algebra/matrices.html" class="internal-link" target="_self" rel="noopener nofollow">Rank</a>
<br><a data-tooltip-position="top" aria-label="Matrices" data-href="Matrices" href="the-guide/mathematics/linear-algebra/matrices.html" class="internal-link" target="_self" rel="noopener nofollow">Determinant</a>
<br><a data-tooltip-position="top" aria-label="Matrices" data-href="Matrices" href="the-guide/mathematics/linear-algebra/matrices.html" class="internal-link" target="_self" rel="noopener nofollow">Trace</a>
<br><a data-tooltip-position="top" aria-label="Eigenvalue Problem" data-href="Eigenvalue Problem" href="the-guide/mathematics/linear-algebra/eigenvalue-problem.html" class="internal-link" target="_self" rel="noopener nofollow">Eigenvalues</a> and their algebraic multiplicities (not the same eigenvectors, transformed)
<br>...
]]></description><link>the-guide/mathematics/linear-algebra/matrix-similarity.html</link><guid isPermaLink="false">The Guide/Mathematics/Linear Algebra/Matrix Similarity.md</guid><pubDate>Sun, 26 Jan 2025 18:42:00 GMT</pubDate></item><item><title><![CDATA[Mercer Theorem]]></title><description><![CDATA[ 
 <br>Theorem
A given kernel <a data-tooltip-position="top" aria-label="Mathematical Relations" data-href="Mathematical Relations" href="the-guide/mathematics/general-stuff/mathematical-relations.html" class="internal-link" target="_self" rel="noopener nofollow">function</a>  is a valid Mercer kernel,meaning that iff for all inputs  () the resulting Kernel <a data-tooltip-position="top" aria-label="Matrices" data-href="Matrices" href="the-guide/mathematics/linear-algebra/matrices.html" class="internal-link" target="_self" rel="noopener nofollow">matrix</a> is <a data-tooltip-position="top" aria-label="Matrices" data-href="Matrices" href="the-guide/mathematics/linear-algebra/matrices.html" class="internal-link" target="_self" rel="noopener nofollow">symmetric and positive semi-definite</a>. 
]]></description><link>the-guide/mathematics/linear-algebra/mercer-theorem.html</link><guid isPermaLink="false">The Guide/Mathematics/Linear Algebra/Mercer Theorem.md</guid><pubDate>Mon, 09 Sep 2024 15:35:46 GMT</pubDate></item><item><title><![CDATA[Moore-Penrose Pseudoinverse]]></title><description><![CDATA[ 
 <br>Info
For a <a data-tooltip-position="top" aria-label="Matrices" data-href="Matrices" href="the-guide/mathematics/linear-algebra/matrices.html" class="internal-link" target="_self" rel="noopener nofollow">matrix</a>  and using the <a data-tooltip-position="top" aria-label="Singular Value Decomposition" data-href="Singular Value Decomposition" href="the-guide/mathematics/linear-algebra/singular-value-decomposition.html" class="internal-link" target="_self" rel="noopener nofollow">SVD</a> , the Moore-Penrose-Pseudoinverse is defined via with 
<br>
<br>If  has linearly independent columns (is <a data-tooltip-position="top" aria-label="Mathematical Relations" data-href="Mathematical Relations" href="the-guide/mathematics/general-stuff/mathematical-relations.html" class="internal-link" target="_self" rel="noopener nofollow">injective</a>,  is invertible), then  can be computed via 
<br>If  has linearly independent rows (is <a data-tooltip-position="top" aria-label="Mathematical Relations" data-href="Mathematical Relations" href="the-guide/mathematics/general-stuff/mathematical-relations.html" class="internal-link" target="_self" rel="noopener nofollow">surjective</a>,  is invertible), then  can be computed via
<br><br><br>
<br>
<br>
<br>
<br>
<br><br><br>Can be used to solve badly-<a data-tooltip-position="top" aria-label="Condition Number" data-href="Condition Number" href="the-guide/mathematics/linear-algebra/condition-number.html" class="internal-link" target="_self" rel="noopener nofollow">conditioned</a> or singular linear problems, e.g. <a data-tooltip-position="top" aria-label="Linear Least Squares and Ridge Regression" data-href="Linear Least Squares and Ridge Regression" href="the-guide/mathematics/optimization/linear-least-squares-and-ridge-regression.html" class="internal-link" target="_self" rel="noopener nofollow">LLS</a>:]]></description><link>the-guide/mathematics/linear-algebra/moore-penrose-pseudoinverse.html</link><guid isPermaLink="false">The Guide/Mathematics/Linear Algebra/Moore-Penrose Pseudoinverse.md</guid><pubDate>Mon, 09 Sep 2024 15:35:46 GMT</pubDate></item><item><title><![CDATA[Norms]]></title><description><![CDATA[ 
 <br>In a Nutshell
<a data-tooltip-position="top" aria-label="Mathematical Relations" data-href="Mathematical Relations" href="the-guide/mathematics/general-stuff/mathematical-relations.html" class="internal-link" target="_self" rel="noopener nofollow">Function</a> from a real or complex <a data-tooltip-position="top" aria-label="Vector Space" data-href="Vector Space" href="the-guide/mathematics/general-stuff/vector-space.html" class="internal-link" target="_self" rel="noopener nofollow">vector space</a> to the non-negative real numbers that behaves in a certain, well-defined way: it commutes with scaling, obeys a form of the triangle inequality and is zero only at the origin.
<br><br>Norm
Given a <a data-tooltip-position="top" aria-label="Vector Space" data-href="Vector Space" href="the-guide/mathematics/general-stuff/vector-space.html" class="internal-link" target="_self" rel="noopener nofollow">vector space</a>  over a field , a norm of  is a real-valued function  that satisfies

<br>Triangle Inequality
<br>Absolute Homogenity
<br>Positive Definiteness

<br>All Norms on finite Vector Spaces are Equivalent
Suppose that  and  are two norms on a vector space . The norms are called equivalent, if there exists positive real constants  and , such that This is alwys true on a finite-dimensional vector space ! However, this does not extend to infinite-dimensional vector spaces.
<br><br>Definition - Matrix or Vector Norm
More specifically, a  <a data-tooltip-position="top" aria-label="Matrices" data-href="Matrices" href="the-guide/mathematics/linear-algebra/matrices.html" class="internal-link" target="_self" rel="noopener nofollow">matrix or vector</a> norm is a <a data-tooltip-position="top" aria-label="Mathematical Relations" data-href="Mathematical Relations" href="the-guide/mathematics/general-stuff/mathematical-relations.html" class="internal-link" target="_self" rel="noopener nofollow">function</a>  satisfying

<br> for all 
<br> for all 
<br> with equality only if 

<br>
<br>
Vector norms<br>
- -norm	- -norm<img alt="450|centre" src="lib/media/pasted-image-20240727182046.png">

<br>
Different Matrix norms

<br>Spectral norm for  

<br>Also from <a data-tooltip-position="top" aria-label="Matrices" data-href="Matrices" href="the-guide/mathematics/linear-algebra/matrices.html" class="internal-link" target="_self" rel="noopener nofollow">spectral radius</a>  (square-root of largest <a data-tooltip-position="top" aria-label="Singular Value Decomposition" data-href="Singular Value Decomposition" href="the-guide/mathematics/linear-algebra/singular-value-decomposition.html" class="internal-link" target="_self" rel="noopener nofollow">singular value</a>) 


<br>Frobenius norm for   

<br>Is equal to , see below


<br>Tschebyschew- / Max-Norm  
<br>-mixed norms over rows  


]]></description><link>the-guide/mathematics/linear-algebra/norms.html</link><guid isPermaLink="false">The Guide/Mathematics/Linear Algebra/Norms.md</guid><pubDate>Thu, 27 Feb 2025 15:21:21 GMT</pubDate><enclosure url="lib/media/pasted-image-20240727182046.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src=&quot;lib/media/pasted-image-20240727182046.png&quot;&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[Numerical Linear Algebra]]></title><description><![CDATA[ 
 <br>In a Nutshell
Subfield of numerics and linear algebra concerned with computation of linear algebra results on a computer, often in very high dimensions.
<br><br>
<br>Eigenvalues and Eigenvectors

<br><a data-href="Eigenvalue Problem" href="the-guide/mathematics/linear-algebra/eigenvalue-problem.html" class="internal-link" target="_self" rel="noopener nofollow">Eigenvalue Problem</a>

<br>Computation of char. polynomial bad <a data-tooltip-position="top" aria-label="Condition Number" data-href="Condition Number" href="the-guide/mathematics/linear-algebra/condition-number.html" class="internal-link" target="_self" rel="noopener nofollow">condition</a>, need iterative algorithms


<br><a data-href="Power Iteration" href="the-guide/mathematics/linear-algebra/power-iteration.html" class="internal-link" target="_self" rel="noopener nofollow">Power Iteration</a>

<br>Compute single eigenvector and corresponding eigenvalue
<br>Good for sparse matrices (reduces matrix-vector cost)


<br><a data-href="QR-Algorithm" href="the-guide/mathematics/linear-algebra/qr-algorithm.html" class="internal-link" target="_self" rel="noopener nofollow">QR-Algorithm</a>

<br>Compute all eigenvalues simultaneously




<br>Singular Values

<br><a data-href="Singular Value Decomposition" href="the-guide/mathematics/linear-algebra/singular-value-decomposition.html" class="internal-link" target="_self" rel="noopener nofollow">Singular Value Decomposition</a>

<br>Low rank approximation + best approx.


<br><a data-href="Moore-Penrose Pseudoinverse" href="the-guide/mathematics/linear-algebra/moore-penrose-pseudoinverse.html" class="internal-link" target="_self" rel="noopener nofollow">Moore-Penrose Pseudoinverse</a>


<br>Large linear Systems

<br><a data-href="Krylov Space Methods" href="the-guide/mathematics/linear-algebra/krylov-space-methods.html" class="internal-link" target="_self" rel="noopener nofollow">Krylov Space Methods</a>
<br><a data-href="Arnoldi Method" href="the-guide/mathematics/linear-algebra/arnoldi-method.html" class="internal-link" target="_self" rel="noopener nofollow">Arnoldi Method</a>
<br><a data-href="Construction of approximate Solution" href="the-guide/mathematics/linear-algebra/construction-of-approximate-solution.html" class="internal-link" target="_self" rel="noopener nofollow">Construction of approximate Solution</a>
<br><a data-href="Lanczos Method" href="the-guide/mathematics/linear-algebra/lanczos-method.html" class="internal-link" target="_self" rel="noopener nofollow">Lanczos Method</a>
<br><a data-href="Conjugate Gradient Method" href="the-guide/mathematics/linear-algebra/conjugate-gradient-method.html" class="internal-link" target="_self" rel="noopener nofollow">Conjugate Gradient Method</a>


]]></description><link>the-guide/mathematics/linear-algebra/numerical-linear-algebra.html</link><guid isPermaLink="false">The Guide/Mathematics/Linear Algebra/Numerical Linear Algebra.md</guid><pubDate>Sun, 26 Jan 2025 18:47:34 GMT</pubDate></item><item><title><![CDATA[Perron-Frobenius Theorem]]></title><description><![CDATA[ 
 <br>Theorem
If  is a <a data-tooltip-position="top" aria-label="Matrices" data-href="Matrices" href="the-guide/mathematics/linear-algebra/matrices.html" class="internal-link" target="_self" rel="noopener nofollow">non-negative matrix</a> (e.g. <a data-href="Adjacency Matrix" href="the-guide/mathematics/graph-theory/adjacency-matrix.html" class="internal-link" target="_self" rel="noopener nofollow">Adjacency Matrix</a> with all positive weights) then:

<br> is a real <a data-tooltip-position="top" aria-label="Eigenvalue Problem" data-href="Eigenvalue Problem" href="the-guide/mathematics/linear-algebra/eigenvalue-problem.html" class="internal-link" target="_self" rel="noopener nofollow">eigenvalue</a> with multiplicity 1 if the graph is connected.

<br>Corresponding eigenvector has all-positive entries.


<br>For any other eigenvalue .
<br>If <a data-tooltip-position="top" aria-label="Graph" data-href="Graph" href="the-guide/mathematics/graph-theory/graph.html" class="internal-link" target="_self" rel="noopener nofollow">graph</a> is also undirected ( symmetric), all eigenvalues are real and 

<br>
<br> if graph is connected


<br>Multiplicity of first eigenvalue corresponds to number of connected components.

]]></description><link>the-guide/mathematics/linear-algebra/perron-frobenius-theorem.html</link><guid isPermaLink="false">The Guide/Mathematics/Linear Algebra/Perron-Frobenius Theorem.md</guid><pubDate>Mon, 09 Sep 2024 15:39:16 GMT</pubDate></item><item><title><![CDATA[Polarization Identity]]></title><description><![CDATA[ 
 <br>In a Nutshell
Family of formulas that expresses the <a data-tooltip-position="top" aria-label="Vector Space" data-href="Vector Space" href="the-guide/mathematics/general-stuff/vector-space.html" class="internal-link" target="_self" rel="noopener nofollow">inner product</a> of two vectors in terms of the norm of a <a data-tooltip-position="top" aria-label="Norms" data-href="Norms" href="the-guide/mathematics/linear-algebra/norms.html" class="internal-link" target="_self" rel="noopener nofollow">normed</a> vector space. If a norm arises from an inner product, the identity can be used to express this inner product entirely through the norm and sometimes to reconstruct the inner product from a given norm.
]]></description><link>the-guide/mathematics/linear-algebra/polarization-identity.html</link><guid isPermaLink="false">The Guide/Mathematics/Linear Algebra/Polarization Identity.md</guid><pubDate>Mon, 24 Feb 2025 23:32:25 GMT</pubDate></item><item><title><![CDATA[Power Iteration]]></title><description><![CDATA[ 
 <br>In a Nutshell
Iterative procedure to compute single <a data-tooltip-position="top" aria-label="Eigenvalue Problem" data-href="Eigenvalue Problem" href="the-guide/mathematics/linear-algebra/eigenvalue-problem.html" class="internal-link" target="_self" rel="noopener nofollow">eigenvalue</a> of quadratic matrix. Ideas can be extended to <a data-tooltip-position="top" aria-label="QR-Algorithm" data-href="QR-Algorithm" href="the-guide/mathematics/linear-algebra/qr-algorithm.html" class="internal-link" target="_self" rel="noopener nofollow">QR-algorithm</a>, which computes the whole spectrum.
<br><br><br>Compute eigenvector of largest eigenvalue  of <a data-tooltip-position="top" aria-label="Matrices" data-href="Matrices" href="the-guide/mathematics/linear-algebra/matrices.html" class="internal-link" target="_self" rel="noopener nofollow">quadratic matrix</a> . Using an initial vector , compute <br>
<br>Converges towards <a data-tooltip-position="top" aria-label="Eigenvalue Problem" data-href="Eigenvalue Problem" href="the-guide/mathematics/linear-algebra/eigenvalue-problem.html" class="internal-link" target="_self" rel="noopener nofollow">eigenvector</a> of largest eigenvalue

<br>If  is negative, convergence is alternating
<br>Normalize between iterations to avoid overflow
<br>Initial vector should not be orthogonal to eigenspace of largest eigenvalue, because it needs component to be stretched

<br>Rounding errors can take care of this




<br>Eigenvalue can be computed after convergence using <a data-tooltip-position="top" aria-label="Rayleigh Quotient" data-href="Rayleigh Quotient" href="the-guide/mathematics/linear-algebra/rayleigh-quotient.html" class="internal-link" target="_self" rel="noopener nofollow">Rayleigh quotient</a> via 
<br>Advantages

<br>Easy, fast


<br>Disadvantages

<br>Only finds Eigenvector to largest absolute eigenvalue
<br>Convergence depends on , can be very slow for close eigenvalues


<br><img alt="center" src="lib/media/pasted-image-20230814153913.png" style="width: 400px; max-width: 100%;"><br><br><br>Estimate  of  given, e.g. via <a data-href="Gerschgorin Circle Theorem" href="the-guide/mathematics/linear-algebra/gerschgorin-circle-theorem.html" class="internal-link" target="_self" rel="noopener nofollow">Gerschgorin Circle Theorem</a> such that <br>
<br>Iterate 	- Use <a data-href="LU-Decomposition" href="the-guide/mathematics/linear-algebra/lu-decomposition.html" class="internal-link" target="_self" rel="noopener nofollow">LU-Decomposition</a> to significantly speed up computation, as every linear system is solved for same matrix<br>
- Convergence depends on , best for well-separated eigenvalues and good initial guess<br>
<img alt="center" src="lib/media/pasted-image-20230824131157.png" style="width: 400px; max-width: 100%;">
<br>Intuition

<br>The shift by  shifts the whole eigenvalue spectrum (imagine a number line)
<br>The direct iteration iterates towards the dominant eigenvalue, a shift can therefore only influence if we iterate towards the last or the first eigenvalue (magnitude)
<br>The inverse iteration converges towards the eigenvalue smallest in magnitude, providing a lot more flexibility in changing towards which value we iterate, because we can shift any value closest to zero (theoretically)

]]></description><link>the-guide/mathematics/linear-algebra/power-iteration.html</link><guid isPermaLink="false">The Guide/Mathematics/Linear Algebra/Power Iteration.md</guid><pubDate>Sun, 26 Jan 2025 18:49:10 GMT</pubDate><enclosure url="lib/media/pasted-image-20230814153913.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src=&quot;lib/media/pasted-image-20230814153913.png&quot;&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[QR-Algorithm]]></title><description><![CDATA[ 
 <br>Info
Algorithm to approximate all <a data-tooltip-position="top" aria-label="Eigenvalue Problem" data-href="Eigenvalue Problem" href="the-guide/mathematics/linear-algebra/eigenvalue-problem.html" class="internal-link" target="_self" rel="noopener nofollow">eigenvalues</a> of a <a data-tooltip-position="top" aria-label="Matrices" data-href="Matrices" href="the-guide/mathematics/linear-algebra/matrices.html" class="internal-link" target="_self" rel="noopener nofollow">matrix</a>  by generalizing the concept of the <a data-tooltip-position="top" aria-label="Power Iteration" data-href="Power Iteration" href="the-guide/mathematics/linear-algebra/power-iteration.html" class="internal-link" target="_self" rel="noopener nofollow">power iteration</a> using simultaneous iteration. The iterates  converge towards the <a data-tooltip-position="top" aria-label="Schur Normal Form > Schur Normal Form" data-href="Schur Normal Form#Schur Normal Form" href="the-guide/mathematics/linear-algebra/schur-normal-form.html#Schur_Normal_Form" class="internal-link" target="_self" rel="noopener nofollow">Schur form</a> of .
<br><br><br>Consider a diagonalizable   with <a data-tooltip-position="top" aria-label="Eigenvalue Problem" data-href="Eigenvalue Problem" href="the-guide/mathematics/linear-algebra/eigenvalue-problem.html" class="internal-link" target="_self" rel="noopener nofollow">eigenvalues</a>  and eigenvectors .<br>
Imagine are iterating on an -dimensional subspace  of . We can define two subspaces  and , such that  ( spans ,  is the "rest") viaGiven a basis  of the subspace , iterating   gives a basis of the space . However, the resulting vectors are all pulled towards the eigenvector of the largest eigenvalue in that subspace (they have a non-zero component to), which leads to bad <a data-tooltip-position="top" aria-label="Condition Number" data-href="Condition Number" href="the-guide/mathematics/linear-algebra/condition-number.html" class="internal-link" target="_self" rel="noopener nofollow">conditioning</a>. To overcome this, we orthonormalize the basis after each transformation:<br>
<br>
<br>Orthogonalize  from left to right to obtain ONB  of .<br>
The resulting basis gets closer and closer to  with iterations. Because of the orthogonalization, we can extend the above to an -dimensional basis, where the first  vectors  converge towards a basis of , while the rest (orthogonal ) converges towards a basis of . Based on this, we compute a matrix  and use it for a <a data-tooltip-position="top" aria-label="Matrix Similarity" data-href="Matrix Similarity" href="the-guide/mathematics/linear-algebra/matrix-similarity.html" class="internal-link" target="_self" rel="noopener nofollow">similarity transform</a> of  via The entry  will converge towards , because . This results in two smaller eigenvalue problems, for which the above results can be used in a recursive fashion, leading to a <a data-tooltip-position="top" aria-label="Schur Normal Form" data-href="Schur Normal Form" href="the-guide/mathematics/linear-algebra/schur-normal-form.html" class="internal-link" target="_self" rel="noopener nofollow">Schur normal form</a> of  for . 
<br><br><br><img alt="center" src="lib/media/pasted-image-20230821094816.png" style="width: 400px; max-width: 100%;"><br>
<br>Convergence

<br>Also converges for non-singular eigenvalues, leaves  blocks (easy eigenvalue computation)
<br>For off-diagonal elements , slow convergence for close-by eigenvalues


<br>Complexity

<br>


<br><br><br>
<br>Transformation to upper Hessenberg form

<br>Before beginning iteration bring matrix  in upper Hessenberg form (tridiagonal in symmetric case)  by <a data-tooltip-position="top" aria-label="Matrix Similarity" data-href="Matrix Similarity" href="the-guide/mathematics/linear-algebra/matrix-similarity.html" class="internal-link" target="_self" rel="noopener nofollow">similarity transformations</a>, e.g. via () <a data-href="Householder Transformation" href="the-guide/mathematics/linear-algebra/householder-transformation.html" class="internal-link" target="_self" rel="noopener nofollow">Householder Transformation</a> or <a data-tooltip-position="top" aria-label="Givens Rotation" data-href="Givens Rotation" href="the-guide/mathematics/linear-algebra/givens-rotation.html" class="internal-link" target="_self" rel="noopener nofollow">Givens rotations</a>:This lowers iteration cost to ) at the expense of the initial transformation ().
<br>Tridiagonality is preserved through iterations, only  <a data-tooltip-position="top" aria-label="Givens Rotation" data-href="Givens Rotation" href="the-guide/mathematics/linear-algebra/givens-rotation.html" class="internal-link" target="_self" rel="noopener nofollow">givens rotations</a> on off-diagonal for <a data-tooltip-position="top" aria-label="QR-Decomposition" data-href="QR-Decomposition" href="the-guide/mathematics/linear-algebra/qr-decomposition.html" class="internal-link" target="_self" rel="noopener nofollow">decomposition</a> needed.


<br>Shift 

<br>Applying a shift  before the <a data-tooltip-position="top" aria-label="QR-Decomposition" data-href="QR-Decomposition" href="the-guide/mathematics/linear-algebra/qr-decomposition.html" class="internal-link" target="_self" rel="noopener nofollow">QR-decomposition</a> at iteration  and removing it afterwards speeds up the convergence by moving all eigenvalues closer to zero
<br>Algorithm<img alt="center" src="lib/media/pasted-image-20230820113637.png" style="width: 500px; max-width: 100%;">
<br>How to choose shift ?

<br>Naive 
<br>Wilkinson 

<br>
<br>






<br>Symmetric Case

<br>Complexity

<br>With Hessenberg 


<br>Deflation

<br>If off-diagonal element shrinks below threshold, set it to zero and continue iteration with smaller matrix / matrices.


<br>Eigenvectors

<br>In this case, iterate converges towards diagonal matrix , columns of  approximate eigenvectors of  (columns of  approximate eigenvector of ). 




]]></description><link>the-guide/mathematics/linear-algebra/qr-algorithm.html</link><guid isPermaLink="false">The Guide/Mathematics/Linear Algebra/QR-Algorithm.md</guid><pubDate>Mon, 09 Sep 2024 15:35:46 GMT</pubDate><enclosure url="lib/media/pasted-image-20230821094816.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src=&quot;lib/media/pasted-image-20230821094816.png&quot;&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[QR-Decomposition]]></title><description><![CDATA[ 
 <br>Theorem - Existence of QR Decomposition
Any real square matrix  can be decomposed as where Q is an <a data-tooltip-position="top" aria-label="Matrices" data-href="Matrices" href="the-guide/mathematics/linear-algebra/matrices.html" class="internal-link" target="_self" rel="noopener nofollow">orthogonal matrix</a> and R is an upper <a data-tooltip-position="top" aria-label="https://en.wikipedia.org/wiki/Triangular_matrix" rel="noopener nofollow" class="external-link" title="Triangular matrix" href="https://en.wikipedia.org/wiki/Triangular_matrix" target="_blank">triangular matrix</a> (also called right triangular matrix). If A is <a data-tooltip-position="top" aria-label="Matrices" data-href="Matrices" href="the-guide/mathematics/linear-algebra/matrices.html" class="internal-link" target="_self" rel="noopener nofollow">invertible</a> then the factorization is unique if we require the diagonal elements of R to be positive.
<br>
<br>In complex square case, matrix  is unitary instead of orthogonal
<br>In rectangular case  has all-zero rows below triagonal structure
<br><br><br>
<br>Use unitary transformations to compute triangular matrix  using one of

<br><a data-href="Householder Transformation" href="the-guide/mathematics/linear-algebra/householder-transformation.html" class="internal-link" target="_self" rel="noopener nofollow">Householder Transformation</a>
<br><a data-href="Givens Rotation" href="the-guide/mathematics/linear-algebra/givens-rotation.html" class="internal-link" target="_self" rel="noopener nofollow">Givens Rotation</a>
<br><a data-href="Gram-Schmidt-Process" href="the-guide/mathematics/linear-algebra/gram-schmidt-process.html" class="internal-link" target="_self" rel="noopener nofollow">Gram-Schmidt-Process</a>


]]></description><link>the-guide/mathematics/linear-algebra/qr-decomposition.html</link><guid isPermaLink="false">The Guide/Mathematics/Linear Algebra/QR-Decomposition.md</guid><pubDate>Sun, 26 Jan 2025 18:49:51 GMT</pubDate></item><item><title><![CDATA[Rayleigh Quotient]]></title><description><![CDATA[ 
 <br>Info
Given a <a data-tooltip-position="top" aria-label="Matrices" data-href="Matrices" href="the-guide/mathematics/linear-algebra/matrices.html" class="internal-link" target="_self" rel="noopener nofollow">symmetric matrix</a>  and a vector , the Rayleigh quotient is 
<br>
<br>If applied to non-symmetric matrix, information about non-symmetric part is lost
<br>If the vector  is a approximation to an eigenvector with error , the Rayleigh Quotient is an approximation to the corresponding eigenvector with error 
]]></description><link>the-guide/mathematics/linear-algebra/rayleigh-quotient.html</link><guid isPermaLink="false">The Guide/Mathematics/Linear Algebra/Rayleigh Quotient.md</guid><pubDate>Mon, 09 Sep 2024 15:35:46 GMT</pubDate></item><item><title><![CDATA[Schur Complement]]></title><description><![CDATA[ 
 <br>In a Nutshell
Key concept in the fields of numerical analysis, statistics (<a data-tooltip-position="top" aria-label="Gaussian Distribution" data-href="Gaussian Distribution" href="the-guide/mathematics/probability-theory/gaussian-distribution.html" class="internal-link" target="_self" rel="noopener nofollow">multivariate Gaussians</a>) and matrix analysis. Results from performing Gaussian elimination on the level of sub-matrices.
<br><br>Schur Complement
Given a <a data-tooltip-position="top" aria-label="Matrices" data-href="Matrices" href="the-guide/mathematics/linear-algebra/matrices.html" class="internal-link" target="_self" rel="noopener nofollow">matrix</a>  of the form with  and , the Schur complement of  is given by provided  is invertible. Equivalently, if  is invertible, its Schur complement is gievn by 
<br>
<br>Resulting Equalities

<br>


<br>Resulting Inequalities

<br>If both  and  are <a data-tooltip-position="top" aria-label="Matrices" data-href="Matrices" href="the-guide/mathematics/linear-algebra/matrices.html" class="internal-link" target="_self" rel="noopener nofollow">positive semi-definite</a>, it holds that The inequality is understood in terms of the eigenvalues.


<br><br><br>The Schur complement arises naturally in solving a system of linear equations such as   Assuming that the submatrix A is invertible, we can eliminate x from the equations, as follows.Substituting this expression into the second equation yields<br>
We refer to this as the ''reduced equation'' obtained by eliminating x from the original equation. The matrix appearing in the reduced equation is called the Schur complement of the first block A in M:Solving the reduced equation, we obtainSubstituting this into the first equation yields]]></description><link>the-guide/mathematics/linear-algebra/schur-complement.html</link><guid isPermaLink="false">The Guide/Mathematics/Linear Algebra/Schur Complement.md</guid><pubDate>Mon, 09 Sep 2024 15:35:46 GMT</pubDate></item><item><title><![CDATA[Schur Normal Form]]></title><description><![CDATA[ 
 <br>Theorem - Complex Version
For a <a data-tooltip-position="top" aria-label="Matrices" data-href="Matrices" href="the-guide/mathematics/linear-algebra/matrices.html" class="internal-link" target="_self" rel="noopener nofollow">quadratic matrix</a>  there exists a unitary matrix , such that  and  is upper triangular.
<br>Theorem - Real Version
For any  there is an orthogonal , such that  is <a data-tooltip-position="top" aria-label="Matrices" data-href="Matrices" href="the-guide/mathematics/linear-algebra/matrices.html" class="internal-link" target="_self" rel="noopener nofollow">quasi-triangular</a> with blocks of maximal dimension  with a pair of complex conjugate eigenvalues (singular eigenvalues else).
]]></description><link>the-guide/mathematics/linear-algebra/schur-normal-form.html</link><guid isPermaLink="false">The Guide/Mathematics/Linear Algebra/Schur Normal Form.md</guid><pubDate>Sun, 26 Jan 2025 18:51:25 GMT</pubDate></item><item><title><![CDATA[Schur-Horn Theorem]]></title><description><![CDATA[ 
 <br>Characterizes the diagonal of a <a data-tooltip-position="top" aria-label="Matrices" data-href="Matrices" href="the-guide/mathematics/linear-algebra/matrices.html" class="internal-link" target="_self" rel="noopener nofollow">hermitian matrix</a> with given <a data-tooltip-position="top" aria-label="Eigenvalue Problem" data-href="Eigenvalue Problem" href="the-guide/mathematics/linear-algebra/eigenvalue-problem.html" class="internal-link" target="_self" rel="noopener nofollow">eigenvalues</a>.<br><br>Theorem
Let  and  be two sequences of real numbers in non-decreasing order. There exists a <a data-tooltip-position="top" aria-label="Matrices" data-href="Matrices" href="the-guide/mathematics/linear-algebra/matrices.html" class="internal-link" target="_self" rel="noopener nofollow">hermitian matrix</a> with <a data-tooltip-position="top" aria-label="Eigenvalue Problem" data-href="Eigenvalue Problem" href="the-guide/mathematics/linear-algebra/eigenvalue-problem.html" class="internal-link" target="_self" rel="noopener nofollow">eigenvalues</a>  and diagonal entries  iffwhich is the weak <a data-tooltip-position="top" aria-label="Majorization" data-href="Majorization" href="the-guide/mathematics/linear-algebra/majorization.html" class="internal-link" target="_self" rel="noopener nofollow">majorization</a> of the diagonal by the eigenvalues and additionally the strong majorization.
]]></description><link>the-guide/mathematics/linear-algebra/schur-horn-theorem.html</link><guid isPermaLink="false">The Guide/Mathematics/Linear Algebra/Schur-Horn Theorem.md</guid><pubDate>Mon, 09 Sep 2024 15:35:46 GMT</pubDate></item><item><title><![CDATA[Singular Value Decomposition]]></title><description><![CDATA[ 
 <br>Info
For every <a data-tooltip-position="top" aria-label="Matrices" data-href="Matrices" href="the-guide/mathematics/linear-algebra/matrices.html" class="internal-link" target="_self" rel="noopener nofollow">matrix</a> , there exists a decomposition with unique singular values  in the form , intuitively decomposing the transformation into "rotation - stretch - rotation" with <a data-tooltip-position="top" aria-label="Matrices > Special Matrix Types" data-href="Matrices#Special Matrix Types" href="the-guide/mathematics/linear-algebra/matrices.html#Special_Matrix_Types" class="internal-link" target="_self" rel="noopener nofollow">unitary</a> non-unique matrices  (information about column space) and  (information about row space). 
<br><img alt="450|center" src="lib/media/pasted-image-20230416154649.png"><br><br>
<br><a data-tooltip-position="top" aria-label="Norms" data-href="Norms" href="the-guide/mathematics/linear-algebra/norms.html" class="internal-link" target="_self" rel="noopener nofollow">Spectral norm</a> 
<br>For <a data-tooltip-position="top" aria-label="Norms > Examples" data-href="Norms#Examples" href="the-guide/mathematics/linear-algebra/norms.html#Examples" class="internal-link" target="_self" rel="noopener nofollow">Frobenius norm</a> it holds 
<br>Squared singular values  are <a data-tooltip-position="top" aria-label="Eigenvalue Problem" data-href="Eigenvalue Problem" href="the-guide/mathematics/linear-algebra/eigenvalue-problem.html" class="internal-link" target="_self" rel="noopener nofollow">eigenvalues</a> of  and .

<br>Eigenvectors are the first  columns of matrices  or , respectively
<br>Transpose in real case


<br>If matrix real,  can be chosen real and orthogonal
<br>Economic Form

<br>If matrix has rank , take only first  columns of  and first  rows of 


<br>For  rank of matrix is equal to number of non-zero singular values. If singular values are sorted as above,
<br><br><br>
<br>SVD yields basis of kernel and image of a matrix  with rank :

<br>
<br>


<br>Low Rank Approximation

<br>SVD tool for data reduction, data-driven generalization of Fourier transform.
<br>Tailor coordinate system / transformation to problem
<br>Error  with  (best approximation)


<br>Can be used to define <a data-tooltip-position="top" aria-label="Moore-Penrose Pseudoinverse" data-href="Moore-Penrose Pseudoinverse" href="the-guide/mathematics/linear-algebra/moore-penrose-pseudoinverse.html" class="internal-link" target="_self" rel="noopener nofollow">Moore-Penrose pseudoinverse</a>.
<br><br><br>
<br>Naive Approach via <a data-tooltip-position="top" aria-label="Eigenvalue Problem" data-href="Eigenvalue Problem" href="the-guide/mathematics/linear-algebra/eigenvalue-problem.html" class="internal-link" target="_self" rel="noopener nofollow">Eigenvalues</a>

<br>Compute  (symmetric, positive-def)
<br>Determine <a data-tooltip-position="top" aria-label="Eigenvalue Problem" data-href="Eigenvalue Problem" href="the-guide/mathematics/linear-algebra/eigenvalue-problem.html" class="internal-link" target="_self" rel="noopener nofollow">Eigenvalue</a> <a data-tooltip-position="top" aria-label="Schur Normal Form" data-href="Schur Normal Form" href="the-guide/mathematics/linear-algebra/schur-normal-form.html" class="internal-link" target="_self" rel="noopener nofollow">decomposition</a>  of 

<br>Leads to a possible 


<br>
<br>Compute  via  using the <a data-href="QR-Decomposition" href="the-guide/mathematics/linear-algebra/qr-decomposition.html" class="internal-link" target="_self" rel="noopener nofollow">QR-Decomposition</a>.
<br>Errors in numeric computation of matrix  have huge effect on singular values.


<br>SVD-Algorithm

<br>(Phase 1) Transform ,  to bi-diagonal form using <a data-tooltip-position="top" aria-label="Householder Transformation" data-href="Householder Transformation" href="the-guide/mathematics/linear-algebra/householder-transformation.html" class="internal-link" target="_self" rel="noopener nofollow">Householder transformation</a> ())

<br>Singular values are invariant under orthogonal transformation
<br>For  there are orthogonal matrices  and  , such that with both  and  bi-diagonal.


<br>(Phase 2) Compute singular values ()

<br>Compute eigenvalues of  (tri-diagonal) using the <a data-tooltip-position="top" aria-label="QR-Algorithm" data-href="QR-Algorithm" href="the-guide/mathematics/linear-algebra/qr-algorithm.html" class="internal-link" target="_self" rel="noopener nofollow">QR-iteration</a> without explicitly building the product 

<br><a data-href="QR-Decomposition" href="the-guide/mathematics/linear-algebra/qr-decomposition.html" class="internal-link" target="_self" rel="noopener nofollow">QR-Decomposition</a> using () <a data-tooltip-position="top" aria-label="Givens Rotation" data-href="Givens Rotation" href="the-guide/mathematics/linear-algebra/givens-rotation.html" class="internal-link" target="_self" rel="noopener nofollow">Givens rotations</a>. The matrices are computable without the product, since all the needed entries are present in .
<br>Iteration of QR can again be written as matrix product with  via which indicates the same structure as before.
<br>Part of the next iterate  can be computed independently by chasing an element down the diagonal

<br>Only guaranteed, if  does not decay !


<br>The bidiagonal matrices converge towards a diagonal matrix of the singular values.




<br>Remark

<br>Also works for <a data-tooltip-position="top" aria-label="QR-Algorithm" data-href="QR-Algorithm" href="the-guide/mathematics/linear-algebra/qr-algorithm.html" class="internal-link" target="_self" rel="noopener nofollow">QR algorithm with shift</a>.




]]></description><link>the-guide/mathematics/linear-algebra/singular-value-decomposition.html</link><guid isPermaLink="false">The Guide/Mathematics/Linear Algebra/Singular Value Decomposition.md</guid><pubDate>Sun, 26 Jan 2025 18:52:29 GMT</pubDate><enclosure url="lib/media/pasted-image-20230416154649.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src=&quot;lib/media/pasted-image-20230416154649.png&quot;&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[Woodbury Identity]]></title><description><![CDATA[<a class="tag" href="?query=tag:Data-Assimilation" style="background-color: rgb(4, 108, 116); color: white; font-weight: 700; border: none; border-radius: 1em; padding: 0.2em 0.5em;">#Data-Assimilation</a> 
 <br>Lemma 4.4 - Woodbury Identity
Given  and assuming  (<a data-tooltip-position="top" aria-label="Matrices" data-href="Matrices" href="the-guide/mathematics/linear-algebra/matrices.html" class="internal-link" target="_self" rel="noopener nofollow">positive semi-definite</a>). Then the term  is invertible and given via 
<br>
<br>Number is from the <a href=".?query=tag:Data-Assimilation" class="tag" target="_blank" rel="noopener nofollow">#Data-Assimilation</a> lecture at uni
]]></description><link>the-guide/mathematics/linear-algebra/woodbury-identity.html</link><guid isPermaLink="false">The Guide/Mathematics/Linear Algebra/Woodbury Identity.md</guid><pubDate>Mon, 03 Mar 2025 17:33:24 GMT</pubDate></item><item><title><![CDATA[Lebesgue Measure]]></title><description><![CDATA[ 
 <br>On uncountable <a data-tooltip-position="top" aria-label="Space" data-href="Space" href="the-guide/mathematics/general-stuff/space.html" class="internal-link" target="_self" rel="noopener nofollow">spaces</a> element-wise allocations do note completely define <a data-tooltip-position="top" aria-label="Measure" data-href="Measure" href="the-guide/mathematics/measure-theory/measure.html" class="internal-link" target="_self" rel="noopener nofollow">measures</a>.<br>...<br>Given a real line with <a data-tooltip-position="top" aria-label="Space" data-href="Space" href="the-guide/mathematics/general-stuff/space.html" class="internal-link" target="_self" rel="noopener nofollow">ordering</a>, <a data-tooltip-position="top" aria-label="Algebra" data-href="Algebra" href="the-guide/mathematics/general-stuff/algebra.html" class="internal-link" target="_self" rel="noopener nofollow">algebra</a>, <a data-tooltip-position="top" aria-label="Metric Space and Completeness" data-href="Metric Space and Completeness" href="the-guide/mathematics/general-stuff/metric-space-and-completeness.html" class="internal-link" target="_self" rel="noopener nofollow">metric</a> and <a data-tooltip-position="top" aria-label="Topology and Topological Space" data-href="Topology and Topological Space" href="the-guide/mathematics/topology/topology-and-topological-space.html" class="internal-link" target="_self" rel="noopener nofollow">topology</a>, we can use the metric to allocate the length of each Borel subset as a measureThis measure is the well-known Lebesgue measure. In contrast to e.g. the counting measure, different real lines with different metrics or parametrizations will result in different Lebesgue measures.<br>
The total Lebesgue measure is infinitely large  and therefore cannot be normalized into a <a data-tooltip-position="top" aria-label="Probability Distribution" data-href="Probability Distribution" href="the-guide/mathematics/probability-theory/probability-distribution.html" class="internal-link" target="_self" rel="noopener nofollow">probability distribution</a>. Additionally, any subset with a countable number of elements is zero (collection of individual points).<br>On multivariate real spaces, the Lebesgue measure can be extended to which is why we can say that the Lebesgue measure over  quantifies volume.]]></description><link>the-guide/mathematics/measure-theory/lebesgue-measure.html</link><guid isPermaLink="false">The Guide/Mathematics/Measure Theory/Lebesgue Measure.md</guid><pubDate>Wed, 23 Apr 2025 08:34:37 GMT</pubDate></item><item><title><![CDATA[Measurable Function]]></title><description><![CDATA[ 
 <br>Definition
Let  and  be <a data-tooltip-position="top" aria-label="Measure" data-href="Measure" href="the-guide/mathematics/measure-theory/measure.html" class="internal-link" target="_self" rel="noopener nofollow">measurable spaces</a>, meaning that  and  are <a data-tooltip-position="top" aria-label="Set" data-href="Set" href="the-guide/mathematics/general-stuff/set.html" class="internal-link" target="_self" rel="noopener nofollow">sets</a> equipped with respective <a data-tooltip-position="top" aria-label="Sigma-Algebra" data-href="Sigma-Algebra" href="the-guide/mathematics/probability-theory/sigma-algebra.html" class="internal-link" target="_self" rel="noopener nofollow">sigma-algebras</a>  and . A <a data-tooltip-position="top" aria-label="Mathematical Relations" data-href="Mathematical Relations" href="the-guide/mathematics/general-stuff/mathematical-relations.html" class="internal-link" target="_self" rel="noopener nofollow">function</a>  is said to be measurable if for every , the pre-image of  under  is in  meaning that for 
<br>
<br>Intuitively gives us a condition such that the underlying <a data-tooltip-position="top" aria-label="Sigma-Algebra" data-href="Sigma-Algebra" href="the-guide/mathematics/probability-theory/sigma-algebra.html" class="internal-link" target="_self" rel="noopener nofollow">sigma-algebras</a> are compatible, such that the mapping / transformation / function  gives us a <a data-tooltip-position="top" aria-label="Probability Distribution" data-href="Probability Distribution" href="the-guide/mathematics/probability-theory/probability-distribution.html" class="internal-link" target="_self" rel="noopener nofollow">probability distribution</a> on .
<br>Every half-interval of outputs pulls back to a measurable subset on .
<br>If a transformation is indeed measurable, we can define a <a data-tooltip-position="top" aria-label="Push-Forward Measure" data-href="Push-Forward Measure" href="the-guide/mathematics/measure-theory/push-forward-measure.html" class="internal-link" target="_self" rel="noopener nofollow">pushforward</a> distribution
<br><br><br>Measurable transformations can be used to project a <a data-tooltip-position="top" aria-label="Probability Distribution" data-href="Probability Distribution" href="the-guide/mathematics/probability-theory/probability-distribution.html" class="internal-link" target="_self" rel="noopener nofollow">probability distribution</a> over a <a data-tooltip-position="top" aria-label="Space" data-href="Space" href="the-guide/mathematics/general-stuff/space.html" class="internal-link" target="_self" rel="noopener nofollow">space</a> onto a probability distribution over a lower-dimensional subspace. This projection, denoted by a projection operator <br><br>]]></description><link>the-guide/mathematics/measure-theory/measurable-function.html</link><guid isPermaLink="false">The Guide/Mathematics/Measure Theory/Measurable Function.md</guid><pubDate>Thu, 15 Aug 2024 12:51:15 GMT</pubDate></item><item><title><![CDATA[Measure]]></title><description><![CDATA[ 
 <br>Generalization and formalization of geometrical measures such as length, volume and area or mass and probability. A measure is any consistent allocation of a quantity  (in probability a quantity with total mass ) to the elements of an <a data-tooltip-position="top" aria-label="Set" data-href="Set" href="the-guide/mathematics/general-stuff/set.html" class="internal-link" target="_self" rel="noopener nofollow">ambient set</a>.<img alt="center" src="lib/media/pasted-image-20231129170022.png" style="width: 300px; max-width: 100%;"><br>Formal Definition of a Measure
Let  be a <a data-tooltip-position="top" aria-label="Set" data-href="Set" href="the-guide/mathematics/general-stuff/set.html" class="internal-link" target="_self" rel="noopener nofollow">set</a> and  a <a data-tooltip-position="top" aria-label="Sigma-Algebra" data-href="Sigma-Algebra" href="the-guide/mathematics/probability-theory/sigma-algebra.html" class="internal-link" target="_self" rel="noopener nofollow">sigma-algebra</a> over . A set function  from the sigma algebra  to the <a data-tooltip-position="top" aria-label="https://en.wikipedia.org/wiki/Extended_real_number_line" rel="noopener nofollow" class="external-link" title="Extended real number line" href="https://en.wikipedia.org/wiki/Extended_real_number_line" target="_blank">extended real number line</a> is called a measure if it satisfies the following properties:

<br>Non-negativity: For all  in , we have 
<br>Null empty set: 
<br>Countable additivity For all countable collections  of pairwise disjoint sets in , 

<br>
<br>The pair  is called measurable space (sometimes Borel space) and the members of the <a data-tooltip-position="top" aria-label="Sigma-Algebra" data-href="Sigma-Algebra" href="the-guide/mathematics/probability-theory/sigma-algebra.html" class="internal-link" target="_self" rel="noopener nofollow">sigma algebra</a>  are called measurable sets.
<br>Adding a measure , the triple  is called measure space, if additionally  (assigns probability to each outcome, sum is 1) it is called probability measure or <a data-tooltip-position="top" aria-label="Probability Space" data-href="Probability Space" href="the-guide/mathematics/probability-theory/probability-space.html" class="internal-link" target="_self" rel="noopener nofollow">probability space</a> (sometimes denoted )
<br>Set of measures on a measurable space  is often denoted 
<br>The set  is often denoted  in connection to probability theory
<br><br><br>Measures, where . They can be reframed via relative allocation, assigning the proportion each element of the total quantity . This leads to proportional measures  with with The collection of variables  is called simplex.<br><br>A discrete measure with weigths  and locations  reads <br>
<br>Intuition with dirac and indicator function, remembering that  is a set function
<br><br>An arbitrary measure  is defined by the fact that it can be integrated against any continuous function  and obtain .]]></description><link>the-guide/mathematics/measure-theory/measure.html</link><guid isPermaLink="false">The Guide/Mathematics/Measure Theory/Measure.md</guid><pubDate>Thu, 15 Aug 2024 12:51:15 GMT</pubDate><enclosure url="lib/media/pasted-image-20231129170022.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src=&quot;lib/media/pasted-image-20231129170022.png&quot;&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[Measure-Informed Integration]]></title><description><![CDATA[ 
 <br><img alt="center" src="lib/media/pasted-image-20240501084113.png" style="width: 500px; max-width: 100%;"><br>
<br>a) For a fixed <a data-tooltip-position="top" aria-label="Mathematical Relations" data-href="Mathematical Relations" href="the-guide/mathematics/general-stuff/mathematical-relations.html" class="internal-link" target="_self" rel="noopener nofollow">function</a> , the measure informed integrals with respect to different test <a data-tooltip-position="top" aria-label="Measure" data-href="Measure" href="the-guide/mathematics/measure-theory/measure.html" class="internal-link" target="_self" rel="noopener nofollow">measures</a> are sensitive to different features of .
<br>b) Integrals of different test functions with respect to the fixed measure  are sensitive to different features of .
<br><br>On finite measure spaces, we can define the integral of  with respect to a measure  via where we combine the weights of the inputs given by  with the weights of the outputs . This gives a number that is sensitive to the interplay between  and .<br><br>On uncountable spaces, element-wise allocations do not completely characterize a measure, which is why the above interpretation cannot completely characterize the interaction between measure and function.<br>
We consider an indicator function that vanishes outside of a given measurable subset which yields the interaction .<br><a data-tooltip-position="top" aria-label="Measurable Function" data-href="Measurable Function" href="the-guide/mathematics/measure-theory/measurable-function.html" class="internal-link" target="_self" rel="noopener nofollow">Measurable functions</a> with non-negative outputs for all  can always be perfectly recovered via a limit of these indicator functions, yielding We can define the measure-informed integral of such a function via this decomposition which yields the same result for any possible decomposition. This process is referred to as Lebesgue Integration. It is extended to general functions by decomposing the function by sign into   and  and then computing <br>Lebesgue Integrable 
A <a data-tooltip-position="top" aria-label="Measurable Function" data-href="Measurable Function" href="the-guide/mathematics/measure-theory/measurable-function.html" class="internal-link" target="_self" rel="noopener nofollow">measurable</a> <a data-tooltip-position="top" aria-label="Mathematical Relations" data-href="Mathematical Relations" href="the-guide/mathematics/general-stuff/mathematical-relations.html" class="internal-link" target="_self" rel="noopener nofollow">function</a>  with is said to be Lebesgue integrable with respect to .
<br>
<br>A real-valued function that is measurable with respect to the ambient <a data-tooltip-position="top" aria-label="Sigma-Algebra" data-href="Sigma-Algebra" href="the-guide/mathematics/probability-theory/sigma-algebra.html" class="internal-link" target="_self" rel="noopener nofollow">sigma algebra</a> and Lebesgue integrable with respect to every relevant measure is denoted integrand
<br>(...)<br><br>]]></description><link>the-guide/mathematics/measure-theory/measure-informed-integration.html</link><guid isPermaLink="false">The Guide/Mathematics/Measure Theory/Measure-Informed Integration.md</guid><pubDate>Thu, 15 Aug 2024 12:51:15 GMT</pubDate><enclosure url="lib/media/pasted-image-20240501084113.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src=&quot;lib/media/pasted-image-20240501084113.png&quot;&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[Push-Forward Measure]]></title><description><![CDATA[ 
 <br>Obtained by transferring a <a data-tooltip-position="top" aria-label="Measure" data-href="Measure" href="the-guide/mathematics/measure-theory/measure.html" class="internal-link" target="_self" rel="noopener nofollow">measure</a> from one measurable space to another using a <a data-tooltip-position="top" aria-label="Measurable Function" data-href="Measurable Function" href="the-guide/mathematics/measure-theory/measurable-function.html" class="internal-link" target="_self" rel="noopener nofollow">measurable function</a>.<br>Definition
Given <a data-tooltip-position="top" aria-label="Measure" data-href="Measure" href="the-guide/mathematics/measure-theory/measure.html" class="internal-link" target="_self" rel="noopener nofollow">measurable spaces</a>  and , a <a data-tooltip-position="top" aria-label="Measurable Function" data-href="Measurable Function" href="the-guide/mathematics/measure-theory/measurable-function.html" class="internal-link" target="_self" rel="noopener nofollow">measurable mapping</a>  and a measure , the push-forward of  is defined to be the measure  given by 
<br>
<br> preserves positivity and total mass
<br> can be interpreted as moving single point from one space to another
<br> is extension, moves entire probability measure to another measure
<br>If  is the pushforward of , then for any measurable set  
<br>Change-of-Variables Formula
A measurable function  on  is integrable with respect to the push-forward <a data-tooltip-position="top" aria-label="Measure" data-href="Measure" href="the-guide/mathematics/measure-theory/measure.html" class="internal-link" target="_self" rel="noopener nofollow">measure</a> , if some  satisfies meaning that the composition  is integrable with respect to the measure .
<br><br><br>
<br>Continuous  
<br>Discrete 

<br>Given a discrete measure , the push forward operator consists in moving the position of all the points in the support of the measure 


<br>Properties

<br>Linear for two measures : 


<br><br><br>
<br>Do not confuse with <a data-tooltip-position="top" aria-label="Mathematical Relations" data-href="Mathematical Relations" href="the-guide/mathematics/general-stuff/mathematical-relations.html" class="internal-link" target="_self" rel="noopener nofollow">pull-back along a function</a> !

<br>Adjoint to pull-back , a linear map  that associates  to . Both concepts are related via <img alt="Pasted image 20230510200950.png" src="lib/media/pasted-image-20230510200950.png">


]]></description><link>the-guide/mathematics/measure-theory/push-forward-measure.html</link><guid isPermaLink="false">The Guide/Mathematics/Measure Theory/Push-Forward Measure.md</guid><pubDate>Thu, 15 Aug 2024 12:51:15 GMT</pubDate><enclosure url="lib/media/pasted-image-20230510200950.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src=&quot;lib/media/pasted-image-20230510200950.png&quot;&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[Radon-Nikodym Theorem]]></title><description><![CDATA[ 
 <br>Theorem
Let  be a <a data-tooltip-position="top" aria-label="Measure" data-href="Measure" href="the-guide/mathematics/measure-theory/measure.html" class="internal-link" target="_self" rel="noopener nofollow">measurable space</a> with two -finite <a data-tooltip-position="top" aria-label="Measure" data-href="Measure" href="the-guide/mathematics/measure-theory/measure.html" class="internal-link" target="_self" rel="noopener nofollow">measures</a>  and . If  is <a data-tooltip-position="top" aria-label="Continuity" data-href="Continuity" href="the-guide/mathematics/analysis-and-calculus/continuity.html" class="internal-link" target="_self" rel="noopener nofollow">absolutely continuous</a> with respect to , then there exists a -<a data-tooltip-position="top" aria-label="Measurable Function" data-href="Measurable Function" href="the-guide/mathematics/measure-theory/measurable-function.html" class="internal-link" target="_self" rel="noopener nofollow">measurable function</a> , such that for any measurable <a data-tooltip-position="top" aria-label="Set" data-href="Set" href="the-guide/mathematics/general-stuff/set.html" class="internal-link" target="_self" rel="noopener nofollow">set</a>  it holds thatThis function is uniquely defined up to a  null set and is denoted the Radon-Nikodym derivative, commonly written
<br>Why Derivative
Reflects the same intuitive idea to a <a data-tooltip-position="top" aria-label="Derivative, Gradient, Jacobian and Hessian" data-href="Derivative, Gradient, Jacobian and Hessian" href="the-guide/mathematics/analysis-and-calculus/derivative,-gradient,-jacobian-and-hessian.html" class="internal-link" target="_self" rel="noopener nofollow">derivative in calculus</a>.  reflects the rate of change of the density of one measure with respect to another.
<br>Examples<br>
<br>If  was the <a data-tooltip-position="top" aria-label="Lebesgue Measure" data-href="Lebesgue Measure" href="the-guide/mathematics/measure-theory/lebesgue-measure.html" class="internal-link" target="_self" rel="noopener nofollow">Lebesgue measure</a> and  represents mass density, then  would assign total mass to spatial regions
]]></description><link>the-guide/mathematics/measure-theory/radon-nikodym-theorem.html</link><guid isPermaLink="false">The Guide/Mathematics/Measure Theory/Radon-Nikodym Theorem.md</guid><pubDate>Sun, 16 Mar 2025 14:23:43 GMT</pubDate></item><item><title><![CDATA[Dual Kantorovich Problem]]></title><description><![CDATA[ 
 <br>In the following the vectors  or the functions  are often refered to as the Kantorovich Potentials.<br><br>Discrete Definition
The <a data-tooltip-position="top" aria-label="Optimal Transport" data-href="Optimal Transport" href="the-guide/mathematics/optimal-transport/optimal-transport.html" class="internal-link" target="_self" rel="noopener nofollow">Kantorovich problem</a> admits the dual problem with admissible set of dual variables 
<br>
<br>Kantorovich Linear Program for the Dual Formulation

<br>Using the real  <a data-tooltip-position="top" aria-label="Matrices" data-href="Matrices" href="the-guide/mathematics/linear-algebra/matrices.html" class="internal-link" target="_self" rel="noopener nofollow">matrix</a> the above problem can be reformulated as an <a data-tooltip-position="top" aria-label="Linear Program (LP)" data-href="Linear Program (LP)" href="the-guide/mathematics/optimization/convex-optimization-lecture/linear-program-(lp).html" class="internal-link" target="_self" rel="noopener nofollow">LP-problem</a> of the form 


<br>Continuous Definition
Given <a data-tooltip-position="top" aria-label="Measure" data-href="Measure" href="the-guide/mathematics/measure-theory/measure.html" class="internal-link" target="_self" rel="noopener nofollow">measures</a>  on , a cost function  and continuous test-functions , the dual Kantorovich problem iswith the <a data-tooltip-position="top" aria-label="Set" data-href="Set" href="the-guide/mathematics/general-stuff/set.html" class="internal-link" target="_self" rel="noopener nofollow">set</a> of admissable dual potentials 
<br>
<br>If  and , the constraint can be dropped and put into the problem as a regularizer, see. (2.27) in book
<br>Intuition

<br>Instead of enforcing a cost for transporting mass from one source point to a target point, split up the price into pick-up and delivery for every "address". In order for this to be a valid offer, the charges should not exceed the initial transportation cost for any pair. The maximization is needed, because otherwise choosing negative prices breaks the scheme
<br>Right-hand-side (without max) is lower bound of the original optimal transport problem, by finding largest lower bound we find original solution (strong duality for linear programs)

<br><br><br>Dual Problem for Entropic Regularization
For the <a data-tooltip-position="top" aria-label="Entropic Regularization of Optimal Transport" data-href="Entropic Regularization of Optimal Transport" href="the-guide/mathematics/optimal-transport/entropic-regularization-of-optimal-transport.html" class="internal-link" target="_self" rel="noopener nofollow">regularized case</a>, the dual problem reads The vectors  and  are linked to the scalings of the <a data-href="Sinkhorn-Knopp Algorithm" href="the-guide/mathematics/optimal-transport/sinkhorn-knopp-algorithm.html" class="internal-link" target="_self" rel="noopener nofollow">Sinkhorn-Knopp Algorithm</a>.
<br><br><br>Proposition 
Assume given optimal solution  of the <a data-tooltip-position="top" aria-label="Optimal Transport" data-href="Optimal Transport" href="the-guide/mathematics/optimal-transport/optimal-transport.html" class="internal-link" target="_self" rel="noopener nofollow">primal problem</a> and optimal dual solutions . Then for any index-pair , it holds that . Therefore, and 
]]></description><link>the-guide/mathematics/optimal-transport/dual-kantorovich-problem.html</link><guid isPermaLink="false">The Guide/Mathematics/Optimal Transport/Dual Kantorovich Problem.md</guid><pubDate>Thu, 17 Apr 2025 16:19:41 GMT</pubDate></item><item><title><![CDATA[Entropic Regularization of Optimal Transport]]></title><description><![CDATA[ 
 <br>Solving normal <a data-tooltip-position="top" aria-label="Optimal Transport" data-href="Optimal Transport" href="the-guide/mathematics/optimal-transport/optimal-transport.html" class="internal-link" target="_self" rel="noopener nofollow">OT</a> via linear programming is convex, but has high complexity. By adding regularization term in form of the transportation plans <a data-tooltip-position="top" aria-label="Shannon Entropy" data-href="Shannon Entropy" href="the-guide/information-theory/information-theory-1/information/shannon-entropy.html" class="internal-link" target="_self" rel="noopener nofollow">entropy</a> we enforce a maximum entropy condition and can solve this approximation a lot more efficient. The new solution still yields a mathematical <a data-tooltip-position="top" aria-label="Metric Space and Completeness" data-href="Metric Space and Completeness" href="the-guide/mathematics/general-stuff/metric-space-and-completeness.html" class="internal-link" target="_self" rel="noopener nofollow">distance</a>, the <a data-href="Sinkhorn Distance" href="the-guide/mathematics/optimal-transport/sinkhorn-distance.html" class="internal-link" target="_self" rel="noopener nofollow">Sinkhorn Distance</a>.<br>
<img alt="center" src="lib/media/pasted-image-20240124192948.png"><br><br><br>Entropic Regularization of Discrete Measures
For discrete <a data-tooltip-position="top" aria-label="Measure" data-href="Measure" href="the-guide/mathematics/measure-theory/measure.html" class="internal-link" target="_self" rel="noopener nofollow">measures</a>, the new problem is stated via
<br>
<br>Converges to maximum entropy solution of standard <a data-tooltip-position="top" aria-label="Optimal Transport" data-href="Optimal Transport" href="the-guide/mathematics/optimal-transport/optimal-transport.html" class="internal-link" target="_self" rel="noopener nofollow">optimal transport problem</a> with 
<br>Intuition

<br>Cost can be higher if <a data-tooltip-position="top" aria-label="Shannon Entropy" data-href="Shannon Entropy" href="the-guide/information-theory/information-theory-1/information/shannon-entropy.html" class="internal-link" target="_self" rel="noopener nofollow">entropy</a> of transportation plan is high, favors more blurred plan / encourages diversity of transportation routes
<br>Leads to less sparsity (optimal plan of original problem had at most  nonzero entries)


<br><br><br>Entropic Regularization of Arbitrary Measures
If considering arbitrary measures, one can replace the entropy by the <a data-tooltip-position="top" aria-label="Kullback-Leibler Divergence" data-href="Kullback-Leibler Divergence" href="the-guide/information-theory/information-theory-1/information/kullback-leibler-divergence.html" class="internal-link" target="_self" rel="noopener nofollow">relative entropy</a> with respect to the product <a data-tooltip-position="top" aria-label="Measure" data-href="Measure" href="the-guide/mathematics/measure-theory/measure.html" class="internal-link" target="_self" rel="noopener nofollow">measure</a> , yielding where the different regularization term results from the problematic of <a data-tooltip-position="top" aria-label="Shannon Entropy" data-href="Shannon Entropy" href="the-guide/information-theory/information-theory-1/information/shannon-entropy.html" class="internal-link" target="_self" rel="noopener nofollow">differential entropy</a> and the <a data-tooltip-position="top" aria-label="Limiting Density of Discrete Points" data-href="Limiting Density of Discrete Points" href="the-guide/information-theory/information-theory-1/information/limiting-density-of-discrete-points.html" class="internal-link" target="_self" rel="noopener nofollow">LDDP</a>.
<br>The Reference Measure doesn't matter
For any  and for any alternative  with the same support, it holds that the <a data-tooltip-position="top" aria-label="Kullback-Leibler Divergence" data-href="Kullback-Leibler Divergence" href="the-guide/information-theory/information-theory-1/information/kullback-leibler-divergence.html" class="internal-link" target="_self" rel="noopener nofollow">KL-Divergence</a> fullfills which means that choosing the alternative measures results in the same solution.
<br>Probabilistic / Information-Theoretic Interpretation
Similar to the normal <a data-tooltip-position="top" aria-label="Optimal Transport" data-href="Optimal Transport" href="the-guide/mathematics/optimal-transport/optimal-transport.html" class="internal-link" target="_self" rel="noopener nofollow">OT objective</a>, the above can be state via with <a data-tooltip-position="top" aria-label="Mutual Information" data-href="Mutual Information" href="the-guide/information-theory/information-theory-1/information/mutual-information.html" class="internal-link" target="_self" rel="noopener nofollow">mutual information</a> .
]]></description><link>the-guide/mathematics/optimal-transport/entropic-regularization-of-optimal-transport.html</link><guid isPermaLink="false">The Guide/Mathematics/Optimal Transport/Entropic Regularization of Optimal Transport.md</guid><pubDate>Mon, 24 Feb 2025 23:49:45 GMT</pubDate><enclosure url="lib/media/pasted-image-20240124192948.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src=&quot;lib/media/pasted-image-20240124192948.png&quot;&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[Gromov-Hausdorff Distance]]></title><description><![CDATA[ 
 <br>Measures the <a data-tooltip-position="top" aria-label="Metric Space and Completeness" data-href="Metric Space and Completeness" href="the-guide/mathematics/general-stuff/metric-space-and-completeness.html" class="internal-link" target="_self" rel="noopener nofollow">distance</a> between two <a data-tooltip-position="top" aria-label="Metric Space and Completeness" data-href="Metric Space and Completeness" href="the-guide/mathematics/general-stuff/metric-space-and-completeness.html" class="internal-link" target="_self" rel="noopener nofollow">metric spaces</a>  and  by quantifying how far these <a data-tooltip-position="top" aria-label="Space" data-href="Space" href="the-guide/mathematics/general-stuff/space.html" class="internal-link" target="_self" rel="noopener nofollow">spaces</a> are from being <a data-tooltip-position="top" aria-label="Mathematical Relations" data-href="Mathematical Relations" href="the-guide/mathematics/general-stuff/mathematical-relations.html" class="internal-link" target="_self" rel="noopener nofollow">isometric</a>.<br>Gromov-Hausdorff Distance
Defined as the minimum <a data-tooltip-position="top" aria-label="Hausdorff Distance" data-href="Hausdorff Distance" href="the-guide/mathematics/optimal-transport/hausdorff-distance.html" class="internal-link" target="_self" rel="noopener nofollow">Hausdorff distance</a> between every possible isometric embedding of the two spaces in a third onewith  being <a data-tooltip-position="top" aria-label="Mathematical Relations" data-href="Mathematical Relations" href="the-guide/mathematics/general-stuff/mathematical-relations.html" class="internal-link" target="_self" rel="noopener nofollow">isometric mappings</a> from  to .
<br>
<br>Leads to highly non-convex optimization problem, can be smoothed by replacing infimum by integration (requires measure)  <a data-href="Gromov-Wasserstein Distance" href="the-guide/mathematics/optimal-transport/gromov-wasserstein-distance.html" class="internal-link" target="_self" rel="noopener nofollow">Gromov-Wasserstein Distance</a> 
]]></description><link>the-guide/mathematics/optimal-transport/gromov-hausdorff-distance.html</link><guid isPermaLink="false">The Guide/Mathematics/Optimal Transport/Gromov-Hausdorff Distance.md</guid><pubDate>Mon, 24 Feb 2025 23:49:45 GMT</pubDate></item><item><title><![CDATA[Gromov-Wasserstein Distance]]></title><description><![CDATA[ 
 <br>Weakens the assumptions of having ground cost  available in the <a data-href="Optimal Transport" href="the-guide/mathematics/optimal-transport/optimal-transport.html" class="internal-link" target="_self" rel="noopener nofollow">Optimal Transport</a> context by assuming that two <a data-tooltip-position="top" aria-label="Matrices" data-href="Matrices" href="the-guide/mathematics/linear-algebra/matrices.html" class="internal-link" target="_self" rel="noopener nofollow">matrices</a>  and  quantify similarity relations between points on which the histograms  are defined. The discrete problem reads using <img alt="center" src="lib/media/pasted-image-20240109091526.png"><br>Continuous Definition
In general, the setting wants to compute couplings between <a data-tooltip-position="top" aria-label="Metric Space and Completeness" data-href="Metric Space and Completeness" href="the-guide/mathematics/general-stuff/metric-space-and-completeness.html" class="internal-link" target="_self" rel="noopener nofollow">metric spaces</a>  and  with <a data-tooltip-position="top" aria-label="Measure" data-href="Measure" href="the-guide/mathematics/measure-theory/measure.html" class="internal-link" target="_self" rel="noopener nofollow">measures</a> . The GW distance defines a distance between metric spaces up to <a data-tooltip-position="top" aria-label="Mathematical Relations" data-href="Mathematical Relations" href="the-guide/mathematics/general-stuff/mathematical-relations.html" class="internal-link" target="_self" rel="noopener nofollow">isometries</a> ( such that  and ). 
<br><br><br>Similarily to <a data-tooltip-position="top" aria-label="Entropic Regularization of Optimal Transport" data-href="Entropic Regularization of Optimal Transport" href="the-guide/mathematics/optimal-transport/entropic-regularization-of-optimal-transport.html" class="internal-link" target="_self" rel="noopener nofollow">entropic regularization of optimal transport</a>, the (discrete) objective can be adapted to which can be solved by a <a data-tooltip-position="top" aria-label="Mirror Descent" data-href="Mirror Descent" href="the-guide/mathematics/optimization/mirror-descent.html" class="internal-link" target="_self" rel="noopener nofollow">mirror descent</a> scheme using the <a data-href="Sinkhorn-Knopp Algorithm" href="the-guide/mathematics/optimal-transport/sinkhorn-knopp-algorithm.html" class="internal-link" target="_self" rel="noopener nofollow">Sinkhorn-Knopp Algorithm</a> to update ]]></description><link>the-guide/mathematics/optimal-transport/gromov-wasserstein-distance.html</link><guid isPermaLink="false">The Guide/Mathematics/Optimal Transport/Gromov-Wasserstein Distance.md</guid><pubDate>Mon, 24 Feb 2025 23:49:45 GMT</pubDate><enclosure url="lib/media/pasted-image-20240109091526.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src=&quot;lib/media/pasted-image-20240109091526.png&quot;&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[Hausdorff Distance]]></title><description><![CDATA[ 
 <br>Measures how far two <a data-tooltip-position="top" aria-label="Set" data-href="Set" href="the-guide/mathematics/general-stuff/set.html" class="internal-link" target="_self" rel="noopener nofollow">subsets</a>  of a <a data-tooltip-position="top" aria-label="Metric Space and Completeness" data-href="Metric Space and Completeness" href="the-guide/mathematics/general-stuff/metric-space-and-completeness.html" class="internal-link" target="_self" rel="noopener nofollow">metric space</a> are from each other. Turns the <a data-tooltip-position="top" aria-label="Set" data-href="Set" href="the-guide/mathematics/general-stuff/set.html" class="internal-link" target="_self" rel="noopener nofollow">set</a> of non-empty compact subsets of a metric space into a metric space in its own right (since it measures distance between those subsets).<img alt="center" src="lib/media/pasted-image-20240109085519.png"><br>
<br> in picture are subsets 
<br>Definition
Given a <a data-tooltip-position="top" aria-label="Metric Space and Completeness" data-href="Metric Space and Completeness" href="the-guide/mathematics/general-stuff/metric-space-and-completeness.html" class="internal-link" target="_self" rel="noopener nofollow">metric space</a>  and two subsets , the Hausdorff Distance is defined as which gives a <a data-tooltip-position="top" aria-label="Metric Space and Completeness" data-href="Metric Space and Completeness" href="the-guide/mathematics/general-stuff/metric-space-and-completeness.html" class="internal-link" target="_self" rel="noopener nofollow">distance</a> between compact sets  of .
<br>Quote
Greatest distance between any point in  and the respective closest point in .
<br><br><br>Can be defined similarly to the <a data-href="Wasserstein Distance" href="the-guide/mathematics/optimal-transport/wasserstein-distance.html" class="internal-link" target="_self" rel="noopener nofollow">Wasserstein Distance</a> with <a data-tooltip-position="top" aria-label="Set" data-href="Set" href="the-guide/mathematics/general-stuff/set.html" class="internal-link" target="_self" rel="noopener nofollow">set</a> couplings replacing <a data-tooltip-position="top" aria-label="Measure" data-href="Measure" href="the-guide/mathematics/measure-theory/measure.html" class="internal-link" target="_self" rel="noopener nofollow">measure</a> couplings:yielding the formulation The support of a measure coupling is a set coupling between the supports. Thereby, the Hausdorff Distance is connected to the <a data-href="Wasserstein Distance" href="the-guide/mathematics/optimal-transport/wasserstein-distance.html" class="internal-link" target="_self" rel="noopener nofollow">Wasserstein Distance</a> via  for any pair of measures  whose supports are .]]></description><link>the-guide/mathematics/optimal-transport/hausdorff-distance.html</link><guid isPermaLink="false">The Guide/Mathematics/Optimal Transport/Hausdorff Distance.md</guid><pubDate>Mon, 24 Feb 2025 23:49:45 GMT</pubDate><enclosure url="lib/media/pasted-image-20240109085519.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src=&quot;lib/media/pasted-image-20240109085519.png&quot;&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[Log-Domain Sinkhorn]]></title><description><![CDATA[ 
 <br>Variant of <a data-tooltip-position="top" aria-label="Sinkhorn-Knopp Algorithm" data-href="Sinkhorn-Knopp Algorithm" href="the-guide/mathematics/optimal-transport/sinkhorn-knopp-algorithm.html" class="internal-link" target="_self" rel="noopener nofollow">Sinkhorn-Knopp algorithm</a> that uses computations in the log-domain to work against the overflowing issues.<br><br>
<br>Block Coordinate Ascent

<br>Alternate between log-updating the <a data-tooltip-position="top" aria-label="Dual Kantorovich Problem" data-href="Dual Kantorovich Problem" href="the-guide/mathematics/optimal-transport/dual-kantorovich-problem.html" class="internal-link" target="_self" rel="noopener nofollow">dual</a> objective  using the gradients 
<br>Resulting updates are
<br>Equivalent result to normal Sinkhorn updates
<br>Prevents overflows etc., but update is no longer just <a data-tooltip-position="top" aria-label="Matrices" data-href="Matrices" href="the-guide/mathematics/linear-algebra/matrices.html" class="internal-link" target="_self" rel="noopener nofollow">matrix-vector</a>, can significantly slow down the computation


<br>Log-Domain Stabilization

<br>Middle way, frequently absorb extreme values into log domain via redundant parametrizationand stabilization of kernel matrix to keep the scaling vectors bounded.
<br>At any absorbtion iteration, compute the vectors  from the formulas above and update , then restart iteration with one-vectors. In the end, the redundant vectors cancel when computing the plan, but prevent extreme values during the iterations.


]]></description><link>the-guide/mathematics/optimal-transport/log-domain-sinkhorn.html</link><guid isPermaLink="false">The Guide/Mathematics/Optimal Transport/Log-Domain Sinkhorn.md</guid><pubDate>Mon, 09 Sep 2024 15:38:50 GMT</pubDate></item><item><title><![CDATA[Monge Problem]]></title><description><![CDATA[ 
 <br><img alt="center" src="lib/media/pasted-image-20231207125429.png" style="width: 300px; max-width: 100%;"><br>Definition
Given two <a data-tooltip-position="top" aria-label="Measure > Spaces" data-href="Measure#Spaces" href="the-guide/mathematics/measure-theory/measure.html#Spaces" class="internal-link" target="_self" rel="noopener nofollow">measurable spaces</a> (more precisely Polish spaces) , a cost <a data-tooltip-position="top" aria-label="Mathematical Relations" data-href="Mathematical Relations" href="the-guide/mathematics/general-stuff/mathematical-relations.html" class="internal-link" target="_self" rel="noopener nofollow">function</a>  and two <a data-tooltip-position="top" aria-label="Measure" data-href="Measure" href="the-guide/mathematics/measure-theory/measure.html" class="internal-link" target="_self" rel="noopener nofollow">measures</a>  in  (sets of measures over spaces), find a map , such that 
<br>
<br>... the <a data-tooltip-position="top" aria-label="Push-Forward Measure" data-href="Push-Forward Measure" href="the-guide/mathematics/measure-theory/push-forward-measure.html" class="internal-link" target="_self" rel="noopener nofollow">push-forward</a> under the map  for measure  is 
<br>Highly non-convex, since it does not allow mass splitting, for some  there might be no .

<br><a data-tooltip-position="top" aria-label="Optimal Transport" data-href="Optimal Transport" href="the-guide/mathematics/optimal-transport/optimal-transport.html" class="internal-link" target="_self" rel="noopener nofollow">Kantorovich</a> relaxation resolves this, leads to <a data-tooltip-position="top" aria-label="Convexity" data-href="Convexity" href="the-guide/mathematics/optimization/convex-optimization-lecture/convexity.html" class="internal-link" target="_self" rel="noopener nofollow">convexity</a>


<br><br><br>Theorem - Monge Map
If  and cost is the squared-euclidean distance, then the map is the gradient of a <a data-tooltip-position="top" aria-label="Convexity" data-href="Convexity" href="the-guide/mathematics/optimization/convex-optimization-lecture/convexity.html" class="internal-link" target="_self" rel="noopener nofollow">convex</a> functionFor any  convex, the gradient  is the Monge map between  and .
]]></description><link>the-guide/mathematics/optimal-transport/monge-problem.html</link><guid isPermaLink="false">The Guide/Mathematics/Optimal Transport/Monge Problem.md</guid><pubDate>Mon, 09 Sep 2024 15:38:50 GMT</pubDate><enclosure url="lib/media/pasted-image-20231207125429.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src=&quot;lib/media/pasted-image-20231207125429.png&quot;&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[Optimal Transport]]></title><description><![CDATA[ 
 <br>Mathematical framework to ...<br><br><br>Original <a data-tooltip-position="top" aria-label="Monge Problem" data-href="Monge Problem" href="the-guide/mathematics/optimal-transport/monge-problem.html" class="internal-link" target="_self" rel="noopener nofollow">Monge problem</a> was limited to deterministic transportation, meaning point  can only be assigned to single point . Kantorovich relaxes this, allowing for mass splitting, therefore using probabilistic maps , also called couplings.<br>Kantorovich Problem for Arbitrary Measures
Given <a data-tooltip-position="top" aria-label="Measure" data-href="Measure" href="the-guide/mathematics/measure-theory/measure.html" class="internal-link" target="_self" rel="noopener nofollow">measures</a>  on , a cost function , the primal Kantorovich problem is  with the <a data-tooltip-position="top" aria-label="Set" data-href="Set" href="the-guide/mathematics/general-stuff/set.html" class="internal-link" target="_self" rel="noopener nofollow">set</a> of feasibel couplings and <a data-tooltip-position="top" aria-label="Push-Forward Measure" data-href="Push-Forward Measure" href="the-guide/mathematics/measure-theory/push-forward-measure.html" class="internal-link" target="_self" rel="noopener nofollow">pushforwards</a>  of the <a data-tooltip-position="top" aria-label="Measurable Function" data-href="Measurable Function" href="the-guide/mathematics/measure-theory/measurable-function.html" class="internal-link" target="_self" rel="noopener nofollow">projections</a> .
<br>
<br>Probabilistic Interpretation with <a data-tooltip-position="top" aria-label="Random Variable" data-href="Random Variable" href="the-guide/mathematics/probability-theory/random-variable.html" class="internal-link" target="_self" rel="noopener nofollow">random variables</a>  over  with the law of  being  and the law of  being , the above problem is equivalent to  with the law of the couple  (<a data-tooltip-position="top" aria-label="Probability Distribution" data-href="Probability Distribution" href="the-guide/mathematics/probability-theory/probability-distribution.html" class="internal-link" target="_self" rel="noopener nofollow">joint distribution</a>) being  over the <a data-tooltip-position="top" aria-label="Product Sets and Spaces" data-href="Product Sets and Spaces" href="Product Sets and Spaces" class="internal-link" target="_self" rel="noopener nofollow">product space</a> . 
<br>Kantorovich Problem for Discrete Measures
Let  be discrete <a data-tooltip-position="top" aria-label="Measure" data-href="Measure" href="the-guide/mathematics/measure-theory/measure.html" class="internal-link" target="_self" rel="noopener nofollow">probability measures</a> with histograms . With the set of bounded of (doubly stochastic) matrices  the Kantorovich optimal transport problem reads 

<br> transportation plan
<br> cost matrix 
<br> encode initial and final <a data-tooltip-position="top" aria-label="Probability Distribution" data-href="Probability Distribution" href="the-guide/mathematics/probability-theory/probability-distribution.html" class="internal-link" target="_self" rel="noopener nofollow">distribution</a>

<br>
<br>Full Transport Costs

<br>Discretization yields <a data-tooltip-position="top" aria-label="Matrices" data-href="Matrices" href="the-guide/mathematics/linear-algebra/matrices.html" class="internal-link" target="_self" rel="noopener nofollow">matrices</a>
<br>Under certain conditions this can be used to define a distance in form of the <a data-href="Wasserstein Distance" href="the-guide/mathematics/optimal-transport/wasserstein-distance.html" class="internal-link" target="_self" rel="noopener nofollow">Wasserstein Distance</a>.


<br>Kantorovich Linear Program for the Primal Formulation

<br>Using the real  <a data-tooltip-position="top" aria-label="Matrices" data-href="Matrices" href="the-guide/mathematics/linear-algebra/matrices.html" class="internal-link" target="_self" rel="noopener nofollow">matrix</a> the above problem can be reformulated as an <a data-tooltip-position="top" aria-label="Linear Program (LP)" data-href="Linear Program (LP)" href="the-guide/mathematics/optimization/convex-optimization-lecture/linear-program-(lp).html" class="internal-link" target="_self" rel="noopener nofollow">LP-problem</a> of the form 


<br><br><br>We can construct an alternative network flow problem on a graph by considering each support element of  as two <a data-tooltip-position="top" aria-label="Set" data-href="Set" href="the-guide/mathematics/general-stuff/set.html" class="internal-link" target="_self" rel="noopener nofollow">sets</a> of <a data-tooltip-position="top" aria-label="Node or Vertex Set" data-href="Node or Vertex Set" href="the-guide/mathematics/graph-theory/node-or-vertex-set.html" class="internal-link" target="_self" rel="noopener nofollow">nodes</a>  and . The transportation between source and target nodes can be encoded in an <a data-tooltip-position="top" aria-label="Graph" data-href="Graph" href="the-guide/mathematics/graph-theory/graph.html" class="internal-link" target="_self" rel="noopener nofollow">egde set</a>  with weights limited by the histograms, resulting in a <a data-tooltip-position="top" aria-label="Graph Coloring" data-href="Graph Coloring" href="the-guide/mathematics/graph-theory/graph-coloring.html" class="internal-link" target="_self" rel="noopener nofollow">bipartite graph</a>.<img alt="center" src="lib/media/pasted-image-20240125194151.png"><br>Lemma
Let  be an extremal point of the polytope  Let  be the <a data-tooltip-position="top" aria-label="Set" data-href="Set" href="the-guide/mathematics/general-stuff/set.html" class="internal-link" target="_self" rel="noopener nofollow">subset</a> of <a data-tooltip-position="top" aria-label="Graph" data-href="Graph" href="the-guide/mathematics/graph-theory/graph.html" class="internal-link" target="_self" rel="noopener nofollow">edges</a> where . Then The graph of the above network flow problem has no cycles. Therefore,  cannot have more than  nonzero entries.
<br>
<br>The proof follows the fact that cycles would contradict the assumptions of an optimal plan, since for cycles, you can construct two alternative plans for which  is equivalent to the average .<img alt="center" src="lib/media/pasted-image-20240125200037.png">Since you need two plans on "opposite sides" of the average and we assumed  to be optimal / on the boundary, this is a contradiction. From graph theory, we know that a bipartite graph without cycles cannot have more than  edges. 
]]></description><link>the-guide/mathematics/optimal-transport/optimal-transport.html</link><guid isPermaLink="false">The Guide/Mathematics/Optimal Transport/Optimal Transport.md</guid><pubDate>Sat, 29 Mar 2025 16:19:05 GMT</pubDate><enclosure url="lib/media/pasted-image-20240125194151.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src=&quot;lib/media/pasted-image-20240125194151.png&quot;&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[Sinkhorn Distance]]></title><description><![CDATA[ 
 <br>Definition
Distance based on the <a data-tooltip-position="top" aria-label="Optimal Transport" data-href="Optimal Transport" href="the-guide/mathematics/optimal-transport/optimal-transport.html" class="internal-link" target="_self" rel="noopener nofollow">optimal transport problem</a> with <a data-tooltip-position="top" aria-label="Entropic Regularization of Optimal Transport" data-href="Entropic Regularization of Optimal Transport" href="the-guide/mathematics/optimal-transport/entropic-regularization-of-optimal-transport.html" class="internal-link" target="_self" rel="noopener nofollow">entropic regularization</a>: with 
<br>
<br> is distance in <a data-tooltip-position="top" aria-label="Kullback-Leibler Divergence" data-href="Kullback-Leibler Divergence" href="the-guide/information-theory/information-theory-1/information/kullback-leibler-divergence.html" class="internal-link" target="_self" rel="noopener nofollow">Kullback-Leibler</a>-sense between transportation plan and <a data-tooltip-position="top" aria-label="Statistical Independence" data-href="Statistical Independence" href="the-guide/mathematics/statistics/statistical-independence.html" class="internal-link" target="_self" rel="noopener nofollow">independence</a> table
<br><a data-tooltip-position="top" aria-label="Metric Space and Completeness" data-href="Metric Space and Completeness" href="the-guide/mathematics/general-stuff/metric-space-and-completeness.html" class="internal-link" target="_self" rel="noopener nofollow">Metric</a> Properties

<br><a data-tooltip-position="top" aria-label="Cutury_Sinkhorn_Computation.pdf > page=3" data-href="Cutury_Sinkhorn_Computation.pdf#page=3" href="Cutury_Sinkhorn_Computation.pdf#page=3" class="internal-link" target="_self" rel="noopener nofollow">1.</a> For  large enough, Sinkhorn distance is the transport distance 
<br><a data-tooltip-position="top" aria-label="Cutury_Sinkhorn_Computation.pdf > page=3" data-href="Cutury_Sinkhorn_Computation.pdf#page=3" href="Cutury_Sinkhorn_Computation.pdf#page=3" class="internal-link" target="_self" rel="noopener nofollow">2.</a> Independence Kernel , independence kernel  is positive definite 
<br><a data-tooltip-position="top" aria-label="Cutury_Sinkhorn_Computation.pdf > page=3" data-href="Cutury_Sinkhorn_Computation.pdf#page=3" href="Cutury_Sinkhorn_Computation.pdf#page=3" class="internal-link" target="_self" rel="noopener nofollow">3.</a> For all   is symmetric and fulfills all triangle equations


<br><br>.... TODO<br><br>
<br>Duality of Sinkhorn distance to dual Sinkhorn Divergence  with 

<br>For every  there is a corresponding <img alt="center" src="lib/media/pasted-image-20230509114642.png" style="width: 550px; max-width: 100%;">


<br>Properties

<br>Leads to approximate solution to <a data-tooltip-position="top" aria-label="Optimal Transport" data-href="Optimal Transport" href="the-guide/mathematics/optimal-transport/optimal-transport.html" class="internal-link" target="_self" rel="noopener nofollow">OT problem</a>

<br>

<br>Optimal solution of original problem with max entropy


<br>

<br>With increasing , the solution becomes less and less sparse






<br>Computation

<br>OT Algorithms normally 
<br><a data-tooltip-position="top" aria-label="Cutury_Sinkhorn_Computation.pdf > page=5" data-href="Cutury_Sinkhorn_Computation.pdf#page=5" href="Cutury_Sinkhorn_Computation.pdf#page=5" class="internal-link" target="_self" rel="noopener nofollow">Lemma 2</a> - For  the solution of above problem is unique and has the form  with 

<br>Approximation via <a data-href="Sinkhorn-Knopp Algorithm" href="the-guide/mathematics/optimal-transport/sinkhorn-knopp-algorithm.html" class="internal-link" target="_self" rel="noopener nofollow">Sinkhorn-Knopp Algorithm</a> is faster


<br>Convergence

<br>Slower convergence with increasing  / decreasing 




]]></description><link>the-guide/mathematics/optimal-transport/sinkhorn-distance.html</link><guid isPermaLink="false">The Guide/Mathematics/Optimal Transport/Sinkhorn Distance.md</guid><pubDate>Mon, 24 Feb 2025 23:49:45 GMT</pubDate><enclosure url="lib/media/pasted-image-20230509114642.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src=&quot;lib/media/pasted-image-20230509114642.png&quot;&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[Sinkhorn-Knopp Algorithm]]></title><description><![CDATA[ 
 <br>Info

<br>Algorithm to compute approximate matrix factorization as in <a data-href="Sinkhorn's Theorem" href="the-guide/mathematics/optimal-transport/sinkhorn's-theorem.html" class="internal-link" target="_self" rel="noopener nofollow">Sinkhorn's Theorem</a> 

<br><a data-tooltip-position="top" aria-label="Matrices" data-href="Matrices" href="the-guide/mathematics/linear-algebra/matrices.html" class="internal-link" target="_self" rel="noopener nofollow">Matrix</a> has to be square and non-negative


<br>Can be used to efficiently compute transportation plan for <a data-tooltip-position="top" aria-label="Entropic Regularization of Optimal Transport" data-href="Entropic Regularization of Optimal Transport" href="the-guide/mathematics/optimal-transport/entropic-regularization-of-optimal-transport.html" class="internal-link" target="_self" rel="noopener nofollow">regularized</a> <a data-tooltip-position="top" aria-label="Optimal Transport" data-href="Optimal Transport" href="the-guide/mathematics/optimal-transport/optimal-transport.html" class="internal-link" target="_self" rel="noopener nofollow">optimal transport</a>

<br>Linear convergence 
<br>Complexity for OT problem is  plus factor depending on chosen regularizer.
<br>Unique up to off-setting scaling factors



<br>
<br>Algorithm

<br>Gibbs Kernel 
<br>With entry-wise division, alternatively perform


<br><br>
<br>Theoretical slowdown as  in <a data-href="Sinkhorn Distance" href="the-guide/mathematics/optimal-transport/sinkhorn-distance.html" class="internal-link" target="_self" rel="noopener nofollow">Sinkhorn Distance</a>, mostly not relevant in practice
<br>Algorithm often fails to terminate because elements in the kernel  become too small, leads to division by zero in the updates.

<br>Carry out computations in <a data-tooltip-position="top" aria-label="Log-Domain Sinkhorn" data-href="Log-Domain Sinkhorn" href="the-guide/mathematics/optimal-transport/log-domain-sinkhorn.html" class="internal-link" target="_self" rel="noopener nofollow">log domain</a> ?


]]></description><link>the-guide/mathematics/optimal-transport/sinkhorn-knopp-algorithm.html</link><guid isPermaLink="false">The Guide/Mathematics/Optimal Transport/Sinkhorn-Knopp Algorithm.md</guid><pubDate>Mon, 09 Sep 2024 15:38:50 GMT</pubDate></item><item><title><![CDATA[Sinkhorn's Theorem]]></title><description><![CDATA[ 
 <br>Theorem
If  with strictly positive entries, there exist diagonal <a data-tooltip-position="top" aria-label="Matrices" data-href="Matrices" href="the-guide/mathematics/linear-algebra/matrices.html" class="internal-link" target="_self" rel="noopener nofollow">matrices</a>  with strictly positive diagonal entries, such that  is <a data-tooltip-position="top" aria-label="Matrices" data-href="Matrices" href="the-guide/mathematics/linear-algebra/matrices.html" class="internal-link" target="_self" rel="noopener nofollow">doubly-stochastic</a>.
<br>
<br>Analogue for unitary Matrices

<br>For every <a data-tooltip-position="top" aria-label="Matrices" data-href="Matrices" href="the-guide/mathematics/linear-algebra/matrices.html" class="internal-link" target="_self" rel="noopener nofollow">unitary matrix</a> , there exist unitary matrices , such that each row and column of  sums up to .


]]></description><link>the-guide/mathematics/optimal-transport/sinkhorn&apos;s-theorem.html</link><guid isPermaLink="false">The Guide/Mathematics/Optimal Transport/Sinkhorn&apos;s Theorem.md</guid><pubDate>Mon, 09 Sep 2024 15:38:50 GMT</pubDate></item><item><title><![CDATA[Unbalanced Optimal Transport]]></title><description><![CDATA[ 
 <br>Bottleneck of the general <a data-tooltip-position="top" aria-label="Optimal Transport" data-href="Optimal Transport" href="the-guide/mathematics/optimal-transport/optimal-transport.html" class="internal-link" target="_self" rel="noopener nofollow">OT</a> framework is the constraint that the two input <a data-tooltip-position="top" aria-label="Measure" data-href="Measure" href="the-guide/mathematics/measure-theory/measure.html" class="internal-link" target="_self" rel="noopener nofollow">measures</a> need to have equal mass. This implication of the <a data-tooltip-position="top" aria-label="Optimal Transport" data-href="Optimal Transport" href="the-guide/mathematics/optimal-transport/optimal-transport.html" class="internal-link" target="_self" rel="noopener nofollow">Kantorovich formulation</a> can be relaxed by penalizing marginal deviation using some <a data-tooltip-position="top" aria-label="Divergence" data-href="Divergence" href="the-guide/information-theory/information-geometry/divergence.html" class="internal-link" target="_self" rel="noopener nofollow">divergence</a> instead of forcing it via the feasible <a data-tooltip-position="top" aria-label="Set" data-href="Set" href="the-guide/mathematics/general-stuff/set.html" class="internal-link" target="_self" rel="noopener nofollow">set</a> of transportation plans.<br>Umbalanced Optimal Transport Problem
Using a <a data-tooltip-position="top" aria-label="Divergence" data-href="Divergence" href="the-guide/information-theory/information-geometry/divergence.html" class="internal-link" target="_self" rel="noopener nofollow">divergence</a> , the unbalanced Optimal Transport problem reads Using the <a data-tooltip-position="top" aria-label="Matrices" data-href="Matrices" href="the-guide/mathematics/linear-algebra/matrices.html" class="internal-link" target="_self" rel="noopener nofollow">matrix</a> notation with transportation plan , this can be rewritten as 
<br><br><br>A generalized version of <a data-tooltip-position="top" aria-label="Sinkhorn-Knopp Algorithm" data-href="Sinkhorn-Knopp Algorithm" href="the-guide/mathematics/optimal-transport/sinkhorn-knopp-algorithm.html" class="internal-link" target="_self" rel="noopener nofollow">Sinkhorn Iterations</a> can be used to solve the above objective.]]></description><link>the-guide/mathematics/optimal-transport/unbalanced-optimal-transport.html</link><guid isPermaLink="false">The Guide/Mathematics/Optimal Transport/Unbalanced Optimal Transport.md</guid><pubDate>Mon, 09 Sep 2024 15:38:50 GMT</pubDate></item><item><title><![CDATA[Wasserstein Distance]]></title><description><![CDATA[ 
 <br>Lemma
If (discrete) source and target <a data-tooltip-position="top" aria-label="Probability Distribution" data-href="Probability Distribution" href="the-guide/mathematics/probability-theory/probability-distribution.html" class="internal-link" target="_self" rel="noopener nofollow">distribution</a> of the <a data-tooltip-position="top" aria-label="Optimal Transport" data-href="Optimal Transport" href="the-guide/mathematics/optimal-transport/optimal-transport.html" class="internal-link" target="_self" rel="noopener nofollow">optimal transport</a> problem lie in the same <a data-tooltip-position="top" aria-label="Metric Space and Completeness" data-href="Metric Space and Completeness" href="the-guide/mathematics/general-stuff/metric-space-and-completeness.html" class="internal-link" target="_self" rel="noopener nofollow">metric space</a> , which is equipped with the distance , then the optimal transport solution itself defines a distance.
<br>Wasserstein Distance
Using the above and the <a data-tooltip-position="top" aria-label="Matrix Norms" data-href="Matrix Norms" href="Matrix Norms" class="internal-link" target="_self" rel="noopener nofollow">p-norm</a>, the p-Wasserstein distance is defined as with  computed via the p-Norm between the support elements. For continuous distributions, we find 
<br>
<br>Properties

<br>
<br>
<br>From <a data-tooltip-position="top" aria-label="Metric Space and Completeness" data-href="Metric Space and Completeness" href="the-guide/mathematics/general-stuff/metric-space-and-completeness.html" class="internal-link" target="_self" rel="noopener nofollow">metric</a>

<br>Non-negative
<br>Equality if zero distance
<br>Triangle equation




]]></description><link>the-guide/mathematics/optimal-transport/wasserstein-distance.html</link><guid isPermaLink="false">The Guide/Mathematics/Optimal Transport/Wasserstein Distance.md</guid><pubDate>Mon, 24 Feb 2025 23:49:45 GMT</pubDate></item><item><title><![CDATA[Weak Optimal Transport]]></title><description><![CDATA[ 
 <br>Generalization of <a data-tooltip-position="top" aria-label="Optimal Transport" data-href="Optimal Transport" href="the-guide/mathematics/optimal-transport/optimal-transport.html" class="internal-link" target="_self" rel="noopener nofollow">optimal transport</a>, where the cost <a data-tooltip-position="top" aria-label="Mathematical Relations" data-href="Mathematical Relations" href="the-guide/mathematics/general-stuff/mathematical-relations.html" class="internal-link" target="_self" rel="noopener nofollow">function</a>  takes in a point  and a <a data-tooltip-position="top" aria-label="Probability Distribution" data-href="Probability Distribution" href="the-guide/mathematics/probability-theory/probability-distribution.html" class="internal-link" target="_self" rel="noopener nofollow">distribution</a>  of , which means that the mass transport is penalized through e.g. mean cost.<br>
The weak OT problem then reads <img alt="center" src="lib/media/pasted-image-20231207125601.png" style="width: 350px; max-width: 100%;"><br>
<br>Generalization, choosing the cost  yields the <a data-tooltip-position="top" aria-label="Optimal Transport" data-href="Optimal Transport" href="the-guide/mathematics/optimal-transport/optimal-transport.html" class="internal-link" target="_self" rel="noopener nofollow">strong formulation</a>.
<br>Exemplary cost is the -weak () Wasserstein-2 cost () for , namely 
<br>Optimal plan exists e.g. if weak cost is lower bounded, <a data-tooltip-position="top" aria-label="Convexity" data-href="Convexity" href="the-guide/mathematics/optimization/convex-optimization-lecture/convexity.html" class="internal-link" target="_self" rel="noopener nofollow">convex</a> in  and (jointly lower semicontinuous)
]]></description><link>the-guide/mathematics/optimal-transport/weak-optimal-transport.html</link><guid isPermaLink="false">The Guide/Mathematics/Optimal Transport/Weak Optimal Transport.md</guid><pubDate>Mon, 09 Sep 2024 15:38:50 GMT</pubDate><enclosure url="lib/media/pasted-image-20231207125601.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src=&quot;lib/media/pasted-image-20231207125601.png&quot;&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[A general Framework for Iterative Optimization Algorithms]]></title><description><![CDATA[ 
 <br>In a Nutshell
General framework to formalize iterative algorithms for <a data-tooltip-position="top" aria-label="Optimization Problem" data-href="Optimization Problem" href="the-guide/mathematics/optimization/convex-optimization-lecture/optimization-problem.html" class="internal-link" target="_self" rel="noopener nofollow">optimization problems</a> and to derive properties based on e.g. <a data-tooltip-position="top" aria-label="Convexity" data-href="Convexity" href="the-guide/mathematics/optimization/convex-optimization-lecture/convexity.html" class="internal-link" target="_self" rel="noopener nofollow">convexity</a>, <a data-tooltip-position="top" aria-label="Pseudo- and Quasiconvexity" data-href="Pseudo- and Quasiconvexity" href="the-guide/mathematics/optimization/convex-optimization-lecture/pseudo-and-quasiconvexity.html" class="internal-link" target="_self" rel="noopener nofollow">pseudo- and quasiconvextiy</a> used in the convex optimization lecture.
<br><br><br>The underlying principle of all iterative <a data-tooltip-position="top" aria-label="Optimization Problem" data-href="Optimization Problem" href="the-guide/mathematics/optimization/convex-optimization-lecture/optimization-problem.html" class="internal-link" target="_self" rel="noopener nofollow">optimization</a> algorithms is to solve a sequence of successively refined approximate algorithms, where ideally each approximate problem is much easier to solve than the original problem, e.g. in closed-form, in parallel or distributed.<img alt="center" src="lib/media/pasted-image-20240716140503.png" style="width: 300px; max-width: 100%;"><br>
Formally, we assume a current iterate  at iteration . We find a new point  using the solution <a data-tooltip-position="top" aria-label="Set" data-href="Set" href="the-guide/mathematics/general-stuff/set.html" class="internal-link" target="_self" rel="noopener nofollow">set</a> of an <a data-tooltip-position="top" aria-label="Mathematical Relations" data-href="Mathematical Relations" href="the-guide/mathematics/general-stuff/mathematical-relations.html" class="internal-link" target="_self" rel="noopener nofollow">approximate function</a> such that the direction  is a "good" update direction and take a scaled step <br>If  is a <a data-tooltip-position="top" aria-label="Convexity" data-href="Convexity" href="the-guide/mathematics/optimization/convex-optimization-lecture/convexity.html" class="internal-link" target="_self" rel="noopener nofollow">convex set</a>, the properties guarantee that any point we may choose during that procedure is feasible again.<img alt="center" src="lib/media/pasted-image-20240716141011.png" style="width: 300px; max-width: 100%;"><br>
In any case, the algorithm which we want to study in this general setting can be expressed as ...<br>Algorithm
Start for any 

<br>Compute  by solving  (at point )
<br>Compute stepsize , e.g. by any line search
<br>Update the point of approximation 

<br><br><br>Definition
Based on the function , we define a general direction  to be a descent direction, if 
<br>Proof
From the first-order Taylor approximationwe see that because  is non-positive, we can always find a , such that the second and third term on the right are overall negativeIn other words, this yields a decrease of the function value if we move in that direction.
<br>Theorem 
If the approximate function  satisfies the conditions

<br><a data-tooltip-position="top" aria-label="Pseudo- and Quasiconvexity" data-href="Pseudo- and Quasiconvexity" href="the-guide/mathematics/optimization/convex-optimization-lecture/pseudo-and-quasiconvexity.html" class="internal-link" target="_self" rel="noopener nofollow">Pseudoconvexity</a> in  for any given 
<br><a data-tooltip-position="top" aria-label="Derivative, Gradient, Jacobian and Hessian" data-href="Derivative, Gradient, Jacobian and Hessian" href="the-guide/mathematics/analysis-and-calculus/derivative,-gradient,-jacobian-and-hessian.html" class="internal-link" target="_self" rel="noopener nofollow">Gradient Consistency</a> exists in  for any fixed . Additionally, the gradient of the approximate function has to be the same in : 
<br>Continuity in  for a fixed ,

then the following holds true:

<br>A point  around which we approximate is a <a data-tooltip-position="top" aria-label="Stationary Points" data-href="Stationary Points" href="the-guide/mathematics/optimization/convex-optimization-lecture/stationary-points.html" class="internal-link" target="_self" rel="noopener nofollow">stationary point</a> of the original problem (P) iff it is already an element of the solution set of the approximate problem .
<br>If  is not in , then  is a descent direction of  at : 

<br>Intuition
If we use a pseudoconvex function as a local approximation and the point at which we perform this is already optimal for the approximation, the only way in which this can happen is if the tangent for both is horizontal.<br>
If we find an optimal solution to the approximate problem that is not the initial  the first order approximation guarantees us that we can find a stepsize, such that we perform a descent.
<br>Proof

<br>"" (in solution set if stationary point) follows from <a data-tooltip-position="top" aria-label="Optimization Problem" data-href="Optimization Problem" href="the-guide/mathematics/optimization/convex-optimization-lecture/optimization-problem.html" class="internal-link" target="_self" rel="noopener nofollow">first-order optimality</a> condition for the approximate function, since it is pseudoconvex
<br>"" (stationary point if in solution set)
<br>Descent direction follows from first order information

<br><br><br>We now know that every  yields a feasible point and that it is possible to find a descent direction. However, we still have to define which concrete step size  we actually take to compute our update If the approximate function is also a global upper bound / a majorizing function of the original function, it can be shown that in this case we can always choose a unit stepsize  in every update to guarantee a decrease. This can take away some computation time and is easier to implement, but the resulting stepsize may be suboptimal.<br>
If the approximation is not an upper bound, a suboptimal choice can even lead to an increase. The easiest, but also "riskiest" methods use <br>
<br>Constant stepsize, where we simply set  and hope that  is sufficiently small.
<br>Decreasing stepsize, where we set These steps might not be optimal at certain , but will be for later iterates in the limit of infinite iterations.
<br>More involved tactics can be derived by considering choosing  as its own <a data-tooltip-position="top" aria-label="Optimization Problem" data-href="Optimization Problem" href="the-guide/mathematics/optimization/convex-optimization-lecture/optimization-problem.html" class="internal-link" target="_self" rel="noopener nofollow">optimization problem</a>.<br>Exact Line Search
Based on solution of Can be solved relatively efficiently, especially for convex functions (bisection).
<br>Armijo Rule
Given DoFs , we try  until we satisfyThe parameter  controls the fraction of the <a data-tooltip-position="top" aria-label="Derivative, Gradient, Jacobian and Hessian" data-href="Derivative, Gradient, Jacobian and Hessian" href="the-guide/mathematics/analysis-and-calculus/derivative,-gradient,-jacobian-and-hessian.html" class="internal-link" target="_self" rel="noopener nofollow">gradient</a> in our last iterate we want to achieve with the update.
<br><img alt="center" src="lib/media/pasted-image-20240716145037.png" style="width: 450px; max-width: 100%;"><br><br><br>Regarding the convergence of the algorithm towards a stationary point of the <a data-tooltip-position="top" aria-label="Optimization Problem" data-href="Optimization Problem" href="the-guide/mathematics/optimization/convex-optimization-lecture/optimization-problem.html" class="internal-link" target="_self" rel="noopener nofollow">original problem</a> (P), we need additional assumptions to formalize <br>Theorem 
If the approximate function  satisfies the conditions

<br><a data-tooltip-position="top" aria-label="Pseudo- and Quasiconvexity" data-href="Pseudo- and Quasiconvexity" href="the-guide/mathematics/optimization/convex-optimization-lecture/pseudo-and-quasiconvexity.html" class="internal-link" target="_self" rel="noopener nofollow">Pseudoconvexity</a> in  for any given 
<br><a data-tooltip-position="top" aria-label="Derivative, Gradient, Jacobian and Hessian" data-href="Derivative, Gradient, Jacobian and Hessian" href="the-guide/mathematics/analysis-and-calculus/derivative,-gradient,-jacobian-and-hessian.html" class="internal-link" target="_self" rel="noopener nofollow">Gradient Consistency</a> exists in  for any fixed . Additionally, the gradient of the approximate function has to be the same in : 
<br>Continuity in  for a fixed 
<br>Existence of a non-empty solution set  for all time steps 
<br>Boundedness of the sequence  given any convergent subsequence 

then the following holds true:

<br> is a decreasing sequence  and any limit point of  is a <a data-tooltip-position="top" aria-label="Stationary Points" data-href="Stationary Points" href="the-guide/mathematics/optimization/convex-optimization-lecture/stationary-points.html" class="internal-link" target="_self" rel="noopener nofollow">stationary point</a> of (P). The stationary point in this setting is always a saddle point or minimum.

<br>Additional Information
Can be used in practice to test whether problem is suitable: if any iterate is larger than the last, some condition is not met or there is an error in the implementation.
<br><br><br>With the theory provided above, we can analyse various optimization algorithms. Some examples are ...<br>
<br><a data-href="Proximal Point Algorithm" href="the-guide/mathematics/optimization/convex-optimization-lecture/algorithms/proximal-point-algorithm.html" class="internal-link" target="_self" rel="noopener nofollow">Proximal Point Algorithm</a>

<br>Zero order Taylor with distance penalty


<br><a data-href="Gradient Descent" href="the-guide/mathematics/optimization/convex-optimization-lecture/algorithms/gradient-descent.html" class="internal-link" target="_self" rel="noopener nofollow">Gradient Descent</a>

<br>First order Taylor 


<br><a data-href="Newton-Raphson Method" href="the-guide/mathematics/optimization/convex-optimization-lecture/algorithms/newton-raphson-method.html" class="internal-link" target="_self" rel="noopener nofollow">Newton-Raphson Method</a>

<br>Use second order Taylor approximation 


<br><a data-href="Block Coordinate Descent Method (BCD)" href="the-guide/mathematics/optimization/convex-optimization-lecture/algorithms/block-coordinate-descent-method-(bcd).html" class="internal-link" target="_self" rel="noopener nofollow">Block Coordinate Descent Method (BCD)</a>
<br><a data-href="The Jacobi Algorithm" href="the-guide/mathematics/optimization/convex-optimization-lecture/algorithms/the-jacobi-algorithm.html" class="internal-link" target="_self" rel="noopener nofollow">The Jacobi Algorithm</a>

<br>For mixed function that is only convex subspace-wise.


<br>...
<br>However, there are algorithms which do not fall under the assumptions made above that still yield viable solutions, e.g.<br>
<br><a data-href="Block Successive Upper-Bound Minimization (BSUM)" href="Block Successive Upper-Bound Minimization (BSUM)" class="internal-link" target="_self" rel="noopener nofollow">Block Successive Upper-Bound Minimization (BSUM)</a>
<br>...
]]></description><link>the-guide/mathematics/optimization/convex-optimization-lecture/algorithms/a-general-framework-for-iterative-optimization-algorithms.html</link><guid isPermaLink="false">The Guide/Mathematics/Optimization/Convex Optimization Lecture/Algorithms/A general Framework for Iterative Optimization Algorithms.md</guid><pubDate>Sat, 25 Jan 2025 17:52:28 GMT</pubDate><enclosure url="lib/media/pasted-image-20240716140503.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src=&quot;lib/media/pasted-image-20240716140503.png&quot;&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[Block Coordinate Descent Method (BCD)]]></title><description><![CDATA[ 
 <br><br>Consider an <a data-tooltip-position="top" aria-label="Optimization Problem" data-href="Optimization Problem" href="the-guide/mathematics/optimization/convex-optimization-lecture/optimization-problem.html" class="internal-link" target="_self" rel="noopener nofollow">optimization problem</a> of the form where  and  is a <a data-tooltip-position="top" aria-label="Product Sets and Spaces" data-href="Product Sets and Spaces" href="Product Sets and Spaces" class="internal-link" target="_self" rel="noopener nofollow">product space</a> where every  is a <a data-tooltip-position="top" aria-label="Convexity" data-href="Convexity" href="the-guide/mathematics/optimization/convex-optimization-lecture/convexity.html" class="internal-link" target="_self" rel="noopener nofollow">convex</a> cartesian <a data-tooltip-position="top" aria-label="Set" data-href="Set" href="the-guide/mathematics/general-stuff/set.html" class="internal-link" target="_self" rel="noopener nofollow">set</a>. <br>Assumption
The algorithm relies on the assumption that every subproblem has at least one solution.
<br>If the so-created sub-problems have a simple or even close-form solution, is it attractive to update each direction or a subset of directions separately by cycling through, yielding the BCD algorithm.<br>Definition
Generate new iterate  by cycling through the updates 
<br>Theorem
If the minimization problem of the assumptions attains a unique minimum  over , while being monotonously non-increasing on the interval , the limit of iterates of the BCD method is a <a data-tooltip-position="top" aria-label="Stationary Points" data-href="Stationary Points" href="the-guide/mathematics/optimization/convex-optimization-lecture/stationary-points.html" class="internal-link" target="_self" rel="noopener nofollow">stationary point</a> of the problem on the product space.
]]></description><link>the-guide/mathematics/optimization/convex-optimization-lecture/algorithms/block-coordinate-descent-method-(bcd).html</link><guid isPermaLink="false">The Guide/Mathematics/Optimization/Convex Optimization Lecture/Algorithms/Block Coordinate Descent Method (BCD).md</guid><pubDate>Mon, 09 Sep 2024 15:35:34 GMT</pubDate></item><item><title><![CDATA[Dual Decomposition Method]]></title><description><![CDATA[ 
 <br>In a Nutshell
Iterative decomposition algorithm that solves optimization problems of a specified form by iteratively updating the dual variables and the primal solution. 
<br><br>Problem Form
Algorithm to solve <a data-tooltip-position="top" aria-label="Optimization Problem" data-href="Optimization Problem" href="the-guide/mathematics/optimization/convex-optimization-lecture/optimization-problem.html" class="internal-link" target="_self" rel="noopener nofollow">optimization problem</a> of the formwhere  is a closed <a data-tooltip-position="top" aria-label="Convexity" data-href="Convexity" href="the-guide/mathematics/optimization/convex-optimization-lecture/convexity.html" class="internal-link" target="_self" rel="noopener nofollow">convex</a> <a data-tooltip-position="top" aria-label="Set" data-href="Set" href="the-guide/mathematics/general-stuff/set.html" class="internal-link" target="_self" rel="noopener nofollow">set</a>, while  and  are arbitrary <a data-tooltip-position="top" aria-label="Mathematical Relations" data-href="Mathematical Relations" href="the-guide/mathematics/general-stuff/mathematical-relations.html" class="internal-link" target="_self" rel="noopener nofollow">functions</a>. 
<br>This yields a <a data-tooltip-position="top" aria-label="Convexity" data-href="Convexity" href="the-guide/mathematics/optimization/convex-optimization-lecture/convexity.html" class="internal-link" target="_self" rel="noopener nofollow">concave</a> <a data-tooltip-position="top" aria-label="Lagrange Dual Problem" data-href="Lagrange Dual Problem" href="the-guide/mathematics/optimization/convex-optimization-lecture/lagrange-dual-problem.html" class="internal-link" target="_self" rel="noopener nofollow">dual function</a>based on the <a data-href="Lagrange Dual Problem" href="the-guide/mathematics/optimization/convex-optimization-lecture/lagrange-dual-problem.html" class="internal-link" target="_self" rel="noopener nofollow">Lagrange Dual Problem</a><br>Assumption
The algorithm assumes that the infimum exists and is a minimum
<br>Then, for an arbitrary , it holds that By extension, we can derive an expression involving a tangent of the dual function viawhere  is a sub-gradient of the dual function at point .<br><br>With the above, we can alternate between primal and dual updates in the following algorithm:<br>Algorithm

<br>Pick initial dual variable vector 
<br>For each iteration do

<br>Primal Update - find local minimum based on fixed  
<br>Dual Update - iterate until convergence.



<br>
<br>Can be extended to decoupled subproblems in the <a data-tooltip-position="top" aria-label="The Jacobi Algorithm" data-href="The Jacobi Algorithm" href="the-guide/mathematics/optimization/convex-optimization-lecture/algorithms/the-jacobi-algorithm.html" class="internal-link" target="_self" rel="noopener nofollow">Jacobi algorithm</a>
]]></description><link>the-guide/mathematics/optimization/convex-optimization-lecture/algorithms/dual-decomposition-method.html</link><guid isPermaLink="false">The Guide/Mathematics/Optimization/Convex Optimization Lecture/Algorithms/Dual Decomposition Method.md</guid><pubDate>Mon, 09 Sep 2024 15:35:34 GMT</pubDate></item><item><title><![CDATA[Gradient Descent]]></title><description><![CDATA[<a class="tag" href="?query=tag:Machine-Learning" style="background-color: rgb(4, 108, 116); color: white; font-weight: 700; border: none; border-radius: 1em; padding: 0.2em 0.5em;">#Machine-Learning</a> <a class="tag" href="?query=tag:Convex-Optimization-Course" style="background-color: rgb(4, 108, 116); color: white; font-weight: 700; border: none; border-radius: 1em; padding: 0.2em 0.5em;">#Convex-Optimization-Course</a> 
 <br>Standard Gradient Descent
We assume the initial problem where we update the parameters according to with learning rate  and <a data-tooltip-position="top" aria-label="Derivative, Gradient, Jacobian and Hessian" data-href="Derivative, Gradient, Jacobian and Hessian" href="the-guide/mathematics/analysis-and-calculus/derivative,-gradient,-jacobian-and-hessian.html" class="internal-link" target="_self" rel="noopener nofollow">gradient</a> from the <a data-tooltip-position="top" aria-label="Forward and Backward Propagation" data-href="Forward and Backward Propagation" href="the-guide/machine-learning/deep-learning/forward-and-backward-propagation.html" class="internal-link" target="_self" rel="noopener nofollow">backpropagation</a>.
<br><br><br><br>
<br>Full Gradient Descent

<br>Use whole training set 
<br>Computationally expensive 


<br>Stochastic Gradient Descent (SGD)

<br>Use one data point of the training set
<br>Needs adaptive learning rate  with  and , see 
<br>High variance estimation


<br>Mini-Batch Gradient Descent

<br>Use a batch of  points of the training set


<br><br>In general, a learning rate that is too small can lead to stagnation. However, if it is chosen too large, the algrotihm may become unstable and diverge<br>
<img alt="center" src="lib/media/pasted-image-20221219182540.png" style="width: 350px; max-width: 100%;"><br>Momentum 
This method updates from  to  in two steps and accumulates a momentum via with momentum coefficient  (sometimes ). The parameters are then updated via
<br>Intuition
This essentially applies exponential moving average smoothing to the gradient. The signal-processing pendant is a discrete-time low pass filter.
<br>Adadelta
Keeps a moving average of the <a data-tooltip-position="top" aria-label="Norms" data-href="Norms" href="the-guide/mathematics/linear-algebra/norms.html" class="internal-link" target="_self" rel="noopener nofollow">squared</a> gradients to estimate how large it ''usually'' is. Based on the update of the last step, we compute a normalized updateTo avoid storing all updates, we accumulate squared updates Finally, we update 
<br>Adam
Combines both of the above, the de-facto standard in <a href=".?query=tag:Machine-Learning" class="tag" target="_blank" rel="noopener nofollow">#Machine-Learning</a> .
<br><br><br>Consider we now want to solve the <a data-tooltip-position="top" aria-label="Optimization Problem" data-href="Optimization Problem" href="the-guide/mathematics/optimization/convex-optimization-lecture/optimization-problem.html" class="internal-link" target="_self" rel="noopener nofollow">optimization</a> problem on the constraint <a data-tooltip-position="top" aria-label="Set" data-href="Set" href="the-guide/mathematics/general-stuff/set.html" class="internal-link" target="_self" rel="noopener nofollow">set</a> , i.e.The projected gradient descent algorithm uses the alternative update where  is the euclidean projection operator, which is in itself an optimization problem of the form In many cases, we consider the squared cost to make the function differentiable.<br><br>In our <a data-tooltip-position="top" aria-label="A general Framework for Iterative Optimization Algorithms" data-href="A general Framework for Iterative Optimization Algorithms" href="the-guide/mathematics/optimization/convex-optimization-lecture/algorithms/a-general-framework-for-iterative-optimization-algorithms.html" class="internal-link" target="_self" rel="noopener nofollow">general framework</a> of the <a href=".?query=tag:Convex-Optimization-Course" class="tag" target="_blank" rel="noopener nofollow">#Convex-Optimization-Course</a> , this corresponds to We can easily verify that is fulfills the assumptions made for the approximate function via ]]></description><link>the-guide/mathematics/optimization/convex-optimization-lecture/algorithms/gradient-descent.html</link><guid isPermaLink="false">The Guide/Mathematics/Optimization/Convex Optimization Lecture/Algorithms/Gradient Descent.md</guid><pubDate>Sat, 05 Apr 2025 14:42:20 GMT</pubDate><enclosure url="lib/media/pasted-image-20221219182540.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src=&quot;lib/media/pasted-image-20221219182540.png&quot;&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[Newton-Raphson Method]]></title><description><![CDATA[ 
 <br><br><br>Root-finding algorithm to produce successively better approximations to the roots of a real-valued <a data-tooltip-position="top" aria-label="Mathematical Relations" data-href="Mathematical Relations" href="the-guide/mathematics/general-stuff/mathematical-relations.html" class="internal-link" target="_self" rel="noopener nofollow">functions</a> via For multivariate functions the derivative yields a <a data-tooltip-position="top" aria-label="Matrices" data-href="Matrices" href="the-guide/mathematics/linear-algebra/matrices.html" class="internal-link" target="_self" rel="noopener nofollow">matrix</a>, which needs to be inverted (in practice we solve linear system) <br>In practice, the above iterations are repeated until a stopping criterion is met, e.g. the change of the iterate.<img alt="center" src="lib/media/newtoniteration_ani.gif" style="width: 300px; max-width: 100%;"><br><br><br>Info
The above concept can be extended to general <a data-tooltip-position="top" aria-label="Optimization Problem" data-href="Optimization Problem" href="the-guide/mathematics/optimization/convex-optimization-lecture/optimization-problem.html" class="internal-link" target="_self" rel="noopener nofollow">optimization problems</a> of the form with twice continuously differentiable . We denote the <a data-tooltip-position="top" aria-label="Derivative, Gradient, Jacobian and Hessian > The Gradient" data-href="Derivative, Gradient, Jacobian and Hessian#The Gradient" href="the-guide/mathematics/analysis-and-calculus/derivative,-gradient,-jacobian-and-hessian.html#The_Gradient" class="internal-link" target="_self" rel="noopener nofollow">gradient</a>  and the <a data-tooltip-position="top" aria-label="Derivative, Gradient, Jacobian and Hessian > The Hessian" data-href="Derivative, Gradient, Jacobian and Hessian#The Hessian" href="the-guide/mathematics/analysis-and-calculus/derivative,-gradient,-jacobian-and-hessian.html#The_Hessian" class="internal-link" target="_self" rel="noopener nofollow">hessian</a> . If the function is <a data-tooltip-position="top" aria-label="Convexity" data-href="Convexity" href="the-guide/mathematics/optimization/convex-optimization-lecture/convexity.html" class="internal-link" target="_self" rel="noopener nofollow">convex</a>, it holds that . This is especially useful in the context of <a data-tooltip-position="top" aria-label="Non-Linear Least Squares Problem" data-href="Non-Linear Least Squares Problem" href="the-guide/mathematics/optimization/non-linear-least-squares-problem.html" class="internal-link" target="_self" rel="noopener nofollow">Non-linear least squares</a>.
<br>Computing the quadratic approximation / second-order Taylor approximation of such a function yieldsComputing the derivative with regard to  and setting it zero equates to<br>Definition
From this, we can rearrange terms and introduce our usual step size as in the <a data-tooltip-position="top" aria-label="A general Framework for Iterative Optimization Algorithms" data-href="A general Framework for Iterative Optimization Algorithms" href="the-guide/mathematics/optimization/convex-optimization-lecture/algorithms/a-general-framework-for-iterative-optimization-algorithms.html" class="internal-link" target="_self" rel="noopener nofollow">general framework</a> to derive an update 
<br>
<br>Reaches quadratic convergence under mild assumptions
<br>Single update for positive definite and quadratic functions
<br>Intuition
We move into the direction of the negative gradient, the steepest descent. However, we incorporate second-order information by scaling with the inverse Hessian:

<br>In direction with very high curvature, the step size is scaled down to not overshooting (high second derivative = small inverse)
<br>In flat directions, the step size is scaled up to lower the required iterations

<br>While having better convergence properties, the <a data-tooltip-position="top" aria-label="Derivative, Gradient, Jacobian and Hessian > The Hessian" data-href="Derivative, Gradient, Jacobian and Hessian#The Hessian" href="the-guide/mathematics/analysis-and-calculus/derivative,-gradient,-jacobian-and-hessian.html#The_Hessian" class="internal-link" target="_self" rel="noopener nofollow">Hessian</a> requires additional computation, which can be infeasible. The <a data-href="Gauss-Newton Method" href="the-guide/mathematics/optimization/gauss-newton-method.html" class="internal-link" target="_self" rel="noopener nofollow">Gauss-Newton Method</a> instead tries to approximate the second order information based on the first order.]]></description><link>the-guide/mathematics/optimization/convex-optimization-lecture/algorithms/newton-raphson-method.html</link><guid isPermaLink="false">The Guide/Mathematics/Optimization/Convex Optimization Lecture/Algorithms/Newton-Raphson Method.md</guid><pubDate>Sat, 25 Jan 2025 17:52:28 GMT</pubDate><enclosure url="lib/media/newtoniteration_ani.gif" length="0" type="image/gif"/><content:encoded>&lt;figure&gt;&lt;img src=&quot;lib/media/newtoniteration_ani.gif&quot;&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[Optimization Algorithms Overview]]></title><description><![CDATA[ 
 <br><br>
<br>
<a data-href="Simplex Algorithm" href="Simplex Algorithm" class="internal-link" target="_self" rel="noopener nofollow">Simplex Algorithm</a> 

<br>
<a data-href="Interior Point Algorithms" href="Interior Point Algorithms" class="internal-link" target="_self" rel="noopener nofollow">Interior Point Algorithms</a>

<br>
<a data-tooltip-position="top" aria-label="A general Framework for Iterative Optimization Algorithms" data-href="A general Framework for Iterative Optimization Algorithms" href="the-guide/mathematics/optimization/convex-optimization-lecture/algorithms/a-general-framework-for-iterative-optimization-algorithms.html" class="internal-link" target="_self" rel="noopener nofollow">Iterative Algorithms</a> - solve alternative simplified problem to obtain good update direction

<br><a data-href="Proximal Point Algorithm" href="the-guide/mathematics/optimization/convex-optimization-lecture/algorithms/proximal-point-algorithm.html" class="internal-link" target="_self" rel="noopener nofollow">Proximal Point Algorithm</a>
<br><a data-href="Gradient Descent" href="the-guide/mathematics/optimization/convex-optimization-lecture/algorithms/gradient-descent.html" class="internal-link" target="_self" rel="noopener nofollow">Gradient Descent</a>
<br><a data-href="Newton-Raphson Method" href="the-guide/mathematics/optimization/convex-optimization-lecture/algorithms/newton-raphson-method.html" class="internal-link" target="_self" rel="noopener nofollow">Newton-Raphson Method</a>
<br><a data-href="Block Coordinate Descent Method (BCD)" href="the-guide/mathematics/optimization/convex-optimization-lecture/algorithms/block-coordinate-descent-method-(bcd).html" class="internal-link" target="_self" rel="noopener nofollow">Block Coordinate Descent Method (BCD)</a>
<br><a data-href="The Jacobi Algorithm" href="the-guide/mathematics/optimization/convex-optimization-lecture/algorithms/the-jacobi-algorithm.html" class="internal-link" target="_self" rel="noopener nofollow">The Jacobi Algorithm</a>


<br>
Decomposition Methods - solve smaller sub-problems that are coordinated by a master problem

<br>Primal Decomposition Method

<br>Complicated variables that couple sub-problems
<br>Primal master problem controls resources


<br><a data-href="Dual Decomposition Method" href="the-guide/mathematics/optimization/convex-optimization-lecture/algorithms/dual-decomposition-method.html" class="internal-link" target="_self" rel="noopener nofollow">Dual Decomposition Method</a>

<br>Complicated constraints couple sub-problems
<br>Dual master problem controls resources
<br>Example is the extension of the <a data-tooltip-position="top" aria-label="The Jacobi Algorithm" data-href="The Jacobi Algorithm" href="the-guide/mathematics/optimization/convex-optimization-lecture/algorithms/the-jacobi-algorithm.html" class="internal-link" target="_self" rel="noopener nofollow">Jacobi algorithm</a>




]]></description><link>the-guide/mathematics/optimization/convex-optimization-lecture/algorithms/optimization-algorithms-overview.html</link><guid isPermaLink="false">The Guide/Mathematics/Optimization/Convex Optimization Lecture/Algorithms/Optimization Algorithms Overview.md</guid><pubDate>Mon, 09 Sep 2024 15:35:34 GMT</pubDate></item><item><title><![CDATA[Partial Linearization Method]]></title><description><![CDATA[ 
 <br>Problem
<a data-tooltip-position="top" aria-label="Optimization Problem" data-href="Optimization Problem" href="the-guide/mathematics/optimization/convex-optimization-lecture/optimization-problem.html" class="internal-link" target="_self" rel="noopener nofollow">Optimization</a> <a data-tooltip-position="top" aria-label="Optimization Algorithms Overview" data-href="Optimization Algorithms Overview" href="the-guide/mathematics/optimization/convex-optimization-lecture/algorithms/optimization-algorithms-overview.html" class="internal-link" target="_self" rel="noopener nofollow">algorithm</a> for problems of the form where  each  is only <a data-tooltip-position="top" aria-label="Convexity" data-href="Convexity" href="the-guide/mathematics/optimization/convex-optimization-lecture/convexity.html" class="internal-link" target="_self" rel="noopener nofollow">convex</a> in .
<br>For each , the problem is the sum of a convex and a non-convex part. The <a data-tooltip-position="top" aria-label="A general Framework for Iterative Optimization Algorithms" data-href="A general Framework for Iterative Optimization Algorithms" href="the-guide/mathematics/optimization/convex-optimization-lecture/algorithms/a-general-framework-for-iterative-optimization-algorithms.html" class="internal-link" target="_self" rel="noopener nofollow">approximate problem for our general framework</a> can be constructed by linearizing the non-convex part via This fulfills the <a data-tooltip-position="top" aria-label="A general Framework for Iterative Optimization Algorithms" data-href="A general Framework for Iterative Optimization Algorithms" href="the-guide/mathematics/optimization/convex-optimization-lecture/algorithms/a-general-framework-for-iterative-optimization-algorithms.html" class="internal-link" target="_self" rel="noopener nofollow">gradient consistency</a>and enables a parallel algorithm:<br>Algorithm

<br>For each iteration

<br>Solve  problems of the form in parallel
<br>Communication the update between threads for 



]]></description><link>the-guide/mathematics/optimization/convex-optimization-lecture/algorithms/partial-linearization-method.html</link><guid isPermaLink="false">The Guide/Mathematics/Optimization/Convex Optimization Lecture/Algorithms/Partial Linearization Method.md</guid><pubDate>Mon, 09 Sep 2024 15:35:34 GMT</pubDate></item><item><title><![CDATA[Proximal Point Algorithm]]></title><description><![CDATA[ 
 <br>Alternative to <a data-tooltip-position="top" aria-label="Gradient Descent" data-href="Gradient Descent" href="the-guide/mathematics/optimization/convex-optimization-lecture/algorithms/gradient-descent.html" class="internal-link" target="_self" rel="noopener nofollow">gradient descent</a> that uses the function itself, but adds a penalty term of the form to create an upper bound <a data-tooltip-position="top" aria-label="Mathematical Relations" data-href="Mathematical Relations" href="the-guide/mathematics/general-stuff/mathematical-relations.html" class="internal-link" target="_self" rel="noopener nofollow">function</a> as the approximate function in our <a data-tooltip-position="top" aria-label="A general Framework for Iterative Optimization Algorithms" data-href="A general Framework for Iterative Optimization Algorithms" href="the-guide/mathematics/optimization/convex-optimization-lecture/algorithms/a-general-framework-for-iterative-optimization-algorithms.html" class="internal-link" target="_self" rel="noopener nofollow">general framework</a>. <br>The approximate function above is <a data-tooltip-position="top" aria-label="Convexity" data-href="Convexity" href="the-guide/mathematics/optimization/convex-optimization-lecture/convexity.html" class="internal-link" target="_self" rel="noopener nofollow">convex</a> if ]]></description><link>the-guide/mathematics/optimization/convex-optimization-lecture/algorithms/proximal-point-algorithm.html</link><guid isPermaLink="false">The Guide/Mathematics/Optimization/Convex Optimization Lecture/Algorithms/Proximal Point Algorithm.md</guid><pubDate>Mon, 09 Sep 2024 15:35:34 GMT</pubDate></item><item><title><![CDATA[The Jacobi Algorithm]]></title><description><![CDATA[ 
 <br>In a Nutshell
Algorithm for special optimization problems, also refered to as best-response algorithm. Can be parallelized in many cases.
<br><br>Problem
<a data-tooltip-position="top" aria-label="Optimization Problem" data-href="Optimization Problem" href="the-guide/mathematics/optimization/convex-optimization-lecture/optimization-problem.html" class="internal-link" target="_self" rel="noopener nofollow">Optimization</a> <a data-tooltip-position="top" aria-label="Optimization Algorithms Overview" data-href="Optimization Algorithms Overview" href="the-guide/mathematics/optimization/convex-optimization-lecture/algorithms/optimization-algorithms-overview.html" class="internal-link" target="_self" rel="noopener nofollow">algorithm</a> for problems of the form where  is convex for each  with all other fixed, but not jointly. 
<br>The <a data-tooltip-position="top" aria-label="A general Framework for Iterative Optimization Algorithms" data-href="A general Framework for Iterative Optimization Algorithms" href="the-guide/mathematics/optimization/convex-optimization-lecture/algorithms/a-general-framework-for-iterative-optimization-algorithms.html" class="internal-link" target="_self" rel="noopener nofollow">approximate problem for our general framework</a> is given by where  denotes all the fixed variables. The assumption of gradient consistency  can easily be verified:<br>
<br>
<a data-tooltip-position="top" aria-label="Derivative, Gradient, Jacobian and Hessian > The Gradient" data-href="Derivative, Gradient, Jacobian and Hessian#The Gradient" href="the-guide/mathematics/analysis-and-calculus/derivative,-gradient,-jacobian-and-hessian.html#The_Gradient" class="internal-link" target="_self" rel="noopener nofollow">Gradient</a> is linear operator, all summands without variable  vanish

<br>
For the last summand it holds that Therefore, the minimization of the approximate problems can be computed in parallel, yielding which can all be applied together. 

<br>
global step size ?

<br><br><br>The above setting can be extended to settings that couple the subspaces via a sum constraint In this case, the Jacobi algorithm can be extended to a <a data-href="Dual Decomposition Method" href="the-guide/mathematics/optimization/convex-optimization-lecture/algorithms/dual-decomposition-method.html" class="internal-link" target="_self" rel="noopener nofollow">Dual Decomposition Method</a>, where the dual problem couples the global resources enforced by the <a data-tooltip-position="top" aria-label="Method of Lagrangian Multipliers" data-href="Method of Lagrangian Multipliers" href="the-guide/mathematics/optimization/convex-optimization-lecture/method-of-lagrangian-multipliers.html" class="internal-link" target="_self" rel="noopener nofollow">Lagrangian</a>The <a data-tooltip-position="top" aria-label="Lagrange Dual Problem" data-href="Lagrange Dual Problem" href="the-guide/mathematics/optimization/convex-optimization-lecture/lagrange-dual-problem.html" class="internal-link" target="_self" rel="noopener nofollow">dual problem</a> is thereby This results in a two-scale algorithm:<br>Algorithm

<br>Init by choosing an initial 
<br>Outer Iteration 

<br>Set fixed dual variable and compute (parallel) to obtain 
<br>Update primal variable with global step size
<br>Inner Iteration with index  to update the dual variable

<br>Compute (parallel) 
<br>Update dual variable globallyby choosing a stepsize as in the vanilla algorithm





]]></description><link>the-guide/mathematics/optimization/convex-optimization-lecture/algorithms/the-jacobi-algorithm.html</link><guid isPermaLink="false">The Guide/Mathematics/Optimization/Convex Optimization Lecture/Algorithms/The Jacobi Algorithm.md</guid><pubDate>Sat, 25 Jan 2025 17:52:28 GMT</pubDate></item><item><title><![CDATA[Compressed Sensing]]></title><description><![CDATA[ 
 <br>In a Nutshell
Signal processing technique to reconstruct a received signal, where we sampled below the <a data-href="Nyquist-Shannon Sampling Theorem" href="the-guide/information-theory/information-theory-1/data-compression/nyquist-shannon-sampling-theorem.html" class="internal-link" target="_self" rel="noopener nofollow">Nyquist-Shannon Sampling Theorem</a> and obtained an under-determined system of equations. Uses the additional assumption of sparsity to uniquely recover the system under certain conditions. 
<br><br>In the simplest case of reconstructing a signal  from measurements  and a known <a data-tooltip-position="top" aria-label="Multi-Antenna Channels" data-href="Multi-Antenna Channels" href="the-guide/information-theory/information-theory-2/multi-antenna-channels.html" class="internal-link" target="_self" rel="noopener nofollow">channel matrix</a> , we obtain the system If the equations are linearly dependent or , this system is under-determined and generally has an infinite amount of solutions.<br>Theorem - Tao 2004
It can be shown that if the vector  is sparse and the columns of  are sufficiently independent, the -<a data-tooltip-position="top" aria-label="Norms" data-href="Norms" href="the-guide/mathematics/linear-algebra/norms.html" class="internal-link" target="_self" rel="noopener nofollow">norm</a> <a data-tooltip-position="top" aria-label="Optimization Problem" data-href="Optimization Problem" href="the-guide/mathematics/optimization/convex-optimization-lecture/optimization-problem.html" class="internal-link" target="_self" rel="noopener nofollow">minimization problem</a> has a unique solution. 
<br>However, this problem is NP-hard (-norm is not convex) and need to be relaxed to be solved with reasonable effort.<br><br><br>A relaxed version of the above problem that is <a data-tooltip-position="top" aria-label="Convexity" data-href="Convexity" href="the-guide/mathematics/optimization/convex-optimization-lecture/convexity.html" class="internal-link" target="_self" rel="noopener nofollow">convex</a> can be obtained by switching the norm to whose solution can coincide with the initial one, if (...)<br>Compressed Sensing for single Instance and Noise
If the measurements are corrupted by noise, whose noise power  we can estimate, we can formulate a related problem viaAn equivalent problem can be obtained via <a data-href="LASSO" href="LASSO" class="internal-link" target="_self" rel="noopener nofollow">LASSO</a>, i.e. where  is a weighting factor.
<br>Intuition
The -norm is the "smallest" (with regard to ) for which the original problem becomes convex.
<br>More generally, for multiple instances of  and , the system where  is assumed to be row-sparse can be solved via <a data-tooltip-position="top" aria-label="LASSO" data-href="LASSO" href="LASSO" class="internal-link" target="_self" rel="noopener nofollow">group-LASSO</a> as This can also be extended to noisy observations, e.g. in <a data-tooltip-position="top" aria-label="Direction-Of-Arrival Estimation" data-href="Direction-Of-Arrival Estimation" href="the-guide/mathematics/optimization/convex-optimization-lecture/applications-in-optimization/direction-of-arrival-estimation.html" class="internal-link" target="_self" rel="noopener nofollow">DoA</a> <a data-tooltip-position="top" aria-label="Estimator" data-href="Estimator" href="the-guide/mathematics/statistics/estimator.html" class="internal-link" target="_self" rel="noopener nofollow">estimation</a>.<br>
]]></description><link>the-guide/mathematics/optimization/convex-optimization-lecture/applications-in-optimization/compressed-sensing.html</link><guid isPermaLink="false">The Guide/Mathematics/Optimization/Convex Optimization Lecture/Applications in Optimization/Compressed Sensing.md</guid><pubDate>Mon, 09 Sep 2024 15:37:19 GMT</pubDate></item><item><title><![CDATA[Direction-Of-Arrival Estimation]]></title><description><![CDATA[ 
 <br>In a Nutshell
Communications problem setting, where a base station is receiving signals from multiple users and wants to estimate the direction from which it receives the data in order to e.g. perform <a data-tooltip-position="top" aria-label="Downlink Beamforming Optimization" data-href="Downlink Beamforming Optimization" href="the-guide/mathematics/optimization/convex-optimization-lecture/applications-in-optimization/downlink-beamforming-optimization.html" class="internal-link" target="_self" rel="noopener nofollow">beamforming</a>. 
<br><img alt="center" src="lib/media/pasted-image-20240625170201.png" style="width: 300px; max-width: 100%;"><br><br>Based on the <a data-tooltip-position="top" aria-label="Compressed Sensing" data-href="Compressed Sensing" href="the-guide/mathematics/optimization/convex-optimization-lecture/applications-in-optimization/compressed-sensing.html" class="internal-link" target="_self" rel="noopener nofollow">compressed sensing</a> problem, we want to determine the direction  (which we denote  in the following) from which we receive  signals from different sources using an array of  sensors.<br><img alt="center" src="lib/media/pasted-image-20240613120401.png" style="width: 300px; max-width: 100%;"><br>For this, we perform  measurements, represented by a measurement <a data-tooltip-position="top" aria-label="Matrices" data-href="Matrices" href="the-guide/mathematics/linear-algebra/matrices.html" class="internal-link" target="_self" rel="noopener nofollow">matrix</a> with <br>
<br>The sensing or dictionary matrix which has dimensions (number of sensors by number of directions). The construction is based on the <a data-tooltip-position="top" aria-label="Steering Vector" data-href="Steering Vector" href="the-guide/mathematics/optimization/convex-optimization-lecture/applications-in-optimization/steering-vector.html" class="internal-link" target="_self" rel="noopener nofollow">steering vector</a>. The needed phases can be determined via where  is the distance between the sensors and wavelength  of the desired signal.
<br>Signal matrix  (number of directions by number of observations)
<br>Measurement matrix .
<br>Noise matrix  (number of sensors by number of observations)<br>
<img alt="center" src="lib/media/pasted-image-20240613120438.png" style="width: 350px; max-width: 100%;">
<br><br>Mixed-Norm Regularization
The idea is to use prior knowledge that we usually have a lot more directions to choose from than there are signals. We can therefore introduce a sparsity regularizer using the -<a data-tooltip-position="top" aria-label="Norms" data-href="Norms" href="the-guide/mathematics/linear-algebra/norms.html" class="internal-link" target="_self" rel="noopener nofollow">norm</a> via where we penalize non-zero rows as in <a data-tooltip-position="top" aria-label="Compressed Sensing" data-href="Compressed Sensing" href="the-guide/mathematics/optimization/convex-optimization-lecture/applications-in-optimization/compressed-sensing.html" class="internal-link" target="_self" rel="noopener nofollow">compressed sensing</a>. For larger , this leads to sparser solutions that are potentially a worse fit to the data. 
<br>
<br>Ideally, we'd have to pick , but non-convex nature of this norm requires approximation, mostly  or , as in <a data-tooltip-position="top" aria-label="Downlink Beamforming Optimization" data-href="Downlink Beamforming Optimization" href="the-guide/mathematics/optimization/convex-optimization-lecture/applications-in-optimization/downlink-beamforming-optimization.html" class="internal-link" target="_self" rel="noopener nofollow">beamforming</a>.
<br>Problem with Feasibility
In general, the <a data-tooltip-position="top" aria-label="Estimator" data-href="Estimator" href="the-guide/mathematics/statistics/estimator.html" class="internal-link" target="_self" rel="noopener nofollow">estimation</a> quality improves with increase of the measurements  and / or the direction resolution . However, the problem above is solved over  complex variables, which makes it intractable for a high direction-resolution and/or a large dataset / many snapshots. This led to the development of an alternative formulation over a reduced variable set.
<br><br>Theorem - SPARROW
The  mixed-norm <a data-tooltip-position="top" aria-label="Optimization Problem" data-href="Optimization Problem" href="the-guide/mathematics/optimization/convex-optimization-lecture/optimization-problem.html" class="internal-link" target="_self" rel="noopener nofollow">minimization problem</a> is equivalent to a SPARse ROW-norm reconstruction problem where  is a positive diagonal matrix and . 
<br>We thereby only have to optimize over  real, non-negative variables, significantly reducing computation times !<br>
Additionally, we usually don't have to go back to the original problem, because we only want to know directions, which are given by the non-zero .<br>Proof

<br>We can reformulate any -norm as where  and . This is derived from the inequalityBy this construction, the optimal solution has to be .
<br>Extend this to the <a data-tooltip-position="top" aria-label="Norms" data-href="Norms" href="the-guide/mathematics/linear-algebra/norms.html" class="internal-link" target="_self" rel="noopener nofollow">mixed-norm</a> scenario, where ,  and  via 
<br>Inserting this  into the original problem yields a closed-form solution for  given a fixed , which can be inserted back into the original problem. Setting  yields the SPARROW formulation.

<br>
<br>Transformation between problems can be obtained via another inverse using  with  ??? this doesn't make any sense
<br>Reconstruct solution via 
<br>(...?...)<br><br><br>While we obtained a massively reduced variable set, we now have to compute a <a data-tooltip-position="top" aria-label="Matrices" data-href="Matrices" href="the-guide/mathematics/linear-algebra/matrices.html" class="internal-link" target="_self" rel="noopener nofollow">matrix</a> inverse to obtain the reformulation, which can in itself be very expensive. However, depending on how many measurements  and sensors  we have, we can use the following reformulations to avoid this:<br>
<br>Oversampled , we have more measurements than sensors. In this case, we use the estimated <a data-tooltip-position="top" aria-label="Covariance and Variance" data-href="Covariance and Variance" href="the-guide/mathematics/statistics/covariance-and-variance.html" class="internal-link" target="_self" rel="noopener nofollow">covariance</a> <a data-tooltip-position="top" aria-label="Matrices" data-href="Matrices" href="the-guide/mathematics/linear-algebra/matrices.html" class="internal-link" target="_self" rel="noopener nofollow">matrix</a>  to solve 

<br>Affine objective, <a data-tooltip-position="top" aria-label="Convexity" data-href="Convexity" href="the-guide/mathematics/optimization/convex-optimization-lecture/convexity.html" class="internal-link" target="_self" rel="noopener nofollow">convex</a> <a data-tooltip-position="top" aria-label="Cones" data-href="Cones" href="the-guide/mathematics/optimization/convex-optimization-lecture/cones.html" class="internal-link" target="_self" rel="noopener nofollow">cone</a> problem because of positive semi-definite constraint
<br>Uses the <a data-tooltip-position="top" aria-label="Schur Complement" data-href="Schur Complement" href="the-guide/mathematics/linear-algebra/schur-complement.html" class="internal-link" target="_self" rel="noopener nofollow">Schur complement</a> for reformulation with 


<br>Undersampled , we have more sensors than measurements. In this case, we solve 
]]></description><link>the-guide/mathematics/optimization/convex-optimization-lecture/applications-in-optimization/direction-of-arrival-estimation.html</link><guid isPermaLink="false">The Guide/Mathematics/Optimization/Convex Optimization Lecture/Applications in Optimization/Direction-Of-Arrival Estimation.md</guid><pubDate>Mon, 09 Sep 2024 15:37:19 GMT</pubDate><enclosure url="lib/media/pasted-image-20240625170201.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src=&quot;lib/media/pasted-image-20240625170201.png&quot;&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[Downlink Beamforming Optimization]]></title><description><![CDATA[ 
 <br>In a Nutshell
Optimization setting for <a data-tooltip-position="top" aria-label="Multiuser Channels" data-href="Multiuser Channels" href="the-guide/information-theory/information-theory-2/multiuser-channels.html" class="internal-link" target="_self" rel="noopener nofollow">multiuser channels</a>, where we have  base stations transmitting distinct signals to  users simultaneously. Can be solved based on a <a data-tooltip-position="top" aria-label="Semidefinite Program (SDP)" data-href="Semidefinite Program (SDP)" href="the-guide/mathematics/optimization/convex-optimization-lecture/semidefinite-program-(sdp).html" class="internal-link" target="_self" rel="noopener nofollow">SDP</a> formulation in <a data-tooltip-position="top" aria-label="Convexity" data-href="Convexity" href="the-guide/mathematics/optimization/convex-optimization-lecture/convexity.html" class="internal-link" target="_self" rel="noopener nofollow">convex</a> <a data-tooltip-position="top" aria-label="Optimization Problem" data-href="Optimization Problem" href="the-guide/mathematics/optimization/convex-optimization-lecture/optimization-problem.html" class="internal-link" target="_self" rel="noopener nofollow">optimization</a>.
<br><br><img alt="center" src="lib/media/pasted-image-20240603121502.png" style="width: 300px; max-width: 100%;"><br>
The total signal that is transmitted by the antenna array is the superposition of all these signals, i.e. with beamforming weights . Additionally, we assume the following common assumption for communication settings:<br>Assumptions

<br>Signals for different users are uncorrelated
<br>Symbols  are zero-<a data-tooltip-position="top" aria-label="Expectations" data-href="Expectations" href="the-guide/mathematics/statistics/expectations.html" class="internal-link" target="_self" rel="noopener nofollow">mean</a>
<br>The symbol power is normalized,  for every user 

<br>With this, we derive some important quantities of our <a data-href="Static and Dynamic Systems" href="the-guide/robotics,-dynamics-and-control/dynamics/static-and-dynamic-systems.html" class="internal-link" target="_self" rel="noopener nofollow">Static and Dynamic Systems</a> model.<br>
<br>Received Signal - at the -th user, the received signal is which is .
<br>Average Total Power transmitted at the basestation

<br>We assume zero-mean, which is why only the weights determine the power.


<br>SINR - signal to interference and noise ratio at user  If we further assume perfect knowledge of the <a data-tooltip-position="top" aria-label="Channel Capacity" data-href="Channel Capacity" href="the-guide/information-theory/information-theory-1/channel-coding/channel-capacity.html" class="internal-link" target="_self" rel="noopener nofollow">channel</a>, we can further simplify toWe now want to guarantee a certain quality-of-service (QoS) to every user, yielding the constraints, formalized via 
Optimization Formulation
Our objective is to minimize the total power while providing the requested QoS, yielding which is non-convex.


<br>This problem is in general very complex to solve and has to be solved many times, as the channel usually changes over time. For this reason, many systems rely on suboptimal solutions that speed up the computations. The most important ones are ...<br>
<br>Matched Filtering - greedy approach, only maximize power at user / receiver  while ignoring interferences this causes at other users. This lowers the dimensionality, because we now use a single scaling factorfor each symbol instead of  weights and only optimize over the power allocations. The optimization now cannot work against interference by individually adapting beamforming weights, it can only adjust the power of symbols. At optimality, we achieve the QoS constraints with equality (higher SINR only via higher power, which we want to minimize), which is why we can write the problem in linear formyielding a linear <a data-href="Static and Dynamic Systems" href="the-guide/robotics,-dynamics-and-control/dynamics/static-and-dynamic-systems.html" class="internal-link" target="_self" rel="noopener nofollow">Static and Dynamic Systems</a> with  conditions and  variables (unique solution).
<br>Zero Forcing - when there are at least as many antennas as there are users , we can completely surpress interferences. To simplify the objective, zero forcing does this while ignoring desired signal powers.This is done via <a data-tooltip-position="top" aria-label="Singular Value Decomposition" data-href="Singular Value Decomposition" href="the-guide/mathematics/linear-algebra/singular-value-decomposition.html" class="internal-link" target="_self" rel="noopener nofollow">SVD</a> of the channel matrix. We choose the beamforming weights as scalar multiples of the columns if , determining the scaling as for matched filtering.
<br><br><br>To derive an optimal formulation that is <a data-tooltip-position="top" aria-label="Convexity" data-href="Convexity" href="the-guide/mathematics/optimization/convex-optimization-lecture/convexity.html" class="internal-link" target="_self" rel="noopener nofollow">convex</a>, we notice that for any optimal solution , we can find infinitely many optimal solutions by simply shifting the phase. <br>Intuition
The formulas only consider the absolute value of the terms involving the beamforming weights, we can always pull out the phase. Physically, we only require the beamforming weights to cancel out interferences, if all phases are shifted accordingly the effect of the phase shift is nullified.
<br>With this, we can introduce an additional constraint , which enables us to remove the norm, because the values are no longer complex. For complex values, manipulating the inequality is problematic as there is no total order relation. The new objective readsThe dual problem ...<br>
<br>For direct approach only the term with negative sign can create zero eigenvalue, which we need for  to be singular
<br><br><br>The derivations above require channel knowledge, which has to be estimated at the users and fed back to the base station. For fast changing channels, this can be infeasible. In those cases, we can instead use an <a data-tooltip-position="top" aria-label="Estimator" data-href="Estimator" href="the-guide/mathematics/statistics/estimator.html" class="internal-link" target="_self" rel="noopener nofollow">estimate</a> of the channel <a data-tooltip-position="top" aria-label="Covariance and Variance" data-href="Covariance and Variance" href="the-guide/mathematics/statistics/covariance-and-variance.html" class="internal-link" target="_self" rel="noopener nofollow">covariance</a> matrix This allows us to perform the feedback less frequently (although in general at the cost of performance). The resulting problemwhich is non-convex because of the constraints.<br>Relaxation Technique
Using the identity we can reformulate the problem above using the rank-1 <a data-href="Matrices" href="the-guide/mathematics/linear-algebra/matrices.html" class="internal-link" target="_self" rel="noopener nofollow">Matrices</a> which yields the problemThis reformulation isolates convex and non-convex parts of the problem. The objective and the first constraint are affine, the <a data-tooltip-position="top" aria-label="Matrices" data-href="Matrices" href="the-guide/mathematics/linear-algebra/matrices.html" class="internal-link" target="_self" rel="noopener nofollow">definiteness</a> constraint is a <a data-tooltip-position="top" aria-label="Cones" data-href="Cones" href="the-guide/mathematics/optimization/convex-optimization-lecture/cones.html" class="internal-link" target="_self" rel="noopener nofollow">convex cone</a>. Only the rank constraint is non-convex. By simply dropping it, we obtain an easier problem. 
<br>
<br>It can be shown that solution is always rank one even without the constraint
<br>For a single global beamformer that all users use, the solution will be of higher rank.
]]></description><link>the-guide/mathematics/optimization/convex-optimization-lecture/applications-in-optimization/downlink-beamforming-optimization.html</link><guid isPermaLink="false">The Guide/Mathematics/Optimization/Convex Optimization Lecture/Applications in Optimization/Downlink Beamforming Optimization.md</guid><pubDate>Mon, 03 Mar 2025 15:30:26 GMT</pubDate><enclosure url="lib/media/pasted-image-20240603121502.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src=&quot;lib/media/pasted-image-20240603121502.png&quot;&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[Hybrid Beamforming]]></title><description><![CDATA[ 
 <br>In the <a data-tooltip-position="top" aria-label="Downlink Beamforming Optimization" data-href="Downlink Beamforming Optimization" href="the-guide/mathematics/optimization/convex-optimization-lecture/applications-in-optimization/downlink-beamforming-optimization.html" class="internal-link" target="_self" rel="noopener nofollow">beamforming</a> setting, we accomplished digital signals weighted accordingly to the expected inference. In applications, these need to be converted into analog signals via RF chains.<br>
<img alt="center" src="lib/media/pasted-image-20240823162859.png" style="width: 500px; max-width: 100%;"><br>
However, these are very expensive and require a lot of power to run. In hybrid beamforming, we try to lower cost and power requirements by lowering the number of RF chains in the signal and instead install a second, analog beamformer that is realized by cheap and power efficient phase shifters.<br>
<img alt="center" src="lib/media/pasted-image-20240823165152.png" style="width: 500px; max-width: 100%;"><br>We base this scenario on a <a data-tooltip-position="top" aria-label="Multiuser Channels" data-href="Multiuser Channels" href="the-guide/information-theory/information-theory-2/multiuser-channels.html" class="internal-link" target="_self" rel="noopener nofollow">multi-user</a> <a data-tooltip-position="top" aria-label="Downlink Beamforming Optimization" data-href="Downlink Beamforming Optimization" href="the-guide/mathematics/optimization/convex-optimization-lecture/applications-in-optimization/downlink-beamforming-optimization.html" class="internal-link" target="_self" rel="noopener nofollow">downlink</a> system with  antennas and  RF chains at the basestation and ...<br>
<br>M-PSK transmit symbols 
<br>Digital beamforming weights 
<br>Analog beamforming weights  collected column-weise in 
<br>A codebook  containing possible analog beamformer constellations
<br> users with single antennas
<br>As the hybrid setting can obviously lead to errors in the transmitter system, we want to design the system in a way that the signals interfere constructively with each other in order to create a safety region. This way, even for non-optimal transmission and noise the chances are high that we are able to recover the correct signal.<img alt="center" src="lib/media/pasted-image-20240823165715.png"><br>
The signal received at the th user can be expressed via Taking the QPSK as an example, we can force the received signal to the safety region by requiring <img alt="center" src="lib/media/pasted-image-20240823172633.png"><br>Optimization Formulation
The joint analog-digital beamforming problem can be stated asThis can be written in a more compact form by substituting  and , yielding 
<br>Problem
The formulation above is highly non-convex, because

<br>The bilinear form is not convex. The matrix  is not psd, which is why the Hessian  is not psd.
<br>The codebook is a discrete <a data-href="Set" href="the-guide/mathematics/general-stuff/set.html" class="internal-link" target="_self" rel="noopener nofollow">Set</a>, breaking <a data-href="Convexity" href="the-guide/mathematics/optimization/convex-optimization-lecture/convexity.html" class="internal-link" target="_self" rel="noopener nofollow">Convexity</a> as well.

<br><br><br>To obtain an approximation to the above problem that is convex, we can decompose it into two subproblems<br>
<br>Analog Beamforming

<br>(...)


<br>Digital Beamforming

<br>(...)


]]></description><link>the-guide/mathematics/optimization/convex-optimization-lecture/applications-in-optimization/hybrid-beamforming.html</link><guid isPermaLink="false">The Guide/Mathematics/Optimization/Convex Optimization Lecture/Applications in Optimization/Hybrid Beamforming.md</guid><pubDate>Wed, 23 Apr 2025 21:55:38 GMT</pubDate><enclosure url="lib/media/pasted-image-20240823162859.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src=&quot;lib/media/pasted-image-20240823162859.png&quot;&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[MIMO Detection]]></title><description><![CDATA[<a class="tag" href="?query=tag:Convex-Optimization-Course" style="background-color: rgb(4, 108, 116); color: white; font-weight: 700; border: none; border-radius: 1em; padding: 0.2em 0.5em;">#Convex-Optimization-Course</a> 
 <br>In a Nutshell
<a data-tooltip-position="top" aria-label="Optimization Problem" data-href="Optimization Problem" href="the-guide/mathematics/optimization/convex-optimization-lecture/optimization-problem.html" class="internal-link" target="_self" rel="noopener nofollow">Optimization</a> scenario in communication systems, where we want to recover a received noisy <a data-tooltip-position="top" aria-label="Multiuser Channels" data-href="Multiuser Channels" href="the-guide/information-theory/information-theory-2/multiuser-channels.html" class="internal-link" target="_self" rel="noopener nofollow">multi-user</a> <a data-tooltip-position="top" aria-label="MIMO Capacity" data-href="MIMO Capacity" href="the-guide/information-theory/information-theory-2/mimo-capacity.html" class="internal-link" target="_self" rel="noopener nofollow">MIMO</a> signal.
<br><br>We assume that we received  based on the system modelwith  and want to reconstruct . The transmitted symbols are taken from a specified modulation set  of symbols , e.g. <br>
<br>BPSK, where the complex values  encode the symbols  and 
<br>QPSK, where the complex values  encode the symbols  and .
<br>Naively, this yields the <a data-tooltip-position="top" aria-label="Maximum Likelihood Estimator" data-href="Maximum Likelihood Estimator" href="the-guide/mathematics/statistics/maximum-likelihood-estimator.html" class="internal-link" target="_self" rel="noopener nofollow">ML</a> optimization objectivewith a highly non-<a data-tooltip-position="top" aria-label="Convexity" data-href="Convexity" href="the-guide/mathematics/optimization/convex-optimization-lecture/convexity.html" class="internal-link" target="_self" rel="noopener nofollow">convex</a> feasible set only consisting of the  modulation symbols. This problem requires exhaustive search algorithms to solve, who have exponential complexity. <br>One approach to overcome this is to relax the feasible set to the <a data-tooltip-position="top" aria-label="Complex Numbers" data-href="Complex Numbers" href="Complex Numbers" class="internal-link" target="_self" rel="noopener nofollow">complex plane</a> , which is called zero-forcing. The idea is to first solve which can be done in closed form using the <a data-href="Moore-Penrose Pseudoinverse" href="the-guide/mathematics/linear-algebra/moore-penrose-pseudoinverse.html" class="internal-link" target="_self" rel="noopener nofollow">Moore-Penrose Pseudoinverse</a> yielding . We then pick the closest feasible symbol as the solution. However, high noise levels can significantly impact the solution of this problem to the point where noise has more influence on the solution than the signal itself.<br><br><br>We can also try to relax the feasible <a data-tooltip-position="top" aria-label="Set" data-href="Set" href="the-guide/mathematics/general-stuff/set.html" class="internal-link" target="_self" rel="noopener nofollow">set</a> in a way that yields a convex problem, which makes optimization a lot more efficient. The most commonly used approaches are ...<br>Square Relaxation
Relax the feasible set to the -<a data-tooltip-position="top" aria-label="Norms" data-href="Norms" href="the-guide/mathematics/linear-algebra/norms.html" class="internal-link" target="_self" rel="noopener nofollow">norm</a> ball, i.e. the unit square in the convex plane
<br>Disc Relaxation
Relax the feasible set to the <a data-tooltip-position="top" aria-label="Norms" data-href="Norms" href="the-guide/mathematics/linear-algebra/norms.html" class="internal-link" target="_self" rel="noopener nofollow">2-norm</a> ball, i.e. the unit disk in the convex planeFor a general radius replace  with .
<br>Diamond Relaxation
Relax the feasible set to the <a data-tooltip-position="top" aria-label="Norms" data-href="Norms" href="the-guide/mathematics/linear-algebra/norms.html" class="internal-link" target="_self" rel="noopener nofollow">1-norm</a> ball, i.e. the unit square in the convex plane
<br><img alt="center" src="lib/media/pasted-image-20240823155355.png" style="width: 400px; max-width: 100%;"><br><br><br>We use the disk relaxation problem above and decouple the solutions into by extracting the -th row from the <a data-tooltip-position="top" aria-label="Matrices" data-href="Matrices" href="the-guide/mathematics/linear-algebra/matrices.html" class="internal-link" target="_self" rel="noopener nofollow">matrix</a>-vector product. By the <a data-tooltip-position="top" aria-label="A general Framework for Iterative Optimization Algorithms" data-href="A general Framework for Iterative Optimization Algorithms" href="the-guide/mathematics/optimization/convex-optimization-lecture/algorithms/a-general-framework-for-iterative-optimization-algorithms.html" class="internal-link" target="_self" rel="noopener nofollow">general framework</a> of the <a href=".?query=tag:Convex-Optimization-Course" class="tag" target="_blank" rel="noopener nofollow">#Convex-Optimization-Course</a>  lecture, we get a descent direction via This problem is separable into  sub-problems, one for each  and can be solved using the <a data-tooltip-position="top" aria-label="The Jacobi Algorithm" data-href="The Jacobi Algorithm" href="the-guide/mathematics/optimization/convex-optimization-lecture/algorithms/the-jacobi-algorithm.html" class="internal-link" target="_self" rel="noopener nofollow">Jacobi algorithm</a>. Additionally, the solution of each subproblem as well as the optimal step can be derived in closed-form.<br>Algorithm

<br>Collect all  and compute every  to assemble the subproblems and solve them in parallel / distributed

<br>Closed form solution and stepsize


<br>Collect all local solutions and update

]]></description><link>the-guide/mathematics/optimization/convex-optimization-lecture/applications-in-optimization/mimo-detection.html</link><guid isPermaLink="false">The Guide/Mathematics/Optimization/Convex Optimization Lecture/Applications in Optimization/MIMO Detection.md</guid><pubDate>Mon, 09 Sep 2024 15:37:20 GMT</pubDate><enclosure url="lib/media/pasted-image-20240823155355.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src=&quot;lib/media/pasted-image-20240823155355.png&quot;&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[Steering Vector]]></title><description><![CDATA[ 
 <br>In a Nutshell
Also denoted steering <a data-tooltip-position="top" aria-label="Matrices" data-href="Matrices" href="the-guide/mathematics/linear-algebra/matrices.html" class="internal-link" target="_self" rel="noopener nofollow">matrix</a>, describes how a signal arrives at an array of antennas considering the far-field assumption (planar wave). The resulting matrix is a <a data-tooltip-position="top" aria-label="Multi-Antenna Channels" data-href="Multi-Antenna Channels" href="the-guide/information-theory/information-theory-2/multi-antenna-channels.html" class="internal-link" target="_self" rel="noopener nofollow">channel</a> matrix for direct line-of-sight communication.
<br><br>We assume an sensor array, consisting of  sensors on a line and  discrete directions, from which there could arrive signals. The sensing or dictionary matrix which has dimensions (number of sensors by number of directions). The construction is based on the <a data-tooltip-position="top" aria-label="Steering Vector" data-href="Steering Vector" href="the-guide/mathematics/optimization/convex-optimization-lecture/applications-in-optimization/steering-vector.html" class="internal-link" target="_self" rel="noopener nofollow">steering vector</a> The needed phases can be determined via where  is the distance between the sensors and wavelength  of the desired signal.<br>Intuition
The lengths can be determined from the diagram below (shows one , one particular column). For the phase shift, the cosine value is adjusted based on the signal wavelenghts via  ( per wavelength).
<br><img alt="center" src="lib/media/pasted-image-20240828145707.png" style="width: 350px; max-width: 100%;">]]></description><link>the-guide/mathematics/optimization/convex-optimization-lecture/applications-in-optimization/steering-vector.html</link><guid isPermaLink="false">The Guide/Mathematics/Optimization/Convex Optimization Lecture/Applications in Optimization/Steering Vector.md</guid><pubDate>Mon, 09 Sep 2024 15:37:20 GMT</pubDate><enclosure url="lib/media/pasted-image-20240828145707.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src=&quot;lib/media/pasted-image-20240828145707.png&quot;&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[Support Vector Machine]]></title><description><![CDATA[ 
 <br>In a Nutshell
<a data-tooltip-position="top" aria-label="- Machine Learning -" data-href="- Machine Learning -" href="the-guide/machine-learning/-machine-learning-.html" class="internal-link" target="_self" rel="noopener nofollow">Supervised learning</a> framework for classification that tries to learn optimal threshold function via hyperplanes.
<br>Given datapoints  containing  dependent features with class labels , we want to find a linear classifier that tells us in which class new data points are.<br><br><br>In this simple scenario, we want to find a single hyperplane that separates two classes in the feature <a data-tooltip-position="top" aria-label="Space" data-href="Space" href="the-guide/mathematics/general-stuff/space.html" class="internal-link" target="_self" rel="noopener nofollow">space</a>. We define the binary classifier The objective is to find the parameters  and  that define the hyperplane separating the two halfspaces.<br>
To formulate an <a data-tooltip-position="top" aria-label="Optimization Problem" data-href="Optimization Problem" href="the-guide/mathematics/optimization/convex-optimization-lecture/optimization-problem.html" class="internal-link" target="_self" rel="noopener nofollow">optimization problem</a>, we define the margin  for data point  as which measures the distance of a point from the plane. A good classifier needs a maximized worst-case margin. The resulting problem is non-<a data-tooltip-position="top" aria-label="Convexity" data-href="Convexity" href="the-guide/mathematics/optimization/convex-optimization-lecture/convexity.html" class="internal-link" target="_self" rel="noopener nofollow">convex</a> and has the formThe problem is the second constraint (imagine two points on the surface of a sphere). However, we can construct a convex problem via the transformation . <br>Convex Optimization Formulation
The resulting problem reads This is a <a data-tooltip-position="top" aria-label="Quadratic Program (QP)" data-href="Quadratic Program (QP)" href="the-guide/mathematics/optimization/convex-optimization-lecture/quadratic-program-(qp).html" class="internal-link" target="_self" rel="noopener nofollow">QP</a> because of the <a data-tooltip-position="top" aria-label="Convexity" data-href="Convexity" href="the-guide/mathematics/optimization/convex-optimization-lecture/convexity.html" class="internal-link" target="_self" rel="noopener nofollow">convex</a> constraints and the euclidean norm. 
<br><img alt="center" src="lib/media/pasted-image-20240603120046.png" style="width: 300px; max-width: 100%;"><br>The <a data-tooltip-position="top" aria-label="Method of Lagrangian Multipliers" data-href="Method of Lagrangian Multipliers" href="the-guide/mathematics/optimization/convex-optimization-lecture/method-of-lagrangian-multipliers.html" class="internal-link" target="_self" rel="noopener nofollow">Lagrangian</a> readswhich yields the <a data-tooltip-position="top" aria-label="Lagrange Dual Problem" data-href="Lagrange Dual Problem" href="the-guide/mathematics/optimization/convex-optimization-lecture/lagrange-dual-problem.html" class="internal-link" target="_self" rel="noopener nofollow">dual problem</a> With this, we can compute the solution of the primal problem and obtain From the <a data-tooltip-position="top" aria-label="Complementary Slackness Condition" data-href="Complementary Slackness Condition" href="the-guide/mathematics/optimization/convex-optimization-lecture/complementary-slackness-condition.html" class="internal-link" target="_self" rel="noopener nofollow">complementary slackness condition</a> we know that many of the parameters are zero, which means the hyperplane is completely determined by a subset of the datapoints, the support vectors. To compute the offset parameter , we can use the support vectors with maximal multiplier from each class and compute the average <br><br><br>The setting above works for very high dimensional data, but is restricted to linear separation of the data. As most data violates this assumption, the kernel trick tries to incorporate non-linearity by introducing a non-linear mapping into a higher-dimensional <a data-tooltip-position="top" aria-label="Space" data-href="Space" href="the-guide/mathematics/general-stuff/space.html" class="internal-link" target="_self" rel="noopener nofollow">space</a>, yielding a transformed dataset . The hope is that the added dimensions enable us to linearly separate the data. The new classifier therefore also requires an extended normal vector  and readsHowever, the kernel trick can require very high dimensions, which makes the computation of the <a data-tooltip-position="top" aria-label="Vector Space" data-href="Vector Space" href="the-guide/mathematics/general-stuff/vector-space.html" class="internal-link" target="_self" rel="noopener nofollow">inner products</a> in the dual formulations above expensive.<br>The Kernel Trick for SVMs
The idea is to replace the inner product by a <a data-tooltip-position="top" aria-label="Mercer Theorem" data-href="Mercer Theorem" href="the-guide/mathematics/linear-algebra/mercer-theorem.html" class="internal-link" target="_self" rel="noopener nofollow">proper Mercer Kernel</a> of the lower dimensional initial feature <a data-href="Space" href="the-guide/mathematics/general-stuff/space.html" class="internal-link" target="_self" rel="noopener nofollow">Space</a> . 
<br>Some examples are ...<br>
<br>Linear Kernel
<br>Polynomial Kernel
<br>RBF Kernel
Numerical Issues
Certain Kernels - data combinations can lead to numerical instabilities. However, we can re-scale the data without changing the outcome, since we compute the classifier based on the relative distances.


<br><br><br>The settings above can fail in optimization contexts because of outliers, e.g. wrongly labeled data or intersecting clusters. To work against that, we introduce a regularizerwhere  introduces a penalty for misclassification. This allows the algorithm to perform a trade-off of going for wider (or any if un-regularized problem was e.g. infeasible) margins, even if there are a few misclassifications for the training data.<br>The corresponding <a data-tooltip-position="top" aria-label="Lagrange Dual Problem" data-href="Lagrange Dual Problem" href="the-guide/mathematics/optimization/convex-optimization-lecture/lagrange-dual-problem.html" class="internal-link" target="_self" rel="noopener nofollow">dual problem</a> reads with <br>
<br>
<br>
<br>, can be replaced with Kernel trick
<br>The offset  can then be computed from the optimal solution by again relying on a support vector. In this setting there will be a single smallest  that satisfies  and . The offset is then given by With this, we can directly perform classification via <br><br><br>All the strategies above classified for binary labels. This can easily be extended by using either ...<br>
<br>"One-against-the-rest" - train  binary classifiers, where the labels for all the  other classes are set to a new summary-label. The classification of unseen data-points is mostly based on majority-vote (for each classifier either one or all but one class get a vote). 
<br>"One-against-one" - Construct  hyperplanes, separating each class from each other by always only considering data with 2 labels. This requires more sophisticated decision-making in the final classification for unseen datapoints.
]]></description><link>the-guide/mathematics/optimization/convex-optimization-lecture/applications-in-optimization/support-vector-machine.html</link><guid isPermaLink="false">The Guide/Mathematics/Optimization/Convex Optimization Lecture/Applications in Optimization/Support Vector Machine.md</guid><pubDate>Thu, 27 Feb 2025 12:05:04 GMT</pubDate><enclosure url="lib/media/pasted-image-20240603120046.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src=&quot;lib/media/pasted-image-20240603120046.png&quot;&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[Affine Sets]]></title><description><![CDATA[ 
 <br><br>Affine Combination
Given a <a data-tooltip-position="top" aria-label="Set" data-href="Set" href="the-guide/mathematics/general-stuff/set.html" class="internal-link" target="_self" rel="noopener nofollow">set</a> of points , the affine combination is a point of the form In contrast to <a data-tooltip-position="top" aria-label="Convexity" data-href="Convexity" href="the-guide/mathematics/optimization/convex-optimization-lecture/convexity.html" class="internal-link" target="_self" rel="noopener nofollow">convex sets</a>, the parameters can be negative.
<br>
<br>Can be described as the solution of a linear equation .
]]></description><link>the-guide/mathematics/optimization/convex-optimization-lecture/affine-sets.html</link><guid isPermaLink="false">The Guide/Mathematics/Optimization/Convex Optimization Lecture/Affine Sets.md</guid><pubDate>Mon, 09 Sep 2024 15:35:34 GMT</pubDate></item><item><title><![CDATA[Complementary Slackness Condition]]></title><description><![CDATA[ 
 <br>In a Nutshell
Statement of mathematical <a data-tooltip-position="top" aria-label="Optimization Problem" data-href="Optimization Problem" href="the-guide/mathematics/optimization/convex-optimization-lecture/optimization-problem.html" class="internal-link" target="_self" rel="noopener nofollow">optimization</a> that connects the optimal values of the <a data-tooltip-position="top" aria-label="Method of Lagrangian Multipliers" data-href="Method of Lagrangian Multipliers" href="the-guide/mathematics/optimization/convex-optimization-lecture/method-of-lagrangian-multipliers.html" class="internal-link" target="_self" rel="noopener nofollow">primal Lagrange problem</a> and the <a data-tooltip-position="top" aria-label="Lagrange Dual Problem" data-href="Lagrange Dual Problem" href="the-guide/mathematics/optimization/convex-optimization-lecture/lagrange-dual-problem.html" class="internal-link" target="_self" rel="noopener nofollow">dual problem</a>. It is a necessary condition for optimality.
<br><br>Assuming <a data-tooltip-position="top" aria-label="Lagrange Dual Problem" data-href="Lagrange Dual Problem" href="the-guide/mathematics/optimization/convex-optimization-lecture/lagrange-dual-problem.html" class="internal-link" target="_self" rel="noopener nofollow">strong duality</a> holds with primal optimum  and dual optimum  at the point , we can derive the inequality because the term  is always non-positive and  has to vanish at optimal points. Therefore, equality has to hold, and the non-positive term has to be an equality.<br>Complementary Slackness Condition
This yields the complementary slackness conditions In other words, if the inequality constraint is oversatisfied, meaning , the respective multiplier has to be inactive . Conversely, a non-zero multiplier implies that the condition is an equality.<br>
As a part of the <a data-tooltip-position="top" aria-label="Karush-Kuhn-Tucker Conditions" data-href="Karush-Kuhn-Tucker Conditions" href="the-guide/mathematics/optimization/convex-optimization-lecture/karush-kuhn-tucker-conditions.html" class="internal-link" target="_self" rel="noopener nofollow">KKT conditions</a>, this is a necessary condition for optimality under <a data-tooltip-position="top" aria-label="Constraint Qualifications" data-href="Constraint Qualifications" href="the-guide/mathematics/optimization/convex-optimization-lecture/constraint-qualifications.html" class="internal-link" target="_self" rel="noopener nofollow">constraint qualifications</a>. This means points can still satisfy complementary slackness even when strong duality does not hold, but they don't have to be optimal in that case.
<br>Intuition

<br>If we somehow oversatisfy an inequality constraint, its evaluation is a negative contribution to the <a data-tooltip-position="top" aria-label="Method of Lagrangian Multipliers" data-href="Method of Lagrangian Multipliers" href="the-guide/mathematics/optimization/convex-optimization-lecture/method-of-lagrangian-multipliers.html" class="internal-link" target="_self" rel="noopener nofollow">Lagrangian</a>. When we perform maximization over all Lagrange multipliers in the <a data-tooltip-position="top" aria-label="Lagrange Dual Problem" data-href="Lagrange Dual Problem" href="the-guide/mathematics/optimization/convex-optimization-lecture/lagrange-dual-problem.html" class="internal-link" target="_self" rel="noopener nofollow">dual problem</a> to reduce the effect of approximating the indicator functions, there is not other way than to set the associated multiplier to zero. 
<br>If the constraint is exactly met, the respective multiplier has no effect and can have any value.

]]></description><link>the-guide/mathematics/optimization/convex-optimization-lecture/complementary-slackness-condition.html</link><guid isPermaLink="false">The Guide/Mathematics/Optimization/Convex Optimization Lecture/Complementary Slackness Condition.md</guid><pubDate>Mon, 14 Apr 2025 15:18:28 GMT</pubDate></item><item><title><![CDATA[Cones]]></title><description><![CDATA[ 
 <br><br>Definition
A <a data-tooltip-position="top" aria-label="Set" data-href="Set" href="the-guide/mathematics/general-stuff/set.html" class="internal-link" target="_self" rel="noopener nofollow">set</a>  is called a cone or non-negative homogeneous, if for every  and , the scaled point  
<br><br>This can be combined with the concept of <a data-tooltip-position="top" aria-label="Convexity" data-href="Convexity" href="the-guide/mathematics/optimization/convex-optimization-lecture/convexity.html" class="internal-link" target="_self" rel="noopener nofollow">convexity</a> to obtain ...<br>Convex Cone
A set is a convex cone, if it is <a data-tooltip-position="top" aria-label="Convexity" data-href="Convexity" href="the-guide/mathematics/optimization/convex-optimization-lecture/convexity.html" class="internal-link" target="_self" rel="noopener nofollow">convex</a> and a cone, meaning for any  and , we have that 
<br><img alt="center" src="lib/media/pasted-image-20240424081950.png" style="width: 300px; max-width: 100%;"><br>
Analogously to the <a data-tooltip-position="top" aria-label="Convexity" data-href="Convexity" href="the-guide/mathematics/optimization/convex-optimization-lecture/convexity.html" class="internal-link" target="_self" rel="noopener nofollow">convex hull</a>, we can define a conic hull via conic combinations of a given set of points  via <img alt="center" src="lib/media/pasted-image-20240424082427.png" style="width: 350px; max-width: 100%;"><br>
<br>Examples

<br>The set of symmetric <a data-tooltip-position="top" aria-label="Matrices" data-href="Matrices" href="the-guide/mathematics/linear-algebra/matrices.html" class="internal-link" target="_self" rel="noopener nofollow">positive semi-definite matrices</a> is a convex cone


<br><br>Dual Cone
The dual cone of a cone  is defined as meaning all vectors that have a maximum angle of  with any vector in the initial cone.
<br><img alt="center" src="lib/media/pasted-image-20240523105012.png" style="width: 400px; max-width: 100%;"><br>Lemma - Convexity of Dual Cones
The dual cone  of an arbitrary cone is always <a data-tooltip-position="top" aria-label="Convexity" data-href="Convexity" href="the-guide/mathematics/optimization/convex-optimization-lecture/convexity.html" class="internal-link" target="_self" rel="noopener nofollow">convex</a>.
<br>
<br>Examples

<br> is self-dual (generalization of first quadrant)
<br>? 
<br> is self-dual (angle of cone is )
<br> yields the dual cone 

<br>-norm yields a rectangular ice cream cone. The opening angle is smaller than  (compare to round -norm cone). Therefore, the dual cone is larger, in this case the encapsulation rectangular cone that is formed by the infinity norm. 




]]></description><link>the-guide/mathematics/optimization/convex-optimization-lecture/cones.html</link><guid isPermaLink="false">The Guide/Mathematics/Optimization/Convex Optimization Lecture/Cones.md</guid><pubDate>Mon, 09 Sep 2024 15:35:34 GMT</pubDate><enclosure url="lib/media/pasted-image-20240424081950.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src=&quot;lib/media/pasted-image-20240424081950.png&quot;&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[Constraint Qualifications]]></title><description><![CDATA[ 
 <br>Constraint qualifications are assumptions on the algebraic description of the feasible <a data-href="Set" href="the-guide/mathematics/general-stuff/set.html" class="internal-link" target="_self" rel="noopener nofollow">Set</a> of an <a data-href="Optimization Problem" href="the-guide/mathematics/optimization/convex-optimization-lecture/optimization-problem.html" class="internal-link" target="_self" rel="noopener nofollow">Optimization Problem</a> that ensures the <a data-tooltip-position="top" aria-label="Karush-Kuhn-Tucker Conditions" data-href="Karush-Kuhn-Tucker Conditions" href="the-guide/mathematics/optimization/convex-optimization-lecture/karush-kuhn-tucker-conditions.html" class="internal-link" target="_self" rel="noopener nofollow">KKT conditions</a> hold at any local minimum.<br><br><br>Simple constraint qualification for <a data-tooltip-position="top" aria-label="Lagrange Dual Problem" data-href="Lagrange Dual Problem" href="the-guide/mathematics/optimization/convex-optimization-lecture/lagrange-dual-problem.html" class="internal-link" target="_self" rel="noopener nofollow">strong duality</a> in the context of <a data-tooltip-position="top" aria-label="Convexity" data-href="Convexity" href="the-guide/mathematics/optimization/convex-optimization-lecture/convexity.html" class="internal-link" target="_self" rel="noopener nofollow">convex</a> <a data-tooltip-position="top" aria-label="Optimization Problem" data-href="Optimization Problem" href="the-guide/mathematics/optimization/convex-optimization-lecture/optimization-problem.html" class="internal-link" target="_self" rel="noopener nofollow">optimization</a>. <br>Slater's Condition
It states that for a convex problem, the <a data-tooltip-position="top" aria-label="Lagrange Dual Problem" data-href="Lagrange Dual Problem" href="the-guide/mathematics/optimization/convex-optimization-lecture/lagrange-dual-problem.html" class="internal-link" target="_self" rel="noopener nofollow">strong duality</a> holds (and the <a data-tooltip-position="top" aria-label="Karush-Kuhn-Tucker Conditions" data-href="Karush-Kuhn-Tucker Conditions" href="the-guide/mathematics/optimization/convex-optimization-lecture/karush-kuhn-tucker-conditions.html" class="internal-link" target="_self" rel="noopener nofollow">KKT conditions</a> are thereby sufficient) if there exists a point  in the relative interior of the domain  such that i.e. the inequality constraints hold with strict inequalities. Such a point is called strictly feasible.
<br>The conditions can be refined, if some of the inequality constraints are <a data-tooltip-position="top" aria-label="Affine Sets" data-href="Affine Sets" href="the-guide/mathematics/optimization/convex-optimization-lecture/affine-sets.html" class="internal-link" target="_self" rel="noopener nofollow">affine</a>. Suppose the first  constraints  are indeed affine, the we can weaken the above condition to: There exists an  as above with <br>Intuition
Ensures that the feasible <a data-tooltip-position="top" aria-label="Set" data-href="Set" href="the-guide/mathematics/general-stuff/set.html" class="internal-link" target="_self" rel="noopener nofollow">set</a> has an interior. Otherwise, we could be dealing with lower dimensional feasible regions (e.g. points, lines) that complicate things:

<br>The dual problem is often unbounded in this case
<br>The original landscape would be infinite everywhere but on this line, making the approximation done in the <a data-tooltip-position="top" aria-label="Method of Lagrangian Multipliers" data-href="Method of Lagrangian Multipliers" href="the-guide/mathematics/optimization/convex-optimization-lecture/method-of-lagrangian-multipliers.html" class="internal-link" target="_self" rel="noopener nofollow">Lagrangian</a> very coarse and not robust

<br><br><br>]]></description><link>the-guide/mathematics/optimization/convex-optimization-lecture/constraint-qualifications.html</link><guid isPermaLink="false">The Guide/Mathematics/Optimization/Convex Optimization Lecture/Constraint Qualifications.md</guid><pubDate>Wed, 23 Apr 2025 21:55:38 GMT</pubDate></item><item><title><![CDATA[Convexity]]></title><description><![CDATA[ 
 <br>In a Nutshell
Important property of <a data-tooltip-position="top" aria-label="Set" data-href="Set" href="the-guide/mathematics/general-stuff/set.html" class="internal-link" target="_self" rel="noopener nofollow">sets</a> and <a data-tooltip-position="top" aria-label="Mathematical Relations" data-href="Mathematical Relations" href="the-guide/mathematics/general-stuff/mathematical-relations.html" class="internal-link" target="_self" rel="noopener nofollow">functions</a> that is widely used in <a data-tooltip-position="top" aria-label="Optimization Problem" data-href="Optimization Problem" href="the-guide/mathematics/optimization/convex-optimization-lecture/optimization-problem.html" class="internal-link" target="_self" rel="noopener nofollow">optimization</a>, because it enables very efficient algorithms with well-understood supporting theory.
<br><br><br>Definition
In <a data-tooltip-position="top" aria-label="https://en.wikipedia.org/wiki/Geometry" rel="noopener nofollow" class="external-link" title="Geometry" href="https://en.wikipedia.org/wiki/Geometry" target="_blank">geometry</a>, a <a data-tooltip-position="top" aria-label="Set" data-href="Set" href="the-guide/mathematics/general-stuff/set.html" class="internal-link" target="_self" rel="noopener nofollow">subset</a>  of  an <a data-tooltip-position="top" aria-label="https://en.wikipedia.org/wiki/Affine_space" rel="noopener nofollow" class="external-link" title="Affine space" href="https://en.wikipedia.org/wiki/Affine_space" target="_blank">affine space</a>  over the <a data-tooltip-position="top" aria-label="https://en.wikipedia.org/wiki/Real_number" rel="noopener nofollow" class="external-link" title="Real number" href="https://en.wikipedia.org/wiki/Real_number" target="_blank">reals</a>, is <a data-tooltip-position="top" aria-label="Convexity" data-href="Convexity" href="the-guide/mathematics/optimization/convex-optimization-lecture/convexity.html" class="internal-link" target="_self" rel="noopener nofollow">convex</a> if, given any two points  in the subset, the subset contains the whole <a data-tooltip-position="top" aria-label="https://en.wikipedia.org/wiki/Line_segment" rel="noopener nofollow" class="external-link" title="Line segment" href="https://en.wikipedia.org/wiki/Line_segment" target="_blank">line segment</a> that joins them. Formally, with parameter  we require For the set of all convex sets over that <a data-href="Space" href="the-guide/mathematics/general-stuff/space.html" class="internal-link" target="_self" rel="noopener nofollow">Space</a>, we write .
<br><img alt="center" src="lib/media/pasted-image-20230523104910.png" style="width: 500px; max-width: 100%;"><br>
<br>Examples

<br>Halfspaces <img alt="center" src="lib/media/pasted-image-20240722101825.png" style="width: 100px; max-width: 100%;">
<br>Hyperplanes <img alt="center" src="lib/media/pasted-image-20240722101853.png" style="width: 100px; max-width: 100%;">
<br>Polyhedra <img alt="center" src="lib/media/pasted-image-20240722102000.png" style="width: 100px; max-width: 100%;">
<br>Norm balls <img alt="center" src="lib/media/pasted-image-20240722101916.png" style="width: 100px; max-width: 100%;">
<br>Norm <a data-tooltip-position="top" aria-label="Cones" data-href="Cones" href="the-guide/mathematics/optimization/convex-optimization-lecture/cones.html" class="internal-link" target="_self" rel="noopener nofollow">cones</a> <img alt="center" src="lib/media/pasted-image-20240722101933.png" style="width: 150px; max-width: 100%;">


<br>Equivalently, a convex set or a convex region is a subset that intersects every <a data-tooltip-position="top" aria-label="https://en.wikipedia.org/wiki/Line_(geometry)" rel="noopener nofollow" class="external-link" title="Line (geometry)" href="https://en.wikipedia.org/wiki/Line_(geometry)" target="_blank">line</a> into a single <a data-tooltip-position="top" aria-label="https://en.wikipedia.org/wiki/Line_segment" rel="noopener nofollow" class="external-link" title="Line segment" href="https://en.wikipedia.org/wiki/Line_segment" target="_blank">line segment</a> (possibly empty). For example, a solid <a data-tooltip-position="top" aria-label="https://en.wikipedia.org/wiki/Cube_(geometry)" rel="noopener nofollow" class="external-link" title="Cube (geometry)" href="https://en.wikipedia.org/wiki/Cube_(geometry)" target="_blank">cube</a> is a <a data-tooltip-position="top" aria-label="Convexity" data-href="Convexity" href="the-guide/mathematics/optimization/convex-optimization-lecture/convexity.html" class="internal-link" target="_self" rel="noopener nofollow">convex</a> <a data-tooltip-position="top" aria-label="Set" data-href="Set" href="the-guide/mathematics/general-stuff/set.html" class="internal-link" target="_self" rel="noopener nofollow">set</a>, but anything that is hollow or has an indent, for example, a <a data-tooltip-position="top" aria-label="https://en.wikipedia.org/wiki/Crescent" rel="noopener nofollow" class="external-link" title="Crescent" href="https://en.wikipedia.org/wiki/Crescent" target="_blank">crescent</a> shape, is not convex.<br><br>
<br>Intersections of convex sets are convex 
<br>Affine mappings  of convex sets and their inverses are convex
<br>Scalings, translations, projections, solutions to linear inequalities (matrices), ... of convex sets are convex
<br>Convex Hull
Given a <a data-tooltip-position="top" aria-label="Set" data-href="Set" href="the-guide/mathematics/general-stuff/set.html" class="internal-link" target="_self" rel="noopener nofollow">set</a> of points , we can create a convex set by using the convex hull creating the smallest convex <a data-href="Set" href="the-guide/mathematics/general-stuff/set.html" class="internal-link" target="_self" rel="noopener nofollow">Set</a> that contains all the given points.
<br><img alt="center" src="lib/media/pasted-image-20240722101244.png" style="width: 200px; max-width: 100%;"><br><br><br>Definition
A <a data-tooltip-position="top" aria-label="Mathematical Relations" data-href="Mathematical Relations" href="the-guide/mathematics/general-stuff/mathematical-relations.html" class="internal-link" target="_self" rel="noopener nofollow">function</a> is convex over a (convex) interval , if for every  and  meaning the function always lies below any chord. 
<br><img alt="center" src="lib/media/pasted-image-20240425161621.png" style="width: 450px; max-width: 100%;"><br>
<br>
The function is strictly convex, if .

<br>
A function is concave if it is convex when multiplied by .

<br>
Examples

<br>Affine functions  are both concave and convex
<br>Exponentials 
<br>Powers 

<br> and  is convex
<br> is concave


<br><a data-tooltip-position="top" aria-label="Norms" data-href="Norms" href="the-guide/mathematics/linear-algebra/norms.html" class="internal-link" target="_self" rel="noopener nofollow">p-Norms</a> 
<br>Negative <a data-tooltip-position="top" aria-label="Shannon Entropy" data-href="Shannon Entropy" href="the-guide/information-theory/information-theory-1/information/shannon-entropy.html" class="internal-link" target="_self" rel="noopener nofollow">Entropy</a> 
<br><a data-tooltip-position="top" aria-label="Norms" data-href="Norms" href="the-guide/mathematics/linear-algebra/norms.html" class="internal-link" target="_self" rel="noopener nofollow">Spectral Norm</a> 
<br><a data-tooltip-position="top" aria-label="Matrices" data-href="Matrices" href="the-guide/mathematics/linear-algebra/matrices.html" class="internal-link" target="_self" rel="noopener nofollow">Trace</a> of a matrix
<br><a data-tooltip-position="top" aria-label="Logarithm and Exponential" data-href="Logarithm and Exponential" href="the-guide/mathematics/general-stuff/logarithm-and-exponential.html" class="internal-link" target="_self" rel="noopener nofollow">Logarithm</a> is concave
<br>Log-<a data-tooltip-position="top" aria-label="Matrices" data-href="Matrices" href="the-guide/mathematics/linear-algebra/matrices.html" class="internal-link" target="_self" rel="noopener nofollow">Determinant</a> is concave for positive definite <a data-href="Matrices" href="the-guide/mathematics/linear-algebra/matrices.html" class="internal-link" target="_self" rel="noopener nofollow">Matrices</a>

<br>We can rewrite this expression using the <a data-tooltip-position="top" aria-label="Eigenvalue Problem" data-href="Eigenvalue Problem" href="the-guide/mathematics/linear-algebra/eigenvalue-problem.html" class="internal-link" target="_self" rel="noopener nofollow">eigenvalues</a> as which is a non-negative weighted sum of concave functions.


<br>...


<br>First-order Condition
A differentiable function  with convex domain  is convex, if Intuitively, this means that the tangent at any point is a global under-<a data-href="Estimator" href="the-guide/mathematics/statistics/estimator.html" class="internal-link" target="_self" rel="noopener nofollow">Estimator</a>.
<br>
<br>On non-convex domains, there might exist points in-between (definition) that are not convex
<br>Second-order Condition
A twice differentiable function  with convex domain  is convex iff the Hessian is <a data-tooltip-position="top" aria-label="Matrices" data-href="Matrices" href="the-guide/mathematics/linear-algebra/matrices.html" class="internal-link" target="_self" rel="noopener nofollow">positive semidefinite</a>It is strictly convex, if the Hessian is positive definite ().
<br><br>
<br>Non-negative weighted sums of convex/concave functions are convex/concave 
<br>Compositions

<br>Compositions of convex functions with affine functions are convex 
<br>Function compositions  are convex, if both functions are convex and  is non-decreasing in each argument 


<br>Pointwise Maximum and Supremum

<br>Given  convex in  (this means  is also defined over a convex set) for every  in an arbitrary <a data-href="Set" href="the-guide/mathematics/general-stuff/set.html" class="internal-link" target="_self" rel="noopener nofollow">Set</a> , then  is also convex.
<br>Can be thought of as intersections of convex sets ! Take for example  with <img alt="center" src="lib/media/pasted-image-20240829145156.png" style="width: 150px; max-width: 100%;">


<br>Minimization

<br>Given a jointly convex , where  is element of a convex <a data-href="Set" href="the-guide/mathematics/general-stuff/set.html" class="internal-link" target="_self" rel="noopener nofollow">Set</a>  (and  as well), then  is also convex.

<br>Infimum is stricter, e.g. above example would fail.




<br><br><br>A stronger version of convexity is achieved by requiring the logarithm of the function to be convex via ...<br>Log-Convexity and Log-Concavity
A function  is (strictly) log-convex or superconvex, if  is a (strictly) convex function as above.<br>
A function sis (strictly) log-concave, if is (strictly-) concave. The relation between the two changes because of log rules:  is lo-convex, iff  is log-concave.
<br>
<br>Examples

<br>Affine functions  are log-concave, if they are positive
<br>Exponentials  are both, since they become linear-affine
<br>Powers 

<br> is log convex 
<br> is log concave


<br>The <a data-tooltip-position="top" aria-label="Cumulative Distribution Function" data-href="Cumulative Distribution Function" href="the-guide/mathematics/probability-theory/cumulative-distribution-function.html" class="internal-link" target="_self" rel="noopener nofollow">cdf</a> of a <a data-tooltip-position="top" aria-label="Gaussian Distribution" data-href="Gaussian Distribution" href="the-guide/mathematics/probability-theory/gaussian-distribution.html" class="internal-link" target="_self" rel="noopener nofollow">Gaussian</a> is log-concave
<br>Determinant of (symmetric)-psd <a data-href="Matrices" href="the-guide/mathematics/linear-algebra/matrices.html" class="internal-link" target="_self" rel="noopener nofollow">Matrices</a> (see above)


]]></description><link>the-guide/mathematics/optimization/convex-optimization-lecture/convexity.html</link><guid isPermaLink="false">The Guide/Mathematics/Optimization/Convex Optimization Lecture/Convexity.md</guid><pubDate>Wed, 23 Apr 2025 21:55:39 GMT</pubDate><enclosure url="lib/media/pasted-image-20230523104910.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src=&quot;lib/media/pasted-image-20230523104910.png&quot;&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[Generalized Inequality Optimization]]></title><description><![CDATA[ 
 <br>In a Nutshell
Generalized setting of <a data-tooltip-position="top" aria-label="Convexity" data-href="Convexity" href="the-guide/mathematics/optimization/convex-optimization-lecture/convexity.html" class="internal-link" target="_self" rel="noopener nofollow">convex</a> <a data-tooltip-position="top" aria-label="Optimization Problem" data-href="Optimization Problem" href="the-guide/mathematics/optimization/convex-optimization-lecture/optimization-problem.html" class="internal-link" target="_self" rel="noopener nofollow">optimization</a> that allows inequality constraints to be vector valued using generalized inequalities.
<br><br>Generalized Inequalities
A generalized inequality, denoted  on  encodes that a vector lies inside a <a data-tooltip-position="top" aria-label="Cones" data-href="Cones" href="the-guide/mathematics/optimization/convex-optimization-lecture/cones.html" class="internal-link" target="_self" rel="noopener nofollow">proper cone</a> , i.e. The index  denotes the dimension of the cone.
<br>Problem Formulation
The new basic problem reads where ,  are <a data-tooltip-position="top" aria-label="Cones" data-href="Cones" href="the-guide/mathematics/optimization/convex-optimization-lecture/cones.html" class="internal-link" target="_self" rel="noopener nofollow">proper cones</a> and the constraint <a data-tooltip-position="top" aria-label="Mathematical Relations" data-href="Mathematical Relations" href="the-guide/mathematics/general-stuff/mathematical-relations.html" class="internal-link" target="_self" rel="noopener nofollow">functions</a>  are -convex. 
<br>The implications of this is that the resulting multipliers in the <a data-tooltip-position="top" aria-label="Method of Lagrangian Multipliers" data-href="Method of Lagrangian Multipliers" href="the-guide/mathematics/optimization/convex-optimization-lecture/method-of-lagrangian-multipliers.html" class="internal-link" target="_self" rel="noopener nofollow">Lagrangian</a> are now vector-valued, i.e. . We get with . The <a data-tooltip-position="top" aria-label="Lagrange Dual Problem" data-href="Lagrange Dual Problem" href="the-guide/mathematics/optimization/convex-optimization-lecture/lagrange-dual-problem.html" class="internal-link" target="_self" rel="noopener nofollow">dual problem</a> yields the dual function  and reads where  denotes the <a data-tooltip-position="top" aria-label="Cones" data-href="Cones" href="the-guide/mathematics/optimization/convex-optimization-lecture/cones.html" class="internal-link" target="_self" rel="noopener nofollow">dual cone</a> of the -th cone.<br>Lower Bound Property
If , then .
]]></description><link>the-guide/mathematics/optimization/convex-optimization-lecture/generalized-inequality-optimization.html</link><guid isPermaLink="false">The Guide/Mathematics/Optimization/Convex Optimization Lecture/Generalized Inequality Optimization.md</guid><pubDate>Mon, 09 Sep 2024 15:35:34 GMT</pubDate></item><item><title><![CDATA[Karush-Kuhn-Tucker Conditions]]></title><description><![CDATA[ 
 <br>In a Nutshell
Generalization of the optimality condition  of unconstrained optimization to the constrained setting. Provide a necessary condition for optimality that can even be sufficient in special cases.
<br><br>Consider the standard <a data-tooltip-position="top" aria-label="Optimization Problem" data-href="Optimization Problem" href="the-guide/mathematics/optimization/convex-optimization-lecture/optimization-problem.html" class="internal-link" target="_self" rel="noopener nofollow">optimization problem</a> (not necessarily <a data-tooltip-position="top" aria-label="Convexity" data-href="Convexity" href="the-guide/mathematics/optimization/convex-optimization-lecture/convexity.html" class="internal-link" target="_self" rel="noopener nofollow">convex</a>) of the form defined on the domain  with optimal value  and differentiable  and .<br>Definition
The KKT conditions for a primal optimum  and dual optimum  of the above problem are ...

<br>Primal Feasibility - The constraints are met, i.e.  and  for  and .
<br>Dual Feasibility - The inequality multipliers are non-negative .
<br><a data-tooltip-position="top" aria-label="Complementary Slackness Condition" data-href="Complementary Slackness Condition" href="the-guide/mathematics/optimization/convex-optimization-lecture/complementary-slackness-condition.html" class="internal-link" target="_self" rel="noopener nofollow">Complementary Slackness</a> - Inequality constraints are either oversatisfied, leading to inactive multipliers or met exactly, leading to positive costs induced by the multipliers .
<br>Stationarity - Similarly to the unconstrained setting, the gradient has to vanish, i.e,

<br>Geometrically, the stationarity condition can be easily visualized by considering the form This essentially says that the active multipliers weight the gradients of the constraints, such that they offset the gradient of the objective (middle and left). If no constraints are violated, the formulation collapses to the regular unconstrained condition (right).<br><img alt="center" src="lib/media/pasted-image-20240502185404.png" style="width: 500px; max-width: 100%;"><br>KKT and Optimality
In the general case, the KKT conditions are well-defined / necessary conditions for optimality of a point, if <a data-href="Constraint Qualifications" href="the-guide/mathematics/optimization/convex-optimization-lecture/constraint-qualifications.html" class="internal-link" target="_self" rel="noopener nofollow">Constraint Qualifications</a> are met.<br>
However, they are not sufficient for either global or local optimality, since the point can be e.g. a saddle point.
<br>For an overview, see the <a data-tooltip-position="top" aria-label="Constraint Qualifications" data-href="Constraint Qualifications" href="the-guide/mathematics/optimization/convex-optimization-lecture/constraint-qualifications.html" class="internal-link" target="_self" rel="noopener nofollow">constraint qualifications note</a>. <br><br><br>Lemma - KKT for Convex Problems
The KKT conditions are necessary and sufficient global optimality conditions for <a data-tooltip-position="top" aria-label="Convexity" data-href="Convexity" href="the-guide/mathematics/optimization/convex-optimization-lecture/convexity.html" class="internal-link" target="_self" rel="noopener nofollow">convex</a> problems with <a data-tooltip-position="top" aria-label="Lagrange Dual Problem" data-href="Lagrange Dual Problem" href="the-guide/mathematics/optimization/convex-optimization-lecture/lagrange-dual-problem.html" class="internal-link" target="_self" rel="noopener nofollow">strong duality</a>, e.g. when Slater's Condition is satisfied.
]]></description><link>the-guide/mathematics/optimization/convex-optimization-lecture/karush-kuhn-tucker-conditions.html</link><guid isPermaLink="false">The Guide/Mathematics/Optimization/Convex Optimization Lecture/Karush-Kuhn-Tucker Conditions.md</guid><pubDate>Wed, 23 Apr 2025 21:55:39 GMT</pubDate><enclosure url="lib/media/pasted-image-20240502185404.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src=&quot;lib/media/pasted-image-20240502185404.png&quot;&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[Lagrange Dual Problem]]></title><description><![CDATA[ 
 <br><br>Based on the reformulated <a data-tooltip-position="top" aria-label="Optimization Problem" data-href="Optimization Problem" href="the-guide/mathematics/optimization/convex-optimization-lecture/optimization-problem.html" class="internal-link" target="_self" rel="noopener nofollow">optimization problem</a> (always assumed a minimization) in form of the <a data-tooltip-position="top" aria-label="Method of Lagrangian Multipliers" data-href="Method of Lagrangian Multipliers" href="the-guide/mathematics/optimization/convex-optimization-lecture/method-of-lagrangian-multipliers.html" class="internal-link" target="_self" rel="noopener nofollow">Lagrangian</a> we can define the dual function  via <br>Definition - Lagrange Dual Problem
This leads to the formulation of the Lagrangian dual problem via 
<br>
<br>Find the tightest = best lower bound on optimal primal function value 
<br>Gives upper bound for maximization problems based on supremum und minimization
<br>Theorem - Concavity of the Dual Function
Because the Lagrangian is affine and therefore <a data-tooltip-position="top" aria-label="Convexity" data-href="Convexity" href="the-guide/mathematics/optimization/convex-optimization-lecture/convexity.html" class="internal-link" target="_self" rel="noopener nofollow">concave</a> in  and  for every choice of , the dual function itself is concave. 
<br>Proof
By the definition of a concave function, we can observe Because the dual function is affine in both  and , we can rearrange term on the right. This yields theThe infimum of a weighted sum is always greater than or equal to the sum of the weighted infima, which leads us to the inequalityexactly the definition of concavity.
<br>Intuition
We are looking for an  that lower bounds the value of the function and the costs induced by violating constraints. However, the original problem theoretically has infinite costs for violating constraints, which is why we take the costs that yield a maximal lower bound.
<br><br><br>Lower Bound Property
If  (all multipliers non-negative), then .
<br>Proof
For any feasible point  and non-negative , we can write simply by definition
<br>Weak Duality
Even if the original problem is not convex, the optimal value of the dual problem  and the optimal function value  of the primal problem fulfill which is denoted weak duality. The difference  is the duality-gap and always non-negative.
<br>Intuition
We approximated the infinite cost with a linear one, which is why we are able to get a "better" solution than the original problem would admit.
<br>In some cases the bound obtained from the <a data-tooltip-position="top" aria-label="Method of Lagrangian Multipliers" data-href="Method of Lagrangian Multipliers" href="the-guide/mathematics/optimization/convex-optimization-lecture/method-of-lagrangian-multipliers.html" class="internal-link" target="_self" rel="noopener nofollow">Lagrangian</a> is tight, in which case we can state another result.<br>Strong Duality
If the duality gap is zero, i.e. we say that strong duality holds. This is only true if additional conditions are satisfied, which are in general denoted <a data-href="Constraint Qualifications" href="the-guide/mathematics/optimization/convex-optimization-lecture/constraint-qualifications.html" class="internal-link" target="_self" rel="noopener nofollow">Constraint Qualifications</a>. In most cases, strong duality holds for <a data-tooltip-position="top" aria-label="Optimization Problem" data-href="Optimization Problem" href="the-guide/mathematics/optimization/convex-optimization-lecture/optimization-problem.html" class="internal-link" target="_self" rel="noopener nofollow">convex optimization problems</a>.
<br>
<br>One simple example of <a data-href="Constraint Qualifications" href="the-guide/mathematics/optimization/convex-optimization-lecture/constraint-qualifications.html" class="internal-link" target="_self" rel="noopener nofollow">Constraint Qualifications</a> is <a data-tooltip-position="top" aria-label="Constraint Qualifications" data-href="Constraint Qualifications" href="the-guide/mathematics/optimization/convex-optimization-lecture/constraint-qualifications.html" class="internal-link" target="_self" rel="noopener nofollow">Slaters condition</a>
]]></description><link>the-guide/mathematics/optimization/convex-optimization-lecture/lagrange-dual-problem.html</link><guid isPermaLink="false">The Guide/Mathematics/Optimization/Convex Optimization Lecture/Lagrange Dual Problem.md</guid><pubDate>Wed, 23 Apr 2025 21:55:39 GMT</pubDate></item><item><title><![CDATA[Linear Program (LP)]]></title><description><![CDATA[ 
 <br>In a Nutshell
Standard form of an <a data-tooltip-position="top" aria-label="Optimization Problem" data-href="Optimization Problem" href="the-guide/mathematics/optimization/convex-optimization-lecture/optimization-problem.html" class="internal-link" target="_self" rel="noopener nofollow">optimization problem</a> with all linear <a data-tooltip-position="top" aria-label="Mathematical Relations" data-href="Mathematical Relations" href="the-guide/mathematics/general-stuff/mathematical-relations.html" class="internal-link" target="_self" rel="noopener nofollow">functions</a> and constraints. 
<br><br>The basic form of the problem can be stated as where any possible constant terms in the objective can be omited. Many special forms of this problem are wide-spread and have been given distinct names. Some are ...<br>Standard Form LP
Encountered if all inequality constraints are component-wise nonnegativity constraints, i.e. 
<br>In this case, the <a data-tooltip-position="top" aria-label="Method of Lagrangian Multipliers" data-href="Method of Lagrangian Multipliers" href="the-guide/mathematics/optimization/convex-optimization-lecture/method-of-lagrangian-multipliers.html" class="internal-link" target="_self" rel="noopener nofollow">Lagrangian</a> can be formulated as yielding the <a data-tooltip-position="top" aria-label="Lagrange Dual Problem" data-href="Lagrange Dual Problem" href="the-guide/mathematics/optimization/convex-optimization-lecture/lagrange-dual-problem.html" class="internal-link" target="_self" rel="noopener nofollow">dual function</a> The associated dual problem reads For a feasible dual problem, we have to incorporte the <a data-tooltip-position="top" aria-label="Lagrange Dual Problem" data-href="Lagrange Dual Problem" href="the-guide/mathematics/optimization/convex-optimization-lecture/lagrange-dual-problem.html" class="internal-link" target="_self" rel="noopener nofollow">lower bound property</a> which states  if . The constraint is adapted to<br><br>Inequality Form LP
Encountered if the problem has only inequality constraints, i.e. 
<br>In this case, the <a data-tooltip-position="top" aria-label="Method of Lagrangian Multipliers" data-href="Method of Lagrangian Multipliers" href="the-guide/mathematics/optimization/convex-optimization-lecture/method-of-lagrangian-multipliers.html" class="internal-link" target="_self" rel="noopener nofollow">Lagrangian</a> can be formulated as yielding the <a data-tooltip-position="top" aria-label="Lagrange Dual Problem" data-href="Lagrange Dual Problem" href="the-guide/mathematics/optimization/convex-optimization-lecture/lagrange-dual-problem.html" class="internal-link" target="_self" rel="noopener nofollow">dual function</a> The associated dual problem reads Here, there was no way to incorporate the lower bound property, which is why it is stated explicitly.<br><br><br>There is a very intuitive interpretation of the LP as a resource allocation problem that is especially useful in <a data-tooltip-position="top" aria-label="Optimal Transport" data-href="Optimal Transport" href="the-guide/mathematics/optimal-transport/optimal-transport.html" class="internal-link" target="_self" rel="noopener nofollow">optimal transport</a>. For the primal problem<br>
<br> is the vector of produced goods, each entry encoding a different product

<br>Cannot be negative 


<br> is the (in our general <a data-tooltip-position="top" aria-label="Optimization Problem" data-href="Optimization Problem" href="the-guide/mathematics/optimization/convex-optimization-lecture/optimization-problem.html" class="internal-link" target="_self" rel="noopener nofollow">framework</a> negative) vector of prices per unit of good
<br> is the vector encoding available raw materials
<br> encodes which product need which raw material<br>
This means that in the standard form LP, we want to maximize our earnings based on our available resources.
<br>The dual problem corresponds to a setting where the factory considers selling raw material and another factory wants to make an offer minimizing its spendings while making the deal profitable for the seller.<br>
<br>The Lagrange multipliers  encode the price for a unit of raw material
<br>The price should not be negative
<br>The seller should not loose money on the deal
<br>Weak duality now means that the seller will make at least as much revenue by selling the raw materials than when producing the goods and selling them afterwards. In strong duality, we get the "equilibrium price".]]></description><link>the-guide/mathematics/optimization/convex-optimization-lecture/linear-program-(lp).html</link><guid isPermaLink="false">The Guide/Mathematics/Optimization/Convex Optimization Lecture/Linear Program (LP).md</guid><pubDate>Mon, 09 Sep 2024 15:35:34 GMT</pubDate></item><item><title><![CDATA[Method of Lagrangian Multipliers]]></title><description><![CDATA[ 
 <br>In a Nutshell
Standard approach for constrained optimization to obtain a formulation that is approachable by the means of analysis.
<br><br>Given a standard <a data-tooltip-position="top" aria-label="Optimization Problem" data-href="Optimization Problem" href="the-guide/mathematics/optimization/convex-optimization-lecture/optimization-problem.html" class="internal-link" target="_self" rel="noopener nofollow">optimization problem</a> (not necessarily <a data-tooltip-position="top" aria-label="Convexity" data-href="Convexity" href="the-guide/mathematics/optimization/convex-optimization-lecture/convexity.html" class="internal-link" target="_self" rel="noopener nofollow">convex</a>) of the form defined on the domain  with optimal value  and differentiable , we can approximate indicator functions  that punish violation of the constraints by infinite values via linear functions. This way, we are intuitively introduce a price for violating constraints. <br>Definition
This yields the Lagrangian with the so-called Lagrange multipliers encoding ... 

<br> is the price for violating  ().
<br> is the price for violating  ().

]]></description><link>the-guide/mathematics/optimization/convex-optimization-lecture/method-of-lagrangian-multipliers.html</link><guid isPermaLink="false">The Guide/Mathematics/Optimization/Convex Optimization Lecture/Method of Lagrangian Multipliers.md</guid><pubDate>Mon, 09 Sep 2024 15:35:34 GMT</pubDate></item><item><title><![CDATA[Optimization Problem]]></title><description><![CDATA[<a class="tag" href="?query=tag:Convex-Optimization-Course" style="background-color: rgb(4, 108, 116); color: white; font-weight: 700; border: none; border-radius: 1em; padding: 0.2em 0.5em;">#Convex-Optimization-Course</a> 
 <br>In a Nutshell
Standard form of optimization problems as introduces in the Convex Optimization lecture ( <a href=".?query=tag:Convex-Optimization-Course" class="tag" target="_blank" rel="noopener nofollow">#Convex-Optimization-Course</a> ).
<br><br>Definition
The standard form of an optimization problem isdefined on the domain  and . 
<br>A point  is denoted feasible, if  and it fulfills all the constraints, it lies inside the feasible set  created by the constraints. This point is then denoted . If a problem has at least one feasible point, it is a feasible problem.<br>The optimal function value is where we say that the problem is infeasible if  and unbounded below if <br>
<br>The optimal solution is obtained, if a feasible  also fulfills . There can be multiple optimal solutions.
<br>A feasible point is locally optimal, if it is an optimal solution for the problem with an additional vicinity constraint, e.g. restriction to a subdomain or a norm constraint
<br>Note that maximizing any function can be translated into minimizing the negative objective.<br><br><br>A special class of optimization problems are those where the constraint set and the objective <a data-tooltip-position="top" aria-label="Mathematical Relations" data-href="Mathematical Relations" href="the-guide/mathematics/general-stuff/mathematical-relations.html" class="internal-link" target="_self" rel="noopener nofollow">function</a> are <a data-tooltip-position="top" aria-label="Convexity" data-href="Convexity" href="the-guide/mathematics/optimization/convex-optimization-lecture/convexity.html" class="internal-link" target="_self" rel="noopener nofollow">convex</a>. In this case, we often denote the feasible set as . This property will allow us to draw global conclusions from local properties. Mathematically, we demand that<br>
<br> is convex
<br> are convex
<br>The equality constraints are <a data-tooltip-position="top" aria-label="Affine Sets" data-href="Affine Sets" href="the-guide/mathematics/optimization/convex-optimization-lecture/affine-sets.html" class="internal-link" target="_self" rel="noopener nofollow">affine</a> 
<br>First-order Optimality Condition for Convex Sets
A feasible point in a convex constraint set  is feasible iff  for all feasible . 
<br><img alt="center" src="lib/media/pasted-image-20240425193042.png" style="width: 400px; max-width: 100%;"><br>
<br>Gradient in direction of steepest descent, if the euclidean inner product is negative, there exist points that are in the opposite direction and therefore "better".
]]></description><link>the-guide/mathematics/optimization/convex-optimization-lecture/optimization-problem.html</link><guid isPermaLink="false">The Guide/Mathematics/Optimization/Convex Optimization Lecture/Optimization Problem.md</guid><pubDate>Mon, 09 Sep 2024 15:35:34 GMT</pubDate><enclosure url="lib/media/pasted-image-20240425193042.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src=&quot;lib/media/pasted-image-20240425193042.png&quot;&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[Pseudo- and Quasiconvexity]]></title><description><![CDATA[ 
 <br>In a Nutshell
Generalizations of the notion of <a data-tooltip-position="top" aria-label="Convexity" data-href="Convexity" href="the-guide/mathematics/optimization/convex-optimization-lecture/convexity.html" class="internal-link" target="_self" rel="noopener nofollow">convexity</a> that in the case if pseudoconvexity inherit enough properties to be useful in <a data-tooltip-position="top" aria-label="Optimization Problem" data-href="Optimization Problem" href="the-guide/mathematics/optimization/convex-optimization-lecture/optimization-problem.html" class="internal-link" target="_self" rel="noopener nofollow">optimization</a> contexts.
<br><img alt="center" src="lib/media/pasted-image-20240704115831.png" style="width: 450px; max-width: 100%;"><br><br><br>Pseudoconvex Functions
A <a data-tooltip-position="top" aria-label="Mathematical Relations" data-href="Mathematical Relations" href="the-guide/mathematics/general-stuff/mathematical-relations.html" class="internal-link" target="_self" rel="noopener nofollow">function</a>  is pseudoconvex, if following the gradient towards smaller function values will always decrease the function. There are no small dips / local minima in the function landscape, i.e. any local minimum is also global.
<br>Follows from contradiction: Assume a <a data-tooltip-position="top" aria-label="Stationary Points" data-href="Stationary Points" href="the-guide/mathematics/optimization/convex-optimization-lecture/stationary-points.html" class="internal-link" target="_self" rel="noopener nofollow">stationary point</a>  and another global minimizer . We can then directly derive <img alt="center" src="lib/media/pasted-image-20240812191603.png" style="width: 200px; max-width: 100%;"><br>Warning
While pseudoconvex functions inherit the global minimization property, they behave differently regarding preservation under operations. For example, the sum of pseudoconvex functions is not necessarily pseudoconvex.
<br><br><br>Quasiconvex Functions
A function  is (strictly) quasiconvex / unimodal, if meaning that every point between two fixed points is smaller or equal than the maximum of the points.
<br><img alt="center" src="lib/media/pasted-image-20240812191735.png" style="width: 200px; max-width: 100%;"><br>Definition via Sublevel Sets
A <a data-tooltip-position="top" aria-label="Mathematical Relations" data-href="Mathematical Relations" href="the-guide/mathematics/general-stuff/mathematical-relations.html" class="internal-link" target="_self" rel="noopener nofollow">function</a>  is called quasiconvex or unimodal, if its domain  and all its sublevel sets are <a data-tooltip-position="top" aria-label="Convexity" data-href="Convexity" href="the-guide/mathematics/optimization/convex-optimization-lecture/convexity.html" class="internal-link" target="_self" rel="noopener nofollow">convex</a>. A function is quasiconcave, if  is quasiconvex or in other words the superlevel sets to be convex instead.. A function that is both is denoted quasilinear, i.e. for every level <a data-href="Set" href="the-guide/mathematics/general-stuff/set.html" class="internal-link" target="_self" rel="noopener nofollow">Set</a> we have equality.
<br><img alt="center" src="lib/media/pasted-image-20240507171939.png" style="width: 350px; max-width: 100%;"><br>For the function above we have the examples of the sublevel <a data-tooltip-position="top" aria-label="Set" data-href="Set" href="the-guide/mathematics/general-stuff/set.html" class="internal-link" target="_self" rel="noopener nofollow">set</a>  and . Both are (possibly infinite) intervals and therefore convex.<br>A well-known inequality for quasiconvex functions is sometimes refered to as <a data-href="Jensen's Inequality" href="the-guide/information-theory/information-theory-1/information/jensen's-inequality.html" class="internal-link" target="_self" rel="noopener nofollow">Jensen's Inequality</a> for quasiconvex functions. It states that such a function has a convex domain  and fulfills for a .<br>First-order Condition
A differentiable function  with convex domain  is quasiconvex, if for any  that fulfill  it holds thatIntuitively, this means that the gradient at any point defines a supporting hyperplane to the sublevel <a data-href="Set" href="the-guide/mathematics/general-stuff/set.html" class="internal-link" target="_self" rel="noopener nofollow">Set</a>.
<br><img alt="center" src="lib/media/pasted-image-20240507174111.png" style="width: 350px; max-width: 100%;"><br>Second-order Condition
If a twice differentiable function  is quasiconvex, then for every  and all  with , we have For a function on  this reduces to the second derivative being non-negative at points with vanishing first derivative.
<br><br>
<br>Non-negative weighted maxima of quasiconvex functions are quasiconvex 
<br>Compositions  of a quasiconvex function  with nondecreasing function  is quasiconvex.
<br>Minimization - If  is jointly quasiconvex in both arguments and  is a convex set, then  is quasiconvex.
]]></description><link>the-guide/mathematics/optimization/convex-optimization-lecture/pseudo-and-quasiconvexity.html</link><guid isPermaLink="false">The Guide/Mathematics/Optimization/Convex Optimization Lecture/Pseudo- and Quasiconvexity.md</guid><pubDate>Sun, 09 Feb 2025 13:41:04 GMT</pubDate><enclosure url="lib/media/pasted-image-20240704115831.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src=&quot;lib/media/pasted-image-20240704115831.png&quot;&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[Quadratic Program (QP)]]></title><description><![CDATA[ 
 <br>In a Nutshell
Standard form of an <a data-tooltip-position="top" aria-label="Optimization Problem" data-href="Optimization Problem" href="the-guide/mathematics/optimization/convex-optimization-lecture/optimization-problem.html" class="internal-link" target="_self" rel="noopener nofollow">optimization problem</a> with quadratic <a data-tooltip-position="top" aria-label="Mathematical Relations" data-href="Mathematical Relations" href="the-guide/mathematics/general-stuff/mathematical-relations.html" class="internal-link" target="_self" rel="noopener nofollow">objective function</a> and linear constraints.
<br><br>Standard Form QP
Assuming a symmetric and <a data-tooltip-position="top" aria-label="Matrices" data-href="Matrices" href="the-guide/mathematics/linear-algebra/matrices.html" class="internal-link" target="_self" rel="noopener nofollow">positive definite matrix</a> , the problem has the form 
<br>In this case, the <a data-tooltip-position="top" aria-label="Method of Lagrangian Multipliers" data-href="Method of Lagrangian Multipliers" href="the-guide/mathematics/optimization/convex-optimization-lecture/method-of-lagrangian-multipliers.html" class="internal-link" target="_self" rel="noopener nofollow">Lagrangian</a> can be formulated as yielding the <a data-tooltip-position="top" aria-label="Lagrange Dual Problem" data-href="Lagrange Dual Problem" href="the-guide/mathematics/optimization/convex-optimization-lecture/lagrange-dual-problem.html" class="internal-link" target="_self" rel="noopener nofollow">dual function</a> <br>Dual Problem
The associated dual problem reads 
]]></description><link>the-guide/mathematics/optimization/convex-optimization-lecture/quadratic-program-(qp).html</link><guid isPermaLink="false">The Guide/Mathematics/Optimization/Convex Optimization Lecture/Quadratic Program (QP).md</guid><pubDate>Mon, 09 Sep 2024 15:35:34 GMT</pubDate></item><item><title><![CDATA[Semidefinite Program (SDP)]]></title><description><![CDATA[ 
 <br>In a Nutshell
Standardform of a <a data-tooltip-position="top" aria-label="Convexity" data-href="Convexity" href="the-guide/mathematics/optimization/convex-optimization-lecture/convexity.html" class="internal-link" target="_self" rel="noopener nofollow">convex</a> <a data-tooltip-position="top" aria-label="Optimization Problem" data-href="Optimization Problem" href="the-guide/mathematics/optimization/convex-optimization-lecture/optimization-problem.html" class="internal-link" target="_self" rel="noopener nofollow">optimization problem</a> with <a data-tooltip-position="top" aria-label="Generalized Inequality Optimization" data-href="Generalized Inequality Optimization" href="the-guide/mathematics/optimization/convex-optimization-lecture/generalized-inequality-optimization.html" class="internal-link" target="_self" rel="noopener nofollow">generalized inequalities</a>.
<br><br>Problem Definition
With  and <a data-tooltip-position="top" aria-label="Matrices" data-href="Matrices" href="the-guide/mathematics/linear-algebra/matrices.html" class="internal-link" target="_self" rel="noopener nofollow">symmetric</a>  and , the problem reads
<br>Based on the implications of <a data-tooltip-position="top" aria-label="Generalized Inequality Optimization" data-href="Generalized Inequality Optimization" href="the-guide/mathematics/optimization/convex-optimization-lecture/generalized-inequality-optimization.html" class="internal-link" target="_self" rel="noopener nofollow">generalized inequalities</a>, there will be a vector of <a data-tooltip-position="top" aria-label="Method of Lagrangian Multipliers" data-href="Method of Lagrangian Multipliers" href="the-guide/mathematics/optimization/convex-optimization-lecture/method-of-lagrangian-multipliers.html" class="internal-link" target="_self" rel="noopener nofollow">Lagrangian multipliers</a> for each . For convenience of notation and implementation, these are encoded in a <a data-tooltip-position="top" aria-label="Matrices" data-href="Matrices" href="the-guide/mathematics/linear-algebra/matrices.html" class="internal-link" target="_self" rel="noopener nofollow">symmetric matrix</a> . With this , the Lagrangian function is Since the <a data-tooltip-position="top" aria-label="Matrices" data-href="Matrices" href="the-guide/mathematics/linear-algebra/matrices.html" class="internal-link" target="_self" rel="noopener nofollow">trace</a> operator is linear, we can reformulate this as This way, we can easily see that the dual problem is The dual problem needs to add the <a data-tooltip-position="top" aria-label="Generalized Inequality Optimization" data-href="Generalized Inequality Optimization" href="the-guide/mathematics/optimization/convex-optimization-lecture/generalized-inequality-optimization.html" class="internal-link" target="_self" rel="noopener nofollow">Lower bound property for generalized inequalities</a>, i.e. ]]></description><link>the-guide/mathematics/optimization/convex-optimization-lecture/semidefinite-program-(sdp).html</link><guid isPermaLink="false">The Guide/Mathematics/Optimization/Convex Optimization Lecture/Semidefinite Program (SDP).md</guid><pubDate>Mon, 09 Sep 2024 15:35:34 GMT</pubDate></item><item><title><![CDATA[Stationary Points]]></title><description><![CDATA[ 
 <br>Definition
Given a <a data-tooltip-position="top" aria-label="Convexity" data-href="Convexity" href="the-guide/mathematics/optimization/convex-optimization-lecture/convexity.html" class="internal-link" target="_self" rel="noopener nofollow">convex</a> <a data-tooltip-position="top" aria-label="Optimization Problem" data-href="Optimization Problem" href="the-guide/mathematics/optimization/convex-optimization-lecture/optimization-problem.html" class="internal-link" target="_self" rel="noopener nofollow">feasible</a> <a data-tooltip-position="top" aria-label="Set" data-href="Set" href="the-guide/mathematics/general-stuff/set.html" class="internal-link" target="_self" rel="noopener nofollow">set</a>  and a <a data-tooltip-position="top" aria-label="Mathematical Relations" data-href="Mathematical Relations" href="the-guide/mathematics/general-stuff/mathematical-relations.html" class="internal-link" target="_self" rel="noopener nofollow">function</a>  with  over which to optimize, a feasible point  that satisfies the first-order optimality condition is a stationary point. For , the condition reduces to the regular .
<br><img alt="center" src="lib/media/pasted-image-20240716134718.png" style="width: 300px; max-width: 100%;"><br>Taking the difference quotient, i.e. the directional derivative, we see that <br>Lemma
For <a data-tooltip-position="top" aria-label="Convexity" data-href="Convexity" href="the-guide/mathematics/optimization/convex-optimization-lecture/convexity.html" class="internal-link" target="_self" rel="noopener nofollow">convex</a> <a data-tooltip-position="top" aria-label="Optimization Problem" data-href="Optimization Problem" href="the-guide/mathematics/optimization/convex-optimization-lecture/optimization-problem.html" class="internal-link" target="_self" rel="noopener nofollow">optimization problems</a> the above condition is sufficient, for general problems it is necessary.
]]></description><link>the-guide/mathematics/optimization/convex-optimization-lecture/stationary-points.html</link><guid isPermaLink="false">The Guide/Mathematics/Optimization/Convex Optimization Lecture/Stationary Points.md</guid><pubDate>Mon, 09 Sep 2024 15:35:34 GMT</pubDate><enclosure url="lib/media/pasted-image-20240716134718.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src=&quot;lib/media/pasted-image-20240716134718.png&quot;&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[Transforming Optimization Problems]]></title><description><![CDATA[ 
 <br>How to show Equivalency
Consider two general <a data-tooltip-position="top" aria-label="Optimization Problem" data-href="Optimization Problem" href="the-guide/mathematics/optimization/convex-optimization-lecture/optimization-problem.html" class="internal-link" target="_self" rel="noopener nofollow">optimization problems</a> of the general form, one with variable  and the other with variable . If these variables are related by a <a data-tooltip-position="top" aria-label="Mathematical Relations" data-href="Mathematical Relations" href="the-guide/mathematics/general-stuff/mathematical-relations.html" class="internal-link" target="_self" rel="noopener nofollow">mapping</a> , the problems are equivalent if the solution <a data-href="Set" href="the-guide/mathematics/general-stuff/set.html" class="internal-link" target="_self" rel="noopener nofollow">Set</a> of one problem can be computed from the solution <a data-href="Set" href="the-guide/mathematics/general-stuff/set.html" class="internal-link" target="_self" rel="noopener nofollow">Set</a> of the other and vice versa. Practically, we'd have to show that ...

<br>From every , we can compute a feasible , yielding 
<br>From every , we can compute a feasible , yielding 

<br><br><br>
<br>Change of Variables

<br>...


<br>Transformation of Objective and Constraints 

<br>...
<br>Especially for complex variables, it might be applicable to shift the phase of any objective without affecting the problem. If the objective function and the constraints only incorporate absolute values as in <a data-href="Downlink Beamforming Optimization" href="the-guide/mathematics/optimization/convex-optimization-lecture/applications-in-optimization/downlink-beamforming-optimization.html" class="internal-link" target="_self" rel="noopener nofollow">Downlink Beamforming Optimization</a>, we can assume any phase shift to add constraints. Similarly, for a complex <a data-tooltip-position="top" aria-label="Bilinear Form" data-href="Bilinear Form" href="the-guide/mathematics/linear-algebra/bilinear-form.html" class="internal-link" target="_self" rel="noopener nofollow">bilinear form</a> the shift would simply offset and yield the same cost 


<br>Introducing Slack Variables
This trick enables us to transform any inequality into an equality constraint at the cost of additional variables over which to optimize. For any , we can say that the constraint is only satisfied, if there is an , such that This creates a new variable and condition for each inequality constraint.
<br>
<br>Eliminating Equality Constraints
]]></description><link>the-guide/mathematics/optimization/convex-optimization-lecture/transforming-optimization-problems.html</link><guid isPermaLink="false">The Guide/Mathematics/Optimization/Convex Optimization Lecture/Transforming Optimization Problems.md</guid><pubDate>Mon, 14 Apr 2025 15:50:57 GMT</pubDate></item><item><title><![CDATA[ADMM]]></title><description><![CDATA[ 
 <br>*Alternating Direction Method of Multipliers*
Solves problems of the form with  and  <a data-tooltip-position="top" aria-label="Convexity" data-href="Convexity" href="the-guide/mathematics/optimization/convex-optimization-lecture/convexity.html" class="internal-link" target="_self" rel="noopener nofollow">convex functions</a> and   closed convex <a data-tooltip-position="top" aria-label="Set" data-href="Set" href="the-guide/mathematics/general-stuff/set.html" class="internal-link" target="_self" rel="noopener nofollow">sets</a>. Adding another quadratic penalty term to the <a data-tooltip-position="top" aria-label="Method of Lagrangian Multipliers" data-href="Method of Lagrangian Multipliers" href="the-guide/mathematics/optimization/convex-optimization-lecture/method-of-lagrangian-multipliers.html" class="internal-link" target="_self" rel="noopener nofollow">Lagrangian</a> yields the augmented Lagrangian 

<br>Scalar penalty term 
<br>Additional quadratic penalty 

<br>Algorithm

<br>For each iteration

<br>
<br>
<br>

<br>Maximizing, bring underestimated solution closer to solution of original problem





]]></description><link>the-guide/mathematics/optimization/admm.html</link><guid isPermaLink="false">The Guide/Mathematics/Optimization/ADMM.md</guid><pubDate>Thu, 15 Aug 2024 12:51:15 GMT</pubDate></item><item><title><![CDATA[Bayesian Optimization]]></title><description><![CDATA[ 
 <br>In a Nutshell
Strategy for optimizing a black-box <a data-tooltip-position="top" aria-label="Mathematical Relations" data-href="Mathematical Relations" href="the-guide/mathematics/general-stuff/mathematical-relations.html" class="internal-link" target="_self" rel="noopener nofollow">function</a> that is expensive to evaluate. 
<br><br><br>
<br>Probabilistic Surrogate Model - in most cases, a <a data-tooltip-position="top" aria-label="Gaussian Process" data-href="Gaussian Process" href="the-guide/mathematics/probability-theory/gaussian-process.html" class="internal-link" target="_self" rel="noopener nofollow">GP</a> serves as a proxy for unknown true distribution / objective function
<br>Aquisition Function - A function that determines the next sampling point by considering the <a data-tooltip-position="top" aria-label="Exploration-Exploitation Trade-Off" data-href="Exploration-Exploitation Trade-Off" href="the-guide/machine-learning/reinforcement-learning/exploration-exploitation-trade-off.html" class="internal-link" target="_self" rel="noopener nofollow">trade-off between exploration and exploitation</a>.
<br>Optimization - The next point is chosen by optimizing the acquisition function with respect to the surrogate model.
<br><br><br>Criterion to pick new evaluation point based on <a data-tooltip-position="top" aria-label="Bayes Theorem" data-href="Bayes Theorem" href="the-guide/mathematics/statistics/bayes-theorem.html" class="internal-link" target="_self" rel="noopener nofollow">posterior</a> / surrogate model . Common choices are ...<br>Probability of Improvement (PI) 
The Probability of Improvement acquisition function aims to find the next point that has the highest probability of improving over the current best observationwhere  is a small positive number that enforces exploration. For the common choice of <a data-tooltip-position="top" aria-label="Gaussian Process" data-href="Gaussian Process" href="the-guide/mathematics/probability-theory/gaussian-process.html" class="internal-link" target="_self" rel="noopener nofollow">GPs</a>, this results inwhere  and  are the posterior mean and standard deviation and  is the <a data-tooltip-position="top" aria-label="Cumulative Distribution Function" data-href="Cumulative Distribution Function" href="the-guide/mathematics/probability-theory/cumulative-distribution-function.html" class="internal-link" target="_self" rel="noopener nofollow">cdf</a> of the <a data-tooltip-position="top" aria-label="Gaussian Distribution" data-href="Gaussian Distribution" href="the-guide/mathematics/probability-theory/gaussian-distribution.html" class="internal-link" target="_self" rel="noopener nofollow">standard normal distribution</a>.
<br>Expected Improvement (EI)
Tries to balance between exploring uncertain areas and exploiting already high values. The general function is given by:  For Gaussian Processes, the EI can be written as: where  is the <a data-tooltip-position="top" aria-label="Probability Density Function" data-href="Probability Density Function" href="the-guide/mathematics/probability-theory/probability-density-function.html" class="internal-link" target="_self" rel="noopener nofollow">pdf</a> and  the <a data-tooltip-position="top" aria-label="Cumulative Distribution Function" data-href="Cumulative Distribution Function" href="the-guide/mathematics/probability-theory/cumulative-distribution-function.html" class="internal-link" target="_self" rel="noopener nofollow">cdf</a> of the standard normal distribution. 
<br>Upper Confidence Bound (UCB)
The Upper Confidence Bound (UCB) acquisition function is defined as: where  is a constant that controls the trade-off between exploration and exploitation. It thereby simply balances predicted mean and uncertainty
<br>Thompson Sampling (TS) 
Selects the next sample based on the maximizer of a sample from the posterior distribution: where  is a sample from the posterior distribution of the surrogate model.
]]></description><link>the-guide/mathematics/optimization/bayesian-optimization.html</link><guid isPermaLink="false">The Guide/Mathematics/Optimization/Bayesian Optimization.md</guid><pubDate>Thu, 12 Dec 2024 18:23:21 GMT</pubDate></item><item><title><![CDATA[Bourbaki-Cheney-Goldstein Inequality]]></title><description><![CDATA[ 
 <br><br>In the context of <a data-tooltip-position="top" aria-label="Optimization Problem" data-href="Optimization Problem" href="the-guide/mathematics/optimization/convex-optimization-lecture/optimization-problem.html" class="internal-link" target="_self" rel="noopener nofollow">optimization</a> with a <a data-tooltip-position="top" aria-label="Convexity" data-href="Convexity" href="the-guide/mathematics/optimization/convex-optimization-lecture/convexity.html" class="internal-link" target="_self" rel="noopener nofollow">convex</a> constraint <a data-tooltip-position="top" aria-label="Set" data-href="Set" href="the-guide/mathematics/general-stuff/set.html" class="internal-link" target="_self" rel="noopener nofollow">set</a> , every point  together with a point  and the euclidean projection  of that point onto the boundary of the set fulfill<img alt="center" src="lib/media/pasted-image-20240428163510.png" style="width: 200px; max-width: 100%;">]]></description><link>the-guide/mathematics/optimization/bourbaki-cheney-goldstein-inequality.html</link><guid isPermaLink="false">The Guide/Mathematics/Optimization/Bourbaki-Cheney-Goldstein Inequality.md</guid><pubDate>Tue, 16 Jul 2024 13:47:16 GMT</pubDate><enclosure url="lib/media/pasted-image-20240428163510.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src=&quot;lib/media/pasted-image-20240428163510.png&quot;&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[Distributed Consensus]]></title><description><![CDATA[ 
 <br>In a Nutshell
Algorithms that enable agents in an <a data-tooltip-position="top" aria-label="Graph" data-href="Graph" href="the-guide/mathematics/graph-theory/graph.html" class="internal-link" target="_self" rel="noopener nofollow">undirected network</a> to reach a consensus via gossiping.
<br><br>
<br>Instead of a central server, all participants reach a conclusion by sharing with neighbours.

<br>Advantages

<br>Fully distributed
<br>Tolerant to failure of nodes
<br>Can adapt to changing values


<br>Disadvantages

<br>Iterative, convergence only over time and asymptotically
<br>Communication costs




<br>

<br>For undirected graphs (here also unweighted)
<br>Consensus by minimizing disagreement (<a data-tooltip-position="top" aria-label="Total Variation" data-href="Total Variation" href="the-guide/integral-transforms-and-signals/total-variation.html" class="internal-link" target="_self" rel="noopener nofollow">total variation</a> or <a data-tooltip-position="top" aria-label="Graph Laplacians" data-href="Graph Laplacians" href="the-guide/mathematics/graph-theory/graph-laplacians.html" class="internal-link" target="_self" rel="noopener nofollow">laplacian potential</a>)
<br>Iterative updating via steepest descent

<br>

<br>Update step-size 
<br>-th node performs 
<br>To ensure convergence 

<br>E.g. 








<br>

<br>We want update <a data-tooltip-position="top" aria-label="Matrices" data-href="Matrices" href="the-guide/mathematics/linear-algebra/matrices.html" class="internal-link" target="_self" rel="noopener nofollow">matrix</a> such that  ().
<br>Only holds if

<br>Matrix is double stochastic

<br>
<br>


<br>The spectral radius  is

<br> 




<br>Convergence Factors

<br>Asymptotic Convergence Factor

<br>


<br>Per-Step Convergence Factor

<br>




<br>Designing such a matrix is <a data-tooltip-position="top" aria-label="Convexity" data-href="Convexity" href="the-guide/mathematics/optimization/convex-optimization-lecture/convexity.html" class="internal-link" target="_self" rel="noopener nofollow">non-convex</a> optimization problem, difficult to solve (assuming symmetric matrix leads to convexity)
<br>Heuristic based on <a data-tooltip-position="top" aria-label="Graph Laplacians" data-href="Graph Laplacians" href="the-guide/mathematics/graph-theory/graph-laplacians.html" class="internal-link" target="_self" rel="noopener nofollow">Laplacian</a>

<br>

<br> are the weights of the edges
<br>If  we have AC protocol






<br>

<br>Init each <a data-tooltip-position="top" aria-label="Node or Vertex Set" data-href="Node or Vertex Set" href="the-guide/mathematics/graph-theory/node-or-vertex-set.html" class="internal-link" target="_self" rel="noopener nofollow">node</a> with sum  and weight .
<br>At each time step

<br> are all pairs sent to -th node a step 
<br> and 
<br>Uniform Gossip

<br>Choose target  randomly
<br>Send pair  to target and itself


<br>Weighted Gossip

<br>Send share  to all neighbors


<br>Algorithm in vector form 

<br>
<br>

<br>e.g. 








<br>

<br>We can construct a sequence of update matrices  that reaches consensus in final time (Push-Sum and AC need infinite time)
<br>Take adaptive step size according to eigenvalues via to reach consensus after  steps.
<br>Design low pass <a data-tooltip-position="top" aria-label="Graph Filters" data-href="Graph Filters" href="the-guide/mathematics/graph-theory/graph-filters.html" class="internal-link" target="_self" rel="noopener nofollow">filter</a> with 

<br>Number of iterations to reach average consensus controlled by filter order
<br>Lower filter order does not result in earlier convergence, as this can led to bad polynomial Fitting
<br>Higher order does not always improve final accuracy because of overfitting




<br><br>
<br>Using inverse <a data-tooltip-position="top" aria-label="Graph Fourier Transform" data-href="Graph Fourier Transform" href="the-guide/mathematics/graph-theory/graph-fourier-transform.html" class="internal-link" target="_self" rel="noopener nofollow">GFT</a>, the average consensus algorithm can be rewritten as  with the <a data-tooltip-position="top" aria-label="Graph Filters" data-href="Graph Filters" href="the-guide/mathematics/graph-theory/graph-filters.html" class="internal-link" target="_self" rel="noopener nofollow">filter</a> 
<br>The protocol reaches average consensus in finite time, if  for .
<br>The protocol reaches asymptotical average consensus if  for .

<br>Only possible if .
<br>Filter needs to be low pass filter


]]></description><link>the-guide/mathematics/optimization/distributed-consensus.html</link><guid isPermaLink="false">The Guide/Mathematics/Optimization/Distributed Consensus.md</guid><pubDate>Sat, 05 Apr 2025 14:09:55 GMT</pubDate></item><item><title><![CDATA[Distributed Optimization]]></title><description><![CDATA[ 
 <br><br>
<br>Conditions

<br>Twice-differentiable
<br>-strongly <a data-tooltip-position="top" aria-label="Convexity" data-href="Convexity" href="the-guide/mathematics/optimization/convex-optimization-lecture/convexity.html" class="internal-link" target="_self" rel="noopener nofollow">convex</a> 

<br>Guarantees unique optimum, guards against ill-conditioning
<br>Can be formulated via 

<br>Minimum curvature  / eigenvalue in every dimension, positive semidefinite




<br>Gradient vector is -Lipschitz

<br>
<br>alt. formulation 




<br>Convergence

<br>If step size is chosen as  and conditions satisfied, then

<br>
<br>


<br>Also possible to choose iteration-dependant step size, two conditions

<br>
<br>




<br>Approximation of Gradient 

<br>E.g. <a data-tooltip-position="top" aria-label="Gradient Descent" data-href="Gradient Descent" href="the-guide/mathematics/optimization/convex-optimization-lecture/algorithms/gradient-descent.html" class="internal-link" target="_self" rel="noopener nofollow">Stochastic Gradient Descent</a>
<br>Conditions on Gradient Noise 

<br>Non-biased, fluctuates around true value 
<br>Bounded, but non-zero variance 

<br>Noise decreases closer to solution, but does not vanish






<br>Examples

<br>Mean-Square-Error Costs

<br>  
<br>Past values , prediction e.g. of future value by linear filter 




<br><br>
<br>Problem

<br>Find 


<br>**<a data-href="Gradient Descent" href="the-guide/mathematics/optimization/convex-optimization-lecture/algorithms/gradient-descent.html" class="internal-link" target="_self" rel="noopener nofollow">Gradient Descent</a>

<br>
<br>E.g. for the Mean-Squared-Error Costs above

<br>




<br>Stochastic Gradient Descent

<br>

<br> is approximation of 
<br>Here  and 




<br><br>
<br>Collection of  agents (<a data-tooltip-position="top" aria-label="Node or Vertex Set" data-href="Node or Vertex Set" href="the-guide/mathematics/graph-theory/node-or-vertex-set.html" class="internal-link" target="_self" rel="noopener nofollow">nodes</a> in a <a data-tooltip-position="top" aria-label="Graph" data-href="Graph" href="the-guide/mathematics/graph-theory/graph.html" class="internal-link" target="_self" rel="noopener nofollow">graph</a>) with individual cost function (data and/or structure). Problem can be stated as
<br>In this case only one cost functions needs to be strongly <a data-tooltip-position="top" aria-label="Convexity" data-href="Convexity" href="the-guide/mathematics/optimization/convex-optimization-lecture/convexity.html" class="internal-link" target="_self" rel="noopener nofollow">convex</a>, while all others only need to be convex.

<br>Centralized Stochastic Gradient Descent

<br>




<br>Incremental Strategy

<br>Determine cyclic path through all agents
<br>Step through cycle, each agent reveices from previous and updates according to own cost function.

<br>


<br>Drawbacks

<br>All agents need update after each cycle
<br>Cycle hard to determine in large graphs (NP)
<br>Limited cooperation, as each agent sends and receives only once per iteration
<br>Sensitive to link or agent failures (new path needed)




<br>Consensus Strategy

<br>Instead of relying on previous node, rely on all neighbouring nodes

<br>Replace global  by combinations of neighbours
<br>Replace global  in gradient by local information


<br>Decentralize update   and 

<br>

<br>Step size  is agent-dependant


<br>Combination Coefficients for each agent satisfy

<br>
<br>
<br> if 


<br>Combination Policies for coefficients

<br>Average Rule
<br>Hasting Rule
<br>Relative-Degree Rule
<br>Laplacian Rules
<br>Metropolis Rules




<br>No path needed, all agent update simultaneously
<br>Asymmetry (value inserted in gradient not the same as from which the step is taken, red vs. blue) may lead to instability, may not decrease cost


<br>Diffusion Strategies

<br>Remove asymmetry of Consensus Strategy
<br>Combine-then-Adapt (CTA)

<br>Combination 
<br>Adaptation 


<br>**Adapt-then-Combine (ATC)

<br>Adaptation 
<br>Combination 


<br>Exact Diffusion Strategy

<br>Introduce Correction Step into ATC
<br>Adaptation 
<br>Correction 

<br>If for one entry neighbours have different value, respect that for own estimate


<br>Combination 




<br>Decentralized Alternating Direction Method of Multipliers (d-<a data-href="ADMM" href="the-guide/mathematics/optimization/admm.html" class="internal-link" target="_self" rel="noopener nofollow">ADMM</a>)

<br>Reformulate optimization problem in ADMM form

<br> subject to 

<br> is local,  global
<br> is consensus constraint


<br>Augmented Lagrangian is 

<br>




<br>Iterate at each node 

<br>Primal descent 
<br>Averaging 

<br>By any <a data-tooltip-position="top" aria-label="Distributed Consensus" data-href="Distributed Consensus" href="the-guide/mathematics/optimization/distributed-consensus.html" class="internal-link" target="_self" rel="noopener nofollow">distributed consensus</a> algorithm


<br>Dual ascent  




]]></description><link>the-guide/mathematics/optimization/distributed-optimization.html</link><guid isPermaLink="false">The Guide/Mathematics/Optimization/Distributed Optimization.md</guid><pubDate>Sat, 29 Mar 2025 16:19:05 GMT</pubDate></item><item><title><![CDATA[Gauss-Newton Method]]></title><description><![CDATA[ 
 <br>In a Nutshell
Optimization algorithm that approximates the <a data-tooltip-position="top" aria-label="Derivative, Gradient, Jacobian and Hessian > The Hessian" data-href="Derivative, Gradient, Jacobian and Hessian#The Hessian" href="the-guide/mathematics/analysis-and-calculus/derivative,-gradient,-jacobian-and-hessian.html#The_Hessian" class="internal-link" target="_self" rel="noopener nofollow">Hessian</a> of the <a data-tooltip-position="top" aria-label="Newton-Raphson Method" data-href="Newton-Raphson Method" href="the-guide/mathematics/optimization/convex-optimization-lecture/algorithms/newton-raphson-method.html" class="internal-link" target="_self" rel="noopener nofollow">Newton-Raphson method</a> via the <a data-tooltip-position="top" aria-label="Derivative, Gradient, Jacobian and Hessian > The Jacobian" data-href="Derivative, Gradient, Jacobian and Hessian#The Jacobian" href="the-guide/mathematics/analysis-and-calculus/derivative,-gradient,-jacobian-and-hessian.html#The_Jacobian" class="internal-link" target="_self" rel="noopener nofollow">Jacobian</a>, trading convergence speed for computational efficiency. 
<br><br>Gauss-Newton Algorithm 
The Gauss-Newton method is used for solving nonlinear least squares problems. Given a nonlinear function  with multiple components, the objective is to find the value of  that minimizes the sum of squared residuals:  where represents the vector of residuals. The algorithm iteratively updates  using the formula: where: 

<br> is the <a data-tooltip-position="top" aria-label="Derivative, Gradient, Jacobian and Hessian > The Jacobian" data-href="Derivative, Gradient, Jacobian and Hessian#The Jacobian" href="the-guide/mathematics/analysis-and-calculus/derivative,-gradient,-jacobian-and-hessian.html#The_Jacobian" class="internal-link" target="_self" rel="noopener nofollow">Jacobian</a> <a data-tooltip-position="top" aria-label="Matrices" data-href="Matrices" href="the-guide/mathematics/linear-algebra/matrices.html" class="internal-link" target="_self" rel="noopener nofollow">matrix</a> of the residual vector  at , 
<br> is the residual vector at the current iteration .

<br>Algorithm

<br>Compute the Residuals: Evaluate . 
<br>Compute the Jacobian: Calculate , the Jacobian of  at . 
<br>Compute the Update: Use the update formula  
<br>Iterate: Repeat until convergence.

<br><br><br>]]></description><link>the-guide/mathematics/optimization/gauss-newton-method.html</link><guid isPermaLink="false">The Guide/Mathematics/Optimization/Gauss-Newton Method.md</guid><pubDate>Sat, 25 Jan 2025 17:52:28 GMT</pubDate></item><item><title><![CDATA[Interior Point Methods]]></title><description><![CDATA[ 
 <br>In a Nutshell
Also called Barrier Methods, family of optimization problems for <a data-tooltip-position="top" aria-label="Convexity" data-href="Convexity" href="the-guide/mathematics/optimization/convex-optimization-lecture/convexity.html" class="internal-link" target="_self" rel="noopener nofollow">convex</a> <a data-tooltip-position="top" aria-label="Optimization Problem" data-href="Optimization Problem" href="the-guide/mathematics/optimization/convex-optimization-lecture/optimization-problem.html" class="internal-link" target="_self" rel="noopener nofollow">optimization problems</a>. While the Simplex algorithm traverses the boundary of the feasible <a data-tooltip-position="top" aria-label="Set" data-href="Set" href="the-guide/mathematics/general-stuff/set.html" class="internal-link" target="_self" rel="noopener nofollow">set</a> and ellipsoid methods bounds it from the outside, interior point methods traverses the interior.
<br>Can be used for e.g. ...<br>
<br><a data-href="Linear Program (LP)" href="the-guide/mathematics/optimization/convex-optimization-lecture/linear-program-(lp).html" class="internal-link" target="_self" rel="noopener nofollow">Linear Program (LP)</a>
<br>Quadratically constrained <a data-href="Quadratic Program (QP)" href="the-guide/mathematics/optimization/convex-optimization-lecture/quadratic-program-(qp).html" class="internal-link" target="_self" rel="noopener nofollow">Quadratic Program (QP)</a>
<br><a data-href="Semidefinite Program (SDP)" href="the-guide/mathematics/optimization/convex-optimization-lecture/semidefinite-program-(sdp).html" class="internal-link" target="_self" rel="noopener nofollow">Semidefinite Program (SDP)</a>
<br>Geometric Programs
<br>...
<br>Generally, we consider three subclasses.<br><br><br>Convert a constrained convex optimization into an unconstrained one by adding barrier functions, who satisfy for any sequence  entirely in the interior of the feasible set. Additionally, we assume that (most general) the <a data-tooltip-position="top" aria-label="Derivative, Gradient, Jacobian and Hessian" data-href="Derivative, Gradient, Jacobian and Hessian" href="the-guide/mathematics/analysis-and-calculus/derivative,-gradient,-jacobian-and-hessian.html" class="internal-link" target="_self" rel="noopener nofollow">Hessian</a> of that barrier <a data-tooltip-position="top" aria-label="Mathematical Relations" data-href="Mathematical Relations" href="the-guide/mathematics/general-stuff/mathematical-relations.html" class="internal-link" target="_self" rel="noopener nofollow">function</a> is <a data-tooltip-position="top" aria-label="Matrices" data-href="Matrices" href="the-guide/mathematics/linear-algebra/matrices.html" class="internal-link" target="_self" rel="noopener nofollow">positive definite</a>. The problem becomesand after choosing the barrier function and the penalty parameter  can be solves via any unconstrained methdos, e.g. <a data-href="Newton-Raphson Method" href="the-guide/mathematics/optimization/convex-optimization-lecture/algorithms/newton-raphson-method.html" class="internal-link" target="_self" rel="noopener nofollow">Newton-Raphson Method</a>.<br><br><br>This variant is based on the <a data-tooltip-position="top" aria-label="Cones" data-href="Cones" href="the-guide/mathematics/optimization/convex-optimization-lecture/cones.html" class="internal-link" target="_self" rel="noopener nofollow">conic</a> form of a <a data-tooltip-position="top" aria-label="Convexity" data-href="Convexity" href="the-guide/mathematics/optimization/convex-optimization-lecture/convexity.html" class="internal-link" target="_self" rel="noopener nofollow">convex</a> problem <br><br>]]></description><link>the-guide/mathematics/optimization/interior-point-methods.html</link><guid isPermaLink="false">The Guide/Mathematics/Optimization/Interior Point Methods.md</guid><pubDate>Sat, 05 Apr 2025 15:14:14 GMT</pubDate></item><item><title><![CDATA[Legendre Transformation]]></title><description><![CDATA[ 
 <br><a data-tooltip-position="top" aria-label="Mathematical Relations" data-href="Mathematical Relations" href="the-guide/mathematics/general-stuff/mathematical-relations.html" class="internal-link" target="_self" rel="noopener nofollow">Involutive transformation</a> (Legendre transform of Legendre transform gives initial function) on real-valued <a data-tooltip-position="top" aria-label="Convexity" data-href="Convexity" href="the-guide/mathematics/optimization/convex-optimization-lecture/convexity.html" class="internal-link" target="_self" rel="noopener nofollow">convex</a> <a data-tooltip-position="top" aria-label="Mathematical Relations" data-href="Mathematical Relations" href="the-guide/mathematics/general-stuff/mathematical-relations.html" class="internal-link" target="_self" rel="noopener nofollow">functions</a> or on a real-valued variable of a multivariate function with respect to which it is convex. The generalization to non-convex functions is the <a data-href="Legendre-Fenchel-Transformation" href="Legendre-Fenchel-Transformation" class="internal-link" target="_self" rel="noopener nofollow">Legendre-Fenchel-Transformation</a>.<br>Can be <br>
<br>Given function , L Transform yields function , which given a tangent slope returns the y-intercept ?
<br>
]]></description><link>the-guide/mathematics/optimization/legendre-transformation.html</link><guid isPermaLink="false">The Guide/Mathematics/Optimization/Legendre Transformation.md</guid><pubDate>Thu, 15 Aug 2024 12:51:15 GMT</pubDate></item><item><title><![CDATA[Levenberg-Marquardt Algorithm]]></title><description><![CDATA[ 
 <br>In a Nutshell
A.k.a damped least squares algorithm, method to solve <a data-tooltip-position="top" aria-label="Non-Linear Least Squares Problem" data-href="Non-Linear Least Squares Problem" href="the-guide/mathematics/optimization/non-linear-least-squares-problem.html" class="internal-link" target="_self" rel="noopener nofollow">non-linear least squares problems</a>. Can be interpreted as an interpolation between <a data-href="Gradient Descent" href="the-guide/mathematics/optimization/convex-optimization-lecture/algorithms/gradient-descent.html" class="internal-link" target="_self" rel="noopener nofollow">Gradient Descent</a> and the <a data-tooltip-position="top" aria-label="Gauss-Newton Method" data-href="Gauss-Newton Method" href="the-guide/mathematics/optimization/gauss-newton-method.html" class="internal-link" target="_self" rel="noopener nofollow">Gauss-Newton method</a>. 
<br><br>This method is generally seen as a more robust improvement over the basic <a data-tooltip-position="top" aria-label="Gauss-Newton Method" data-href="Gauss-Newton Method" href="the-guide/mathematics/optimization/gauss-newton-method.html" class="internal-link" target="_self" rel="noopener nofollow">Gauss-Newton method</a>.<br>Levenberg-Marquart Method
It is used for solving nonlinear least squares problems. Given a nonlinear function  with multiple components, the objective is to find the value of  that minimizes the sum of squared residuals:  where represents the vector of residuals. The algorithm iteratively updates  using the formula:where 

<br> is the <a data-tooltip-position="top" aria-label="Derivative, Gradient, Jacobian and Hessian > The Jacobian" data-href="Derivative, Gradient, Jacobian and Hessian#The Jacobian" href="the-guide/mathematics/analysis-and-calculus/derivative,-gradient,-jacobian-and-hessian.html#The_Jacobian" class="internal-link" target="_self" rel="noopener nofollow">Jacobian</a> <a data-tooltip-position="top" aria-label="Matrices" data-href="Matrices" href="the-guide/mathematics/linear-algebra/matrices.html" class="internal-link" target="_self" rel="noopener nofollow">matrix</a> of the residual vector  at , 
<br> is the residual vector at the current iteration .
<br> is a damping parameter.

<br>Damping Parameter Influence
The damping parameter  can be adjusted at each iteration based on the reduction in the objective function: 

<br>If the update reduces the objective function significantly, decrease   (making the update closer to Gauss-Newton). 
<br>If the update does not reduce the objective function, increase  (making the update closer to gradient descent).<br>
This balance between Gauss-Newton and gradient descent improves stability:

<br>Algorithm

<br>Compute the Residuals: Evaluate . 
<br>Compute the Jacobian: Calculate , the Jacobian of  at . 
<br>Compute the Update: Use the update formula  
<br>Adjust : Modify based on the reduction in the objective function. 
<br>Iterate: Repeat until convergence.

]]></description><link>the-guide/mathematics/optimization/levenberg-marquardt-algorithm.html</link><guid isPermaLink="false">The Guide/Mathematics/Optimization/Levenberg-Marquardt Algorithm.md</guid><pubDate>Sat, 25 Jan 2025 17:52:28 GMT</pubDate></item><item><title><![CDATA[Linear Least Squares and Ridge Regression]]></title><description><![CDATA[ 
 <br>Question
In <a data-tooltip-position="top" aria-label="- Regression -" data-href="- Regression -" href="the-guide/mathematics/statistics/-regression-.html" class="internal-link" target="_self" rel="noopener nofollow">linear regression</a>, we assume given features  with <a data-tooltip-position="top" aria-label="Matrices" data-href="Matrices" href="the-guide/mathematics/linear-algebra/matrices.html" class="internal-link" target="_self" rel="noopener nofollow">vector</a>  encoding desired linear combination and e.g. polynomial  or sinusoidal . How can we compute  ?
<br>We can compute the desired  by minimizing the loss using a set of training data .<br><br><br>
<br>Design Matrix 
<br>Target Vector 
<br>Model Fitting 
<br>Model Prediction 
<br><br><br>Definition
Variant of the above Least squares method that penalizes high parameters by replacing  by  and normalize by  with diagonal elements  and zero else. This yields  with .
<br>
<br>Intuition

<br>If the variance of the input is high ...?


<br>Results in <a data-href="Bias-Variance Tradeoff" href="the-guide/machine-learning/reinforcement-learning/theorems/bias-variance-tradeoff.html" class="internal-link" target="_self" rel="noopener nofollow">Bias-Variance Tradeoff</a>

<br>Bias increases as  increases.
<br>Variance decreases as  increases.


]]></description><link>the-guide/mathematics/optimization/linear-least-squares-and-ridge-regression.html</link><guid isPermaLink="false">The Guide/Mathematics/Optimization/Linear Least Squares and Ridge Regression.md</guid><pubDate>Thu, 17 Apr 2025 16:39:15 GMT</pubDate></item><item><title><![CDATA[Mirror Descent]]></title><description><![CDATA[ 
 <br>Info
An iterative optimization algorithm to find the local minimum of a differentiable <a data-tooltip-position="top" aria-label="Mathematical Relations" data-href="Mathematical Relations" href="the-guide/mathematics/general-stuff/mathematical-relations.html" class="internal-link" target="_self" rel="noopener nofollow">function</a>. It represents a generalization of the well-known <a data-tooltip-position="top" aria-label="Gradient Descent" data-href="Gradient Descent" href="the-guide/mathematics/optimization/convex-optimization-lecture/algorithms/gradient-descent.html" class="internal-link" target="_self" rel="noopener nofollow">gradient descent</a> that adapts to the underlying geometry of the problem by using a more general <a data-tooltip-position="top" aria-label="Bregman Divergence" data-href="Bregman Divergence" href="the-guide/information-theory/information-geometry/bregman-divergence.html" class="internal-link" target="_self" rel="noopener nofollow">Bregman distance</a> instead of a euclidean distance. 
<br>
<br>Motivation

<br>Standard <a data-tooltip-position="top" aria-label="Gradient Descent" data-href="Gradient Descent" href="the-guide/mathematics/optimization/convex-optimization-lecture/algorithms/gradient-descent.html" class="internal-link" target="_self" rel="noopener nofollow">gradient descent</a> assumes function to minimize is Lipschitz with respect to euclidean norm, general functions can be L-Lipschitz with respect to another norm.
<br>Euclidean norm has spherical symmetry 
<br>Gradient technically lives in dual space, linear combination of regular iterate has to pay attention to that mapping


<br>Idea<img alt="center" src="lib/media/pasted-image-20230605101211.png" style="width: 400px; max-width: 100%;">

<br>Map current point  to a point in dual space using gradient of convex function  
<br>Take gradient step in dual space 
<br>Map point  back to  via inverse map
<br>In constrained case project point  to "close" feasible point in feasible region  using <a data-href="Bregman Divergence" href="the-guide/information-theory/information-geometry/bregman-divergence.html" class="internal-link" target="_self" rel="noopener nofollow">Bregman Divergence</a> of function  


<br>Update
<br>Alternative Formulation

<br>Approximate by linear function and minimize it instead, but add penalty to move not too far away.
<br>Obtain update by inserting definition of <a data-href="Bregman Divergence" href="the-guide/information-theory/information-geometry/bregman-divergence.html" class="internal-link" target="_self" rel="noopener nofollow">Bregman Divergence</a> and setting gradient to zero.


]]></description><link>the-guide/mathematics/optimization/mirror-descent.html</link><guid isPermaLink="false">The Guide/Mathematics/Optimization/Mirror Descent.md</guid><pubDate>Thu, 15 Aug 2024 12:51:15 GMT</pubDate><enclosure url="lib/media/pasted-image-20230605101211.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src=&quot;lib/media/pasted-image-20230605101211.png&quot;&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[Non-Linear Least Squares Problem]]></title><description><![CDATA[<a class="tag" href="?query=tag:Machine-Learning" style="background-color: rgb(4, 108, 116); color: white; font-weight: 700; border: none; border-radius: 1em; padding: 0.2em 0.5em;">#Machine-Learning</a> 
 <br>In a Nusthell
Mathematical optimization problem extending <a data-tooltip-position="top" aria-label="Linear Least Squares and Ridge Regression" data-href="Linear Least Squares and Ridge Regression" href="the-guide/mathematics/optimization/linear-least-squares-and-ridge-regression.html" class="internal-link" target="_self" rel="noopener nofollow">Linear Least Squares</a> to arbitrary <a data-tooltip-position="top" aria-label="Mathematical Relations" data-href="Mathematical Relations" href="the-guide/mathematics/general-stuff/mathematical-relations.html" class="internal-link" target="_self" rel="noopener nofollow">functions</a> / models. The goal is again to find parameters that minimize the squared distance between data and prediction.
<br>Since this is mostly relevant to me in the <a href=".?query=tag:Machine-Learning" class="tag" target="_blank" rel="noopener nofollow">#Machine-Learning</a> context, I'll use the notation of that domain directly.<br><br>N-LLS Optimization Problem 
Given a set of datapoints  with observations , the objective is to find parameters  to minimize In contrast to <a data-tooltip-position="top" aria-label="Linear Least Squares and Ridge Regression" data-href="Linear Least Squares and Ridge Regression" href="the-guide/mathematics/optimization/linear-least-squares-and-ridge-regression.html" class="internal-link" target="_self" rel="noopener nofollow">LLS</a>, there is no constraint on the <a data-tooltip-position="top" aria-label="Mathematical Relations" data-href="Mathematical Relations" href="the-guide/mathematics/general-stuff/mathematical-relations.html" class="internal-link" target="_self" rel="noopener nofollow">function</a>  other than being differentiable. In a more general optimization contexts, we often simply define the residual between data and model prediction as the function to minimize, yielding 
<br><br><br>Most algorithms are <a data-tooltip-position="top" aria-label="A general Framework for Iterative Optimization Algorithms" data-href="A general Framework for Iterative Optimization Algorithms" href="the-guide/mathematics/optimization/convex-optimization-lecture/algorithms/a-general-framework-for-iterative-optimization-algorithms.html" class="internal-link" target="_self" rel="noopener nofollow">iterative algorithms</a> based on an initial guess .<br>
<br><a data-href="Gradient Descent" href="the-guide/mathematics/optimization/convex-optimization-lecture/algorithms/gradient-descent.html" class="internal-link" target="_self" rel="noopener nofollow">Gradient Descent</a> (first order information)
<br><a data-tooltip-position="top" aria-label="Gauss-Newton Method" data-href="Gauss-Newton Method" href="the-guide/mathematics/optimization/gauss-newton-method.html" class="internal-link" target="_self" rel="noopener nofollow">Gauss-Newton</a>, a taylored version of the <a data-tooltip-position="top" aria-label="Newton-Raphson Method" data-href="Newton-Raphson Method" href="the-guide/mathematics/optimization/convex-optimization-lecture/algorithms/newton-raphson-method.html" class="internal-link" target="_self" rel="noopener nofollow">Newton-Raphson algorithm</a> (approximate second order information to gain efficiency) 
]]></description><link>the-guide/mathematics/optimization/non-linear-least-squares-problem.html</link><guid isPermaLink="false">The Guide/Mathematics/Optimization/Non-Linear Least Squares Problem.md</guid><pubDate>Fri, 08 Nov 2024 13:51:05 GMT</pubDate></item><item><title><![CDATA[Stochastic Optimization]]></title><description><![CDATA[ 
 <br>In a Nutshell
Family of <a data-tooltip-position="top" aria-label="A general Framework for Iterative Optimization Algorithms" data-href="A general Framework for Iterative Optimization Algorithms" href="the-guide/mathematics/optimization/convex-optimization-lecture/algorithms/a-general-framework-for-iterative-optimization-algorithms.html" class="internal-link" target="_self" rel="noopener nofollow">iterative algorithms</a>, typically used for root-finding or <a data-tooltip-position="top" aria-label="Optimization Problem" data-href="Optimization Problem" href="the-guide/mathematics/optimization/convex-optimization-lecture/optimization-problem.html" class="internal-link" target="_self" rel="noopener nofollow">optimization</a>.
<br><br><br>We assume that we want to optimize a <a data-tooltip-position="top" aria-label="Mathematical Relations" data-href="Mathematical Relations" href="the-guide/mathematics/general-stuff/mathematical-relations.html" class="internal-link" target="_self" rel="noopener nofollow">function</a> . For example, we can consider finding the root of the equationprovided a unique  exists. In this setting, it is assumed that we cannot directly evaluate , but we can obtain measurements of a <a data-tooltip-position="top" aria-label="Random Variable" data-href="Random Variable" href="the-guide/mathematics/probability-theory/random-variable.html" class="internal-link" target="_self" rel="noopener nofollow">random variable</a> , where <br>Definition
The algorithm uses a sequence of positive steps  to perform the iterative update 
<br>Theorem
Robbins, Monro and Blum proved that this converges to  provided that 

<br> is uniformly bound (upper bound on magnitude)
<br> is non-decreasing
<br> exists and is positive
<br>The sequence of step sizes satisfies 

<br>
<br>The last condition is often referred to as Robbins-Monroe condition
]]></description><link>the-guide/mathematics/optimization/stochastic-optimization.html</link><guid isPermaLink="false">The Guide/Mathematics/Optimization/Stochastic Optimization.md</guid><pubDate>Fri, 20 Dec 2024 11:50:53 GMT</pubDate></item><item><title><![CDATA[Borel Set and Borel Sigma Algebra]]></title><description><![CDATA[ 
 <br>For any collection of sets , there is a smallest sigma algebra that includes :We say that  is the sigma algebra generated by .<br>For example for  and , we get <br>In the continuous case, the definition is more involved. <br>Borel Sigma Algebra
For a <a data-tooltip-position="top" aria-label="Topology and Topological Space" data-href="Topology and Topological Space" href="the-guide/mathematics/topology/topology-and-topological-space.html" class="internal-link" target="_self" rel="noopener nofollow">topological space</a> , the Borel-<a data-tooltip-position="top" aria-label="Sigma-Algebra" data-href="Sigma-Algebra" href="the-guide/mathematics/probability-theory/sigma-algebra.html" class="internal-link" target="_self" rel="noopener nofollow">sigma algebra</a>, is the sigma-algebra generated by the <a data-tooltip-position="top" aria-label="Set" data-href="Set" href="the-guide/mathematics/general-stuff/set.html" class="internal-link" target="_self" rel="noopener nofollow">open sets</a>. Its elements are denoted Borel sets.
]]></description><link>the-guide/mathematics/probability-theory/borel-set-and-borel-sigma-algebra.html</link><guid isPermaLink="false">The Guide/Mathematics/Probability Theory/Borel Set and Borel Sigma Algebra.md</guid><pubDate>Wed, 23 Apr 2025 08:34:37 GMT</pubDate></item><item><title><![CDATA[Chebyshev or Markov Inequality]]></title><description><![CDATA[ 
 <br>Theorem
In <a data-tooltip-position="top" aria-label="https://en.wikipedia.org/wiki/Probability_theory" rel="noopener nofollow" class="external-link" title="Probability theory" href="https://en.wikipedia.org/wiki/Probability_theory" target="_blank">probability theory</a>, Chebyshev's inequality (also called the Bienaymé–Chebyshev inequality) guarantees that, for a wide class of <a data-tooltip-position="top" aria-label="Probability Distribution" data-href="Probability Distribution" href="the-guide/mathematics/probability-theory/probability-distribution.html" class="internal-link" target="_self" rel="noopener nofollow">probability distributions</a> no more than a certain fraction of values can be more than a certain distance from the <a data-tooltip-position="top" aria-label="Expectations" data-href="Expectations" href="the-guide/mathematics/statistics/expectations.html" class="internal-link" target="_self" rel="noopener nofollow">mean</a>.
<br>In the language of <a data-tooltip-position="top" aria-label="Measure" data-href="Measure" href="the-guide/mathematics/measure-theory/measure.html" class="internal-link" target="_self" rel="noopener nofollow">measure</a> thery, the inequality states that if  is a <a data-tooltip-position="top" aria-label="Measure" data-href="Measure" href="the-guide/mathematics/measure-theory/measure.html" class="internal-link" target="_self" rel="noopener nofollow">measurable space</a>,  is a real-valued measurable <a data-tooltip-position="top" aria-label="Mathematical Relations" data-href="Mathematical Relations" href="the-guide/mathematics/general-stuff/mathematical-relations.html" class="internal-link" target="_self" rel="noopener nofollow">function</a> and , it holds that ]]></description><link>the-guide/mathematics/probability-theory/chebyshev-or-markov-inequality.html</link><guid isPermaLink="false">The Guide/Mathematics/Probability Theory/Chebyshev or Markov Inequality.md</guid><pubDate>Tue, 20 Aug 2024 13:11:42 GMT</pubDate></item><item><title><![CDATA[Convergence of Measures]]></title><description><![CDATA[ 
 <br>...<br><br><br>A sequence of <a data-tooltip-position="top" aria-label="Measure" data-href="Measure" href="the-guide/mathematics/measure-theory/measure.html" class="internal-link" target="_self" rel="noopener nofollow">probability measures</a>  on  converges weakly to a limiting measure , if for all continuous bounded functions  we have ]]></description><link>the-guide/mathematics/probability-theory/convergence-of-measures.html</link><guid isPermaLink="false">The Guide/Mathematics/Probability Theory/Convergence of Measures.md</guid><pubDate>Tue, 20 Aug 2024 13:14:32 GMT</pubDate></item><item><title><![CDATA[Cumulative Distribution Function]]></title><description><![CDATA[ 
 <br>Discrete CDF
For discrete <a data-tooltip-position="top" aria-label="Random Variable" data-href="Random Variable" href="the-guide/mathematics/probability-theory/random-variable.html" class="internal-link" target="_self" rel="noopener nofollow">random variables</a>, the CDF is defined as<br>

<br>Continuous CDF
For continuous <a data-tooltip-position="top" aria-label="Random Variable" data-href="Random Variable" href="the-guide/mathematics/probability-theory/random-variable.html" class="internal-link" target="_self" rel="noopener nofollow">random variables</a>, the CDF is defined isIf the CDF is differentiable, the derivative is defined as the <a data-tooltip-position="top" aria-label="Probability Density Function" data-href="Probability Density Function" href="the-guide/mathematics/probability-theory/probability-density-function.html" class="internal-link" target="_self" rel="noopener nofollow">probability density function</a> .
<br>Additional Information
Mathematically, the density is then only defined with regard to the <a data-tooltip-position="top" aria-label="Measure" data-href="Measure" href="the-guide/mathematics/measure-theory/measure.html" class="internal-link" target="_self" rel="noopener nofollow">measure</a>, in almost every application this is the <a data-tooltip-position="top" aria-label="Lebesgue Measure" data-href="Lebesgue Measure" href="the-guide/mathematics/measure-theory/lebesgue-measure.html" class="internal-link" target="_self" rel="noopener nofollow">Lebesgue measure</a>.
<br><br>]]></description><link>the-guide/mathematics/probability-theory/cumulative-distribution-function.html</link><guid isPermaLink="false">The Guide/Mathematics/Probability Theory/Cumulative Distribution Function.md</guid><pubDate>Mon, 14 Apr 2025 15:16:21 GMT</pubDate></item><item><title><![CDATA[Euler-Maruyama Method]]></title><description><![CDATA[ 
 <br>In a Nutshell
Method to approximate the numerical solution of a <a data-tooltip-position="top" aria-label="Stochastic Differential Equation" data-href="Stochastic Differential Equation" href="the-guide/mathematics/probability-theory/stochastic-differential-equation.html" class="internal-link" target="_self" rel="noopener nofollow">SDE</a> by extending the well-known Euler method to SDEs.
<br><br>Definition
Considering the SDE with initial condition  and <a data-tooltip-position="top" aria-label="Wiener Process" data-href="Wiener Process" href="the-guide/mathematics/probability-theory/wiener-process.html" class="internal-link" target="_self" rel="noopener nofollow">Wiener process</a> , we want to solce this equation on a time intervall . The Euler-Maruyama Approximation to the true solution is the <a data-href="Markov Chain" href="the-guide/computational-statistics/data-assimilation/markov-chain.html" class="internal-link" target="_self" rel="noopener nofollow">Markov Chain</a>  defined via using 

<br>Partition of the time intervall  and 
<br>Initial value 
<br><a data-tooltip-position="top" aria-label="Random Variable" data-href="Random Variable" href="the-guide/mathematics/probability-theory/random-variable.html" class="internal-link" target="_self" rel="noopener nofollow">Random variables</a> (random fluctuations)that are <a data-tooltip-position="top" aria-label="Statistical Independence" data-href="Statistical Independence" href="the-guide/mathematics/statistics/statistical-independence.html" class="internal-link" target="_self" rel="noopener nofollow">i.i.d.</a> <a data-tooltip-position="top" aria-label="Gaussian Distribution" data-href="Gaussian Distribution" href="the-guide/mathematics/probability-theory/gaussian-distribution.html" class="internal-link" target="_self" rel="noopener nofollow">normal</a> distributed with <a data-tooltip-position="top" aria-label="Expectations" data-href="Expectations" href="the-guide/mathematics/statistics/expectations.html" class="internal-link" target="_self" rel="noopener nofollow">expectation</a>  and <a data-tooltip-position="top" aria-label="Covariance and Variance" data-href="Covariance and Variance" href="the-guide/mathematics/statistics/covariance-and-variance.html" class="internal-link" target="_self" rel="noopener nofollow">variance</a> .

]]></description><link>the-guide/mathematics/probability-theory/euler-maruyama-method.html</link><guid isPermaLink="false">The Guide/Mathematics/Probability Theory/Euler-Maruyama Method.md</guid><pubDate>Mon, 14 Apr 2025 15:14:01 GMT</pubDate></item><item><title><![CDATA[Evidence Lower Bound]]></title><description><![CDATA[ 
 <br>In a Nutshell
Lower (worst case) bound on the <a data-tooltip-position="top" aria-label="Likelihood Function" data-href="Likelihood Function" href="the-guide/mathematics/statistics/likelihood-function.html" class="internal-link" target="_self" rel="noopener nofollow">log-likelihood</a> of observed data that is often used as a loss function for improving a parameterized model.
<br><br>We want to fit a parameterized model  to observations , i.e. maximize the likelihood This objective is based on the following theorem.<br>Theorem
Minimizing the <a data-tooltip-position="top" aria-label="Kullback-Leibler Divergence" data-href="Kullback-Leibler Divergence" href="the-guide/information-theory/information-theory-1/information/kullback-leibler-divergence.html" class="internal-link" target="_self" rel="noopener nofollow">KL-divergence</a> between two distributions, here a parameterized  and a "true"  is equivalent to maximizing the <a data-tooltip-position="top" aria-label="Likelihood Function" data-href="Likelihood Function" href="the-guide/mathematics/statistics/likelihood-function.html" class="internal-link" target="_self" rel="noopener nofollow">log-likelihood</a>, because
<br>This means that we can maximize the likelihood on the left in order to find a good approximation . This approximation can be computed explicitly using calculus of variations and a parameterized family of distributions, but these are too simplistic in many cases. <br><br><br>To overcome this, we employ implicitly parameterized distributions, which samples a <a data-tooltip-position="top" aria-label="Latent Variable Models" data-href="Latent Variable Models" href="the-guide/machine-learning/generative-models/latent-variable-models.html" class="internal-link" target="_self" rel="noopener nofollow">latent variable</a>  from a pre-defined simple distribution  and then converts the sample using complicated functions, e.g. <a data-tooltip-position="top" aria-label="Artificial Neural Network" data-href="Artificial Neural Network" href="the-guide/machine-learning/deep-learning/artificial-neural-network.html" class="internal-link" target="_self" rel="noopener nofollow">neural networks</a>, into parameters of other simple distributions . <br>This results in a family of <a data-tooltip-position="top" aria-label="Probability Distribution" data-href="Probability Distribution" href="the-guide/mathematics/probability-theory/probability-distribution.html" class="internal-link" target="_self" rel="noopener nofollow">joint distributions</a>  over , from which we can easily sample by first sampling , put it into the function and them sample from the resulting distribution. The parameters  in this case are the combined parameters of the latent distribution  and of the function mapping .<br>To perform the initial maximization with this new model, we'd have to compute the term marginalizing over the latent space for every sample, which is usually impossible in closed-form and infeasible in practice. We can use <a data-tooltip-position="top" aria-label="Monte Carlo Integration" data-href="Monte Carlo Integration" href="the-guide/mathematics/probability-theory/monte-carlo-integration.html" class="internal-link" target="_self" rel="noopener nofollow">MC integration</a> and <a data-tooltip-position="top" aria-label="Importance Sampling" data-href="Importance Sampling" href="the-guide/mathematics/probability-theory/importance-sampling.html" class="internal-link" target="_self" rel="noopener nofollow">importance sampling</a> using a discriminative model  of the latent to <a data-tooltip-position="top" aria-label="Estimator" data-href="Estimator" href="the-guide/mathematics/statistics/estimator.html" class="internal-link" target="_self" rel="noopener nofollow">estimate</a> (unbiased) the term via With this, we can sample  (and then easily compute )) as above. However, since the logarithm is not linear, we cannot pull it inside the <a data-tooltip-position="top" aria-label="Expectations" data-href="Expectations" href="the-guide/mathematics/statistics/expectations.html" class="internal-link" target="_self" rel="noopener nofollow">expectation</a>. <br>ELBO
What we do get is a lower bound by <a data-href="Jensen's Inequality" href="the-guide/information-theory/information-theory-1/information/jensen's-inequality.html" class="internal-link" target="_self" rel="noopener nofollow">Jensen's Inequality</a> in form of which is denoted the Evidence Lower Bound, short ELBO.  The ELBO can also be written as a joint probability and relates to information-theoretic quantities via
<br>
<br>Instead of having to marginalize over  for every sample, we can now use  to generate samples. However, we now have to maximize over  to maximize the likelihood and minimize the KL over   to move the estimated  closer to . 
<br>Maximizing the ELBO is a widely-used trick in in <a data-tooltip-position="top" aria-label="Variational Inference" data-href="Variational Inference" href="the-guide/computational-statistics/variational-inference.html" class="internal-link" target="_self" rel="noopener nofollow">Variational Inference</a> or <a data-href="- Bayesian Statistics -" href="the-guide/mathematics/statistics/-bayesian-statistics-.html" class="internal-link" target="_self" rel="noopener nofollow">- Bayesian Statistics -</a> in general, because we can now see from the theorem above that the maximization of the ELBO yields the minimum of the <a data-tooltip-position="top" aria-label="Kullback-Leibler Divergence" data-href="Kullback-Leibler Divergence" href="the-guide/information-theory/information-theory-1/information/kullback-leibler-divergence.html" class="internal-link" target="_self" rel="noopener nofollow">KL-Divergence</a> (<a data-tooltip-position="top" aria-label="Shannon Entropy" data-href="Shannon Entropy" href="the-guide/information-theory/information-theory-1/information/shannon-entropy.html" class="internal-link" target="_self" rel="noopener nofollow">entropy</a> constant) and

<br>Maximizing over  yields generative model
<br>Maximizing over  yields descriminative model
<br>


]]></description><link>the-guide/mathematics/probability-theory/evidence-lower-bound.html</link><guid isPermaLink="false">The Guide/Mathematics/Probability Theory/Evidence Lower Bound.md</guid><pubDate>Thu, 17 Apr 2025 16:23:36 GMT</pubDate></item><item><title><![CDATA[Filtrations]]></title><description><![CDATA[ 
 <br>In probability theory, more precisely in the study of <a data-tooltip-position="top" aria-label="Stochastic Process" data-href="Stochastic Process" href="the-guide/mathematics/probability-theory/stochastic-process.html" class="internal-link" target="_self" rel="noopener nofollow">stochastic processes</a>, filtrations are totally ordered collections of <a data-tooltip-position="top" aria-label="Set" data-href="Set" href="the-guide/mathematics/general-stuff/set.html" class="internal-link" target="_self" rel="noopener nofollow">subsets</a> that are used to model the available information at a given point. <br>Definition
Given a <a data-tooltip-position="top" aria-label="Probability Space" data-href="Probability Space" href="the-guide/mathematics/probability-theory/probability-space.html" class="internal-link" target="_self" rel="noopener nofollow">probability space</a>  and an index <a data-tooltip-position="top" aria-label="Set" data-href="Set" href="the-guide/mathematics/general-stuff/set.html" class="internal-link" target="_self" rel="noopener nofollow">set</a>  with total order , let  be a <a data-tooltip-position="top" aria-label="Sigma-Algebra" data-href="Sigma-Algebra" href="the-guide/mathematics/probability-theory/sigma-algebra.html" class="internal-link" target="_self" rel="noopener nofollow">sub-sigma-algebra</a> of  for every . Then is called a filtration, if  for all . Therefore, filtrations are families of <a data-tooltip-position="top" aria-label="Sigma-Algebra" data-href="Sigma-Algebra" href="the-guide/mathematics/probability-theory/sigma-algebra.html" class="internal-link" target="_self" rel="noopener nofollow">sigma-algebras</a> that are ordered non-decreasingly.
<br>
<br>For a filtration ,  is called a filtered probability space 
<br><br><br>
<br>Right-continuous
<br>Complete
<br><br>]]></description><link>the-guide/mathematics/probability-theory/filtrations.html</link><guid isPermaLink="false">The Guide/Mathematics/Probability Theory/Filtrations.md</guid><pubDate>Thu, 15 Aug 2024 12:51:52 GMT</pubDate></item><item><title><![CDATA[Fokker-Planck Equation]]></title><description><![CDATA[ 
 <br>In a Nutshell
Partial differential equation that describes the deterministic evolution of a <a data-tooltip-position="top" aria-label="Probability Distribution" data-href="Probability Distribution" href="the-guide/mathematics/probability-theory/probability-distribution.html" class="internal-link" target="_self" rel="noopener nofollow">probability distribution</a> over time under the effect of drift forces and random forces / noise. 
<br><br><br>Assume an <a data-tooltip-position="top" aria-label="Itô Diffusion" data-href="Itô Diffusion" href="the-guide/mathematics/probability-theory/itô-diffusion.html" class="internal-link" target="_self" rel="noopener nofollow">Itô process</a> driven by a <a data-tooltip-position="top" aria-label="Wiener Process" data-href="Wiener Process" href="the-guide/mathematics/probability-theory/wiener-process.html" class="internal-link" target="_self" rel="noopener nofollow">Wiener process</a>  with diffusion coefficient  and drift term  of the form The Fokker-Planck equation for the <a data-tooltip-position="top" aria-label="Probability Density Function" data-href="Probability Density Function" href="the-guide/mathematics/probability-theory/probability-density-function.html" class="internal-link" target="_self" rel="noopener nofollow">probability density</a>  of the <a data-tooltip-position="top" aria-label="Random Variable" data-href="Random Variable" href="the-guide/mathematics/probability-theory/random-variable.html" class="internal-link" target="_self" rel="noopener nofollow">random variable</a>  is <br>Intuition
For <a data-tooltip-position="top" aria-label="Stochastic Differential Equation" data-href="Stochastic Differential Equation" href="the-guide/mathematics/probability-theory/stochastic-differential-equation.html" class="internal-link" target="_self" rel="noopener nofollow">SDEs</a>, we cannot make prediction about single points due to the randomness. However, we can focus on the evolution of the <a data-tooltip-position="top" aria-label="Probability Density Function" data-href="Probability Density Function" href="the-guide/mathematics/probability-theory/probability-density-function.html" class="internal-link" target="_self" rel="noopener nofollow">probability density</a> instead, which is governed by the above differential equation. 
<br><br><br>Definition
Assume an -dimensional <a data-tooltip-position="top" aria-label="Itô Diffusion" data-href="Itô Diffusion" href="the-guide/mathematics/probability-theory/itô-diffusion.html" class="internal-link" target="_self" rel="noopener nofollow">Itô process</a> driven by an -dimensional <a data-tooltip-position="top" aria-label="Wiener Process" data-href="Wiener Process" href="the-guide/mathematics/probability-theory/wiener-process.html" class="internal-link" target="_self" rel="noopener nofollow">Wiener process</a> , formally with  and . The <a data-tooltip-position="top" aria-label="Probability Density Function" data-href="Probability Density Function" href="the-guide/mathematics/probability-theory/probability-density-function.html" class="internal-link" target="_self" rel="noopener nofollow">pdf</a>  of  satisfies the Fokker-Planck equation where  is the diffusion <a data-tooltip-position="top" aria-label="Tensor" data-href="Tensor" href="Tensor" class="internal-link" target="_self" rel="noopener nofollow">tensor</a> with components 
]]></description><link>the-guide/mathematics/probability-theory/fokker-planck-equation.html</link><guid isPermaLink="false">The Guide/Mathematics/Probability Theory/Fokker-Planck Equation.md</guid><pubDate>Wed, 23 Apr 2025 09:58:33 GMT</pubDate></item><item><title><![CDATA[Gaussian Distribution]]></title><description><![CDATA[ 
 <br>Definition
Maximum <a data-tooltip-position="top" aria-label="Shannon Entropy" data-href="Shannon Entropy" href="the-guide/information-theory/information-theory-1/information/shannon-entropy.html" class="internal-link" target="_self" rel="noopener nofollow">entropy</a> distribution given first and second moment. The <a data-tooltip-position="top" aria-label="Probability Distribution" data-href="Probability Distribution" href="the-guide/mathematics/probability-theory/probability-distribution.html" class="internal-link" target="_self" rel="noopener nofollow">distribution</a> has the parameters  and , encoding the <a data-tooltip-position="top" aria-label="Probability Density Function" data-href="Probability Density Function" href="the-guide/mathematics/probability-theory/probability-density-function.html" class="internal-link" target="_self" rel="noopener nofollow">pdf</a> for the scalar case. More generally, a higher dimensional Multivariate Gaussian is defined bywhere the <a data-tooltip-position="top" aria-label="Matrices" data-href="Matrices" href="the-guide/mathematics/linear-algebra/matrices.html" class="internal-link" target="_self" rel="noopener nofollow">positive-definite</a> <a data-tooltip-position="top" aria-label="Covariance and Variance" data-href="Covariance and Variance" href="the-guide/mathematics/statistics/covariance-and-variance.html" class="internal-link" target="_self" rel="noopener nofollow">covariance</a> matrix captures correlations between the dimensions.
<br>Lemma 1.6
Define with  <a data-tooltip-position="top" aria-label="Matrices" data-href="Matrices" href="the-guide/mathematics/linear-algebra/matrices.html" class="internal-link" target="_self" rel="noopener nofollow">symmetric and positive</a>. Then  can be normalized to produce the <a data-tooltip-position="top" aria-label="Probability Density Function" data-href="Probability Density Function" href="the-guide/mathematics/probability-theory/probability-density-function.html" class="internal-link" target="_self" rel="noopener nofollow">pdf</a> of a Gaussian <a data-tooltip-position="top" aria-label="Random Variable" data-href="Random Variable" href="the-guide/mathematics/probability-theory/random-variable.html" class="internal-link" target="_self" rel="noopener nofollow">random variable</a>  with  being the <a data-tooltip-position="top" aria-label="Covariance and Variance" data-href="Covariance and Variance" href="the-guide/mathematics/statistics/covariance-and-variance.html" class="internal-link" target="_self" rel="noopener nofollow">precision matrix</a>.
<br><br><br>The family of Gaussian-distributed <a data-tooltip-position="top" aria-label="Random Variable" data-href="Random Variable" href="the-guide/mathematics/probability-theory/random-variable.html" class="internal-link" target="_self" rel="noopener nofollow">random variables</a> is, generally speaking, well-behaved under numerous operations, such as ...<br>
<br>
Product - given two normal distributions  and , their product is distributed with the law This can be simplified if you work with <a data-tooltip-position="top" aria-label="Covariance and Variance" data-href="Covariance and Variance" href="the-guide/mathematics/statistics/covariance-and-variance.html" class="internal-link" target="_self" rel="noopener nofollow">precision matrices</a> instead. Then, the product of  Gaussians is distributed with the law 

<br>
Sum - given two normally distributed <a data-tooltip-position="top" aria-label="Random Variable" data-href="Random Variable" href="the-guide/mathematics/probability-theory/random-variable.html" class="internal-link" target="_self" rel="noopener nofollow">random variables</a>  and , their sum  is distributed with  

<br>
Linear Transformations - if , then the transformation  is distributed according to 

<br>
<a data-href="Marginal Distribution" href="the-guide/mathematics/probability-theory/marginal-distribution.html" class="internal-link" target="_self" rel="noopener nofollow">Marginal Distribution</a> / <a data-tooltip-position="top" aria-label="Probability Distribution" data-href="Probability Distribution" href="the-guide/mathematics/probability-theory/probability-distribution.html" class="internal-link" target="_self" rel="noopener nofollow">Conditional Distribution</a> - Suppose we have two gaussian <a data-tooltip-position="top" aria-label="Random Variable" data-href="Random Variable" href="the-guide/mathematics/probability-theory/random-variable.html" class="internal-link" target="_self" rel="noopener nofollow">random variables</a>  and  with covariance matrices  and . The covariance between the two variables is denoted . Consequently, the joint covariance is The <a data-tooltip-position="top" aria-label="Probability Distribution" data-href="Probability Distribution" href="the-guide/mathematics/probability-theory/probability-distribution.html" class="internal-link" target="_self" rel="noopener nofollow">conditional distribution</a> of  given  is then given by another <a data-tooltip-position="top" aria-label="Gaussian Distribution" data-href="Gaussian Distribution" href="the-guide/mathematics/probability-theory/gaussian-distribution.html" class="internal-link" target="_self" rel="noopener nofollow">Gaussian</a> defined bywhich is based on the <a data-tooltip-position="top" aria-label="Schur Complement" data-href="Schur Complement" href="the-guide/mathematics/linear-algebra/schur-complement.html" class="internal-link" target="_self" rel="noopener nofollow">Schur complement</a>.

<br>
<a data-href="Bayes Theorem" href="the-guide/mathematics/statistics/bayes-theorem.html" class="internal-link" target="_self" rel="noopener nofollow">Bayes Theorem</a> consider the problem . If then the left-hand side of the equation is distributed as with .

<br>
... tbc. ...

<br>Deriving Mean and Variance
In scenarios where we already know a distribution will be Gaussian, but we have a complicated derviation formula, we can observe that By equating coefficents that involve  and , we can derive expressions that reveal  and , while everything else is absorbed into a constant. This can be very useful, especially for large expressions where we do not care about the constant.
<br>Covariance and Hessian
Taking the exponential form of the theorem above, there is a simple but powerful connection between the <a data-tooltip-position="top" aria-label="Covariance and Variance" data-href="Covariance and Variance" href="the-guide/mathematics/statistics/covariance-and-variance.html" class="internal-link" target="_self" rel="noopener nofollow">covariance</a> and the <a data-tooltip-position="top" aria-label="Derivative, Gradient, Jacobian and Hessian > The Hessian" data-href="Derivative, Gradient, Jacobian and Hessian#The Hessian" href="the-guide/mathematics/analysis-and-calculus/derivative,-gradient,-jacobian-and-hessian.html#The_Hessian" class="internal-link" target="_self" rel="noopener nofollow">Hessian</a> of the exponent via This simply follows using rules of <a data-tooltip-position="top" aria-label="Matrix Calculus" data-href="Matrix Calculus" href="the-guide/mathematics/analysis-and-calculus/matrix-calculus.html" class="internal-link" target="_self" rel="noopener nofollow">matrix calculus</a> and the fact that the covariance is positive semi-definite.
<br><br><br>The complex Gaussian distribution  can be described using <br>
<br>The complex mean , an -dimensional complex vector
<br>The <a data-tooltip-position="top" aria-label="Covariance and Variance" data-href="Covariance and Variance" href="the-guide/mathematics/statistics/covariance-and-variance.html" class="internal-link" target="_self" rel="noopener nofollow">covariance</a> <a data-tooltip-position="top" aria-label="Matrices" data-href="Matrices" href="the-guide/mathematics/linear-algebra/matrices.html" class="internal-link" target="_self" rel="noopener nofollow">matrix</a> , a hermitian and non-negative definite matrix
<br>The relation <a data-tooltip-position="top" aria-label="Matrices" data-href="Matrices" href="the-guide/mathematics/linear-algebra/matrices.html" class="internal-link" target="_self" rel="noopener nofollow">matrix</a>  or pseudo-covariance, a symmetric matrix<br>
The circularly symmetric case is  with . In this case the <a data-tooltip-position="top" aria-label="Probability Distribution" data-href="Probability Distribution" href="the-guide/mathematics/probability-theory/probability-distribution.html" class="internal-link" target="_self" rel="noopener nofollow">distribution</a> is invariant under rotations. 
]]></description><link>the-guide/mathematics/probability-theory/gaussian-distribution.html</link><guid isPermaLink="false">The Guide/Mathematics/Probability Theory/Gaussian Distribution.md</guid><pubDate>Wed, 23 Apr 2025 21:55:39 GMT</pubDate></item><item><title><![CDATA[Gaussian Process]]></title><description><![CDATA[ 
 <br>In a Nutshell
<a data-tooltip-position="top" aria-label="Stochastic Process" data-href="Stochastic Process" href="the-guide/mathematics/probability-theory/stochastic-process.html" class="internal-link" target="_self" rel="noopener nofollow">Stochastic process</a>, where every finite collection of <a data-tooltip-position="top" aria-label="Random Variable" data-href="Random Variable" href="the-guide/mathematics/probability-theory/random-variable.html" class="internal-link" target="_self" rel="noopener nofollow">random variables</a> has a multivariate <a data-tooltip-position="top" aria-label="Gaussian Distribution" data-href="Gaussian Distribution" href="the-guide/mathematics/probability-theory/gaussian-distribution.html" class="internal-link" target="_self" rel="noopener nofollow">normal</a> <a data-tooltip-position="top" aria-label="Probability Distribution" data-href="Probability Distribution" href="the-guide/mathematics/probability-theory/probability-distribution.html" class="internal-link" target="_self" rel="noopener nofollow">distribution</a>. Sometimes considered a semi-parametric supervised <a data-tooltip-position="top" aria-label="- Machine Learning -" data-href="- Machine Learning -" href="the-guide/machine-learning/-machine-learning-.html" class="internal-link" target="_self" rel="noopener nofollow">machine learning</a> algorithm, because no assumptions are made on the model type, but on the way on how predictions are calculated.
<br><br><br>A Gaussian Process is a <a data-tooltip-position="top" aria-label="Probability Distribution" data-href="Probability Distribution" href="the-guide/mathematics/probability-theory/probability-distribution.html" class="internal-link" target="_self" rel="noopener nofollow">distribution</a> over continuous <a data-tooltip-position="top" aria-label="Mathematical Relations" data-href="Mathematical Relations" href="the-guide/mathematics/general-stuff/mathematical-relations.html" class="internal-link" target="_self" rel="noopener nofollow">functions</a>, where the mean function is denoted  and the <a data-tooltip-position="top" aria-label="Covariance and Variance" data-href="Covariance and Variance" href="the-guide/mathematics/statistics/covariance-and-variance.html" class="internal-link" target="_self" rel="noopener nofollow">covariance</a>  encodes how points correlate. These two components then completely define the Gaussian Process We can sample from such a process by simply sampling from a <a data-tooltip-position="top" aria-label="Gaussian Distribution" data-href="Gaussian Distribution" href="the-guide/mathematics/probability-theory/gaussian-distribution.html" class="internal-link" target="_self" rel="noopener nofollow">multivariate normal</a> distribution, whose dimensions equal the resolution  of the desired function. To do so, we pick a mean function and a covariance function / kernel, compute and use the result as mean and covariance of a Gaussian.<img alt="center" src="lib/media/pasted-image-20240626083034.png" style="width: 300px; max-width: 100%;"><br><br><br>The choice of the kernel / <a data-tooltip-position="top" aria-label="Covariance and Variance" data-href="Covariance and Variance" href="the-guide/mathematics/statistics/covariance-and-variance.html" class="internal-link" target="_self" rel="noopener nofollow">covariance</a> function significantly impacts the behavior of sampled functions and the shape of the posterior.  Some popular choices are ...<br>RBF Kernel
A smooth kernel, given by resulting in very smooth GPs. It thereby implicitly assumes that the function we are trying to learn is infinitely differentiable. The hyperparameters are

<br>Length-scale , which scales how fast the effect of neighboring points influencing each other decays. Can be chosen differently for every dimension, resulting in an anisotropic kernel.
<br><a data-tooltip-position="top" aria-label="Metric Space and Completeness" data-href="Metric Space and Completeness" href="the-guide/mathematics/general-stuff/metric-space-and-completeness.html" class="internal-link" target="_self" rel="noopener nofollow">Distance function</a> , which controls the range in which neighboring points influence each other

<br>Matérn Kernel
A generalization of the RBF kernel, which allows us to control the smoothness of the resulting functions, weakening the implicit assumption on its differentiabiliy. It is computed as where  is the gamma function and  a modified Bessel function.<br>
In addition to the hyperparameters for the RBF kernel, we additionally have

<br> to control the smoothness (the higher the smoother)

<br><br>]]></description><link>the-guide/mathematics/probability-theory/gaussian-process.html</link><guid isPermaLink="false">The Guide/Mathematics/Probability Theory/Gaussian Process.md</guid><pubDate>Thu, 06 Mar 2025 12:11:20 GMT</pubDate><enclosure url="lib/media/pasted-image-20240626083034.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src=&quot;lib/media/pasted-image-20240626083034.png&quot;&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[Importance Sampling]]></title><description><![CDATA[ 
 <br>Info
<a data-tooltip-position="top" aria-label="Estimator" data-href="Estimator" href="the-guide/mathematics/statistics/estimator.html" class="internal-link" target="_self" rel="noopener nofollow">Estimate</a> the <a data-tooltip-position="top" aria-label="Expectations" data-href="Expectations" href="the-guide/mathematics/statistics/expectations.html" class="internal-link" target="_self" rel="noopener nofollow">expectation</a> of a <a data-tooltip-position="top" aria-label="Probability Distribution" data-href="Probability Distribution" href="the-guide/mathematics/probability-theory/probability-distribution.html" class="internal-link" target="_self" rel="noopener nofollow">distribution</a> that is hard to draw samples from by sampling from a proposal <a data-tooltip-position="top" aria-label="Probability Distribution" data-href="Probability Distribution" href="the-guide/mathematics/probability-theory/probability-distribution.html" class="internal-link" target="_self" rel="noopener nofollow">distribution</a> and adding a weighting term via
<br>The hope is to find a proposal distribution , such that the <a data-tooltip-position="top" aria-label="Covariance and Variance" data-href="Covariance and Variance" href="the-guide/mathematics/statistics/covariance-and-variance.html" class="internal-link" target="_self" rel="noopener nofollow">variance</a> of the computed expected value  (if theoretically repeating the estimation) is lower than if simply sampling from  (standard <a data-tooltip-position="top" aria-label="Monte Carlo Integration" data-href="Monte Carlo Integration" href="the-guide/mathematics/probability-theory/monte-carlo-integration.html" class="internal-link" target="_self" rel="noopener nofollow">MC</a>) or to enable computing the value in the first place if  is hard or impossible to sample from. The result is a <a data-tooltip-position="top" aria-label="Estimator" data-href="Estimator" href="the-guide/mathematics/statistics/estimator.html" class="internal-link" target="_self" rel="noopener nofollow">consistent and unbiased estimator</a> as long as original <a data-tooltip-position="top" aria-label="Expectations" data-href="Expectations" href="the-guide/mathematics/statistics/expectations.html" class="internal-link" target="_self" rel="noopener nofollow">expectation</a> exists.<br>Problems

<br> can lead to lower variance or enable computation, but poor choice can blow up variance, even to infinity. This results from situations in which  yields low values while  is high. It is therefore adviced to pick a proposal distribution such that this ratio is bounded.
<br> has to have same support as , same reason as above

<br><br><br>
<br>Self-Normalized Importance Sampling

<br>Applicable if normalization constants of <a data-tooltip-position="top" aria-label="Probability Density Function" data-href="Probability Density Function" href="the-guide/mathematics/probability-theory/probability-density-function.html" class="internal-link" target="_self" rel="noopener nofollow">densities</a> are unknown

<br><a data-tooltip-position="top" aria-label="Estimator" data-href="Estimator" href="the-guide/mathematics/statistics/estimator.html" class="internal-link" target="_self" rel="noopener nofollow">Consistent, but bias</a> of order  if estimating over  samples




<br>Multiple Importance Sampling

<br>Sampling <a data-tooltip-position="top" aria-label="Statistical Independence" data-href="Statistical Independence" href="the-guide/mathematics/statistics/statistical-independence.html" class="internal-link" target="_self" rel="noopener nofollow">independently</a> from many () proposal distributions  each with possibly different number of samples , such that . The MIS <a data-tooltip-position="top" aria-label="Estimator" data-href="Estimator" href="the-guide/mathematics/statistics/estimator.html" class="internal-link" target="_self" rel="noopener nofollow">estimator</a> is with a non-negative weighting <a data-tooltip-position="top" aria-label="Mathematical Relations" data-href="Mathematical Relations" href="the-guide/mathematics/general-stuff/mathematical-relations.html" class="internal-link" target="_self" rel="noopener nofollow">function</a>  fullfilling . This can be chosen in varying ways. A common way is the balance heuristicwhich leads to samples being computed as if they were drawn from a mixture distribution (weighted combination of the  with weights ).


<br>Adaptive Importance Sampling

<br>General class that uses iterative process to update proposal distribution(s) to approximate a target distribution, generally three steps
<br>Generate sample from proposal distribution
<br>Compute importance weights
<br>Adapt proposal distribution(s) based on weights


<br><br><br>
<br><a rel="noopener nofollow" class="external-link" href="https://www.google.com/search?client=ubuntu-sn&amp;hs=coI&amp;channel=fs&amp;sxsrf=APwXEded_e968ZvdWaRsQRJAIBUODgI0lQ:1687100539141&amp;q=importance+sampling&amp;tbm=vid&amp;sa=X&amp;ved=2ahUKEwiNyNeii83_AhXYO-wKHQIOAk8Q0pQJegQIChAB&amp;biw=1046&amp;bih=995&amp;dpr=1#fpstate=ive&amp;vld=cid:4d9be7e2,vid:C3p2wI4RAi8" target="_blank">https://www.google.com/search?client=ubuntu-sn&amp;hs=coI&amp;channel=fs&amp;sxsrf=APwXEded_e968ZvdWaRsQRJAIBUODgI0lQ:1687100539141&amp;q=importance+sampling&amp;tbm=vid&amp;sa=X&amp;ved=2ahUKEwiNyNeii83_AhXYO-wKHQIOAk8Q0pQJegQIChAB&amp;biw=1046&amp;bih=995&amp;dpr=1#fpstate=ive&amp;vld=cid:4d9be7e2,vid:C3p2wI4RAi8</a>
<br><a rel="noopener nofollow" class="external-link" href="https://maurocamaraescudero.netlify.app/post/towards-smc-importance-sampling-explained/" target="_blank">https://maurocamaraescudero.netlify.app/post/towards-smc-importance-sampling-explained/</a>
]]></description><link>the-guide/mathematics/probability-theory/importance-sampling.html</link><guid isPermaLink="false">The Guide/Mathematics/Probability Theory/Importance Sampling.md</guid><pubDate>Thu, 19 Sep 2024 09:42:32 GMT</pubDate></item><item><title><![CDATA[Itô Calculus]]></title><description><![CDATA[ 
 <br>Extension of calculus to <a data-tooltip-position="top" aria-label="Stochastic Process" data-href="Stochastic Process" href="the-guide/mathematics/probability-theory/stochastic-process.html" class="internal-link" target="_self" rel="noopener nofollow">stochastic processes</a>, such as <a data-tooltip-position="top" aria-label="Wiener Process" data-href="Wiener Process" href="the-guide/mathematics/probability-theory/wiener-process.html" class="internal-link" target="_self" rel="noopener nofollow">Brownian Motion</a>. Evolves around the concept of a stochastic generalization of the Riemann-Stieltjes integral in analysis. In this Itô stochastic integral, the integrand and the integrators are <a data-tooltip-position="top" aria-label="Stochastic Process" data-href="Stochastic Process" href="the-guide/mathematics/probability-theory/stochastic-process.html" class="internal-link" target="_self" rel="noopener nofollow">stochastic processes</a> where  is a locally square-integrable process adapted to the filtration generated by .<br>...]]></description><link>the-guide/mathematics/probability-theory/itô-calculus.html</link><guid isPermaLink="false">The Guide/Mathematics/Probability Theory/Itô Calculus.md</guid><pubDate>Thu, 15 Aug 2024 12:51:52 GMT</pubDate></item><item><title><![CDATA[Itô Diffusion]]></title><description><![CDATA[ 
 <br>Definition
A (time-homogeneous) Itô diffusion in  is a <a data-tooltip-position="top" aria-label="Stochastic Process" data-href="Stochastic Process" href="the-guide/mathematics/probability-theory/stochastic-process.html" class="internal-link" target="_self" rel="noopener nofollow">stochastic process</a> defined on a <a data-tooltip-position="top" aria-label="Probability Space" data-href="Probability Space" href="the-guide/mathematics/probability-theory/probability-space.html" class="internal-link" target="_self" rel="noopener nofollow">probability space</a>  and satisfying a stochastic differential equation of the form is an -dimensional Brownian motion, while  and  satisfy the Lipschitz continuity function <br>
for some constant  and all .
<br>
<br>The Lipschitz condition ensures the existence of a unique strong solution  to the stochastic differential equation given above. 
<br>The vector field  is known as the drift coefficient of 
<br>The tensor field σ is known as the diffusion coefficient of X 
<br>Neither  nor  depend upon time; if they were to depend upon time, ''X'' would be referred to only as an <a data-tooltip-position="top" aria-label="Itô Calculus" data-href="Itô Calculus" href="the-guide/mathematics/probability-theory/itô-calculus.html" class="internal-link" target="_self" rel="noopener nofollow">Itô process</a>, not a diffusion. 
<br>,,,<br>Itô diffusions have a number of nice properties, which include<br>
<br>the <a data-tooltip-position="top" aria-label="Markov Chain" data-href="Markov Chain" href="the-guide/computational-statistics/data-assimilation/markov-chain.html" class="internal-link" target="_self" rel="noopener nofollow">(strong Markov property)</a>
<br>...
<br>In particular, an Itô diffusion is a continuous, strongly Markovian process such that the domain of its characteristic operator includes all twice-continuously differentiable functions, so it is a ''diffusion'' in the sense defined by Dynkin (1965).]]></description><link>the-guide/mathematics/probability-theory/itô-diffusion.html</link><guid isPermaLink="false">The Guide/Mathematics/Probability Theory/Itô Diffusion.md</guid><pubDate>Thu, 15 Aug 2024 12:51:52 GMT</pubDate></item><item><title><![CDATA[Marginal Distribution]]></title><description><![CDATA[ 
 <br><br>Definition
The marginal distribution of a <a data-tooltip-position="top" aria-label="Set" data-href="Set" href="the-guide/mathematics/general-stuff/set.html" class="internal-link" target="_self" rel="noopener nofollow">subset</a> of a collection of <a data-tooltip-position="top" aria-label="Random Variable" data-href="Random Variable" href="the-guide/mathematics/probability-theory/random-variable.html" class="internal-link" target="_self" rel="noopener nofollow">random variables</a> is the <a data-tooltip-position="top" aria-label="Probability Distribution" data-href="Probability Distribution" href="the-guide/mathematics/probability-theory/probability-distribution.html" class="internal-link" target="_self" rel="noopener nofollow">probability distribution</a> of the variables contained in this subset. Given a joint <a data-tooltip-position="top" aria-label="Probability Density Function" data-href="Probability Density Function" href="the-guide/mathematics/probability-theory/probability-density-function.html" class="internal-link" target="_self" rel="noopener nofollow">pdf</a>, we can retrieve the marginal <a data-tooltip-position="top" aria-label="Probability Distribution" data-href="Probability Distribution" href="the-guide/mathematics/probability-theory/probability-distribution.html" class="internal-link" target="_self" rel="noopener nofollow">distribution</a> viaor given a <a data-tooltip-position="top" aria-label="Probability Mass Function" data-href="Probability Mass Function" href="the-guide/mathematics/probability-theory/probability-mass-function.html" class="internal-link" target="_self" rel="noopener nofollow">pmf</a> via 
]]></description><link>the-guide/mathematics/probability-theory/marginal-distribution.html</link><guid isPermaLink="false">The Guide/Mathematics/Probability Theory/Marginal Distribution.md</guid><pubDate>Mon, 09 Sep 2024 15:33:49 GMT</pubDate></item><item><title><![CDATA[Mixture Distribution]]></title><description><![CDATA[ 
 <br><a data-tooltip-position="top" aria-label="Probability Distribution" data-href="Probability Distribution" href="the-guide/mathematics/probability-theory/probability-distribution.html" class="internal-link" target="_self" rel="noopener nofollow">Distribution</a> of a <a data-tooltip-position="top" aria-label="Random Variable" data-href="Random Variable" href="the-guide/mathematics/probability-theory/random-variable.html" class="internal-link" target="_self" rel="noopener nofollow">random variable</a>  that is constructed by<br>
<br>Fixing a collection of  other <a data-tooltip-position="top" aria-label="Random Variable" data-href="Random Variable" href="the-guide/mathematics/probability-theory/random-variable.html" class="internal-link" target="_self" rel="noopener nofollow">random variables</a> with potentially different <a data-tooltip-position="top" aria-label="Probability Distribution" data-href="Probability Distribution" href="the-guide/mathematics/probability-theory/probability-distribution.html" class="internal-link" target="_self" rel="noopener nofollow">distributions</a>, denoted Mixture components withfor a <a data-tooltip-position="top" aria-label="Cumulative Distribution Function" data-href="Cumulative Distribution Function" href="the-guide/mathematics/probability-theory/cumulative-distribution-function.html" class="internal-link" target="_self" rel="noopener nofollow">cdf</a> and 
<br>Selecting one of those random variables  according to a high-level distribution given by the weights
<br>Realizing the chosen random variable according to its distribution
<br><br><br>
<br>The resulting linear combination is only a <a data-tooltip-position="top" aria-label="Probability Density Function" data-href="Probability Density Function" href="the-guide/mathematics/probability-theory/probability-density-function.html" class="internal-link" target="_self" rel="noopener nofollow">density</a>, if is is a <a data-tooltip-position="top" aria-label="Convexity" data-href="Convexity" href="the-guide/mathematics/optimization/convex-optimization-lecture/convexity.html" class="internal-link" target="_self" rel="noopener nofollow">convex</a> combination (non-negative, sum up to )
<br>For any <a data-tooltip-position="top" aria-label="Mathematical Relations" data-href="Mathematical Relations" href="the-guide/mathematics/general-stuff/mathematical-relations.html" class="internal-link" target="_self" rel="noopener nofollow">function</a>  with the high-level random variable , for which the <a data-tooltip-position="top" aria-label="Expectations" data-href="Expectations" href="the-guide/mathematics/statistics/expectations.html" class="internal-link" target="_self" rel="noopener nofollow">expectation</a> exists, it holds that 
<br>Formulas for general moments available, see <a rel="noopener nofollow" class="external-link" href="https://en.wikipedia.org/wiki/Mixture_distribution" target="_blank">https://en.wikipedia.org/wiki/Mixture_distribution</a>
]]></description><link>the-guide/mathematics/probability-theory/mixture-distribution.html</link><guid isPermaLink="false">The Guide/Mathematics/Probability Theory/Mixture Distribution.md</guid><pubDate>Thu, 15 Aug 2024 12:51:52 GMT</pubDate></item><item><title><![CDATA[Monte Carlo Gradient Estimator]]></title><description><![CDATA[ 
 <br>Question
How can we find the gradient of an <a data-tooltip-position="top" aria-label="Expectations" data-href="Expectations" href="the-guide/mathematics/statistics/expectations.html" class="internal-link" target="_self" rel="noopener nofollow">expectation</a> w.r.t. the parameters  if we don't know the <a data-tooltip-position="top" aria-label="Probability Distribution" data-href="Probability Distribution" href="the-guide/mathematics/probability-theory/probability-distribution.html" class="internal-link" target="_self" rel="noopener nofollow">distribution</a> ?<br>

<br><br>
<br>Law of the Unconscious Statistician

<br>We want expected value of function, but we only know distribution of functions input


<br>Reparametrization Gradient Estimator
<br>Likelihood-Ratio Gradient Estimator

<br>Unbiased, but high variance


<br><br>
<br>Distribution is parametrized by parameters , e.g. a Gaussian   
]]></description><link>the-guide/mathematics/probability-theory/monte-carlo-gradient-estimator.html</link><guid isPermaLink="false">The Guide/Mathematics/Probability Theory/Monte Carlo Gradient Estimator.md</guid><pubDate>Thu, 15 Aug 2024 12:51:52 GMT</pubDate></item><item><title><![CDATA[Monte Carlo Integration]]></title><description><![CDATA[ 
 <br>In a Nutshell
Simple approach to estimate an <a data-tooltip-position="top" aria-label="Expectations" data-href="Expectations" href="the-guide/mathematics/statistics/expectations.html" class="internal-link" target="_self" rel="noopener nofollow">expectation</a> of a <a data-tooltip-position="top" aria-label="Mathematical Relations" data-href="Mathematical Relations" href="the-guide/mathematics/general-stuff/mathematical-relations.html" class="internal-link" target="_self" rel="noopener nofollow">function</a> w.r.t. a <a data-tooltip-position="top" aria-label="Probability Distribution" data-href="Probability Distribution" href="the-guide/mathematics/probability-theory/probability-distribution.html" class="internal-link" target="_self" rel="noopener nofollow">distribution</a>. Necessary in most cases, because integrating over support is expensive or sampling is easier.
<br><br>Monte Carlo Integration
By <a data-tooltip-position="top" aria-label="Law of the unconscious Statistician" data-href="Law of the unconscious Statistician" href="the-guide/mathematics/statistics/law-of-the-unconscious-statistician.html" class="internal-link" target="_self" rel="noopener nofollow">LOTUS</a>, you can simply evaluate samples of the input <a data-tooltip-position="top" aria-label="Probability Distribution" data-href="Probability Distribution" href="the-guide/mathematics/probability-theory/probability-distribution.html" class="internal-link" target="_self" rel="noopener nofollow">distribution</a> and compute
<br>More formally, we approximate a <a data-tooltip-position="top" aria-label="Measure" data-href="Measure" href="the-guide/mathematics/measure-theory/measure.html" class="internal-link" target="_self" rel="noopener nofollow">measure</a>  by  samples , approximately drawn from . We obtain another measure ]]></description><link>the-guide/mathematics/probability-theory/monte-carlo-integration.html</link><guid isPermaLink="false">The Guide/Mathematics/Probability Theory/Monte Carlo Integration.md</guid><pubDate>Thu, 15 Aug 2024 12:51:52 GMT</pubDate></item><item><title><![CDATA[Ornstein-Uhlenbeck Process]]></title><description><![CDATA[ 
 <br><a data-href="Stochastic Process" href="the-guide/mathematics/probability-theory/stochastic-process.html" class="internal-link" target="_self" rel="noopener nofollow">Stochastic Process</a> that is a Gaussian process, a Markov process and temporally homogeneous.<br>...]]></description><link>the-guide/mathematics/probability-theory/ornstein-uhlenbeck-process.html</link><guid isPermaLink="false">The Guide/Mathematics/Probability Theory/Ornstein-Uhlenbeck Process.md</guid><pubDate>Thu, 15 Aug 2024 12:51:52 GMT</pubDate></item><item><title><![CDATA[Probability Density Function]]></title><description><![CDATA[ 
 <br>If the <a data-tooltip-position="top" aria-label="Cumulative Distribution Function" data-href="Cumulative Distribution Function" href="the-guide/mathematics/probability-theory/cumulative-distribution-function.html" class="internal-link" target="_self" rel="noopener nofollow">CMF</a> of a continuous <a data-tooltip-position="top" aria-label="Random Variable" data-href="Random Variable" href="the-guide/mathematics/probability-theory/random-variable.html" class="internal-link" target="_self" rel="noopener nofollow">random variable</a> is differentiable, the PDF is defined as<br><br><br>
<br> 
<br>
<br>For continuous <a data-tooltip-position="top" aria-label="Random Variable" data-href="Random Variable" href="the-guide/mathematics/probability-theory/random-variable.html" class="internal-link" target="_self" rel="noopener nofollow">random variable</a>, 
]]></description><link>the-guide/mathematics/probability-theory/probability-density-function.html</link><guid isPermaLink="false">The Guide/Mathematics/Probability Theory/Probability Density Function.md</guid><pubDate>Thu, 15 Aug 2024 12:51:52 GMT</pubDate></item><item><title><![CDATA[Probability Distribution]]></title><description><![CDATA[ 
 <br>Formal Definition
Mathematically a <a data-tooltip-position="top" aria-label="Measure" data-href="Measure" href="the-guide/mathematics/measure-theory/measure.html" class="internal-link" target="_self" rel="noopener nofollow">proportional measure</a> that assigns proportions to a simplex, a collection of variables. Given a <a data-tooltip-position="top" aria-label="Measure" data-href="Measure" href="the-guide/mathematics/measure-theory/measure.html" class="internal-link" target="_self" rel="noopener nofollow">measurable space</a> , a probability distribution maps each element of the <a data-tooltip-position="top" aria-label="Sigma-Algebra" data-href="Sigma-Algebra" href="the-guide/mathematics/probability-theory/sigma-algebra.html" class="internal-link" target="_self" rel="noopener nofollow">sigma algebra</a> to the unit interval.
<br><img alt="center" src="lib/media/pasted-image-20240430193121.png" style="width: 450px; max-width: 100%;"><br>
Equipping a set with a sigma-algebra and a probability distribution yields a <a data-tooltip-position="top" aria-label="Probability Space" data-href="Probability Space" href="the-guide/mathematics/probability-theory/probability-space.html" class="internal-link" target="_self" rel="noopener nofollow">probability space.</a> If a <a data-tooltip-position="top" aria-label="Random Variable" data-href="Random Variable" href="the-guide/mathematics/probability-theory/random-variable.html" class="internal-link" target="_self" rel="noopener nofollow">random variable</a> takes values in a set  that is equipped with a probability distribution , we write . <br>A probability distribution is normally represented by<br>
<br>Discrete case  

<br><a data-href="Cumulative Distribution Function" href="the-guide/mathematics/probability-theory/cumulative-distribution-function.html" class="internal-link" target="_self" rel="noopener nofollow">Cumulative Distribution Function</a> 
<br><a data-href="Probability Mass Function" href="the-guide/mathematics/probability-theory/probability-mass-function.html" class="internal-link" target="_self" rel="noopener nofollow">Probability Mass Function</a> 


<br>Continuous case

<br><a data-href="Cumulative Distribution Function" href="the-guide/mathematics/probability-theory/cumulative-distribution-function.html" class="internal-link" target="_self" rel="noopener nofollow">Cumulative Distribution Function</a> , which (if differentiable) leads to

<br><a data-href="Probability Density Function" href="the-guide/mathematics/probability-theory/probability-density-function.html" class="internal-link" target="_self" rel="noopener nofollow">Probability Density Function</a> 




<br><br>
<br>1 - <a data-href="Expectations" href="the-guide/mathematics/statistics/expectations.html" class="internal-link" target="_self" rel="noopener nofollow">Expectations</a>
<br>2 - <a data-href="Covariance and Variance" href="the-guide/mathematics/statistics/covariance-and-variance.html" class="internal-link" target="_self" rel="noopener nofollow">Covariance and Variance</a> characterizes how strongly the measure  concentrates around its centrality
<br>3 - Skewness (standardized, dimensionless)characterizes how symmetric the concentration of  is around its centrality.
<br>4 - Kurtosis (standardized, dimensionless)
<br><br><br>
<br>Bernoulli Distribution <br>
Let  be a binary random variable, e.g.  with  and  for a parameter . 
<br>Binomial Distribution <br>
Assume we draw  samples from a Bernoulli distribution with parameter .<br>
Let  represent the number of successes (≡ number of ones) not considering the<br>
order of drawing.
<br>Poisson Distribution <br>
The Poisson distribution can be derived as a limit of the Binomial distribution. Consider  and a parameter .  
<br><br><br>
<br>Uniform Distribution 


<br><a data-href="Gaussian Distribution" href="the-guide/mathematics/probability-theory/gaussian-distribution.html" class="internal-link" target="_self" rel="noopener nofollow">Gaussian Distribution</a>  <br>
With  and  
<br><br><br>The joint probability distribution is the <a data-tooltip-position="top" aria-label="Probability Mass Function" data-href="Probability Mass Function" href="the-guide/mathematics/probability-theory/probability-mass-function.html" class="internal-link" target="_self" rel="noopener nofollow">probability mass function</a> of a vector of <a data-tooltip-position="top" aria-label="Random Variable" data-href="Random Variable" href="the-guide/mathematics/probability-theory/random-variable.html" class="internal-link" target="_self" rel="noopener nofollow">random variables</a>.  The bivariate case reads <br><br><br>For , the probability of an <a data-tooltip-position="top" aria-label="Sigma-Algebra" data-href="Sigma-Algebra" href="the-guide/mathematics/probability-theory/sigma-algebra.html" class="internal-link" target="_self" rel="noopener nofollow">event</a> , given that event  already happened is<br>
For <a data-tooltip-position="top" aria-label="Random Variable" data-href="Random Variable" href="the-guide/mathematics/probability-theory/random-variable.html" class="internal-link" target="_self" rel="noopener nofollow">random variables</a> , the conditional probability is<br><br><br>
<br>
<br><br>
<br><a rel="noopener nofollow" class="external-link" href="https://betanalpha.github.io/assets/case_studies/probability_densities.html" target="_blank">https://betanalpha.github.io/assets/case_studies/probability_densities.html</a>
]]></description><link>the-guide/mathematics/probability-theory/probability-distribution.html</link><guid isPermaLink="false">The Guide/Mathematics/Probability Theory/Probability Distribution.md</guid><pubDate>Thu, 15 Aug 2024 12:51:52 GMT</pubDate><enclosure url="lib/media/pasted-image-20240430193121.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src=&quot;lib/media/pasted-image-20240430193121.png&quot;&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[Probability Mass Function]]></title><description><![CDATA[ 
 <br>For a discrete <a data-tooltip-position="top" aria-label="Random Variable" data-href="Random Variable" href="the-guide/mathematics/probability-theory/random-variable.html" class="internal-link" target="_self" rel="noopener nofollow">random variable</a> , the PMF is defined as a function mapping <br><br>This concept can be expanded to a vector of random variables, yielding the <a data-tooltip-position="top" aria-label="Probability Distribution" data-href="Probability Distribution" href="the-guide/mathematics/probability-theory/probability-distribution.html" class="internal-link" target="_self" rel="noopener nofollow">joint probability distribution</a><br><br>Probability mass functions transform naturally along a <a data-tooltip-position="top" aria-label="Measurable Function" data-href="Measurable Function" href="the-guide/mathematics/measure-theory/measurable-function.html" class="internal-link" target="_self" rel="noopener nofollow">measurable function</a>  via ]]></description><link>the-guide/mathematics/probability-theory/probability-mass-function.html</link><guid isPermaLink="false">The Guide/Mathematics/Probability Theory/Probability Mass Function.md</guid><pubDate>Thu, 15 Aug 2024 12:51:52 GMT</pubDate></item><item><title><![CDATA[Probability Space]]></title><description><![CDATA[ 
 <br>Definition
Also called probability triplet, denotes the triplet of  with 

<br><a data-tooltip-position="top" aria-label="Sigma-Algebra" data-href="Sigma-Algebra" href="the-guide/mathematics/probability-theory/sigma-algebra.html" class="internal-link" target="_self" rel="noopener nofollow">Sample Space</a> , <a data-tooltip-position="top" aria-label="Set" data-href="Set" href="the-guide/mathematics/general-stuff/set.html" class="internal-link" target="_self" rel="noopener nofollow">set</a> of all conceptually possible outcomes 
<br><a data-href="Sigma-Algebra" href="the-guide/mathematics/probability-theory/sigma-algebra.html" class="internal-link" target="_self" rel="noopener nofollow">Sigma-Algebra</a> , <a data-tooltip-position="top" aria-label="Set" data-href="Set" href="the-guide/mathematics/general-stuff/set.html" class="internal-link" target="_self" rel="noopener nofollow">subsets</a> of 
<br><a data-tooltip-position="top" aria-label="Measure" data-href="Measure" href="the-guide/mathematics/measure-theory/measure.html" class="internal-link" target="_self" rel="noopener nofollow">Probability Measure</a> , <a data-tooltip-position="top" aria-label="Mathematical Relations" data-href="Mathematical Relations" href="the-guide/mathematics/general-stuff/mathematical-relations.html" class="internal-link" target="_self" rel="noopener nofollow">function</a> that assigns probability between  and  to each event in 

<br><img alt="center" src="lib/media/pasted-image-20240114181413.png" style="width: 400px; max-width: 100%;"><br>Intuition
Goddess of Chance chooses a point  at random according to the law  that such an event  has a probability given by . We can choose  by some physical process/experiment, as long as it is random.
]]></description><link>the-guide/mathematics/probability-theory/probability-space.html</link><guid isPermaLink="false">The Guide/Mathematics/Probability Theory/Probability Space.md</guid><pubDate>Mon, 14 Apr 2025 15:10:45 GMT</pubDate><enclosure url="lib/media/pasted-image-20240114181413.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src=&quot;lib/media/pasted-image-20240114181413.png&quot;&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[Random Variable]]></title><description><![CDATA[ 
 <br>Definition
A random variable  is a <a data-tooltip-position="top" aria-label="Mathematical Relations" data-href="Mathematical Relations" href="the-guide/mathematics/general-stuff/mathematical-relations.html" class="internal-link" target="_self" rel="noopener nofollow">mapping</a> / deterministic, <a data-tooltip-position="top" aria-label="Measure" data-href="Measure" href="the-guide/mathematics/measure-theory/measure.html" class="internal-link" target="_self" rel="noopener nofollow">measurable</a> function that assigns an element  of a measurable space  (e.g. real numbers) to each outcome (element of <a data-tooltip-position="top" aria-label="Sigma-Algebra" data-href="Sigma-Algebra" href="the-guide/mathematics/probability-theory/sigma-algebra.html" class="internal-link" target="_self" rel="noopener nofollow">sample space</a>  of a <a data-tooltip-position="top" aria-label="Probability Space" data-href="Probability Space" href="the-guide/mathematics/probability-theory/probability-space.html" class="internal-link" target="_self" rel="noopener nofollow">probability space</a> ) :It thereby induces a <a data-tooltip-position="top" aria-label="Push-Forward Measure" data-href="Push-Forward Measure" href="the-guide/mathematics/measure-theory/push-forward-measure.html" class="internal-link" target="_self" rel="noopener nofollow">pushforward measure</a> , which is denoted the law or <a data-tooltip-position="top" aria-label="Probability Distribution" data-href="Probability Distribution" href="the-guide/mathematics/probability-theory/probability-distribution.html" class="internal-link" target="_self" rel="noopener nofollow">distribution</a> or .
<br>We distinguish between discrete random variables with  (or another ordered finite or countable space) and continuous random variables with .<br>
<br>Example

<br><a data-tooltip-position="top" aria-label="Sigma-Algebra" data-href="Sigma-Algebra" href="the-guide/mathematics/probability-theory/sigma-algebra.html" class="internal-link" target="_self" rel="noopener nofollow">Sample Space</a> is Head and Tails of a coin , assigned to measurable space  


<br><br><br>A complex random variable  on the <a data-tooltip-position="top" aria-label="Measure" data-href="Measure" href="the-guide/mathematics/measure-theory/measure.html" class="internal-link" target="_self" rel="noopener nofollow">probability space</a>  is a function , such that both its real part  and its imaginary part  are real random variables on .]]></description><link>the-guide/mathematics/probability-theory/random-variable.html</link><guid isPermaLink="false">The Guide/Mathematics/Probability Theory/Random Variable.md</guid><pubDate>Sat, 20 Jul 2024 16:52:57 GMT</pubDate></item><item><title><![CDATA[Sigma-Algebra]]></title><description><![CDATA[ 
 <br>In a Nutshell
Collection of sets that is closed under countable unions, intersections, and complements.
<br><br>Definition
Let  be a <a data-tooltip-position="top" aria-label="Set" data-href="Set" href="the-guide/mathematics/general-stuff/set.html" class="internal-link" target="_self" rel="noopener nofollow">set</a>. A collection of subsets (family of events)  is called a -algebra on  if 

<br>
<br>
<br> 

<br>
<br>The pair of a <a data-tooltip-position="top" aria-label="Set" data-href="Set" href="the-guide/mathematics/general-stuff/set.html" class="internal-link" target="_self" rel="noopener nofollow">set</a> and its Sigma-Algebra is a <a data-tooltip-position="top" aria-label="Measure" data-href="Measure" href="the-guide/mathematics/measure-theory/measure.html" class="internal-link" target="_self" rel="noopener nofollow">measurable space</a>, because we can then define a function that measures the abstract size of every element, leading to a <a data-tooltip-position="top" aria-label="Measure" data-href="Measure" href="the-guide/mathematics/measure-theory/measure.html" class="internal-link" target="_self" rel="noopener nofollow">measure space</a>. 
<br><br>Sample Space 
<a data-tooltip-position="top" aria-label="Set" data-href="Set" href="the-guide/mathematics/general-stuff/set.html" class="internal-link" target="_self" rel="noopener nofollow">Set</a> of all possible outcomes (atomic events) of an experiment. Number of atomic events can be finite, infinite and countable or uncountable. 
<br><img alt="center" src="lib/media/pasted-image-20240114180015.png" style="width: 200px; max-width: 100%;"><br>
<br>Example

<br>flipping of two coins  


<br>Event
<a data-tooltip-position="top" aria-label="Set" data-href="Set" href="the-guide/mathematics/general-stuff/set.html" class="internal-link" target="_self" rel="noopener nofollow">Subset</a> of the <a data-tooltip-position="top" aria-label="Sigma-Algebra" data-href="Sigma-Algebra" href="the-guide/mathematics/probability-theory/sigma-algebra.html" class="internal-link" target="_self" rel="noopener nofollow">sample space</a> 
<br>
<br>Example

<br>At least one tail: 


<br><br><br>Event Space
Set  of all defined <a data-tooltip-position="top" aria-label="Sigma-Algebra" data-href="Sigma-Algebra" href="the-guide/mathematics/probability-theory/sigma-algebra.html" class="internal-link" target="_self" rel="noopener nofollow">events</a> of interest, to which we want to assign a probability.<br>
Must be a <a data-href="Sigma-Algebra" href="the-guide/mathematics/probability-theory/sigma-algebra.html" class="internal-link" target="_self" rel="noopener nofollow">Sigma-Algebra</a>. In general, the event space therefore includes trivial elements, all events and their complements as well as all possible unions.
<br><img alt="center" src="lib/media/pasted-image-20240114180402.png" style="width: 300px; max-width: 100%;"><br>
<br>Example

<br>


]]></description><link>the-guide/mathematics/probability-theory/sigma-algebra.html</link><guid isPermaLink="false">The Guide/Mathematics/Probability Theory/Sigma-Algebra.md</guid><pubDate>Tue, 20 Aug 2024 13:39:18 GMT</pubDate><enclosure url="lib/media/pasted-image-20240114180015.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src=&quot;lib/media/pasted-image-20240114180015.png&quot;&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[Stochastic Differential Equation]]></title><description><![CDATA[ 
 <br>
<br><a rel="noopener nofollow" class="external-link" href="https://betanalpha.github.io/assets/case_studies/stochastic_differential_equations.html" target="_blank">https://betanalpha.github.io/assets/case_studies/stochastic_differential_equations.html</a>
]]></description><link>the-guide/mathematics/probability-theory/stochastic-differential-equation.html</link><guid isPermaLink="false">The Guide/Mathematics/Probability Theory/Stochastic Differential Equation.md</guid><pubDate>Mon, 09 Sep 2024 15:33:49 GMT</pubDate></item><item><title><![CDATA[Stochastic Process]]></title><description><![CDATA[ 
 <br>Definition
Collection of <a data-tooltip-position="top" aria-label="Random Variable" data-href="Random Variable" href="the-guide/mathematics/probability-theory/random-variable.html" class="internal-link" target="_self" rel="noopener nofollow">random variables</a> indexed by some <a data-tooltip-position="top" aria-label="Set" data-href="Set" href="the-guide/mathematics/general-stuff/set.html" class="internal-link" target="_self" rel="noopener nofollow">set</a> (index set), usually time  on a common <a data-tooltip-position="top" aria-label="Probability Space" data-href="Probability Space" href="the-guide/mathematics/probability-theory/probability-space.html" class="internal-link" target="_self" rel="noopener nofollow">probability space</a>  with <a data-tooltip-position="top" aria-label="Sigma-Algebra" data-href="Sigma-Algebra" href="the-guide/mathematics/probability-theory/sigma-algebra.html" class="internal-link" target="_self" rel="noopener nofollow">sample-space</a> , <a data-tooltip-position="top" aria-label="Sigma-Algebra" data-href="Sigma-Algebra" href="the-guide/mathematics/probability-theory/sigma-algebra.html" class="internal-link" target="_self" rel="noopener nofollow">Sigma-Algebra</a>  and <a data-tooltip-position="top" aria-label="Measure" data-href="Measure" href="the-guide/mathematics/measure-theory/measure.html" class="internal-link" target="_self" rel="noopener nofollow">probability measure</a> .
Each random variable, indexed by an  takes on values in the same <a data-tooltip-position="top" aria-label="Space" data-href="Space" href="the-guide/mathematics/general-stuff/space.html" class="internal-link" target="_self" rel="noopener nofollow">mathematical space</a> , which must be <a data-tooltip-position="top" aria-label="Measure" data-href="Measure" href="the-guide/mathematics/measure-theory/measure.html" class="internal-link" target="_self" rel="noopener nofollow">measurable</a> with respect to some <a data-tooltip-position="top" aria-label="Sigma-Algebra" data-href="Sigma-Algebra" href="the-guide/mathematics/probability-theory/sigma-algebra.html" class="internal-link" target="_self" rel="noopener nofollow">Sigma-Algebra</a> . The collection of these random variables is denoted 
<br>
<br>Categories

<br>Stationary Process - unconditional <a data-tooltip-position="top" aria-label="Probability Distribution" data-href="Probability Distribution" href="the-guide/mathematics/probability-theory/probability-distribution.html" class="internal-link" target="_self" rel="noopener nofollow">joint distribution</a> does not change when shifted in tome, <a data-tooltip-position="top" aria-label="Expectations" data-href="Expectations" href="the-guide/mathematics/statistics/expectations.html" class="internal-link" target="_self" rel="noopener nofollow">mean</a> and <a data-tooltip-position="top" aria-label="Covariance and Variance" data-href="Covariance and Variance" href="the-guide/mathematics/statistics/covariance-and-variance.html" class="internal-link" target="_self" rel="noopener nofollow">variance</a> do not change in time. Resulting graph does not trend up or down


<br><br><br>A single outcome of a stochastic process is called a sample function, a sample path, a trajectory , a path function or, a realization. It is formed by taking a single value of each <a data-tooltip-position="top" aria-label="Random Variable" data-href="Random Variable" href="the-guide/mathematics/probability-theory/random-variable.html" class="internal-link" target="_self" rel="noopener nofollow">random variable</a> of the stochastic process. More precisely, if  is a stochastic process, then for any point , the <a data-tooltip-position="top" aria-label="Mathematical Relations" data-href="Mathematical Relations" href="the-guide/mathematics/general-stuff/mathematical-relations.html" class="internal-link" target="_self" rel="noopener nofollow">mapping</a><br>
is a sample function of the stochastic process . <br><br><br>
<br><a data-href="Bernoulli Process" href="Bernoulli Process" class="internal-link" target="_self" rel="noopener nofollow">Bernoulli Process</a>

<br>No dependence between the random variables


<br><a data-href="Markov Chain" href="the-guide/computational-statistics/data-assimilation/markov-chain.html" class="internal-link" target="_self" rel="noopener nofollow">Markov Chain</a> 

<br>Dependence between the random variables


<br><br>
<br>Examples - most important stochastic processes are

<br><a data-href="Wiener Process" href="the-guide/mathematics/probability-theory/wiener-process.html" class="internal-link" target="_self" rel="noopener nofollow">Wiener Process</a>, Brownian Motion Process
<br><a data-href="Poisson Process" href="Poisson Process" class="internal-link" target="_self" rel="noopener nofollow">Poisson Process</a>


]]></description><link>the-guide/mathematics/probability-theory/stochastic-process.html</link><guid isPermaLink="false">The Guide/Mathematics/Probability Theory/Stochastic Process.md</guid><pubDate>Thu, 15 Aug 2024 12:51:52 GMT</pubDate></item><item><title><![CDATA[Strong Law of Large Numbers]]></title><description><![CDATA[ 
 <br>TODO<br>
<a rel="noopener nofollow" class="external-link" href="https://en.wikipedia.org/wiki/Law_of_large_numbers" target="_blank">https://en.wikipedia.org/wiki/Law_of_large_numbers</a>]]></description><link>the-guide/mathematics/probability-theory/strong-law-of-large-numbers.html</link><guid isPermaLink="false">The Guide/Mathematics/Probability Theory/Strong Law of Large Numbers.md</guid><pubDate>Thu, 15 Aug 2024 12:51:52 GMT</pubDate></item><item><title><![CDATA[The Reparametrization Trick]]></title><description><![CDATA[ 
 <br>In a Nutshell
Transformation that allows us to decouple the parameters of a  <a data-tooltip-position="top" aria-label="Gaussian Distribution" data-href="Gaussian Distribution" href="the-guide/mathematics/probability-theory/gaussian-distribution.html" class="internal-link" target="_self" rel="noopener nofollow">Gaussian</a> <a data-tooltip-position="top" aria-label="Probability Distribution" data-href="Probability Distribution" href="the-guide/mathematics/probability-theory/probability-distribution.html" class="internal-link" target="_self" rel="noopener nofollow">distributions</a> from its stochasticity. Enables us to <a data-tooltip-position="top" aria-label="Forward and Backward Propagation" data-href="Forward and Backward Propagation" href="the-guide/machine-learning/deep-learning/forward-and-backward-propagation.html" class="internal-link" target="_self" rel="noopener nofollow">propagate</a> <a data-tooltip-position="top" aria-label="Derivative, Gradient, Jacobian and Hessian" data-href="Derivative, Gradient, Jacobian and Hessian" href="the-guide/mathematics/analysis-and-calculus/derivative,-gradient,-jacobian-and-hessian.html" class="internal-link" target="_self" rel="noopener nofollow">gradients</a> of these parameters via computational path.
<br><br>In many computational statistics problems and in deep learning, taking <a data-tooltip-position="top" aria-label="Derivative, Gradient, Jacobian and Hessian" data-href="Derivative, Gradient, Jacobian and Hessian" href="the-guide/mathematics/analysis-and-calculus/derivative,-gradient,-jacobian-and-hessian.html" class="internal-link" target="_self" rel="noopener nofollow">derivatives</a> w.r.t. parameters  of <a data-tooltip-position="top" aria-label="Expectations" data-href="Expectations" href="the-guide/mathematics/statistics/expectations.html" class="internal-link" target="_self" rel="noopener nofollow">expectations</a> likeis a common problem, since samples themselves depend on . Initially, there is no meaningful way to derive a gradient.<br>However, if we can split the sampling process into a stochastic and deterministic part by applying a <a data-tooltip-position="top" aria-label="Transformations of Random Variables" data-href="Transformations of Random Variables" href="the-guide/mathematics/probability-theory/transformations-of-random-variables.html" class="internal-link" target="_self" rel="noopener nofollow">transformation of the random variable</a>we can rewrite the objectiveThis way, when computing the gradient , we can now pull the operator inside the expectationand the derivative follows simply by <a data-tooltip-position="top" aria-label="Derivative Tricks and Theorems" data-href="Derivative Tricks and Theorems" href="the-guide/mathematics/analysis-and-calculus/derivative-tricks-and-theorems.html" class="internal-link" target="_self" rel="noopener nofollow">chain rule</a> <br><br><br>Consider e.g. a <a data-tooltip-position="top" aria-label="Gaussian Distribution" data-href="Gaussian Distribution" href="the-guide/mathematics/probability-theory/gaussian-distribution.html" class="internal-link" target="_self" rel="noopener nofollow">Gaussian</a> <a data-tooltip-position="top" aria-label="Probability Distribution" data-href="Probability Distribution" href="the-guide/mathematics/probability-theory/probability-distribution.html" class="internal-link" target="_self" rel="noopener nofollow">distribution</a> where  and  are parameterized by . Since the <a data-tooltip-position="top" aria-label="Covariance and Variance" data-href="Covariance and Variance" href="the-guide/mathematics/statistics/covariance-and-variance.html" class="internal-link" target="_self" rel="noopener nofollow">covariance</a> <a data-tooltip-position="top" aria-label="Matrices" data-href="Matrices" href="the-guide/mathematics/linear-algebra/matrices.html" class="internal-link" target="_self" rel="noopener nofollow">matrix</a> can be decomposed via <a data-href="Cholesky Decomposition" href="Cholesky Decomposition" class="internal-link" target="_self" rel="noopener nofollow">Cholesky Decomposition</a> (it is positive definite), we can writewith  lower-triangular. From this, we can create the transformation asto take the gradient as described above<br>Proof
To prove that the transformed random variable describes the same distribution, we computeand
<br><img alt="center" src="lib/media/pasted-image-20250227003544.png" style="width: 500px; max-width: 100%;">]]></description><link>the-guide/mathematics/probability-theory/the-reparametrization-trick.html</link><guid isPermaLink="false">The Guide/Mathematics/Probability Theory/The Reparametrization Trick.md</guid><pubDate>Fri, 28 Feb 2025 21:47:24 GMT</pubDate><enclosure url="lib/media/pasted-image-20250227003544.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src=&quot;lib/media/pasted-image-20250227003544.png&quot;&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[Transformations of Random Variables]]></title><description><![CDATA[ 
 <br>In a Nutshell
When a <a data-tooltip-position="top" aria-label="Random Variable" data-href="Random Variable" href="the-guide/mathematics/probability-theory/random-variable.html" class="internal-link" target="_self" rel="noopener nofollow">random variable</a> is transformed via a one-to-one transformation, the resulting <a data-tooltip-position="top" aria-label="Probability Density Function" data-href="Probability Density Function" href="the-guide/mathematics/probability-theory/probability-density-function.html" class="internal-link" target="_self" rel="noopener nofollow">density function</a> can be explicitly stated via the <a data-href="Inverse Function Theorem" href="the-guide/mathematics/analysis-and-calculus/inverse-function-theorem.html" class="internal-link" target="_self" rel="noopener nofollow">Inverse Function Theorem</a>.
<br><br>Theorem
Suppose  is a <a data-tooltip-position="top" aria-label="Random Variable" data-href="Random Variable" href="the-guide/mathematics/probability-theory/random-variable.html" class="internal-link" target="_self" rel="noopener nofollow">random variable</a> taking values in  with a <a data-tooltip-position="top" aria-label="Continuity" data-href="Continuity" href="the-guide/mathematics/analysis-and-calculus/continuity.html" class="internal-link" target="_self" rel="noopener nofollow">continuous</a> <a data-tooltip-position="top" aria-label="Probability Distribution" data-href="Probability Distribution" href="the-guide/mathematics/probability-theory/probability-distribution.html" class="internal-link" target="_self" rel="noopener nofollow">distribution</a> with <a data-tooltip-position="top" aria-label="Probability Density Function" data-href="Probability Density Function" href="the-guide/mathematics/probability-theory/probability-density-function.html" class="internal-link" target="_self" rel="noopener nofollow">probability density</a> . In addition, suppose with  <a data-tooltip-position="top" aria-label="Mathematical Relations" data-href="Mathematical Relations" href="the-guide/mathematics/general-stuff/mathematical-relations.html" class="internal-link" target="_self" rel="noopener nofollow">bijective</a>. Then  has a continuous distribution with density function
<br>Proof
Let  be a <a data-tooltip-position="top" aria-label="Measure" data-href="Measure" href="the-guide/mathematics/measure-theory/measure.html" class="internal-link" target="_self" rel="noopener nofollow">measurable</a> <a data-tooltip-position="top" aria-label="Set" data-href="Set" href="the-guide/mathematics/general-stuff/set.html" class="internal-link" target="_self" rel="noopener nofollow">set</a>. The accumulated probability of  taking a value in  is where  follows from bijectivity of  and  from <a data-tooltip-position="top" aria-label="Integration Tricks and Theorems" data-href="Integration Tricks and Theorems" href="the-guide/mathematics/analysis-and-calculus/integration-tricks-and-theorems.html" class="internal-link" target="_self" rel="noopener nofollow">integration by substitution</a>. Therefore, by <a data-href="Inverse Function Theorem" href="the-guide/mathematics/analysis-and-calculus/inverse-function-theorem.html" class="internal-link" target="_self" rel="noopener nofollow">Inverse Function Theorem</a>
]]></description><link>the-guide/mathematics/probability-theory/transformations-of-random-variables.html</link><guid isPermaLink="false">The Guide/Mathematics/Probability Theory/Transformations of Random Variables.md</guid><pubDate>Mon, 14 Apr 2025 15:12:52 GMT</pubDate></item><item><title><![CDATA[Weak Law of Large Numbers]]></title><description><![CDATA[ 
 <br>Weak Law of Large Numbers
For <a data-tooltip-position="top" aria-label="Statistical Independence" data-href="Statistical Independence" href="the-guide/mathematics/statistics/statistical-independence.html" class="internal-link" target="_self" rel="noopener nofollow">independent, identically distributed</a> samples of a <a data-tooltip-position="top" aria-label="Random Variable" data-href="Random Variable" href="the-guide/mathematics/probability-theory/random-variable.html" class="internal-link" target="_self" rel="noopener nofollow">random variables</a> with <a data-tooltip-position="top" aria-label="Expectations" data-href="Expectations" href="the-guide/mathematics/statistics/expectations.html" class="internal-link" target="_self" rel="noopener nofollow">expectation</a> , the sample mean  converges in probability to the expected value
<br>
<br>For specified large , the average  is likely to be near . However, is is still possible that  happens an infinite number of times, although at infrequent intervals.
]]></description><link>the-guide/mathematics/probability-theory/weak-law-of-large-numbers.html</link><guid isPermaLink="false">The Guide/Mathematics/Probability Theory/Weak Law of Large Numbers.md</guid><pubDate>Thu, 15 Aug 2024 12:51:52 GMT</pubDate></item><item><title><![CDATA[Wiener Process]]></title><description><![CDATA[ 
 <br>Real-valued continuous-time <a data-tooltip-position="top" aria-label="Stochastic Process" data-href="Stochastic Process" href="the-guide/mathematics/probability-theory/stochastic-process.html" class="internal-link" target="_self" rel="noopener nofollow">stochastic process</a>, often referred to as Brownian motion. The process is a <a data-tooltip-position="top" aria-label="Markov Decision Process" data-href="Markov Decision Process" href="the-guide/machine-learning/reinforcement-learning/markov-decision-process.html" class="internal-link" target="_self" rel="noopener nofollow">Markov process</a> characterized by the following properties:<br>Definition Wiener Process
A real-valued <a data-tooltip-position="top" aria-label="Stochastic Process" data-href="Stochastic Process" href="the-guide/mathematics/probability-theory/stochastic-process.html" class="internal-link" target="_self" rel="noopener nofollow">stochastic process</a>, more precisely an <a data-href="Itô Diffusion" href="the-guide/mathematics/probability-theory/itô-diffusion.html" class="internal-link" target="_self" rel="noopener nofollow">Itô Diffusion</a>  defined on a <a data-tooltip-position="top" aria-label="Probability Space" data-href="Probability Space" href="the-guide/mathematics/probability-theory/probability-space.html" class="internal-link" target="_self" rel="noopener nofollow">probability space</a>  is a standard Wiener process, if:

<br>The initial value is zero with probability 1.
<br>The increment  is independent of the past  with .
<br>The increment  is a <a data-tooltip-position="top" aria-label="Gaussian Distribution" data-href="Gaussian Distribution" href="the-guide/mathematics/probability-theory/gaussian-distribution.html" class="internal-link" target="_self" rel="noopener nofollow">normal</a> variable with <a data-tooltip-position="top" aria-label="Expectations" data-href="Expectations" href="the-guide/mathematics/statistics/expectations.html" class="internal-link" target="_self" rel="noopener nofollow">mean</a>  and <a data-tooltip-position="top" aria-label="Variance" data-href="Variance" href="Variance" class="internal-link" target="_self" rel="noopener nofollow">variance</a> .
<br>For  the increment  is equal in <a data-tooltip-position="top" aria-label="Probability Distribution" data-href="Probability Distribution" href="the-guide/mathematics/probability-theory/probability-distribution.html" class="internal-link" target="_self" rel="noopener nofollow">distribution</a> to $W_<a class="internal-link" data-href="Covariance and Variance.md" href="the-guide/mathematics/statistics/covariance-and-variance.html" target="_self" rel="noopener nofollow"></a>age 20231211090729.png|center]]

<br><br><br>
<br><a rel="noopener nofollow" class="external-link" href="https://hpaulkeeler.com/wiener-or-brownian-motion-process/" target="_blank">https://hpaulkeeler.com/wiener-or-brownian-motion-process/</a>
]]></description><link>the-guide/mathematics/probability-theory/wiener-process.html</link><guid isPermaLink="false">The Guide/Mathematics/Probability Theory/Wiener Process.md</guid><pubDate>Thu, 15 Aug 2024 12:51:52 GMT</pubDate></item><item><title><![CDATA[- Bayesian Statistics -]]></title><description><![CDATA[ 
 <br>In a Nutshell
Theory in statistics based on the Bayesian interpretation of probability, where probability expresses degree of belief in an <a data-tooltip-position="top" aria-label="Sigma-Algebra" data-href="Sigma-Algebra" href="the-guide/mathematics/probability-theory/sigma-algebra.html" class="internal-link" target="_self" rel="noopener nofollow">event</a> (<a data-href="Frequentist vs. Bayesian Statistics" href="the-guide/mathematics/statistics/frequentist-vs.-bayesian-statistics.html" class="internal-link" target="_self" rel="noopener nofollow">Frequentist vs. Bayesian Statistics</a>). The degree of belief may be based on prior knowledge about the event, such as the results of previous experiments, or on personal beliefs about the event. 
<br><br><br><br>Method of statistical inference that uses <a data-href="Bayes Theorem" href="the-guide/mathematics/statistics/bayes-theorem.html" class="internal-link" target="_self" rel="noopener nofollow">Bayes Theorem</a> to update probability for a hypothesis as more evidence or information becomes available.<br>
<br>
Formal Definition with Parametric Distributions

<br>Data points  are distributed  using parametric distributions with hyperparameters  (prior) with fixed . Assume we observed a set of  samples  and want to update our prior in order to predict the distribution for a new sample . This is done via which can be written as with  for multiple observations (<a data-tooltip-position="top" aria-label="Statistical Independence" data-href="Statistical Independence" href="the-guide/mathematics/statistics/statistical-independence.html" class="internal-link" target="_self" rel="noopener nofollow">i.i.d.</a>).

<br>The denominator is often impossible to compute efficiently, because is scales exponentially with the number of parameters !




<br>
Bayesian Prediction

<br>After we compute the posterior, we can predict the distribution of a new sample  based on the observations by marginalizing over the posterior via the posterior predictive distribution
<br>Instead marginalizing over the prior yields the prior predictive distribution


<br>
Toy Example

<br>Assume prior that one of two success models is correct, one optimistic with  and one pessimistic with . We begin by assuming a <a data-tooltip-position="top" aria-label="Probability Distribution" data-href="Probability Distribution" href="the-guide/mathematics/probability-theory/probability-distribution.html" class="internal-link" target="_self" rel="noopener nofollow">uniform distribution</a> over these models . If we observe a binary variable (success and failure), this yields the initial table<img alt="center" src="lib/media/pasted-image-20240117104519.png" style="width: 300px; max-width: 100%;">with the marginal likelihood  and the prior 
<br>Assume we observe a success, we want to compute the posterior . This can be done via  and .


<br><br><br>Approach in <a data-tooltip-position="top" aria-label="Bayes Theorem" data-href="Bayes Theorem" href="the-guide/mathematics/statistics/bayes-theorem.html" class="internal-link" target="_self" rel="noopener nofollow">Bayesian</a> statistics and <a data-tooltip-position="top" aria-label="- Machine Learning -" data-href="- Machine Learning -" href="the-guide/machine-learning/-machine-learning-.html" class="internal-link" target="_self" rel="noopener nofollow">machine learning</a> to update <a data-tooltip-position="top" aria-label="Bayes Theorem" data-href="Bayes Theorem" href="the-guide/mathematics/statistics/bayes-theorem.html" class="internal-link" target="_self" rel="noopener nofollow">prior</a> belief based on integration of new evidence (data), leading to <a data-tooltip-position="top" aria-label="Bayes Theorem" data-href="Bayes Theorem" href="the-guide/mathematics/statistics/bayes-theorem.html" class="internal-link" target="_self" rel="noopener nofollow">posterior</a>. In most applications, this would require <a data-tooltip-position="top" aria-label="Marginal Distribution" data-href="Marginal Distribution" href="the-guide/mathematics/probability-theory/marginal-distribution.html" class="internal-link" target="_self" rel="noopener nofollow">marginalizing</a> over the whole parameter / <a data-tooltip-position="top" aria-label="Latent Variable Models" data-href="Latent Variable Models" href="the-guide/machine-learning/generative-models/latent-variable-models.html" class="internal-link" target="_self" rel="noopener nofollow">latent</a> <a data-tooltip-position="top" aria-label="Space" data-href="Space" href="the-guide/mathematics/general-stuff/space.html" class="internal-link" target="_self" rel="noopener nofollow">space</a>, which is infeasible for all but simple systems. Therefore, many approximative approaches have been developed.<br>
<br><a data-href="Variational Inference" href="the-guide/computational-statistics/variational-inference.html" class="internal-link" target="_self" rel="noopener nofollow">Variational Inference</a>

<br>Category of algorithms in statistics and Bayesian machine learning that tries to solve the problem of dealing with intractable <a data-tooltip-position="top" aria-label="Probability Distribution" data-href="Probability Distribution" href="the-guide/mathematics/probability-theory/probability-distribution.html" class="internal-link" target="_self" rel="noopener nofollow">distributions</a>. Variational inference seeks to approximate the problematic distribution  by another approximate distribution .

<br><a data-tooltip-position="top" aria-label="Stein Variational Gradient Descent A General Purpose Bayesian Inference Algorithm" data-href="Stein Variational Gradient Descent A General Purpose Bayesian Inference Algorithm" href="research-papers/master/stein-variational-gradient-descent-a-general-purpose-bayesian-inference-algorithm.html" class="internal-link" target="_self" rel="noopener nofollow">Stein Variational Gradient Descent</a>




<br><a data-href="Markov-Chain Monte Carlo Methods" href="the-guide/computational-statistics/data-assimilation/smoothing-algorithms/markov-chain-monte-carlo-methods.html" class="internal-link" target="_self" rel="noopener nofollow">Markov-Chain Monte Carlo Methods</a>

<br>Category of algorithms for sampling from a <a data-tooltip-position="top" aria-label="Probability Distribution" data-href="Probability Distribution" href="the-guide/mathematics/probability-theory/probability-distribution.html" class="internal-link" target="_self" rel="noopener nofollow">probability distribution</a> by constructing a <a data-href="Markov Chain" href="the-guide/computational-statistics/data-assimilation/markov-chain.html" class="internal-link" target="_self" rel="noopener nofollow">Markov Chain</a> that has the desired distribution as its <a data-tooltip-position="top" aria-label="Markov Chain" data-href="Markov Chain" href="the-guide/computational-statistics/data-assimilation/markov-chain.html" class="internal-link" target="_self" rel="noopener nofollow">equilibrium distribution</a>.

<br><a data-href="Hamiltonian Monte Carlo" href="the-guide/computational-statistics/hamiltonian-monte-carlo.html" class="internal-link" target="_self" rel="noopener nofollow">Hamiltonian Monte Carlo</a>
<br><a data-href="No-U-Turn Sampler (NUTS)" href="the-guide/computational-statistics/no-u-turn-sampler-(nuts).html" class="internal-link" target="_self" rel="noopener nofollow">No-U-Turn Sampler (NUTS)</a>
<br><a data-href="Markov-Chain Monte Carlo Methods" href="the-guide/computational-statistics/data-assimilation/smoothing-algorithms/markov-chain-monte-carlo-methods.html" class="internal-link" target="_self" rel="noopener nofollow">Markov-Chain Monte Carlo Methods</a>
<br><a data-href="Metropolis-Adjusted Langevin Algorithm" href="the-guide/computational-statistics/metropolis-adjusted-langevin-algorithm.html" class="internal-link" target="_self" rel="noopener nofollow">Metropolis-Adjusted Langevin Algorithm</a>




<br><a data-href="Approximate Bayesian Computation" href="the-guide/computational-statistics/approximate-bayesian-computation.html" class="internal-link" target="_self" rel="noopener nofollow">Approximate Bayesian Computation</a>

<br>


]]></description><link>the-guide/mathematics/statistics/-bayesian-statistics-.html</link><guid isPermaLink="false">The Guide/Mathematics/Statistics/- Bayesian Statistics -.md</guid><pubDate>Thu, 27 Feb 2025 12:05:07 GMT</pubDate><enclosure url="lib/media/pasted-image-20240117104519.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src=&quot;lib/media/pasted-image-20240117104519.png&quot;&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[- Regression -]]></title><description><![CDATA[ 
 <br>Methods for statistical modelling that want to find relationships between  <br><br><br>Assumes that the dependent variable is a linear combination of the parameters, but not necessarily linear in the independent variables (features linearly combined, but not necessarily linear themselves, e.f.  with  polynomials or sinusoidal).<br>
<br><a data-href="Linear Least Squares and Ridge Regression" href="the-guide/mathematics/optimization/linear-least-squares-and-ridge-regression.html" class="internal-link" target="_self" rel="noopener nofollow">Linear Least Squares and Ridge Regression</a>
]]></description><link>the-guide/mathematics/statistics/-regression-.html</link><guid isPermaLink="false">The Guide/Mathematics/Statistics/- Regression -.md</guid><pubDate>Thu, 15 Aug 2024 12:51:52 GMT</pubDate></item><item><title><![CDATA[Bayes Theorem]]></title><description><![CDATA[ 
 <br>In a Nutshell
Update prior believes about parameters  due to evidence  considering the base rate. Probability that hypothesis (prior)  holds, given that some evidence  is true.
<br><br>This leads to the formulawhere  and  are both <a data-tooltip-position="top" aria-label="Sigma-Algebra" data-href="Sigma-Algebra" href="the-guide/mathematics/probability-theory/sigma-algebra.html" class="internal-link" target="_self" rel="noopener nofollow">events</a> (both have distributions). The theorem may be derived from the <a data-tooltip-position="top" aria-label="Probability Distribution" data-href="Probability Distribution" href="the-guide/mathematics/probability-theory/probability-distribution.html" class="internal-link" target="_self" rel="noopener nofollow">conditional probability</a>.<br>Bayes Theorem as a Nonlinear Map
The formula above may be viewed as a <a data-tooltip-position="top" aria-label="Mathematical Relations" data-href="Mathematical Relations" href="the-guide/mathematics/general-stuff/mathematical-relations.html" class="internal-link" target="_self" rel="noopener nofollow">map</a> from the prior  to the posterior , i.e. with nonlinear map .
<br>Intuition
If the evidence observed regarding the prior applies to a large portion of e.g. the population, the evidence does not change the prior believes much. ex
]]></description><link>the-guide/mathematics/statistics/bayes-theorem.html</link><guid isPermaLink="false">The Guide/Mathematics/Statistics/Bayes Theorem.md</guid><pubDate>Thu, 17 Apr 2025 16:23:10 GMT</pubDate></item><item><title><![CDATA[Covariance and Variance]]></title><description><![CDATA[ 
 <br><br><br>In a Nutshell
Captures the relationship between coupled <a data-tooltip-position="top" aria-label="Random Variable" data-href="Random Variable" href="the-guide/mathematics/probability-theory/random-variable.html" class="internal-link" target="_self" rel="noopener nofollow">random variables</a> / dimensions of a multidimensional random variable. The sign of each entry encodes the linear relationship between the two components. A positive sign signals that greater /  lesser values of the one variable tend to correspond to greater / lesser values in the other. A negative sign indicates that they behave opposed to each other, i.e. greater values for one tend to correspond to lesser values in the other.
<br><img alt="center" src="lib/media/pasted-image-20240516093856.png" style="width: 400px; max-width: 100%;"><br>Definition
The covariance is a measure of the joint variability of two or more <a data-tooltip-position="top" aria-label="Random Variable" data-href="Random Variable" href="the-guide/mathematics/probability-theory/random-variable.html" class="internal-link" target="_self" rel="noopener nofollow">random variables</a>. It is greater than zero if on average, larger values of one variable correspond to larger values of the other one v.v..With more than two variables, the covariance is computed by the sum of all pairs.
<br>Properties <br>
<br>The total <a data-tooltip-position="top" aria-label="Covariance and Variance" data-href="Covariance and Variance" href="the-guide/mathematics/statistics/covariance-and-variance.html" class="internal-link" target="_self" rel="noopener nofollow">variance</a> of two <a data-tooltip-position="top" aria-label="Random Variable" data-href="Random Variable" href="the-guide/mathematics/probability-theory/random-variable.html" class="internal-link" target="_self" rel="noopener nofollow">random variables</a> is 
<br>The Covariance of two <a data-tooltip-position="top" aria-label="Statistical Independence" data-href="Statistical Independence" href="the-guide/mathematics/statistics/statistical-independence.html" class="internal-link" target="_self" rel="noopener nofollow">i.i.d.</a> variables is zero However, if the covariance is zero the variables are not necessarily independent.
<br><br>Covariance and Precision Matrix
The <a data-tooltip-position="top" aria-label="Matrices" data-href="Matrices" href="the-guide/mathematics/linear-algebra/matrices.html" class="internal-link" target="_self" rel="noopener nofollow">matrix</a> equivalent of the above is computed via yielding a symmetric <a data-tooltip-position="top" aria-label="Matrices" data-href="Matrices" href="the-guide/mathematics/linear-algebra/matrices.html" class="internal-link" target="_self" rel="noopener nofollow">positive semidefinite matrix</a>. If the underlying <a data-tooltip-position="top" aria-label="Probability Distribution" data-href="Probability Distribution" href="the-guide/mathematics/probability-theory/probability-distribution.html" class="internal-link" target="_self" rel="noopener nofollow">distribution</a> has zero mean, the formula is simplified to an expected outer product : The inverse of the above covariance matrix is denoted the precision matrix 
<br><br><br>Variance
When only considering a single <a data-tooltip-position="top" aria-label="Random Variable" data-href="Random Variable" href="the-guide/mathematics/probability-theory/random-variable.html" class="internal-link" target="_self" rel="noopener nofollow">random variable</a>, the variance is a measure of spread of a <a data-tooltip-position="top" aria-label="Probability Distribution" data-href="Probability Distribution" href="the-guide/mathematics/probability-theory/probability-distribution.html" class="internal-link" target="_self" rel="noopener nofollow">distribution</a> around its <a data-tooltip-position="top" aria-label="Expectations" data-href="Expectations" href="the-guide/mathematics/statistics/expectations.html" class="internal-link" target="_self" rel="noopener nofollow">mean</a>. It is defined as In the continuous case, it can be computed using 
<br>
<br>The standard deviation  is the square-root of the variance
<br><br><br>... needed for <a data-tooltip-position="top" aria-label="Estimator" data-href="Estimator" href="the-guide/mathematics/statistics/estimator.html" class="internal-link" target="_self" rel="noopener nofollow">unbiased estimator</a>]]></description><link>the-guide/mathematics/statistics/covariance-and-variance.html</link><guid isPermaLink="false">The Guide/Mathematics/Statistics/Covariance and Variance.md</guid><pubDate>Fri, 14 Mar 2025 10:07:40 GMT</pubDate><enclosure url="lib/media/pasted-image-20240516093856.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src=&quot;lib/media/pasted-image-20240516093856.png&quot;&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[Estimator]]></title><description><![CDATA[ 
 <br>Info
An estimator  is a <a data-tooltip-position="top" aria-label="Statistic" data-href="Statistic" href="the-guide/mathematics/statistics/statistic.html" class="internal-link" target="_self" rel="noopener nofollow">statistic</a> that is used to compute an estimate of a quantity  (e.g. <a data-tooltip-position="top" aria-label="Expectations" data-href="Expectations" href="the-guide/mathematics/statistics/expectations.html" class="internal-link" target="_self" rel="noopener nofollow">mean</a>, ... ), based on observed data. 
<br><br><br>
<br>Bias

<br>Distance between average of the collection of estimates and the parameter being estimated. This leads to the definition Intuitively, if you keep repeating an estimation then the resulting distribution of the estimate will be centered around the true value, if the estimator is unbiased. 


<br><a data-tooltip-position="top" aria-label="Covariance and Variance" data-href="Covariance and Variance" href="the-guide/mathematics/statistics/covariance-and-variance.html" class="internal-link" target="_self" rel="noopener nofollow">Variance</a>

<br>Indicate how far, on average the collection of estimates are from the <a data-tooltip-position="top" aria-label="Expectations" data-href="Expectations" href="the-guide/mathematics/statistics/expectations.html" class="internal-link" target="_self" rel="noopener nofollow">expected</a> value (that doesn't necessarily coincide with the true value):


<br><br><br>
<br>Consistency

<br>Increasing the sample size increases the probability of the estimator being close to the true parameter.
<br>


<br><br><br>
<br><a data-href="Maximum Likelihood Estimator" href="the-guide/mathematics/statistics/maximum-likelihood-estimator.html" class="internal-link" target="_self" rel="noopener nofollow">Maximum Likelihood Estimator</a> - find maximum of the <a data-tooltip-position="top" aria-label="Likelihood Function" data-href="Likelihood Function" href="the-guide/mathematics/statistics/likelihood-function.html" class="internal-link" target="_self" rel="noopener nofollow">likelihood function</a>
<br><a data-href="Maximum A Posteriori Estimator" href="the-guide/mathematics/statistics/maximum-a-posteriori-estimator.html" class="internal-link" target="_self" rel="noopener nofollow">Maximum A Posteriori Estimator</a> - find maximum of the <a data-tooltip-position="top" aria-label="Bayes Theorem" data-href="Bayes Theorem" href="the-guide/mathematics/statistics/bayes-theorem.html" class="internal-link" target="_self" rel="noopener nofollow">posterior</a>
]]></description><link>the-guide/mathematics/statistics/estimator.html</link><guid isPermaLink="false">The Guide/Mathematics/Statistics/Estimator.md</guid><pubDate>Mon, 09 Sep 2024 15:34:56 GMT</pubDate></item><item><title><![CDATA[Expectations]]></title><description><![CDATA[ 
 <br>Definition
The expected value is the arithmetic mean of independently selected outcomes of a random variable. For a discrete <a data-tooltip-position="top" aria-label="Random Variable" data-href="Random Variable" href="the-guide/mathematics/probability-theory/random-variable.html" class="internal-link" target="_self" rel="noopener nofollow">random variable</a>, it is defined aswith  being the <a data-tooltip-position="top" aria-label="Probability Mass Function" data-href="Probability Mass Function" href="the-guide/mathematics/probability-theory/probability-mass-function.html" class="internal-link" target="_self" rel="noopener nofollow">pmf</a>of . For a continuous <a data-tooltip-position="top" aria-label="Random Variable" data-href="Random Variable" href="the-guide/mathematics/probability-theory/random-variable.html" class="internal-link" target="_self" rel="noopener nofollow">random variable</a> (here ), it is defined via
<br>
<br>Properties

<br>Linear 

<br>


<br>Monotonous 

<br>, if  


<br>If two random variables are <a data-tooltip-position="top" aria-label="Statistical Independence" data-href="Statistical Independence" href="the-guide/mathematics/statistics/statistical-independence.html" class="internal-link" target="_self" rel="noopener nofollow">independent</a>, the equality  holds for the joint distribution.


<br>Theorem
The results of the <a data-href="Cauchy-Schwartz Inequality" href="Cauchy-Schwartz Inequality" class="internal-link" target="_self" rel="noopener nofollow">Cauchy-Schwartz Inequality</a> can be used to derive 
<br>If a <a data-tooltip-position="top" aria-label="Mathematical Relations" data-href="Mathematical Relations" href="the-guide/mathematics/general-stuff/mathematical-relations.html" class="internal-link" target="_self" rel="noopener nofollow">function</a> represents a <a data-tooltip-position="top" aria-label="Probability Distribution" data-href="Probability Distribution" href="the-guide/mathematics/probability-theory/probability-distribution.html" class="internal-link" target="_self" rel="noopener nofollow">probability distribution</a>, the expectation is called the first moment.<br><br><br>Expectation of a random variable under the assumption of another variable.]]></description><link>the-guide/mathematics/statistics/expectations.html</link><guid isPermaLink="false">The Guide/Mathematics/Statistics/Expectations.md</guid><pubDate>Mon, 09 Sep 2024 15:34:56 GMT</pubDate></item><item><title><![CDATA[Fisher Information Matrix]]></title><description><![CDATA[ 
 <br>In a Nutshell
Captures the amount of information an observable <a data-tooltip-position="top" aria-label="Random Variable" data-href="Random Variable" href="the-guide/mathematics/probability-theory/random-variable.html" class="internal-link" target="_self" rel="noopener nofollow">random variable</a>  carries about unknown parameter  by describing the curvature of the <a data-tooltip-position="top" aria-label="Likelihood Function" data-href="Likelihood Function" href="the-guide/mathematics/statistics/likelihood-function.html" class="internal-link" target="_self" rel="noopener nofollow">log-likelihood</a> on the space of parameters.
<br><br>Definition
Given a <a data-tooltip-position="top" aria-label="Probability Density Function" data-href="Probability Density Function" href="the-guide/mathematics/probability-theory/probability-density-function.html" class="internal-link" target="_self" rel="noopener nofollow">pdf</a> or <a data-tooltip-position="top" aria-label="Probability Mass Function" data-href="Probability Mass Function" href="the-guide/mathematics/probability-theory/probability-mass-function.html" class="internal-link" target="_self" rel="noopener nofollow">pmf</a> ,  the Fisher Information is defined as the <a data-tooltip-position="top" aria-label="Covariance and Variance" data-href="Covariance and Variance" href="the-guide/mathematics/statistics/covariance-and-variance.html" class="internal-link" target="_self" rel="noopener nofollow">variance</a> of the <a data-tooltip-position="top" aria-label="Likelihood Function" data-href="Likelihood Function" href="the-guide/mathematics/statistics/likelihood-function.html" class="internal-link" target="_self" rel="noopener nofollow">score</a> which -if the score is twice differentiable- can be written as 
<br>
<br>
Approximation of the <a data-href="Kullback-Leibler Divergence" href="the-guide/information-theory/information-theory-1/information/kullback-leibler-divergence.html" class="internal-link" target="_self" rel="noopener nofollow">Kullback-Leibler Divergence</a> by second-order Taylor Approximation, valid for small variations or parameters of<a class="internal-link" data-href="Covariance and Variance.md" href="the-guide/mathematics/statistics/covariance-and-variance.html" target="_self" rel="noopener nofollow"></a>ribution|distribution]].

<br>

<br>
<br>Captures information how much each parameter influences the <a data-tooltip-position="top" aria-label="Probability Distribution" data-href="Probability Distribution" href="the-guide/mathematics/probability-theory/probability-distribution.html" class="internal-link" target="_self" rel="noopener nofollow">distribution</a>




<br>
Used to compute jeffreys prior

<br><br><br>
<br><a rel="noopener nofollow" class="external-link" href="https://www.youtube.com/watch?v=pneluWj-U-o" target="_blank">https://www.youtube.com/watch?v=pneluWj-U-o</a>
]]></description><link>the-guide/mathematics/statistics/fisher-information-matrix.html</link><guid isPermaLink="false">The Guide/Mathematics/Statistics/Fisher Information Matrix.md</guid><pubDate>Mon, 04 Nov 2024 14:17:43 GMT</pubDate></item><item><title><![CDATA[Frequentist vs. Bayesian Statistics]]></title><description><![CDATA[ 
 <br>Two schools of statistics with different interpretation / philisophical views of uncertainty and probability.<br>
<br>Bayesian

<br>Subjective view, depends on available information
<br>Prior belief is updated (<a data-href="Bayes Theorem" href="the-guide/mathematics/statistics/bayes-theorem.html" class="internal-link" target="_self" rel="noopener nofollow">Bayes Theorem</a>) once new information is available
<br>Data is fixed, model varies around it.


<br>Frequentist

<br>Probability as limit of taking infinitely many trials
<br>Model is fixed, data varies around it.


]]></description><link>the-guide/mathematics/statistics/frequentist-vs.-bayesian-statistics.html</link><guid isPermaLink="false">The Guide/Mathematics/Statistics/Frequentist vs. Bayesian Statistics.md</guid><pubDate>Mon, 09 Sep 2024 15:34:56 GMT</pubDate></item><item><title><![CDATA[Law of the unconscious Statistician]]></title><description><![CDATA[ 
 <br>LOTUS
Theorem in probability theory and statistic that expresses the <a data-tooltip-position="top" aria-label="Expectations" data-href="Expectations" href="the-guide/mathematics/statistics/expectations.html" class="internal-link" target="_self" rel="noopener nofollow">expected value</a> of a <a data-tooltip-position="top" aria-label="Mathematical Relations" data-href="Mathematical Relations" href="the-guide/mathematics/general-stuff/mathematical-relations.html" class="internal-link" target="_self" rel="noopener nofollow">function</a>  of a <a data-tooltip-position="top" aria-label="Random Variable" data-href="Random Variable" href="the-guide/mathematics/probability-theory/random-variable.html" class="internal-link" target="_self" rel="noopener nofollow">random variable</a>  in terms of the function and the random variables <a data-tooltip-position="top" aria-label="Probability Distribution" data-href="Probability Distribution" href="the-guide/mathematics/probability-theory/probability-distribution.html" class="internal-link" target="_self" rel="noopener nofollow">distribution</a>.

<br><a data-tooltip-position="top" aria-label="Random Variable" data-href="Random Variable" href="the-guide/mathematics/probability-theory/random-variable.html" class="internal-link" target="_self" rel="noopener nofollow">Discrete Case</a>, assuming a known <a data-tooltip-position="top" aria-label="Probability Mass Function" data-href="Probability Mass Function" href="the-guide/mathematics/probability-theory/probability-mass-function.html" class="internal-link" target="_self" rel="noopener nofollow">pmf</a>  of , we get 
<br><a data-tooltip-position="top" aria-label="Random Variable" data-href="Random Variable" href="the-guide/mathematics/probability-theory/random-variable.html" class="internal-link" target="_self" rel="noopener nofollow">Continuous Case</a>, assuming a known <a data-tooltip-position="top" aria-label="Probability Density Function" data-href="Probability Density Function" href="the-guide/mathematics/probability-theory/probability-density-function.html" class="internal-link" target="_self" rel="noopener nofollow">pdf</a>  of , we get 

<br>
<br>Name is due to the fact that the above is often seen as the very definition of the <a data-tooltip-position="top" aria-label="Expectations" data-href="Expectations" href="the-guide/mathematics/statistics/expectations.html" class="internal-link" target="_self" rel="noopener nofollow">expected</a> value, while mathematically, it is a consequence of it.
]]></description><link>the-guide/mathematics/statistics/law-of-the-unconscious-statistician.html</link><guid isPermaLink="false">The Guide/Mathematics/Statistics/Law of the unconscious Statistician.md</guid><pubDate>Mon, 09 Sep 2024 15:34:56 GMT</pubDate></item><item><title><![CDATA[Likelihood Function]]></title><description><![CDATA[ 
 <br>When observing a ...<br>
<br><a data-tooltip-position="top" aria-label="Random Variable" data-href="Random Variable" href="the-guide/mathematics/probability-theory/random-variable.html" class="internal-link" target="_self" rel="noopener nofollow">Continuous random variable</a> with parametrized <a data-tooltip-position="top" aria-label="Probability Density Function" data-href="Probability Density Function" href="the-guide/mathematics/probability-theory/probability-density-function.html" class="internal-link" target="_self" rel="noopener nofollow">pdf</a> , one can treat these parameters themselves as <a data-tooltip-position="top" aria-label="Random Variable" data-href="Random Variable" href="the-guide/mathematics/probability-theory/random-variable.html" class="internal-link" target="_self" rel="noopener nofollow">random variables</a> (<a data-tooltip-position="top" aria-label="Bayes Theorem" data-href="Bayes Theorem" href="the-guide/mathematics/statistics/bayes-theorem.html" class="internal-link" target="_self" rel="noopener nofollow">Bayesian</a>) and define as the likelihood-function.
<br><a data-tooltip-position="top" aria-label="Random Variable" data-href="Random Variable" href="the-guide/mathematics/probability-theory/random-variable.html" class="internal-link" target="_self" rel="noopener nofollow">Discrete random variable</a> with parametrized <a data-tooltip-position="top" aria-label="Probability Mass Function" data-href="Probability Mass Function" href="the-guide/mathematics/probability-theory/probability-mass-function.html" class="internal-link" target="_self" rel="noopener nofollow">pmf</a> , one can treat these parameters themselves as <a data-tooltip-position="top" aria-label="Random Variable" data-href="Random Variable" href="the-guide/mathematics/probability-theory/random-variable.html" class="internal-link" target="_self" rel="noopener nofollow">random variables</a> (<a data-tooltip-position="top" aria-label="Bayes Theorem" data-href="Bayes Theorem" href="the-guide/mathematics/statistics/bayes-theorem.html" class="internal-link" target="_self" rel="noopener nofollow">Bayesian</a>) and define as the likelihood-function.<br>
In <a data-tooltip-position="top" aria-label="Maximum Likelihood Estimator" data-href="Maximum Likelihood Estimator" href="the-guide/mathematics/statistics/maximum-likelihood-estimator.html" class="internal-link" target="_self" rel="noopener nofollow">maximum likelihood estimation</a>, the  over the parameters serves as an <a data-tooltip-position="top" aria-label="Estimator" data-href="Estimator" href="the-guide/mathematics/statistics/estimator.html" class="internal-link" target="_self" rel="noopener nofollow">estimate</a> for , while the <a data-tooltip-position="top" aria-label="Fisher Information Matrix" data-href="Fisher Information Matrix" href="the-guide/mathematics/statistics/fisher-information-matrix.html" class="internal-link" target="_self" rel="noopener nofollow">Fisher Information</a> indicates precision.
<br><br><br>The natural logarithm of the <a data-tooltip-position="top" aria-label="Bayes Theorem" data-href="Bayes Theorem" href="the-guide/mathematics/statistics/bayes-theorem.html" class="internal-link" target="_self" rel="noopener nofollow">likelihood function</a>, denoted as The logarithm is a <a data-tooltip-position="top" aria-label="Mathematical Relations" data-href="Mathematical Relations" href="the-guide/mathematics/general-stuff/mathematical-relations.html" class="internal-link" target="_self" rel="noopener nofollow">strictly positive monotonic function</a>, which is why extrema of the normal and logarithmic version coincide. however, the logarithm enables easier computation in many cases.<br><br><br>In models with parameter vector , the gradient of the log-likelihood w.r.t. that parameter vector is denoted the score function ]]></description><link>the-guide/mathematics/statistics/likelihood-function.html</link><guid isPermaLink="false">The Guide/Mathematics/Statistics/Likelihood Function.md</guid><pubDate>Mon, 09 Sep 2024 15:34:56 GMT</pubDate></item><item><title><![CDATA[Maximum A Posteriori Estimator]]></title><description><![CDATA[ 
 <br>MAP Estimator
Principle in <a data-href="- Bayesian Statistics -" href="the-guide/mathematics/statistics/-bayesian-statistics-.html" class="internal-link" target="_self" rel="noopener nofollow">- Bayesian Statistics -</a> that yields <a data-tooltip-position="top" aria-label="Estimator" data-href="Estimator" href="the-guide/mathematics/statistics/estimator.html" class="internal-link" target="_self" rel="noopener nofollow">estimators</a> of an unknown quantity  by computing the mode (max density) of the the   of <a data-href="Bayes Theorem" href="the-guide/mathematics/statistics/bayes-theorem.html" class="internal-link" target="_self" rel="noopener nofollow">Bayes Theorem</a> using empirical data .
<br>
<br>In contrast to <a data-tooltip-position="top" aria-label="Maximum Likelihood Estimator" data-href="Maximum Likelihood Estimator" href="the-guide/mathematics/statistics/maximum-likelihood-estimator.html" class="internal-link" target="_self" rel="noopener nofollow">Maximum Likelihood estimators</a>, MAP also incorporates prior knowledge in form of 

<br>ML equals MAP if prior is uniform


<br><br><br>
<br>Gaussians Using the log-trick (maximizing the log yields the same result) for the right side in above equation yields 

<br>first term is prediction loss
<br>second term is regularization, e.g. additional constraints


<br>Using <a data-href="Linear Least Squares and Ridge Regression" href="the-guide/mathematics/optimization/linear-least-squares-and-ridge-regression.html" class="internal-link" target="_self" rel="noopener nofollow">Linear Least Squares and Ridge Regression</a>, this yields 
]]></description><link>the-guide/mathematics/statistics/maximum-a-posteriori-estimator.html</link><guid isPermaLink="false">The Guide/Mathematics/Statistics/Maximum A Posteriori Estimator.md</guid><pubDate>Thu, 17 Apr 2025 16:23:37 GMT</pubDate></item><item><title><![CDATA[Maximum Likelihood Estimator]]></title><description><![CDATA[ 
 <br>In a Nutshell
Fundamental approach to compute <a data-tooltip-position="top" aria-label="Estimator" data-href="Estimator" href="the-guide/mathematics/statistics/estimator.html" class="internal-link" target="_self" rel="noopener nofollow">estimators</a>. Based on the idea that the estimator should have high probability for observed data points.
<br><br>ML Estimator
Statistical method to obtain <a data-tooltip-position="top" aria-label="Estimator" data-href="Estimator" href="the-guide/mathematics/statistics/estimator.html" class="internal-link" target="_self" rel="noopener nofollow">estimators</a> for an unknown quantity  given observed data. Uses the point in parameter space that maximize the likelihood (<a data-href="Likelihood Function" href="the-guide/mathematics/statistics/likelihood-function.html" class="internal-link" target="_self" rel="noopener nofollow">Likelihood Function</a>) of the outcomes under the assumption that data  is <a data-tooltip-position="top" aria-label="Statistical Independence" data-href="Statistical Independence" href="the-guide/mathematics/statistics/statistical-independence.html" class="internal-link" target="_self" rel="noopener nofollow">i.i.d</a> 
<br>
<br>If data is non-i.i.d. <a data-tooltip-position="top" aria-label="Maximum A Posteriori Estimator" data-href="Maximum A Posteriori Estimator" href="the-guide/mathematics/statistics/maximum-a-posteriori-estimator.html" class="internal-link" target="_self" rel="noopener nofollow">MAP</a> estimation can be used
<br>In many cases, transformation of the above expression into log-space is very useful, because it decouples the expression while leaving the maximization unaffected. <br>From an information-theoretic perspective, the following theorem gives a useful connection to the <a data-tooltip-position="top" aria-label="Kullback-Leibler Divergence" data-href="Kullback-Leibler Divergence" href="the-guide/information-theory/information-theory-1/information/kullback-leibler-divergence.html" class="internal-link" target="_self" rel="noopener nofollow">KL divergence</a>:<br>Theorem
Minimizing the <a data-tooltip-position="top" aria-label="Kullback-Leibler Divergence" data-href="Kullback-Leibler Divergence" href="the-guide/information-theory/information-theory-1/information/kullback-leibler-divergence.html" class="internal-link" target="_self" rel="noopener nofollow">KL-divergence</a> between two distributions, here a parameterized  and a "true"  is equivalent to maximizing the <a data-tooltip-position="top" aria-label="Likelihood Function" data-href="Likelihood Function" href="the-guide/mathematics/statistics/likelihood-function.html" class="internal-link" target="_self" rel="noopener nofollow">log-likelihood</a>, because In practice, we can use the reformulation 
<br><br>
<br>Examples

<br><a data-tooltip-position="top" aria-label="Gaussian Distribution" data-href="Gaussian Distribution" href="the-guide/mathematics/probability-theory/gaussian-distribution.html" class="internal-link" target="_self" rel="noopener nofollow">Gaussian</a> Setting - data , we want to find parameters . Likelihood is given by To decouple this expression, we can use the <a data-tooltip-position="top" aria-label="Logarithm and Exponential" data-href="Logarithm and Exponential" href="the-guide/mathematics/general-stuff/logarithm-and-exponential.html" class="internal-link" target="_self" rel="noopener nofollow">log-trick</a> by transforming the expression into log-space (does not influence extrema). We then find the maximum likelihood estimators viayielding


]]></description><link>the-guide/mathematics/statistics/maximum-likelihood-estimator.html</link><guid isPermaLink="false">The Guide/Mathematics/Statistics/Maximum Likelihood Estimator.md</guid><pubDate>Tue, 10 Sep 2024 07:32:56 GMT</pubDate></item><item><title><![CDATA[Statistic]]></title><description><![CDATA[ 
 <br>Any quantity  computed from values in a sample  which is considered for a statistical purpose. When used for estimating a population parameter, the statistic is called an <a data-tooltip-position="top" aria-label="Estimator" data-href="Estimator" href="the-guide/mathematics/statistics/estimator.html" class="internal-link" target="_self" rel="noopener nofollow">estimator</a>.<br><br><br>
<br>Observability

<br>Has to be based on actual measurements, e.g. average height of 100 individuals is statistic, average height of all possible individuals is a parameter.


<br>Sufficiency

<br>A statistic is sufficient for a parameter  if once computed, no additional information can be gathered from the data set about the parameter. In other words, the <a data-tooltip-position="top" aria-label="Data Processing Inequality" data-href="Data Processing Inequality" href="the-guide/information-theory/information-theory-1/channel-coding/data-processing-inequality.html" class="internal-link" target="_self" rel="noopener nofollow">data processing inequality</a> becomes an equality 


<br><br><br>Examples<br>
<br>Sample mean / median / quantiles 
<br>Sample max / min
]]></description><link>the-guide/mathematics/statistics/statistic.html</link><guid isPermaLink="false">The Guide/Mathematics/Statistics/Statistic.md</guid><pubDate>Mon, 09 Sep 2024 15:34:56 GMT</pubDate></item><item><title><![CDATA[Statistical Independence]]></title><description><![CDATA[ 
 <br><br>Two <a data-tooltip-position="top" aria-label="Sigma-Algebra" data-href="Sigma-Algebra" href="the-guide/mathematics/probability-theory/sigma-algebra.html" class="internal-link" target="_self" rel="noopener nofollow">events</a> are statistically independent, if	<br><br>In terms of <a data-tooltip-position="top" aria-label="Probability Distribution" data-href="Probability Distribution" href="the-guide/mathematics/probability-theory/probability-distribution.html" class="internal-link" target="_self" rel="noopener nofollow">conditional probability</a> this means that the gathered information does not have any effect on the probability  or  respectively.<br><br><br>The <a data-tooltip-position="top" aria-label="Random Variable" data-href="Random Variable" href="the-guide/mathematics/probability-theory/random-variable.html" class="internal-link" target="_self" rel="noopener nofollow">random variables</a>  are independent, if for every   it satisfies<br><br>If all  have the same marginal distribution  and are independent, we say that the variables are independent and identically distributed (i.i.d.) and can be seen as a sample from the distribution.]]></description><link>the-guide/mathematics/statistics/statistical-independence.html</link><guid isPermaLink="false">The Guide/Mathematics/Statistics/Statistical Independence.md</guid><pubDate>Mon, 09 Sep 2024 15:34:56 GMT</pubDate></item><item><title><![CDATA[Statistics Overview]]></title><description><![CDATA[ 
 <br><br><br><br>In a Nutshell
Process of using data to deduce properties of an underlying <a data-tooltip-position="top" aria-label="Probability Distribution" data-href="Probability Distribution" href="the-guide/mathematics/probability-theory/probability-distribution.html" class="internal-link" target="_self" rel="noopener nofollow">probability distribution</a>, i.e. reaching conclusions about populations based on data.
<br><br>
<br>
Population - entire group about which informaiton is desired

<br>
Sample - subset of the population to analyse

<br>
parameter -  numerical characteristic about population

<br>
<a data-href="Statistic" href="the-guide/mathematics/statistics/statistic.html" class="internal-link" target="_self" rel="noopener nofollow">Statistic</a> - numerical characteristic of a sample

<br><br>
<br>Point Estimation 
<br>Interval Estimation
<br>Hypothesis Testing
]]></description><link>the-guide/mathematics/statistics/statistics-overview.html</link><guid isPermaLink="false">The Guide/Mathematics/Statistics/Statistics Overview.md</guid><pubDate>Mon, 09 Sep 2024 15:34:56 GMT</pubDate></item><item><title><![CDATA[Stein's Identity]]></title><description><![CDATA[ 
 <br>In a Nutshell
Theorem that relates the <a data-tooltip-position="top" aria-label="Likelihood Function > Score-Function" data-href="Likelihood Function#Score-Function" href="the-guide/mathematics/statistics/likelihood-function.html#Score-Function" class="internal-link" target="_self" rel="noopener nofollow">score function</a> of a <a data-tooltip-position="top" aria-label="Probability Distribution" data-href="Probability Distribution" href="the-guide/mathematics/probability-theory/probability-distribution.html" class="internal-link" target="_self" rel="noopener nofollow">probability distribution</a> to <a data-tooltip-position="top" aria-label="Expectations" data-href="Expectations" href="the-guide/mathematics/statistics/expectations.html" class="internal-link" target="_self" rel="noopener nofollow">expectations</a> involving <a data-tooltip-position="top" aria-label="Mathematical Relations" data-href="Mathematical Relations" href="the-guide/mathematics/general-stuff/mathematical-relations.html" class="internal-link" target="_self" rel="noopener nofollow">functions</a> of <a data-tooltip-position="top" aria-label="Random Variable" data-href="Random Variable" href="the-guide/mathematics/probability-theory/random-variable.html" class="internal-link" target="_self" rel="noopener nofollow">random variables</a>. Underlies many other derivations, as it formalizes the connections between the score and the properties of the underlying distribution.
<br><br>Theorem
Let  be a <a data-tooltip-position="top" aria-label="Random Variable" data-href="Random Variable" href="the-guide/mathematics/probability-theory/random-variable.html" class="internal-link" target="_self" rel="noopener nofollow">random variable</a> with <a data-tooltip-position="top" aria-label="Probability Density Function" data-href="Probability Density Function" href="the-guide/mathematics/probability-theory/probability-density-function.html" class="internal-link" target="_self" rel="noopener nofollow">density</a>  and parmaters from a parameter <a data-tooltip-position="top" aria-label="Space" data-href="Space" href="the-guide/mathematics/general-stuff/space.html" class="internal-link" target="_self" rel="noopener nofollow">space</a> . For a differentiable <a data-tooltip-position="top" aria-label="Mathematical Relations" data-href="Mathematical Relations" href="the-guide/mathematics/general-stuff/mathematical-relations.html" class="internal-link" target="_self" rel="noopener nofollow">function</a> , it holds that
]]></description><link>the-guide/mathematics/statistics/stein&apos;s-identity.html</link><guid isPermaLink="false">The Guide/Mathematics/Statistics/Stein&apos;s Identity.md</guid><pubDate>Fri, 12 Jul 2024 12:58:21 GMT</pubDate></item><item><title><![CDATA[Compactness]]></title><description><![CDATA[ 
 <br>In a Nutshell
Property of a <a data-tooltip-position="top" aria-label="Space" data-href="Space" href="the-guide/mathematics/general-stuff/space.html" class="internal-link" target="_self" rel="noopener nofollow">space</a> that generalizes the notion of closed and bounded subsets in Euclidean space to more general <a data-tooltip-position="top" aria-label="Topology and Topological Space" data-href="Topology and Topological Space" href="the-guide/mathematics/topology/topology-and-topological-space.html" class="internal-link" target="_self" rel="noopener nofollow">topological spaces</a>.
<br><br>Compact Space
A topological space  is compact if every open <a data-tooltip-position="top" aria-label="Überlagerung" data-href="Überlagerung" href="the-guide/mathematics/differential-geometry/riemannian-geometry/mannigfaltigkeiten-und-differenzierbare-abbildungen/überlagerung.html" class="internal-link" target="_self" rel="noopener nofollow">cover</a> has a finite subcover. This means that if  is covered by a collection of open sets, then there is a finite subset of these <a data-tooltip-position="top" aria-label="Set" data-href="Set" href="the-guide/mathematics/general-stuff/set.html" class="internal-link" target="_self" rel="noopener nofollow">open sets</a> that still covers . 
<br>Intuition
The idea is that a compact space has no "punctures" or "missing endpoints", i.e., it includes all limiting values of points.
<br>Sequentially Compact Space
A topological space  is sequentially compact if every sequence of points has a convergent subsequence.
<br>
<br>Equivalent for <a data-href="Metric Space and Completeness" href="the-guide/mathematics/general-stuff/metric-space-and-completeness.html" class="internal-link" target="_self" rel="noopener nofollow">Metric Space and Completeness</a>
]]></description><link>the-guide/mathematics/topology/compactness.html</link><guid isPermaLink="false">The Guide/Mathematics/Topology/Compactness.md</guid><pubDate>Wed, 23 Apr 2025 08:36:12 GMT</pubDate></item><item><title><![CDATA[Connectedness]]></title><description><![CDATA[ 
 <br>In a Nutshell
Grasps the abstract notion of how together a <a data-tooltip-position="top" aria-label="Space" data-href="Space" href="the-guide/mathematics/general-stuff/space.html" class="internal-link" target="_self" rel="noopener nofollow">space</a> is, i.e. if we can move from one point to another without leaving the space.
<br><br>Connected Topological Space
A topology is connected (zusammenhängend), if it cannot be represented as a union of multiple disjoint non-empty open subsets (A). If it can be represented like that (B), it is disconnected.
<br><img alt="center" src="lib/media/pasted-image-20240718174434.png" style="width: 300px; max-width: 100%;"><br>Connected Component
The connected component (Zusammenhangskomponente) of a point  is the union of all subsets that contain  and therefore the unique largest connected subset of  that contains  (by definition of topology).
<br>TODO - sketch<br>Path Connectedness
A stronger notion of connectedness that requires the structure of a path from point  to point  in , which is a <a data-tooltip-position="top" aria-label="Mathematical Relations" data-href="Mathematical Relations" href="the-guide/mathematics/general-stuff/mathematical-relations.html" class="internal-link" target="_self" rel="noopener nofollow">continuous function</a> from the unit intervall to . It induces a <a data-tooltip-position="top" aria-label="Equivalence Relation and Class" data-href="Equivalence Relation and Class" href="the-guide/mathematics/general-stuff/equivalence-relation-and-class.html" class="internal-link" target="_self" rel="noopener nofollow">equivalence relation</a>, which makes the points equivalent if such a path exists. 
<br>TODO - sketch<br>Simply-Connected Space
A simply- or 1-connected (einfach zusammenhängend) topological space is a path connected topological space, where every path can be continuously transformed into a point, it is homotope to a point.
]]></description><link>the-guide/mathematics/topology/connectedness.html</link><guid isPermaLink="false">The Guide/Mathematics/Topology/Connectedness.md</guid><pubDate>Wed, 23 Apr 2025 08:40:57 GMT</pubDate><enclosure url="lib/media/pasted-image-20240718174434.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src=&quot;lib/media/pasted-image-20240718174434.png&quot;&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[Homeomorphism]]></title><description><![CDATA[ 
 <br>Definition
A <a data-tooltip-position="top" aria-label="Mathematical Relations" data-href="Mathematical Relations" href="the-guide/mathematics/general-stuff/mathematical-relations.html" class="internal-link" target="_self" rel="noopener nofollow">function</a>  between two <a data-tooltip-position="top" aria-label="Topology and Topological Space" data-href="Topology and Topological Space" href="the-guide/mathematics/topology/topology-and-topological-space.html" class="internal-link" target="_self" rel="noopener nofollow">topological</a> <a data-tooltip-position="top" aria-label="Space" data-href="Space" href="the-guide/mathematics/general-stuff/space.html" class="internal-link" target="_self" rel="noopener nofollow">spaces</a> is a homeomorphisms, if it is 

<br>Bijective 
<br>Continuous. 
<br>has a <a data-tooltip-position="top" aria-label="Continuity" data-href="Continuity" href="the-guide/mathematics/analysis-and-calculus/continuity.html" class="internal-link" target="_self" rel="noopener nofollow">continuous</a> inverse

It is therefore the <a data-tooltip-position="top" aria-label="Morphisms" data-href="Morphisms" href="the-guide/mathematics/category-theory/morphisms.html" class="internal-link" target="_self" rel="noopener nofollow">isomorphism</a> for the <a data-tooltip-position="top" aria-label="Category" data-href="Category" href="the-guide/mathematics/category-theory/category.html" class="internal-link" target="_self" rel="noopener nofollow">category</a> topological space.
<br>Theorem
Being homeomorphic is an <a data-tooltip-position="top" aria-label="Equivalence Relation and Class" data-href="Equivalence Relation and Class" href="the-guide/mathematics/general-stuff/equivalence-relation-and-class.html" class="internal-link" target="_self" rel="noopener nofollow">equivalence relation</a> for <a data-tooltip-position="top" aria-label="Topology and Topological Space" data-href="Topology and Topological Space" href="the-guide/mathematics/topology/topology-and-topological-space.html" class="internal-link" target="_self" rel="noopener nofollow">topological</a> <a data-tooltip-position="top" aria-label="Space" data-href="Space" href="the-guide/mathematics/general-stuff/space.html" class="internal-link" target="_self" rel="noopener nofollow">spaces</a>.
]]></description><link>the-guide/mathematics/topology/homeomorphism.html</link><guid isPermaLink="false">The Guide/Mathematics/Topology/Homeomorphism.md</guid><pubDate>Wed, 23 Apr 2025 08:34:38 GMT</pubDate></item><item><title><![CDATA[Topology and Topological Space]]></title><description><![CDATA[ 
 <br>Definition
Any collection of <a data-tooltip-position="top" aria-label="Set" data-href="Set" href="the-guide/mathematics/general-stuff/set.html" class="internal-link" target="_self" rel="noopener nofollow">subsets</a> that ...

<br>contain an empty set  and the full set  
<br>that is closed under unions and finite intersections. 

<br>The pair  of set and topology is called a topological space. The subsets  are referred to as open subsets, if they are or can be derived from a <a data-tooltip-position="top" aria-label="Metric Space and Completeness" data-href="Metric Space and Completeness" href="the-guide/mathematics/general-stuff/metric-space-and-completeness.html" class="internal-link" target="_self" rel="noopener nofollow">metric</a>, the topology is called metric topology. Each subset defines a general notion of neighborhood without the need of a metric.<img alt="center" src="lib/media/pasted-image-20231129190225.png">The overlap of topological neighborhoods around distinct elements determines how well the topology can distinguish those elements from each other. <br>If one of two points is contained in a set the other is not, the points are topologically distinguishable.<br>Larger topologies contain more neighborhoods that better resolve the individual elements in the underlying set than smaller topologies. In other words, the larger the topology ,the closer we can "zoom into" the individual elements.<br>Hausdorff Topological Space
Every pair of elements of the set can be contained by non-overlapping neighborhoods, mirroring the separation of open sets on metric spaces. Thereby, we are able to distinguish every element from every other.
<br><img alt="center" src="lib/media/pasted-image-20231129192015.png"><br>Important concepts in topology are <a data-href="Compactness" href="the-guide/mathematics/topology/compactness.html" class="internal-link" target="_self" rel="noopener nofollow">Compactness</a> and <a data-href="Connectedness" href="the-guide/mathematics/topology/connectedness.html" class="internal-link" target="_self" rel="noopener nofollow">Connectedness</a>.<br><br><br>First Axiom of Countability
A <a data-tooltip-position="top" aria-label="Space" data-href="Space" href="the-guide/mathematics/general-stuff/space.html" class="internal-link" target="_self" rel="noopener nofollow">space</a>  is first-countable if each point has a local neighborhood base, i.e. for each point , there is a sequence  of neighborhoods, such that for any neighborhood of  there is an  with  contained in .
<br>Second Axiom of Countability
A topological space  is second-countable or completely seperable, if there is a countable collection of open subsets of , such that any open subset of  can be written as a union of elements  of some subfamily of .
<br><br><br>
<br>Trivial Topology 

<br>We cannot discriminate any element from each other


<br>Discrete Topology 

<br>Entire power set, each element can be contained in a neighborhood by itself.


]]></description><link>the-guide/mathematics/topology/topology-and-topological-space.html</link><guid isPermaLink="false">The Guide/Mathematics/Topology/Topology and Topological Space.md</guid><pubDate>Wed, 23 Apr 2025 08:42:59 GMT</pubDate><enclosure url="lib/media/pasted-image-20231129190225.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src=&quot;lib/media/pasted-image-20231129190225.png&quot;&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[Adaptive Control]]></title><description><![CDATA[ 
 <br>Controller parameters are time varying, mechanism for adjusting these parameters online based on some signals in the closed-loop systems. By the use of such control scheme, the control goal can be achieved even if there is parametric uncertainty in the plant (Robotic Bartender).<br>Linear Parametrization
Based on <a data-tooltip-position="top" aria-label="Static and Dynamic Systems" data-href="Static and Dynamic Systems" href="the-guide/robotics,-dynamics-and-control/dynamics/static-and-dynamic-systems.html" class="internal-link" target="_self" rel="noopener nofollow">dynamics model</a> with joint friction . There always exists a  reformulation of that system in the formwhere the vector  contains unknown or uncertain coefficients consisting of combinations of physical parameters of the modeled system. Therefore, the regression <a data-tooltip-position="top" aria-label="Matrices" data-href="Matrices" href="the-guide/mathematics/linear-algebra/matrices.html" class="internal-link" target="_self" rel="noopener nofollow">matrix</a>  is linear in , quadratic in  (kinetic energy) and nonlinear in  (<a data-tooltip-position="top" aria-label="Kinematics" data-href="Kinematics" href="the-guide/robotics,-dynamics-and-control/kinematics/kinematics.html" class="internal-link" target="_self" rel="noopener nofollow">kinematics</a>)
<br><br>
<br>Goal

<br>Given twice differentiable desired joint trajectory  with known desired velocities  and acceleration , execute this trajectory under maximum uncertainty and with asymptotically vanishing  tracking errors  and 
<br>Guarantee global <a data-tooltip-position="top" aria-label="Ljapunov Stability of Dynamical Systems" data-href="Ljapunov Stability of Dynamical Systems" href="the-guide/robotics,-dynamics-and-control/dynamics/ljapunov-stability-of-dynamical-systems.html" class="internal-link" target="_self" rel="noopener nofollow">stability</a> no matter how far the initial parameter estimates  and  are from the true values


<br>Problem

<br><a data-tooltip-position="top" aria-label="Model-Based Control" data-href="Model-Based Control" href="the-guide/robotics,-dynamics-and-control/control/model-based-control.html" class="internal-link" target="_self" rel="noopener nofollow">Model-based control laws</a> are hard to turn into adaptive schemes


<br>Idea - Combine

<br>Nonlinear trajectory tracking control based on <a data-tooltip-position="top" aria-label="Static and Dynamic Systems" data-href="Static and Dynamic Systems" href="the-guide/robotics,-dynamics-and-control/dynamics/static-and-dynamic-systems.html" class="internal-link" target="_self" rel="noopener nofollow">inverse dynamics</a> (global <a data-tooltip-position="top" aria-label="Ljapunov Stability of Dynamical Systems" data-href="Ljapunov Stability of Dynamical Systems" href="the-guide/robotics,-dynamics-and-control/dynamics/ljapunov-stability-of-dynamical-systems.html" class="internal-link" target="_self" rel="noopener nofollow">asymptotic stability</a>, vanishing velocity error but residual error stays) 
<br>Use reference velocity instead of desired that is modified online according to tracking error typically  with diagonal <a data-tooltip-position="top" aria-label="Matrices" data-href="Matrices" href="the-guide/mathematics/linear-algebra/matrices.html" class="internal-link" target="_self" rel="noopener nofollow">matrices</a> in order to work against position error. Can be interpreted as a mobile reference we are lagging behind at constant speed<img alt="center" src="lib/media/pasted-image-20240315085441.png" style="width: 450px; max-width: 100%;"> 
<br>Substituting  and  into nonlinear controller based on current estimates and rearranging yields 
<br>Update estimates via 

<br>, diagonal - estimation gains for each parameter




<br><br><br>
<br>Indirect adaptive control assumes changing dynamic coefficients that need to be estimated online, here only direct adaptive control 
<br>Setting

<br>Known Parameters

<br>Robot <a data-href="Kinematics" href="the-guide/robotics,-dynamics-and-control/kinematics/kinematics.html" class="internal-link" target="_self" rel="noopener nofollow">Kinematics</a>, e.g. <a data-tooltip-position="top" aria-label="Denavit-Hartenberg Convention" data-href="Denavit-Hartenberg Convention" href="the-guide/robotics,-dynamics-and-control/kinematics/denavit-hartenberg-convention.html" class="internal-link" target="_self" rel="noopener nofollow">Denavit-Hartenberg</a> parameters


<br>Uncertain Parameters - can be identified offline

<br>Masses, CoMs, inertias
<br>Slowly varying forces like viscosity, frictions


<br>Unknown Parameters 

<br>mass, CoMs inertia matrix or payload




]]></description><link>the-guide/robotics,-dynamics-and-control/control/adaptive-control.html</link><guid isPermaLink="false">The Guide/Robotics, Dynamics and Control/Control/Adaptive Control.md</guid><pubDate>Mon, 03 Mar 2025 15:30:26 GMT</pubDate><enclosure url="lib/media/pasted-image-20240315085441.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src=&quot;lib/media/pasted-image-20240315085441.png&quot;&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[Admittance Control]]></title><description><![CDATA[ 
 <br>In a Nutshell
Approach to <a data-tooltip-position="top" aria-label="Static and Dynamic Systems" data-href="Static and Dynamic Systems" href="the-guide/robotics,-dynamics-and-control/dynamics/static-and-dynamic-systems.html" class="internal-link" target="_self" rel="noopener nofollow">dynamic</a> <a data-tooltip-position="top" aria-label="Control Overview" data-href="Control Overview" href="the-guide/robotics,-dynamics-and-control/control/control-overview.html" class="internal-link" target="_self" rel="noopener nofollow">control</a>, specifically <a data-tooltip-position="top" aria-label="Interaction Control" data-href="Interaction Control" href="the-guide/robotics,-dynamics-and-control/control/interaction-control.html" class="internal-link" target="_self" rel="noopener nofollow">interaction control</a> where e.g. the robot adjusts its position or velocity in response to forces applied to it (at the TCP), allowing for more adaptive behavior in contact. 
<br><img alt="center" src="lib/media/pasted-image-20250122112153.png" style="width: 400px; max-width: 100%;"><br>Can used when we want to react to external forces, but we don't have access to low-level robot torque commands as in <a data-href="Impedance Control" href="the-guide/robotics,-dynamics-and-control/control/impedance-control.html" class="internal-link" target="_self" rel="noopener nofollow">Impedance Control</a>:<br>
<br>For admittance control, we only require a force sensor at the TCP instead of torque sensors at the joints
<br>Force is regulated through position or velocity at TCP (picture shows position case)
<br>Relation to Admittance and Impedance in physical Systems
In Admittance Control the controller is an admittance (force inputs  motion outputs) and the manipulator is an impedance (motion inputs  force outputs).
<br>Avantages

<br>Can be done with external force sensors at TCP instead of torque sensors inside robot.

<br>Disadvantage

<br>Less sophisticated behaviors compared to impedance control
<br>Higher computational demand than impedance control

<br><br><br>Denoting the measured forces at the TCP , we encode a desired trajectory via  into relate it to a resulting desired acceleration. The behavior of the end-effector in cartesian direction and orientation can be encoded via the <a data-tooltip-position="top" aria-label="Matrices" data-href="Matrices" href="the-guide/mathematics/linear-algebra/matrices.html" class="internal-link" target="_self" rel="noopener nofollow">matrices</a>  and .<br>
From this we can compute and feed that into any position controller.<br><br><br>Mapping the experienced forces  to torques at the joints , we can write<br>Resulting Behavior
The robot will interpret external forces as commands and move in this direction. 

<br>The inertia matrix  will scale this effect, higher inertias make the system less compliant (inverse)
<br>The stiffness  controls the effect of the initial desired trajectory / resting position. The higher the values, the more the system wants to stick to 

]]></description><link>the-guide/robotics,-dynamics-and-control/control/admittance-control.html</link><guid isPermaLink="false">The Guide/Robotics, Dynamics and Control/Control/Admittance Control.md</guid><pubDate>Mon, 10 Mar 2025 15:08:37 GMT</pubDate><enclosure url="lib/media/pasted-image-20250122112153.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src=&quot;lib/media/pasted-image-20250122112153.png&quot;&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[Control Overview]]></title><description><![CDATA[ 
 <br>In a Nutshell
Field of applied mathematics dealing with control of <a data-tooltip-position="top" aria-label="Static and Dynamic Systems" data-href="Static and Dynamic Systems" href="the-guide/robotics,-dynamics-and-control/dynamics/static-and-dynamic-systems.html" class="internal-link" target="_self" rel="noopener nofollow">dynamical</a> <a data-tooltip-position="top" aria-label="Static and Dynamic Systems" data-href="Static and Dynamic Systems" href="the-guide/robotics,-dynamics-and-control/dynamics/static-and-dynamic-systems.html" class="internal-link" target="_self" rel="noopener nofollow">systems</a>.
<br><img alt="center" src="lib/media/pasted-image-20230216195735.png" style="width: 400px; max-width: 100%;"><br><br><br>Control in Joint Space
Control based on error between actual and desired joint positions and velocities.
<br>Control in Joint Space
Control based on desired end-effector state in cartesian coordinates , requiring <a data-tooltip-position="top" aria-label="Inverse Kinematics Problem" data-href="Inverse Kinematics Problem" href="the-guide/robotics,-dynamics-and-control/kinematics/inverse-kinematics-problem.html" class="internal-link" target="_self" rel="noopener nofollow">solving inverse kinematics to map them into joint space</a>.
<br>Open-Loop Control
Control strategy where the controller sends control commands to the system without using feedback from the system's output (i.e., without measuring the system's current state). In other words, the control action is predetermined and not adjusted based on the system's actual performance.
<br><br>Closed-Loop Control
Uses feedback from the system's output to adjust the input in real-time, allowing the system to self-correct its behavior and maintain the desired performance.
<br><br><br><br>
<br>Position Control: Directly regulates the position of the robot. Commonly uses basic <a data-tooltip-position="top" aria-label="Linear Controllers" data-href="Linear Controllers" href="the-guide/robotics,-dynamics-and-control/control/linear-controllers.html" class="internal-link" target="_self" rel="noopener nofollow">PD or PID controllers</a>.

<br>Governs the robot's joint positions by minimizing the error between the current and desired positions.
<br>Often used in tasks that require precise positioning.


<br>Velocity Control: Focuses on controlling the velocity of the robot. Often used in high-speed motion tasks.

<br>Involves controlling the joint velocities by minimizing the error between the current and desired velocities.
<br>Useful in tasks that require fast movements or when position tracking errors need to be minimized dynamically.


<br>Torque Control: Directly controls the torques applied to the robot's joints. Often used in force-based tasks.

<br>Allows direct manipulation of forces and is typically used when compliance or interaction with an environment is required.


<br><br><br>
<br>
<a data-href="Interaction Control" href="the-guide/robotics,-dynamics-and-control/control/interaction-control.html" class="internal-link" target="_self" rel="noopener nofollow">Interaction Control</a> - Tasks involving interaction with the environment

<br><a data-href="Admittance Control" href="the-guide/robotics,-dynamics-and-control/control/admittance-control.html" class="internal-link" target="_self" rel="noopener nofollow">Admittance Control</a>
<br><a data-href="Impedance Control" href="the-guide/robotics,-dynamics-and-control/control/impedance-control.html" class="internal-link" target="_self" rel="noopener nofollow">Impedance Control</a>


<br>
<a data-href="Model Predictive Control" href="the-guide/robotics,-dynamics-and-control/control/model-predictive-control.html" class="internal-link" target="_self" rel="noopener nofollow">Model Predictive Control</a>

<br>...


<br>
<a data-href="Model-Based Control" href="the-guide/robotics,-dynamics-and-control/control/model-based-control.html" class="internal-link" target="_self" rel="noopener nofollow">Model-Based Control</a>

<br>...


<br>
<a data-href="Adaptive Control" href="the-guide/robotics,-dynamics-and-control/control/adaptive-control.html" class="internal-link" target="_self" rel="noopener nofollow">Adaptive Control</a>

<br>Tasks with dynamic parameters


]]></description><link>the-guide/robotics,-dynamics-and-control/control/control-overview.html</link><guid isPermaLink="false">The Guide/Robotics, Dynamics and Control/Control/Control Overview.md</guid><pubDate>Sun, 30 Mar 2025 12:38:03 GMT</pubDate><enclosure url="lib/media/pasted-image-20230216195735.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src=&quot;lib/media/pasted-image-20230216195735.png&quot;&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[DMPs - Dynamic Movement Primitives]]></title><description><![CDATA[ 
 <br>In a Nutshell
Framework for motion generation used in robotics and machine learning. Encodes desired motion pattern into robust <a data-tooltip-position="top" aria-label="ODEs - Ordinary Differential Equations" data-href="ODEs - Ordinary Differential Equations" href="the-guide/mathematics/differential-equations/odes-ordinary-differential-equations.html" class="internal-link" target="_self" rel="noopener nofollow">ODE</a> system encoding a <a data-tooltip-position="top" aria-label="Ljapunov Stability of Dynamical Systems" data-href="Ljapunov Stability of Dynamical Systems" href="the-guide/robotics,-dynamics-and-control/dynamics/ljapunov-stability-of-dynamical-systems.html" class="internal-link" target="_self" rel="noopener nofollow">stable</a> <a data-tooltip-position="top" aria-label="Static and Dynamic Systems" data-href="Static and Dynamic Systems" href="the-guide/robotics,-dynamics-and-control/dynamics/static-and-dynamic-systems.html" class="internal-link" target="_self" rel="noopener nofollow">dynamical system</a>.
<br>In the following, we assume trajectory data  and (goal state ) ... . The concepts in this note introduce the basic idea of classical DMPs. There are numerous specialized extensions, for example ...<br>
<br>...
<br><br><br>DMP Dynamics Equation
The dynamics (here: joint space component of robot) of the system is set to be governed by the dynamics The first part represents a simple spring-damper <a data-tooltip-position="top" aria-label="Static and Dynamic Systems" data-href="Static and Dynamic Systems" href="the-guide/robotics,-dynamics-and-control/dynamics/static-and-dynamic-systems.html" class="internal-link" target="_self" rel="noopener nofollow">system</a> that pulls the system towards the state , while the second imprints a non-linear behavior that can be learned to fit any motion profile.
<br>Effect of Hyperparameters

<br>The convergence rate   controls the convergence to the goal state
<br>The damping term 

<br>The components of the nonlinear part are ...<br>Canonical System
The DMP is driven by a phase variable or normalized time in order to control the timing of trajectory execution. Typically this is represented as so that  decays from  to .
<br>Effect of Canonical System Decay 
The canonical's system decay variable  controls how quickly the system goes through the motion encoded into the system via the learned superposition of basis <a data-tooltip-position="top" aria-label="Mathematical Relations" data-href="Mathematical Relations" href="the-guide/mathematics/general-stuff/mathematical-relations.html" class="internal-link" target="_self" rel="noopener nofollow">functions</a>.
<br>Superposition of Basis Functions
To encode the nonlinear part of the motion profile, classic DMPs use a superposition of basis functions, most commonly Gaussian basis functionsWeighting these functions each with a set of weights  for each dimension  allows for a wide range of motions to be encoded. 
]]></description><link>the-guide/robotics,-dynamics-and-control/control/dmps-dynamic-movement-primitives.html</link><guid isPermaLink="false">The Guide/Robotics, Dynamics and Control/Control/DMPs - Dynamic Movement Primitives.md</guid><pubDate>Mon, 03 Mar 2025 15:30:26 GMT</pubDate></item><item><title><![CDATA[Generalized Momentum Observer]]></title><description><![CDATA[ 
 <br>Based on the general <a data-tooltip-position="top" aria-label="Static and Dynamic Systems" data-href="Static and Dynamic Systems" href="the-guide/robotics,-dynamics-and-control/dynamics/static-and-dynamic-systems.html" class="internal-link" target="_self" rel="noopener nofollow">dynamics</a> equation of an -DOF rigid body manipulatorwith  <a data-tooltip-position="top" aria-label="Matrices" data-href="Matrices" href="the-guide/mathematics/linear-algebra/matrices.html" class="internal-link" target="_self" rel="noopener nofollow">symmetric and positive definite</a>, we can construct  (non-unique). One common way is to use the mass matrix as a <a data-tooltip-position="top" aria-label="(Semi-) Riemannsche Metriken" data-href="(Semi-) Riemannsche Metriken" href="the-guide/mathematics/differential-geometry/riemannian-geometry/metriken-und-zusammenhänge/(semi-)-riemannsche-metriken.html" class="internal-link" target="_self" rel="noopener nofollow">metric</a> in <a data-tooltip-position="top" aria-label="Generalized Coordinates, Configuration or Joint Space" data-href="Generalized Coordinates, Configuration or Joint Space" href="the-guide/robotics,-dynamics-and-control/generalized-coordinates,-configuration-or-joint-space.html" class="internal-link" target="_self" rel="noopener nofollow">configuration space</a> and define  based on the <a data-tooltip-position="top" aria-label="Christoffel-Symbol" data-href="Christoffel-Symbol" href="the-guide/mathematics/differential-geometry/elementary-differential-geometry/intrinsische-geometrie-von-hyperflächen/christoffel-symbol.html" class="internal-link" target="_self" rel="noopener nofollow">Christoffel-Symbols</a> via<br>We can show that for the coriolis term not to inject artificial energy (just redistribution), it has to fulfill a skew-symmetry condition, which yields another skew-symmetry property forThis can be derived from the definition of the Christoffel symbols via <a data-tooltip-position="top" aria-label="Derivative, Gradient, Jacobian and Hessian" data-href="Derivative, Gradient, Jacobian and Hessian" href="the-guide/mathematics/analysis-and-calculus/derivative,-gradient,-jacobian-and-hessian.html" class="internal-link" target="_self" rel="noopener nofollow">derivatives</a> of  above. Based on skew symmetry argument we can derive the equation<br><br><br>The generalizes momentum is defined via Taking the derivative, the product rule giveswhich we can combine with the general dynamics equation to obtainInserting the equation derived from the skew-symmetry argument, we getCollecting all torques in , each component of the change in momentum is the decoupled with respect to these torques<br><br><br>Based on the steps above, we can define a residual vectorwith . The dynamics of this residual is given bywith component-wise transfer function <a data-tooltip-position="top" aria-label="Laplace Transform" data-href="Laplace Transform" href="Laplace Transform" class="internal-link" target="_self" rel="noopener nofollow">in the Laplace domain</a> of the form<br>In practice, we therefore want to choose  as large as possible to estimate the external torques based on<br>Advantages

<br>No acceleration required
<br>No <a data-tooltip-position="top" aria-label="Matrices" data-href="Matrices" href="the-guide/mathematics/linear-algebra/matrices.html" class="internal-link" target="_self" rel="noopener nofollow">matrix</a> inversion required

<br>Disadvantage / Problems
Accuracy depends on how well we can model the remainder of the system, as all errors will be interpreted as external. We can work against this with a threshold, but only to some extend.
]]></description><link>the-guide/robotics,-dynamics-and-control/control/generalized-momentum-observer.html</link><guid isPermaLink="false">The Guide/Robotics, Dynamics and Control/Control/Generalized Momentum Observer.md</guid><pubDate>Sun, 16 Mar 2025 12:06:46 GMT</pubDate></item><item><title><![CDATA[Hybrid Impedance and Admittance Control]]></title><description><![CDATA[ 
 <br>In a Nutshell
Formulation based on the paper Unified Impedance and Admittance Control, unifying <a data-tooltip-position="top" aria-label="Admittance Control" data-href="Admittance Control" href="the-guide/robotics,-dynamics-and-control/control/admittance-control.html" class="internal-link" target="_self" rel="noopener nofollow">admittance control</a> and <a data-tooltip-position="top" aria-label="Impedance Control" data-href="Impedance Control" href="the-guide/robotics,-dynamics-and-control/control/impedance-control.html" class="internal-link" target="_self" rel="noopener nofollow">impedance control</a> as extreme cases of one general <a data-tooltip-position="top" aria-label="Control Overview" data-href="Control Overview" href="the-guide/robotics,-dynamics-and-control/control/control-overview.html" class="internal-link" target="_self" rel="noopener nofollow">control</a> framework.
<br><br>We consider a <a data-tooltip-position="top" aria-label="Static and Dynamic Systems" data-href="Static and Dynamic Systems" href="the-guide/robotics,-dynamics-and-control/dynamics/static-and-dynamic-systems.html" class="internal-link" target="_self" rel="noopener nofollow">system</a> with control force  and external forces , where all parameters and variables may be vector-valued. The general goal in <a data-tooltip-position="top" aria-label="Interaction Control" data-href="Interaction Control" href="the-guide/robotics,-dynamics-and-control/control/interaction-control.html" class="internal-link" target="_self" rel="noopener nofollow">interaction control tasks</a> is to generate a control force to enforce a desired relationship between the external force and the deviations  and . ]]></description><link>the-guide/robotics,-dynamics-and-control/control/hybrid-impedance-and-admittance-control.html</link><guid isPermaLink="false">The Guide/Robotics, Dynamics and Control/Control/Hybrid Impedance and Admittance Control.md</guid><pubDate>Mon, 03 Mar 2025 15:30:26 GMT</pubDate></item><item><title><![CDATA[Impedance Control]]></title><description><![CDATA[ 
 <br>In a Nutshell
Approach to <a data-tooltip-position="top" aria-label="Static and Dynamic Systems" data-href="Static and Dynamic Systems" href="the-guide/robotics,-dynamics-and-control/dynamics/static-and-dynamic-systems.html" class="internal-link" target="_self" rel="noopener nofollow">dynamic</a> <a data-tooltip-position="top" aria-label="Control Overview" data-href="Control Overview" href="the-guide/robotics,-dynamics-and-control/control/control-overview.html" class="internal-link" target="_self" rel="noopener nofollow">control</a>, specifically <a data-tooltip-position="top" aria-label="Interaction Control" data-href="Interaction Control" href="the-guide/robotics,-dynamics-and-control/control/interaction-control.html" class="internal-link" target="_self" rel="noopener nofollow">interaction control</a> that controls motion indirectly via torque control based on measured external forces at the TCP. This response to displacement is similar to a virtual "spring-damper" <a data-tooltip-position="top" aria-label="Static and Dynamic Systems" data-href="Static and Dynamic Systems" href="the-guide/robotics,-dynamics-and-control/dynamics/static-and-dynamic-systems.html" class="internal-link" target="_self" rel="noopener nofollow">system</a>.
<br><br><br>The key idea of impedance control is to control the system in a way that is behaves like a compliant physical system with desired dynamic properties, e.g. spring-damper under displacements. In robotics this generally concerns the TCP of the <a data-tooltip-position="top" aria-label="End-Effector Pose Representations" data-href="End-Effector Pose Representations" href="the-guide/robotics,-dynamics-and-control/kinematics/end-effector-pose-representations.html" class="internal-link" target="_self" rel="noopener nofollow">end-effector</a>. For this, we first formalize the desired <a data-tooltip-position="top" aria-label="Static and Dynamic Systems" data-href="Static and Dynamic Systems" href="the-guide/robotics,-dynamics-and-control/dynamics/static-and-dynamic-systems.html" class="internal-link" target="_self" rel="noopener nofollow">dynamic</a> behavior aswhere  is the inertia matrix,  is a damping term in <a data-tooltip-position="top" aria-label="Matrices" data-href="Matrices" href="the-guide/mathematics/linear-algebra/matrices.html" class="internal-link" target="_self" rel="noopener nofollow">matrix</a> form used to absorb energy (often for <a data-tooltip-position="top" aria-label="Ljapunov Stability of Dynamical Systems" data-href="Ljapunov Stability of Dynamical Systems" href="the-guide/robotics,-dynamics-and-control/dynamics/ljapunov-stability-of-dynamical-systems.html" class="internal-link" target="_self" rel="noopener nofollow">stability</a>) and  a stiffness <a data-tooltip-position="top" aria-label="Matrices" data-href="Matrices" href="the-guide/mathematics/linear-algebra/matrices.html" class="internal-link" target="_self" rel="noopener nofollow">matrix</a>.  describes position and orientation of the TCP.<br>
<img alt="center" src="lib/media/pasted-image-20250122112117.png" style="width: 400px; max-width: 100%;"><br>
Impedance Control is often used when the manipulator, e.g. a robot interacts with its environment (<a data-href="Interaction Control" href="the-guide/robotics,-dynamics-and-control/control/interaction-control.html" class="internal-link" target="_self" rel="noopener nofollow">Interaction Control</a>). By controlling impedance, we control the energy exchange during an interaction.<br>Using generalized cartesian forces , a <a data-tooltip-position="top" aria-label="Static and Dynamic Systems" data-href="Static and Dynamic Systems" href="the-guide/robotics,-dynamics-and-control/dynamics/static-and-dynamic-systems.html" class="internal-link" target="_self" rel="noopener nofollow">dynamics</a> model of a robot with  can be written as where  are the control inputs for <a data-tooltip-position="top" aria-label="Control Overview" data-href="Control Overview" href="the-guide/robotics,-dynamics-and-control/control/control-overview.html" class="internal-link" target="_self" rel="noopener nofollow">torque control</a>. We now have to incorporate our desired behavior into this control model.<br>Relation to Admittance and Impedance in physical Systems
In Impedance Control the controller is an impedance (motion inputs  force outputs) and the manipulator is an admittance (force inputs  motion outputs).
<br>Implications of this Approach on Performance
In general, robotic systems with Impedance Control have stable dynamic interaction with stiff environments but have poor accuracy in free-space due to friction and other unmodeled dynamics.
<br>Avantages

<br>Enables sophisticated dynamic behavior at the TCP for robot interation

<br>Disadvantage

<br>Requires torque sensors at the robots joints to measure . 

<br><br><br>Based on the above <a data-tooltip-position="top" aria-label="Static and Dynamic Systems" data-href="Static and Dynamic Systems" href="the-guide/robotics,-dynamics-and-control/dynamics/static-and-dynamic-systems.html" class="internal-link" target="_self" rel="noopener nofollow">dynamics</a> model of a robot with  where  are the control inputs for <a data-tooltip-position="top" aria-label="Control Overview" data-href="Control Overview" href="the-guide/robotics,-dynamics-and-control/control/control-overview.html" class="internal-link" target="_self" rel="noopener nofollow">torque control</a>, we can adapt out reference acceleration to enforce the desired behavior.<br>Based on feedback linearization in cartesian / task <a data-href="Space" href="the-guide/mathematics/general-stuff/space.html" class="internal-link" target="_self" rel="noopener nofollow">Space</a> (with cartesian force measure ), we build our controller based on with the <a data-tooltip-position="top" aria-label="Derivative, Gradient, Jacobian and Hessian" data-href="Derivative, Gradient, Jacobian and Hessian" href="the-guide/mathematics/analysis-and-calculus/derivative,-gradient,-jacobian-and-hessian.html" class="internal-link" target="_self" rel="noopener nofollow">geometric Jacobian</a> . If we choose our new reference acceleration  as we implicitly define a <a data-tooltip-position="top" aria-label="Control Overview" data-href="Control Overview" href="the-guide/robotics,-dynamics-and-control/control/control-overview.html" class="internal-link" target="_self" rel="noopener nofollow">closed-loop control</a> for initial desired dynamics.<br>Intuition 

<br>Our systems dynamics is influenced by external forces , stiffness and damping determine how it reacts.
<br>E.g. foreven slight deviations in the -direction lead to a large term that is subtracted from the external force, which limits possible movement. For , the low coefficients enable the <a data-href="Static and Dynamic Systems" href="the-guide/robotics,-dynamics-and-control/dynamics/static-and-dynamic-systems.html" class="internal-link" target="_self" rel="noopener nofollow">Static and Dynamic Systems</a> to move easily under external forces.

<br>Effect of Hyperparameters
Stiffness coefficient along any cartesian direction results in trade-off between contact forces and position accuracy (the stiffer, the more accurate, but can lead to higher forces).
In many cases, it is reasonable to mimic a human arms behavior, fast and stiff in free motion and slow and compliant in guarded motions. This can be encoded into the above control law.

<br>Large  and small  where contact is foreseen to reach low / regulated contact forces
<br>Large  and small  in directions that are free to reach good tracking of desired trajectory
<br>Damping  for transient behavior

In many applications the desired motion  is chosen to be ...

<br>... slightly inside the environment to allow constant contact and application of force
<br>... constant, yielding a free cartesian rest pose the robot tries to get back to.

<br><br><br>Impedance control is generally a control application in task space, as compliance in joint space does not really have applications (?).]]></description><link>the-guide/robotics,-dynamics-and-control/control/impedance-control.html</link><guid isPermaLink="false">The Guide/Robotics, Dynamics and Control/Control/Impedance Control.md</guid><pubDate>Mon, 03 Mar 2025 15:30:26 GMT</pubDate><enclosure url="lib/media/pasted-image-20250122112117.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src=&quot;lib/media/pasted-image-20250122112117.png&quot;&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[Interaction Control]]></title><description><![CDATA[<a class="tag" href="?query=tag:Robotics" style="background-color: rgb(4, 108, 116); color: white; font-weight: 700; border: none; border-radius: 1em; padding: 0.2em 0.5em;">#Robotics</a> 
 <br>In a Nutshell
Category of <a data-tooltip-position="top" aria-label="Control Overview" data-href="Control Overview" href="the-guide/robotics,-dynamics-and-control/control/control-overview.html" class="internal-link" target="_self" rel="noopener nofollow">control strategies</a> used in <a href=".?query=tag:Robotics" class="tag" target="_blank" rel="noopener nofollow">#Robotics</a>  and mechatronics to manage how robots interact with their environment, particularly when they apply or experience forces and torques.
<br>A robot may interact with its environment or a tool, e.g. modifying it (pick-and-place) or exchanging forces with it (polishing task etc.). Tools are generally described using their center point, the TCP (Tool Center Point).<br>Unclear Definitions
All definitions in this context are sometimes captured under the umbrella term compliant control or just called impedance control.
<br><br>Impedance and Admittance in Systems
A physical <a data-tooltip-position="top" aria-label="Static and Dynamic Systems" data-href="Static and Dynamic Systems" href="the-guide/robotics,-dynamics-and-control/dynamics/static-and-dynamic-systems.html" class="internal-link" target="_self" rel="noopener nofollow">system</a> that accepts motion inputs and yields force outputs is defined as an impedance. A system that accepts force inputs and yields motion outputs is defined as an admittance.
<br>In general, interaction control relies on one of the following strategies<br>
<br>
<a data-href="Admittance Control" href="the-guide/robotics,-dynamics-and-control/control/admittance-control.html" class="internal-link" target="_self" rel="noopener nofollow">Admittance Control</a> - robot controls its position or velocity in response to forces applied to it (at the TCP), allowing it to comply.

<br>
<a data-tooltip-position="top" aria-label="Impedance Control" data-href="Impedance Control" href="the-guide/robotics,-dynamics-and-control/control/impedance-control.html" class="internal-link" target="_self" rel="noopener nofollow">Impedance Control</a>- controls exerted force indirectly based on measured motion / position deviation at the TCP.

<br>
<a data-href="Hybrid Impedance and Admittance Control" href="the-guide/robotics,-dynamics-and-control/control/hybrid-impedance-and-admittance-control.html" class="internal-link" target="_self" rel="noopener nofollow">Hybrid Impedance and Admittance Control</a> - combines the former two in one framework, where they are the extreme cases. This enables a combination of both behaviors

<br>Admittance converts external forces into motion commands to comply
<br>Impedance regulates the robots dynamics to ensure a certain response behavior


]]></description><link>the-guide/robotics,-dynamics-and-control/control/interaction-control.html</link><guid isPermaLink="false">The Guide/Robotics, Dynamics and Control/Control/Interaction Control.md</guid><pubDate>Mon, 03 Mar 2025 15:30:26 GMT</pubDate></item><item><title><![CDATA[Linear Controllers]]></title><description><![CDATA[ 
 <br>In a Nutshell
<a data-tooltip-position="top" aria-label="Control Overview" data-href="Control Overview" href="the-guide/robotics,-dynamics-and-control/control/control-overview.html" class="internal-link" target="_self" rel="noopener nofollow">Feedback</a> controllers that use a control input based on linear combination of system state errors, namely velocity and acceleration. For <a data-tooltip-position="top" aria-label="Inverse Kinematics Problem" data-href="Inverse Kinematics Problem" href="the-guide/robotics,-dynamics-and-control/kinematics/inverse-kinematics-problem.html" class="internal-link" target="_self" rel="noopener nofollow">control in task space</a>, the laws may require <a data-tooltip-position="top" aria-label="Mathematical Relations" data-href="Mathematical Relations" href="the-guide/mathematics/general-stuff/mathematical-relations.html" class="internal-link" target="_self" rel="noopener nofollow">mapping</a> via the <a data-tooltip-position="top" aria-label="Derivative, Gradient, Jacobian and Hessian" data-href="Derivative, Gradient, Jacobian and Hessian" href="the-guide/mathematics/analysis-and-calculus/derivative,-gradient,-jacobian-and-hessian.html" class="internal-link" target="_self" rel="noopener nofollow">Jacobian</a>.
<br><br><br><br>Proportional Controller
Adjusts the control input based only on the position error (the difference between the desired and actual positions).Usually suitable for systems with relatively small inertia and damping or where precision is not critical / external disturbances are minimal.
<br>Advantages

<br>Simplicity: Easy to implement and computationally efficient.
<br>Fast Response: Provides a quick reaction to changes in the desired position.
<br>Low Computational Cost: No need for complex calculations or state measurements (e.g., velocity).

<br>Disadvantages

<br>Oscillations: A high KP\mathbf{K}_PKP​ can lead to overshooting and oscillations, as it does not consider the system's dynamics like velocity or damping.
<br>Steady-State Error: It cannot eliminate steady-state errors (i.e., the system may never perfectly reach the desired position due to the lack of compensation for constant disturbances or system imperfections).
<br>Lack of Robustness: It is susceptible to external disturbances, which can lead to sustained error.

<br><br><br>Proportional-Derivative Controller
Extends the P-Controller by adding a derivative term that is based on the velocity errorThe derivative term helps reduce oscillations and improve the system's damping, making it more stable than a P-Controller. Usually suitable for systems with speed and dynamic response demands.
<br>Advantages

<br>Reduced Oscillations: The derivative term helps smooth the system's response by accounting for the rate of change of the position (velocity), reducing overshoot and oscillations.
<br>Faster Settling Time: By including velocity feedback, the controller is able to respond to dynamic changes more effectively.

<br>Disadvantages

<br>Steady-State Error: Like the P-Controller, the PD-Controller does not eliminate steady-state error. The system may still be unable to reach the desired position, especially in the presence of external disturbances or constant forces.
<br>Sensitivity to Noise: The derivative term is sensitive to high-frequency noise, which can cause the controller to react to small, rapid fluctuations in the system's velocity, leading to instability if not properly filtered.
<br>Tuning Complexity: Finding the correct balance between the ​ and  gains is crucial for achieving a stable and responsive system.

<br><br><br>PD-Controller with Gravity Compensation
Improves the basic PD controller by adding a compensation term that accounts for the robot's gravitational forces, reducing the need for additional torque to counterbalance the weight of the robotSuitable for systems with heavy payloads.
<br>Advantages

<br>Gravity Compensation: The inclusion of the gravity compensation term makes the controller capable of handling the robot's weight, reducing the load on the actuators and improving efficiency.
<br>Improved Stability: With the inclusion of the gravitational term, the robot is better able to maintain stable positions in the presence of gravity, especially for large or heavy robots.
<br>Reduced Actuator Load: By compensating for the gravitational forces, the controller reduces the need for high torque inputs to maintain the robot's posture.

<br>Disadvantages

<br>Requires Accurate Model: The compensation term  requires an accurate model of the robot's dynamics (specifically, its gravitational forces). If the model is inaccurate or incomplete, the controller will not function optimally.
<br>High Gains and Stiffness: In some cases, small errors in estimating the gravitational forces or system dynamics can require large gains in the controller, making the system stiff and potentially costly to implement.
<br>Complexity: More complex than a simple PD controller due to the need for precise modeling of the gravitational components.

<br><br><br>Proportional-Integral-Derivative Controller
Adds integral term to eliminate constant deviation (offset) from the desired positionUseful for systems that need steady state accuracy, such as CNC machines, industrial robots or temperature regulators or with persistent disturbances such as high friction.
<br>Advantages

<br>Eliminates Steady-State Error: The integral term ensures that the system will eventually eliminate any persistent error or offset, even in the presence of constant disturbances.
<br>Good for Steady-State Systems: Very effective in systems that require precise steady-state control, where long-term accuracy is more important than fast transient responses.
<br>Robustness: Well-suited for systems with unknown or varying dynamics, especially when there is no precise model.

<br>Disadvantages

<br>Risk of Over-Compensation (Wind-Up): If the system accumulates too much error in the integral term, it can cause integral wind-up, where the control signal becomes excessively large, leading to overshoot and instability.
<br>Slower Response: The integral action introduces a delay in the system's response because it is based on accumulated error, leading to slower convergence compared to PD controllers.
<br>Difficult to Tune: Finding the right balance between the proportional, derivative, and integral gains requires careful tuning and can be difficult, especially in non-linear systems.

<br><br><br>If we require asymptotic stabilization of the end-effector ( and ), we can use an approach based on <a data-tooltip-position="top" aria-label="Linear Controllers" data-href="Linear Controllers" href="the-guide/robotics,-dynamics-and-control/control/linear-controllers.html" class="internal-link" target="_self" rel="noopener nofollow">linear control</a> via  with  <a data-tooltip-position="top" aria-label="Matrices" data-href="Matrices" href="the-guide/mathematics/linear-algebra/matrices.html" class="internal-link" target="_self" rel="noopener nofollow">symmetric</a>.<br>Alternatively, if the joint velocities aren't available, the all cartesian PD control with gravity compensation can be employed via with  <a data-tooltip-position="top" aria-label="Matrices" data-href="Matrices" href="the-guide/mathematics/linear-algebra/matrices.html" class="internal-link" target="_self" rel="noopener nofollow">symmetric</a>.]]></description><link>the-guide/robotics,-dynamics-and-control/control/linear-controllers.html</link><guid isPermaLink="false">The Guide/Robotics, Dynamics and Control/Control/Linear Controllers.md</guid><pubDate>Sun, 30 Mar 2025 12:38:04 GMT</pubDate></item><item><title><![CDATA[LQG Regulator]]></title><description><![CDATA[ 
 <br>In a Nutshell
<a data-tooltip-position="top" aria-label="Optimal Control and LQR" data-href="Optimal Control and LQR" href="the-guide/robotics,-dynamics-and-control/control/optimal-control-and-lqr.html" class="internal-link" target="_self" rel="noopener nofollow">Optimal control</a> strategy that can be derived analytically, if the <a data-tooltip-position="top" aria-label="Static and Dynamic Systems" data-href="Static and Dynamic Systems" href="the-guide/robotics,-dynamics-and-control/dynamics/static-and-dynamic-systems.html" class="internal-link" target="_self" rel="noopener nofollow">system</a> of interest has

<br>linear <a data-tooltip-position="top" aria-label="Static and Dynamic Systems" data-href="Static and Dynamic Systems" href="the-guide/robotics,-dynamics-and-control/dynamics/static-and-dynamic-systems.html" class="internal-link" target="_self" rel="noopener nofollow">dynamics</a>
<br>a quadratic reward / cost function
<br>all <a data-tooltip-position="top" aria-label="Gaussian Distribution" data-href="Gaussian Distribution" href="the-guide/mathematics/probability-theory/gaussian-distribution.html" class="internal-link" target="_self" rel="noopener nofollow">Gaussian</a> noise

<br><br><br>Mathematically, the system in question is defined as<br>Linear Quadratic Gaussian Systems
Given a continuous <a data-tooltip-position="top" aria-label="Markov Decision Process" data-href="Markov Decision Process" href="the-guide/machine-learning/reinforcement-learning/markov-decision-process.html" class="internal-link" target="_self" rel="noopener nofollow">state</a> <a data-href="Space" href="the-guide/mathematics/general-stuff/space.html" class="internal-link" target="_self" rel="noopener nofollow">Space</a> , a continuous <a data-tooltip-position="top" aria-label="Markov Decision Process" data-href="Markov Decision Process" href="the-guide/machine-learning/reinforcement-learning/markov-decision-process.html" class="internal-link" target="_self" rel="noopener nofollow">action</a> space , the transition dynamics is linear and can therefore be described using a system <a data-tooltip-position="top" aria-label="Matrices" data-href="Matrices" href="the-guide/mathematics/linear-algebra/matrices.html" class="internal-link" target="_self" rel="noopener nofollow">matrix</a> , control <a data-tooltip-position="top" aria-label="Matrices" data-href="Matrices" href="the-guide/mathematics/linear-algebra/matrices.html" class="internal-link" target="_self" rel="noopener nofollow">matrix</a>  and drift term  with noise as a density Additionally, the cost / reward has to have a quadratic form where  punishes state deviation and  punishes control ressource usage.
<br><br><br>If we want to solve the optimal control problem using <a data-tooltip-position="top" aria-label="Dynamic Programming" data-href="Dynamic Programming" href="the-guide/machine-learning/reinforcement-learning/dynamic-programming.html" class="internal-link" target="_self" rel="noopener nofollow">value iteration</a>, we have to solve two problems at the same time during the backwards iteration: the <a data-tooltip-position="top" aria-label="Expectations" data-href="Expectations" href="the-guide/mathematics/statistics/expectations.html" class="internal-link" target="_self" rel="noopener nofollow">expectation</a> over the next value and the maximum operator in this continuous setting. For a <a data-tooltip-position="top" aria-label="- Reinforcement Learning -" data-href="- Reinforcement Learning -" href="the-guide/machine-learning/reinforcement-learning/-reinforcement-learning-.html" class="internal-link" target="_self" rel="noopener nofollow">finite horizon</a> (here without linear terms), we can derive the optimal <a data-href="Policy" href="the-guide/machine-learning/reinforcement-learning/policy.html" class="internal-link" target="_self" rel="noopener nofollow">Policy</a> and value function via Bellmans Recipe<br>
<br>At last time step T, <a data-tooltip-position="top" aria-label="Value Functions" data-href="Value Functions" href="the-guide/machine-learning/reinforcement-learning/value-functions.html" class="internal-link" target="_self" rel="noopener nofollow">value function</a> is given via reward function 
<br>Iterate backwards  in time and compute <a data-tooltip-position="top" aria-label="Q-Function" data-href="Q-Function" href="the-guide/machine-learning/reinforcement-learning/q-function.html" class="internal-link" target="_self" rel="noopener nofollow">Q-Function</a>
<br>Then <a data-tooltip-position="top" aria-label="Policy" data-href="Policy" href="the-guide/machine-learning/reinforcement-learning/policy.html" class="internal-link" target="_self" rel="noopener nofollow">optimal policy</a> (<a data-href="Set" href="the-guide/mathematics/general-stuff/set.html" class="internal-link" target="_self" rel="noopener nofollow">Set</a> derivative of equation above to zero and solve for )
<br>And then the <a data-tooltip-position="top" aria-label="Value Functions" data-href="Value Functions" href="the-guide/machine-learning/reinforcement-learning/value-functions.html" class="internal-link" target="_self" rel="noopener nofollow">Value Function</a> for time step t (update).
<br>Intuition
The resulting <a data-tooltip-position="top" aria-label="Policy" data-href="Policy" href="the-guide/machine-learning/reinforcement-learning/policy.html" class="internal-link" target="_self" rel="noopener nofollow">optimal policy</a> is a time-dependant <a data-tooltip-position="top" aria-label="Linear Controllers" data-href="Linear Controllers" href="the-guide/robotics,-dynamics-and-control/control/linear-controllers.html" class="internal-link" target="_self" rel="noopener nofollow">linear feedback controller</a> with time-dependent offset.
]]></description><link>the-guide/robotics,-dynamics-and-control/control/lqg-regulator.html</link><guid isPermaLink="false">The Guide/Robotics, Dynamics and Control/Control/LQG Regulator.md</guid><pubDate>Wed, 23 Apr 2025 21:56:28 GMT</pubDate></item><item><title><![CDATA[Model Predictive Control]]></title><description><![CDATA[ 
 <br>In a Nutshell
<a data-tooltip-position="top" aria-label="Optimization Problem" data-href="Optimization Problem" href="the-guide/mathematics/optimization/convex-optimization-lecture/optimization-problem.html" class="internal-link" target="_self" rel="noopener nofollow">Optimization</a>-based control strategies in time domain, often running in real time by repeatedly solving a short horizon problem. 
<br><br>While classical control approaches usually rely on analysis of closed-loop behavior in the <a data-tooltip-position="top" aria-label="Laplace Transform" data-href="Laplace Transform" href="Laplace Transform" class="internal-link" target="_self" rel="noopener nofollow">frequency domain</a> analysis and derived tools such as Bode plots, constrains yield nonlinear <a data-tooltip-position="top" aria-label="Static and Dynamic Systems" data-href="Static and Dynamic Systems" href="the-guide/robotics,-dynamics-and-control/dynamics/static-and-dynamic-systems.html" class="internal-link" target="_self" rel="noopener nofollow">dynamic systems</a>, which cannot be analyzed in this way.<br>Intuition
The promise of frequency analysis is that we can reconstruct the full behavior of the system by superposition of its frequency responses. Additionally, we assume that its behavior does not change over time (LTI assumption = linear time invariant) If that fundamental property is violated, e.g. non-linear problem.
<br>However, even if a system is non-linear, the first order Taylor approximation around the current point offers a good linear approximation  ]]></description><link>the-guide/robotics,-dynamics-and-control/control/model-predictive-control.html</link><guid isPermaLink="false">The Guide/Robotics, Dynamics and Control/Control/Model Predictive Control.md</guid><pubDate>Thu, 10 Apr 2025 21:53:47 GMT</pubDate></item><item><title><![CDATA[Model-Based Control]]></title><description><![CDATA[ 
 <br>Exploit <a data-tooltip-position="top" aria-label="Static and Dynamic Systems" data-href="Static and Dynamic Systems" href="the-guide/robotics,-dynamics-and-control/dynamics/static-and-dynamic-systems.html" class="internal-link" target="_self" rel="noopener nofollow">inverse dynamics</a> as <a data-tooltip-position="top" aria-label="Control Overview" data-href="Control Overview" href="the-guide/robotics,-dynamics-and-control/control/control-overview.html" class="internal-link" target="_self" rel="noopener nofollow">control</a> policy to achieve desired <a data-tooltip-position="top" aria-label="Static and Dynamic Systems" data-href="Static and Dynamic Systems" href="the-guide/robotics,-dynamics-and-control/dynamics/static-and-dynamic-systems.html" class="internal-link" target="_self" rel="noopener nofollow">system</a> behavior.<br><br>Model-Based Feedback Control
Non-linear control based on feedback linearization (FBL), linearly adapt your desired velocity based on the measured errorsvia <a data-tooltip-position="top" aria-label="Linear Controllers" data-href="Linear Controllers" href="the-guide/robotics,-dynamics-and-control/control/linear-controllers.html" class="internal-link" target="_self" rel="noopener nofollow">linear control</a> laws and then insert them into the <a data-tooltip-position="top" aria-label="Static and Dynamic Systems" data-href="Static and Dynamic Systems" href="the-guide/robotics,-dynamics-and-control/dynamics/static-and-dynamic-systems.html" class="internal-link" target="_self" rel="noopener nofollow">inverse dynamics model</a> via 
<br><img alt="center" src="lib/media/pasted-image-20230216205406.png" style="width: 500px; max-width: 100%;"><br><br>Inverse Dynamics Feedforward Control
Assume  and  and use torque prediction combining an <a data-tooltip-position="top" aria-label="Static and Dynamic Systems" data-href="Static and Dynamic Systems" href="the-guide/robotics,-dynamics-and-control/dynamics/static-and-dynamic-systems.html" class="internal-link" target="_self" rel="noopener nofollow">inverse dynamics model</a>and a <a data-tooltip-position="top" aria-label="Linear Controllers" data-href="Linear Controllers" href="the-guide/robotics,-dynamics-and-control/control/linear-controllers.html" class="internal-link" target="_self" rel="noopener nofollow">linear PD control law</a> of the form
<br><img alt="center" src="lib/media/pasted-image-20230217093133.png" style="width: 400px; max-width: 100%;"><br>
<br>Advantages

<br>Pre-Computable, less real-time computation
<br>Generally more stable, even with bad models


]]></description><link>the-guide/robotics,-dynamics-and-control/control/model-based-control.html</link><guid isPermaLink="false">The Guide/Robotics, Dynamics and Control/Control/Model-Based Control.md</guid><pubDate>Mon, 03 Mar 2025 15:30:26 GMT</pubDate><enclosure url="lib/media/pasted-image-20230216205406.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src=&quot;lib/media/pasted-image-20230216205406.png&quot;&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[Nullspace Control]]></title><description><![CDATA[ 
 <br>If we want to use the redundancy of the <a data-tooltip-position="top" aria-label="Kinematics" data-href="Kinematics" href="the-guide/robotics,-dynamics-and-control/kinematics/kinematics.html" class="internal-link" target="_self" rel="noopener nofollow">kinematic</a> to tackle some sort of secondary subtask, we can use an alternative approach<br>Quadratic Programming
Solve optimization problem with constraints

<br>, guarantee cartesian velocities
<br>, with secondary task objective <a data-tooltip-position="top" aria-label="Mathematical Relations" data-href="Mathematical Relations" href="the-guide/mathematics/general-stuff/mathematical-relations.html" class="internal-link" target="_self" rel="noopener nofollow">function</a> .
<br>

<br>Alternatively, we can add a term that tries to minimize a cost function  solely by operationg in the nullspace via <br>
<br> is a sensitivity <a data-tooltip-position="top" aria-label="Matrices" data-href="Matrices" href="the-guide/mathematics/linear-algebra/matrices.html" class="internal-link" target="_self" rel="noopener nofollow">matrix</a> that can be used to make the nullspace control more responsive for certain joints.
]]></description><link>the-guide/robotics,-dynamics-and-control/control/nullspace-control.html</link><guid isPermaLink="false">The Guide/Robotics, Dynamics and Control/Control/Nullspace Control.md</guid><pubDate>Sun, 30 Mar 2025 12:39:12 GMT</pubDate></item><item><title><![CDATA[Optimal Control and LQR]]></title><description><![CDATA[ 
 <br>In a Nutshell
Control strategy that derived optimal control inputs to <a data-tooltip-position="top" aria-label="Static and Dynamic Systems" data-href="Static and Dynamic Systems" href="the-guide/robotics,-dynamics-and-control/dynamics/static-and-dynamic-systems.html" class="internal-link" target="_self" rel="noopener nofollow">dynamical systems</a> via <a data-tooltip-position="top" aria-label="Optimization Problem" data-href="Optimization Problem" href="the-guide/mathematics/optimization/convex-optimization-lecture/optimization-problem.html" class="internal-link" target="_self" rel="noopener nofollow">optimization</a> formulations.
<br>Since most controllers are digital, the more interesting case in physics and engineering is the discrete one.<br><br><br>Choose optimal input sequence  over a finite or infinite horizon to apply to a system with initial state . Optimal means to minimize a costwhile adhering to certain constraints. The minimal case requires to satisfy the (in general nonlinear) system dynamicsand possibly input constraintsAdditionally, we usually solve for a given initial state and (mostly in the finite horizon) for a desired final state <a data-tooltip-position="top" aria-label="Set" data-href="Set" href="the-guide/mathematics/general-stuff/set.html" class="internal-link" target="_self" rel="noopener nofollow">set</a><br><br><br>Simple case to understand the basic concepts.<br>Assumptions
Linear dynamics, quadratic cost function. In the following, no state or input constraints and the goal state is the origin.
<br>If the system is noisy, a <a data-tooltip-position="top" aria-label="Gaussian Distribution" data-href="Gaussian Distribution" href="the-guide/mathematics/probability-theory/gaussian-distribution.html" class="internal-link" target="_self" rel="noopener nofollow">Gaussian</a> assumption on this noise yields an explicit solution in form of the <a data-href="LQG Regulator" href="the-guide/robotics,-dynamics-and-control/control/lqg-regulator.html" class="internal-link" target="_self" rel="noopener nofollow">LQG Regulator</a>.]]></description><link>the-guide/robotics,-dynamics-and-control/control/optimal-control-and-lqr.html</link><guid isPermaLink="false">The Guide/Robotics, Dynamics and Control/Control/Optimal Control and LQR.md</guid><pubDate>Sun, 13 Apr 2025 19:21:30 GMT</pubDate></item><item><title><![CDATA[Controllability and Stabilizability of LTI Systems]]></title><description><![CDATA[<a class="tag" href="?query=tag:Linear-Algebra" style="background-color: rgb(4, 108, 116); color: white; font-weight: 700; border: none; border-radius: 1em; padding: 0.2em 0.5em;">#Linear-Algebra</a> 
 <br>Controllability
A linear system, e.g. <a data-tooltip-position="top" aria-label="LTI - Linear Time Invariant Systems" data-href="LTI - Linear Time Invariant Systems" href="the-guide/robotics,-dynamics-and-control/dynamics/lti-linear-time-invariant-systems.html" class="internal-link" target="_self" rel="noopener nofollow">LTI</a> is controllable or reachable, if for any pair of states , there exists a finite time  and a control sequence , such that . 
<br>Proposition
For an LTI system with -dimensional state, the system is controllable if the controllability <a data-tooltip-position="top" aria-label="Matrices" data-href="Matrices" href="the-guide/mathematics/linear-algebra/matrices.html" class="internal-link" target="_self" rel="noopener nofollow">matrix</a> has full rankThis condition is necessary and sufficient.
<br>Proof
Inserting a control sequence of arbitrary length  in the dynamics equation recursively, we obtainSince our state is -dimensional, we know by the <a data-href="Cayley-Hamilton Theorem" href="Cayley-Hamilton Theorem" class="internal-link" target="_self" rel="noopener nofollow">Cayley-Hamilton Theorem</a> that This means, we can solve the systemto generate such a sequence. A solution exists, if  has full rank.
<br><br>Stabilizability
A linear system is stabilizable, if there exists an input sequence  that returns any arbitrary initial state  to the origin.
<br>Theorem
A system is stabilizable if all of its uncontrollable modes are stable. Therefore, ifthe system  with the eigenvalues  outside the unit ball is stabilizable.
<br>Proof (Informal)
From <a href=".?query=tag:Linear-Algebra" class="tag" target="_blank" rel="noopener nofollow">#Linear-Algebra</a> we know that the LTI system is <a data-tooltip-position="top" aria-label="Stability of Numerical Methods" data-href="Stability of Numerical Methods" href="the-guide/mathematics/general-stuff/stability-of-numerical-methods.html" class="internal-link" target="_self" rel="noopener nofollow">stable</a>, if all <a data-tooltip-position="top" aria-label="Eigenvalue Problem" data-href="Eigenvalue Problem" href="the-guide/mathematics/linear-algebra/eigenvalue-problem.html" class="internal-link" target="_self" rel="noopener nofollow">eigenvalues</a> are within the unit ball.
The control input allows us to affect the controllable modes and force them to be stable. If the uncontrollable ones are already stable, we can make the whole system stable.
<br>Lemma
Controllability implies Stabilizability.
<br>Proof (Informal)
Since controllability means there are no uncontrollable modes, we are automatically stabilizable, because we can affect all modes.
]]></description><link>the-guide/robotics,-dynamics-and-control/dynamics/controllability-and-stabilizability-of-lti-systems.html</link><guid isPermaLink="false">The Guide/Robotics, Dynamics and Control/Dynamics/Controllability and Stabilizability of LTI Systems.md</guid><pubDate>Sun, 13 Apr 2025 17:44:35 GMT</pubDate></item><item><title><![CDATA[Friction Models]]></title><description><![CDATA[<a class="tag" href="?query=tag:Robotics" style="background-color: rgb(4, 108, 116); color: white; font-weight: 700; border: none; border-radius: 1em; padding: 0.2em 0.5em;">#Robotics</a> 
 <br>In a Nutshell
Approaches to model the friction term of a rigid body moving in euclidean space or for <a data-tooltip-position="top" aria-label="Robotic Joints" data-href="Robotic Joints" href="the-guide/robotics,-dynamics-and-control/robotic-joints.html" class="internal-link" target="_self" rel="noopener nofollow">robot joints</a> in <a data-tooltip-position="top" aria-label="Generalized Coordinates, Configuration or Joint Space" data-href="Generalized Coordinates, Configuration or Joint Space" href="the-guide/robotics,-dynamics-and-control/generalized-coordinates,-configuration-or-joint-space.html" class="internal-link" target="_self" rel="noopener nofollow">configuration space</a>.
<br>The following formulations are based on the <a href=".?query=tag:Robotics" class="tag" target="_blank" rel="noopener nofollow">#Robotics</a> notation in <a data-href="- Notation - ODEs, Robotics, Dynamics and Control -" href="notation-world/-notation-odes,-robotics,-dynamics-and-control-.html" class="internal-link" target="_self" rel="noopener nofollow">- Notation - ODEs, Robotics, Dynamics and Control -</a> and therefore relate to robot joint torques. For the euclidean case, simply switch out all torques with force components in -dimensional space.<br><br><br>Simplest model, only uses a single parameter for statis and dynamic friction.<br>
<br>Static friction (stiction): When the velocity is zero, the friction force can take any value within a bounded interval to prevent motion.
<br>Dynamic (kinetic) friction: When the velocity is nonzero, the friction force has a constant magnitude and opposes the direction of motion.
<br>Coulomb Friction Model
Mathematically, the torque resulting from friction in a joint based on the Coulomb model is expressed as
<br><br><br>Adds a linear proportionality to the velocity, resulting in a damping term. Especially useful in systems involving fluids.<br>Coulomb Model with Viscous Friction
Mathematically, it extends the above to
<br><br><br>Accounts for the Stribeck effect, i.e. the observation that friction rapidly decreases when joints begin moving.<br>Stribeck Friction Model
Mathematically, this is achieved by adding an exponentially decaying term via where the upper index  denotes asymptotic friction components,  is a characteristic velocity and  is a shaping parameter.
<br><br>Potentially add:<br>
<br>LuGre Model
<br>Dahl Model
]]></description><link>the-guide/robotics,-dynamics-and-control/dynamics/friction-models.html</link><guid isPermaLink="false">The Guide/Robotics, Dynamics and Control/Dynamics/Friction Models.md</guid><pubDate>Tue, 11 Mar 2025 18:12:34 GMT</pubDate></item><item><title><![CDATA[Hamiltonian Mechanics]]></title><description><![CDATA[ 
 <br>Given a <a data-tooltip-position="top" aria-label="Static and Dynamic Systems" data-href="Static and Dynamic Systems" href="the-guide/robotics,-dynamics-and-control/dynamics/static-and-dynamic-systems.html" class="internal-link" target="_self" rel="noopener nofollow">mechanical system</a>  with <a data-tooltip-position="top" aria-label="Generalized Coordinates, Configuration or Joint Space" data-href="Generalized Coordinates, Configuration or Joint Space" href="the-guide/robotics,-dynamics-and-control/generalized-coordinates,-configuration-or-joint-space.html" class="internal-link" target="_self" rel="noopener nofollow">configuration space</a>  and (smooth) <a data-tooltip-position="top" aria-label="Lagrangian Mechanics" data-href="Lagrangian Mechanics" href="the-guide/robotics,-dynamics-and-control/dynamics/lagrangian-mechanics.html" class="internal-link" target="_self" rel="noopener nofollow">Lagrangian</a> , select a standard coordinate system  on . Defining the momentaand the Legendre transformation of , a smooth map , the Lagrangian is transformed to the HamiltonianThereby, the <a data-tooltip-position="top" aria-label="Lagrangian Mechanics" data-href="Lagrangian Mechanics" href="the-guide/robotics,-dynamics-and-control/dynamics/lagrangian-mechanics.html" class="internal-link" target="_self" rel="noopener nofollow">Euler-Lagrange equation</a> in  dimensions becomes Hamilton's equations in  dimensions <br>
<br>Remarks

<br>Instead of using <a data-tooltip-position="top" aria-label="Generalized Coordinates, Configuration or Joint Space" data-href="Generalized Coordinates, Configuration or Joint Space" href="the-guide/robotics,-dynamics-and-control/generalized-coordinates,-configuration-or-joint-space.html" class="internal-link" target="_self" rel="noopener nofollow">configuration space</a> as in <a data-tooltip-position="top" aria-label="Lagrangian Mechanics" data-href="Lagrangian Mechanics" href="the-guide/robotics,-dynamics-and-control/dynamics/lagrangian-mechanics.html" class="internal-link" target="_self" rel="noopener nofollow">Lagrangian mechanics</a>, use phase space to track motion.


<br><br><br>A path  is a stationary point of  iff the path in phase space  obeys Hamilton's equations]]></description><link>the-guide/robotics,-dynamics-and-control/dynamics/hamiltonian-mechanics.html</link><guid isPermaLink="false">The Guide/Robotics, Dynamics and Control/Dynamics/Hamiltonian Mechanics.md</guid><pubDate>Tue, 11 Mar 2025 15:43:32 GMT</pubDate></item><item><title><![CDATA[Lagrangian Mechanics]]></title><description><![CDATA[ 
 <br>Info
Formulation of classical mechanics based on the principal of least action, using energies instead of forces in the <a data-tooltip-position="top" aria-label="Static and Dynamic Systems" data-href="Static and Dynamic Systems" href="the-guide/robotics,-dynamics-and-control/dynamics/static-and-dynamic-systems.html" class="internal-link" target="_self" rel="noopener nofollow">system</a>. Describes a mechanical system as a pair  consisting of <a data-tooltip-position="top" aria-label="Generalized Coordinates, Configuration or Joint Space" data-href="Generalized Coordinates, Configuration or Joint Space" href="the-guide/robotics,-dynamics-and-control/generalized-coordinates,-configuration-or-joint-space.html" class="internal-link" target="_self" rel="noopener nofollow">configuration space</a>  and a smooth function  called the Lagrangian. For many systems in engineering (non-relativistic, no electromagnetic field) where  is the kinetic energy (scalar pendant to vector-valued motion) and  the potential energy (scalar pendant to vector-valued location). The Lagrangian does not represent any physical phenomena.
<br>Newton vs. Lagrange
Newtonian mechanics tracks motion based on forces (cause and effect) in D position space, while Lagrangian mechanics looks at complete paths between two events in <a data-tooltip-position="top" aria-label="Generalized Coordinates, Configuration or Joint Space" data-href="Generalized Coordinates, Configuration or Joint Space" href="the-guide/robotics,-dynamics-and-control/generalized-coordinates,-configuration-or-joint-space.html" class="internal-link" target="_self" rel="noopener nofollow">configuration space</a> and uses principle of stationary action.
<br><br><br>The Action Functional
The time integral of the Lagrangian with  being <a data-tooltip-position="top" aria-label="Generalized Coordinates, Configuration or Joint Space" data-href="Generalized Coordinates, Configuration or Joint Space" href="the-guide/robotics,-dynamics-and-control/generalized-coordinates,-configuration-or-joint-space.html" class="internal-link" target="_self" rel="noopener nofollow">generalized coordinates</a> is denoted action and is a functional. The resulting unit is Joule-second, which is the same as for angular momentum. 
]]></description><link>the-guide/robotics,-dynamics-and-control/dynamics/lagrangian-mechanics.html</link><guid isPermaLink="false">The Guide/Robotics, Dynamics and Control/Dynamics/Lagrangian Mechanics.md</guid><pubDate>Sun, 16 Mar 2025 16:43:33 GMT</pubDate></item><item><title><![CDATA[Langevin Dynamics]]></title><description><![CDATA[<a class="tag" href="?query=tag:Computational-Statistics" style="background-color: rgb(4, 108, 116); color: white; font-weight: 700; border: none; border-radius: 1em; padding: 0.2em 0.5em;">#Computational-Statistics</a> 
 <br>In a Nutshell
<a data-tooltip-position="top" aria-label="Stochastic Differential Equation" data-href="Stochastic Differential Equation" href="the-guide/mathematics/probability-theory/stochastic-differential-equation.html" class="internal-link" target="_self" rel="noopener nofollow">SDE</a> used to model the evolution of a <a data-href="Static and Dynamic Systems" href="the-guide/robotics,-dynamics-and-control/dynamics/static-and-dynamic-systems.html" class="internal-link" target="_self" rel="noopener nofollow">Static and Dynamic Systems</a>'s position and velocity under the influence of both deterministic forces and random noise (representing thermal fluctuations, for example). It is widely used in statistical mechanics to simulate the behavior of particles in a fluid or a system in contact with a heat bath.
<br><br>Classical Langevin Equation
In the classical form, the Langevin equation describes the evolution of a particle’s position and velocity  under the influence of friction, a conservative force (derived from a potential), and random noise. The generalized Langevin equation for a particle of mass  is given as:In most applications, this is directily rewritten as a first order system of <a data-tooltip-position="top" aria-label="ODEs - Ordinary Differential Equations" data-href="ODEs - Ordinary Differential Equations" href="the-guide/mathematics/differential-equations/odes-ordinary-differential-equations.html" class="internal-link" target="_self" rel="noopener nofollow">ODEs</a> via 
<br>
<br> is the velocity of a particle
<br> is the damping / friction coefficient
<br> is a deterministic force, often derived from a potential
<br> is Boltzmann's constant
<br> is the temperature
<br> is <a data-tooltip-position="top" aria-label="Gaussian Distribution" data-href="Gaussian Distribution" href="the-guide/mathematics/probability-theory/gaussian-distribution.html" class="internal-link" target="_self" rel="noopener nofollow">Gaussian</a> noise with zero mean and <a data-tooltip-position="top" aria-label="Covariance and Variance" data-href="Covariance and Variance" href="the-guide/mathematics/statistics/covariance-and-variance.html" class="internal-link" target="_self" rel="noopener nofollow">variance</a> correlated via the time difference
<br>In many applications, especially in the application of <a href=".?query=tag:Computational-Statistics" class="tag" target="_blank" rel="noopener nofollow">#Computational-Statistics</a> , the high-friction limit (large ) yields a simplified equation denoted overdamped <br>Additional Information
The name stems from the fact that it is for scenarios in which the inertia is nearly completely absorbed by the damping, such that the system only and immediately reacts to external forces in its velocity.
<br>Connection to Fokker-Planck
The Langevin equation is equivalent to a <a data-tooltip-position="top" aria-label="Fokker-Planck Equation" data-href="Fokker-Planck Equation" href="the-guide/mathematics/probability-theory/fokker-planck-equation.html" class="internal-link" target="_self" rel="noopener nofollow">Fokker-Planck equation</a> for the <a data-href="Probability Distribution" href="the-guide/mathematics/probability-theory/probability-distribution.html" class="internal-link" target="_self" rel="noopener nofollow">Probability Distribution</a>  of the <a data-href="Static and Dynamic Systems" href="the-guide/robotics,-dynamics-and-control/dynamics/static-and-dynamic-systems.html" class="internal-link" target="_self" rel="noopener nofollow">Static and Dynamic Systems</a>’s state. The Fokker-Planck equation associated with the Langevin equation is:In the overdamped limit, this reduces to:
<br><br><br>For numerical simulations, Langevin dynamics is discretized using finite time steps. Let  be the time step, and denote the position and velocity at discrete time  as  and ​. A common discretization scheme (<a data-tooltip-position="top" aria-label="Euler-Maruyama Method" data-href="Euler-Maruyama Method" href="the-guide/mathematics/probability-theory/euler-maruyama-method.html" class="internal-link" target="_self" rel="noopener nofollow">Euler-Maruyama</a>) is given by:Here  is drawn from a unit normal distribution.]]></description><link>the-guide/robotics,-dynamics-and-control/dynamics/langevin-dynamics.html</link><guid isPermaLink="false">The Guide/Robotics, Dynamics and Control/Dynamics/Langevin Dynamics.md</guid><pubDate>Wed, 23 Apr 2025 21:55:39 GMT</pubDate></item><item><title><![CDATA[Ljapunov Stability for LTI Systems]]></title><description><![CDATA[ 
 <br>For a (discrete) <a data-tooltip-position="top" aria-label="LTI - Linear Time Invariant Systems" data-href="LTI - Linear Time Invariant Systems" href="the-guide/robotics,-dynamics-and-control/dynamics/lti-linear-time-invariant-systems.html" class="internal-link" target="_self" rel="noopener nofollow">LTI</a> <a data-tooltip-position="top" aria-label="Static and Dynamic Systems" data-href="Static and Dynamic Systems" href="the-guide/robotics,-dynamics-and-control/dynamics/static-and-dynamic-systems.html" class="internal-link" target="_self" rel="noopener nofollow">dynamic system</a>, we can use the concepts from <a data-href="Ljapunov Stability of Dynamical Systems" href="the-guide/robotics,-dynamics-and-control/dynamics/ljapunov-stability-of-dynamical-systems.html" class="internal-link" target="_self" rel="noopener nofollow">Ljapunov Stability of Dynamical Systems</a> to show global asymptotic stability. Additionally, linearity results in these results are not only sufficient, but also necessary.<br><br>For the discretized LTI system we can take to automatically satisfy ,  and the limit conditions from the <a data-tooltip-position="top" aria-label="Ljapunov Stability of Dynamical Systems" data-href="Ljapunov Stability of Dynamical Systems" href="the-guide/robotics,-dynamics-and-control/dynamics/ljapunov-stability-of-dynamical-systems.html" class="internal-link" target="_self" rel="noopener nofollow">Ljapunov Theorem</a>. <br>Theorem
The discrete time LTI Ljapunov equation above has a unique solution  if and only if  has all <a data-tooltip-position="top" aria-label="Eigenvalue Problem" data-href="Eigenvalue Problem" href="the-guide/mathematics/linear-algebra/eigenvalue-problem.html" class="internal-link" target="_self" rel="noopener nofollow">eigenvalues</a> inside the unit circle, i.e. the system is stable. 
If this is the case, this condition is sufficient and necessary and the system is global asymptotically stable-
]]></description><link>the-guide/robotics,-dynamics-and-control/dynamics/ljapunov-stability-for-lti-systems.html</link><guid isPermaLink="false">The Guide/Robotics, Dynamics and Control/Dynamics/Ljapunov Stability for LTI Systems.md</guid><pubDate>Sun, 13 Apr 2025 17:39:23 GMT</pubDate></item><item><title><![CDATA[Ljapunov Stability of Dynamical Systems]]></title><description><![CDATA[ 
 <br>Assume a (generally nonlinear) <a data-tooltip-position="top" aria-label="Static and Dynamic Systems" data-href="Static and Dynamic Systems" href="the-guide/robotics,-dynamics-and-control/dynamics/static-and-dynamic-systems.html" class="internal-link" target="_self" rel="noopener nofollow">dynamic</a> <a data-tooltip-position="top" aria-label="Static and Dynamic Systems" data-href="Static and Dynamic Systems" href="the-guide/robotics,-dynamics-and-control/dynamics/static-and-dynamic-systems.html" class="internal-link" target="_self" rel="noopener nofollow">system</a> that has been transformed to first order via using the <a data-tooltip-position="top" aria-label="Mathematical Relations" data-href="Mathematical Relations" href="the-guide/mathematics/general-stuff/mathematical-relations.html" class="internal-link" target="_self" rel="noopener nofollow">functionals</a> Stability is defined for so-called equilibrium states , solutions of the above system that lead to either without requiring motor commands (, unforced) or by requiring motor commands (, forced).<br>Simplification
In many formulations, authors consider a shift of coordinates , such that the equilibrium in question is at the origin  (without loss of generality).
<br><br><br>Ljapunov Stability
An equilibrium state  is Ljapunov stable, if for every , there exists a , such that if then for every  Solutions starting close enough to the equilibrium will remain close enough to it forever. 
<br>Asymptotical Stability
An equilibrium state  is asymptotically stable, if it is Ljapunov stable and there exists a , such that if thenSolutions that start close enough stay close enough to the equilibrium and converge to it.<br>
Some authors differentiate between asymptotically stable in  and globally asymptotically stable if .
<br>Exponential Stability
An equilibrium state  is exponentially stable, if it is asymptotically stable and there exists , such that if thenSolutions that start close enough stay close enough to the equilibrium and converge to it faster than a particular rate.
<br><br><br>To prove stability, we generalize energy via so-called Ljapunov <a data-tooltip-position="top" aria-label="Mathematical Relations" data-href="Mathematical Relations" href="the-guide/mathematics/general-stuff/mathematical-relations.html" class="internal-link" target="_self" rel="noopener nofollow">functions</a>. Then, we consider a system asymptotically stable, if the total energy decreases over time. In the following, we consider a discretized systemwith equilibrium point .<br>Definition
With this equilibrium point , let  be a <a data-tooltip-position="top" aria-label="Set" data-href="Set" href="the-guide/mathematics/general-stuff/set.html" class="internal-link" target="_self" rel="noopener nofollow">closed and bounded set</a> containing that point. A <a data-tooltip-position="top" aria-label="Mathematical Relations" data-href="Mathematical Relations" href="the-guide/mathematics/general-stuff/mathematical-relations.html" class="internal-link" target="_self" rel="noopener nofollow">function</a> , continuous around  and finite on  andwith  <a data-tooltip-position="top" aria-label="Continuity" data-href="Continuity" href="the-guide/mathematics/analysis-and-calculus/continuity.html" class="internal-link" target="_self" rel="noopener nofollow">continuous</a> and locally positive definite is called a Ljapunov function.
<br><img alt="center" src="lib/media/pasted-image-20250412154551.png" style="width: 350px; max-width: 100%;"><br>Ljapunov Theorem - Local
If a system as in the introduction admits a Ljapunov function  on , then  is asymptotically stable in  (sufficient condition). 
<br>Ljapunov Theorem - Global
If a system as in the introduction admits a Ljapunov function  on , and additionallythen  is globally asymptotically stable (sufficient condition).
]]></description><link>the-guide/robotics,-dynamics-and-control/dynamics/ljapunov-stability-of-dynamical-systems.html</link><guid isPermaLink="false">The Guide/Robotics, Dynamics and Control/Dynamics/Ljapunov Stability of Dynamical Systems.md</guid><pubDate>Sun, 13 Apr 2025 19:11:27 GMT</pubDate><enclosure url="lib/media/pasted-image-20250412154551.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src=&quot;lib/media/pasted-image-20250412154551.png&quot;&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[LTI - Linear Time Invariant Systems]]></title><description><![CDATA[ 
 <br>As mentioned in <a data-href="- Notation - ODEs, Robotics, Dynamics and Control -" href="notation-world/-notation-odes,-robotics,-dynamics-and-control-.html" class="internal-link" target="_self" rel="noopener nofollow">- Notation - ODEs, Robotics, Dynamics and Control -</a>, certain subfields may use slightly different notation based on the application.<br>Intuition
LTI systems are a special case of a <a data-tooltip-position="top" aria-label="Static and Dynamic Systems" data-href="Static and Dynamic Systems" href="the-guide/robotics,-dynamics-and-control/dynamics/static-and-dynamic-systems.html" class="internal-link" target="_self" rel="noopener nofollow">dynamical system</a> that is extensively studied in control theory. They are based on the assumption that the systems behavior is constant in time (governed by physical law) and that the system is linear (we can understand the whole by analyzing its parts). 
<br>Mathematically, the functional is not explicitly dependent on the independent variable . Additionally, we assume the <a data-tooltip-position="top" aria-label="ODEs - Ordinary Differential Equations" data-href="ODEs - Ordinary Differential Equations" href="the-guide/mathematics/differential-equations/odes-ordinary-differential-equations.html" class="internal-link" target="_self" rel="noopener nofollow">reformulation to a first order system</a>. This yields the general formulation <br>In practice, controllers are implemented on a digital controller and therefore require a discretized versionUnder these assumptions, we can conduct a vast amount of analysis and derive an abundance of properties. For details , see<br>
<br><a data-href="Controllability and Stabilizability of LTI Systems" href="the-guide/robotics,-dynamics-and-control/dynamics/controllability-and-stabilizability-of-lti-systems.html" class="internal-link" target="_self" rel="noopener nofollow">Controllability and Stabilizability of LTI Systems</a>
<br><a data-href="Observability and Detectability of LTI Systems" href="the-guide/robotics,-dynamics-and-control/dynamics/observability-and-detectability-of-lti-systems.html" class="internal-link" target="_self" rel="noopener nofollow">Observability and Detectability of LTI Systems</a>
<br><a data-href="Ljapunov Stability for LTI Systems" href="the-guide/robotics,-dynamics-and-control/dynamics/ljapunov-stability-for-lti-systems.html" class="internal-link" target="_self" rel="noopener nofollow">Ljapunov Stability for LTI Systems</a>
<br>Non-Uniqueness of State Representation
The input-output behavior  is entirely defined by the initial condition  and the control input sequence . For LTI systems, the choice of a state representation is not unique. 
<br>Proof
Consider any <a data-tooltip-position="top" aria-label="Mathematical Relations" data-href="Mathematical Relations" href="the-guide/mathematics/general-stuff/mathematical-relations.html" class="internal-link" target="_self" rel="noopener nofollow">bijective</a> <a data-tooltip-position="top" aria-label="Matrices" data-href="Matrices" href="the-guide/mathematics/linear-algebra/matrices.html" class="internal-link" target="_self" rel="noopener nofollow">linear transformation</a> and insert it into our systemTherefore, we can simply redefine all the matrices with the state  while leaving input and output unchanged.
<br>Intuition
Since the transformation is bijective, we essentially just say that as long as the new state representation does not collapse dimensions / looses information, the basis in which we express the state does not matter.
]]></description><link>the-guide/robotics,-dynamics-and-control/dynamics/lti-linear-time-invariant-systems.html</link><guid isPermaLink="false">The Guide/Robotics, Dynamics and Control/Dynamics/LTI - Linear Time Invariant Systems.md</guid><pubDate>Sun, 13 Apr 2025 17:37:26 GMT</pubDate></item><item><title><![CDATA[Observability and Detectability of LTI Systems]]></title><description><![CDATA[ 
 <br>The following considers an <a data-tooltip-position="top" aria-label="LTI - Linear Time Invariant Systems" data-href="LTI - Linear Time Invariant Systems" href="the-guide/robotics,-dynamics-and-control/dynamics/lti-linear-time-invariant-systems.html" class="internal-link" target="_self" rel="noopener nofollow">LTI</a> system without control input of the form<br>Observability
A system is observable, if there exists a finite , such that for every initial  the measurements uniquely distinguish said .
<br>Proposition
For the above LTI system  with -dimensional state, the system is observable if the  <a data-tooltip-position="top" aria-label="Matrices" data-href="Matrices" href="the-guide/mathematics/linear-algebra/matrices.html" class="internal-link" target="_self" rel="noopener nofollow">matrix</a> has full rankThis condition is necessary and sufficient.
<br>Proof
The proof follows the exact arguments of the proof for <a data-tooltip-position="top" aria-label="Controllability and Stabilizability of LTI Systems" data-href="Controllability and Stabilizability of LTI Systems" href="the-guide/robotics,-dynamics-and-control/dynamics/controllability-and-stabilizability-of-lti-systems.html" class="internal-link" target="_self" rel="noopener nofollow">controllability of an LTI</a> <a data-tooltip-position="top" aria-label="Static and Dynamic Systems" data-href="Static and Dynamic Systems" href="the-guide/robotics,-dynamics-and-control/dynamics/static-and-dynamic-systems.html" class="internal-link" target="_self" rel="noopener nofollow">system</a> with the initial linear system
<br><br>Detectability
A system is detectable, if it if possible to reconstruct a sequence of state estimates from the measurements that, starting from an arbitrary initial , converges to the true state asymptotically.
<br>Theorem
A system is detectable if all of its unobservable modes are stable. Therefore, ifthe system  with the eigenvalues  outside the unit ball is observable.
<br>Intuition
Because stable modes will converge towards a known equilibrium and we can uniquely identify the state in the observable subspace.
<br>Lemma
Observability implies Detectability.
<br>Proof (Informal)
Since observability means that we can uniquely identify the state, the desired sequence for detectability is simply given by this state. 
]]></description><link>the-guide/robotics,-dynamics-and-control/dynamics/observability-and-detectability-of-lti-systems.html</link><guid isPermaLink="false">The Guide/Robotics, Dynamics and Control/Dynamics/Observability and Detectability of LTI Systems.md</guid><pubDate>Sun, 13 Apr 2025 17:38:57 GMT</pubDate></item><item><title><![CDATA[Static and Dynamic Systems]]></title><description><![CDATA[<a class="tag" href="?query=tag:Dynamics" style="background-color: rgb(4, 108, 116); color: white; font-weight: 700; border: none; border-radius: 1em; padding: 0.2em 0.5em;">#Dynamics</a> <a class="tag" href="?query=tag:Control" style="background-color: rgb(4, 108, 116); color: white; font-weight: 700; border: none; border-radius: 1em; padding: 0.2em 0.5em;">#Control</a> <a class="tag" href="?query=tag:Robotics" style="background-color: rgb(4, 108, 116); color: white; font-weight: 700; border: none; border-radius: 1em; padding: 0.2em 0.5em;">#Robotics</a> <a class="tag" href="?query=tag:Data-Assimilation" style="background-color: rgb(4, 108, 116); color: white; font-weight: 700; border: none; border-radius: 1em; padding: 0.2em 0.5em;">#Data-Assimilation</a> <a class="tag" href="?query=tag:Control" style="background-color: rgb(4, 108, 116); color: white; font-weight: 700; border: none; border-radius: 1em; padding: 0.2em 0.5em;">#Control</a> 
 <br>In a Nutshell
Basic mathematical model used in <a href=".?query=tag:Dynamics" class="tag" target="_blank" rel="noopener nofollow">#Dynamics</a> , <a href=".?query=tag:Control" class="tag" target="_blank" rel="noopener nofollow">#Control</a> and more specifically <a href=".?query=tag:Robotics" class="tag" target="_blank" rel="noopener nofollow">#Robotics</a> . Mathematically, is is simply a system of <a data-tooltip-position="top" aria-label="ODEs - Ordinary Differential Equations" data-href="ODEs - Ordinary Differential Equations" href="the-guide/mathematics/differential-equations/odes-ordinary-differential-equations.html" class="internal-link" target="_self" rel="noopener nofollow">differential equations</a>.
<br>For a very important special case, see <a data-href="LTI - Linear Time Invariant Systems" href="the-guide/robotics,-dynamics-and-control/dynamics/lti-linear-time-invariant-systems.html" class="internal-link" target="_self" rel="noopener nofollow">LTI - Linear Time Invariant Systems</a>.<br><br><br>Dynamic System
Mathematical description of a <a data-tooltip-position="top" aria-label="Static and Dynamic Systems" data-href="Static and Dynamic Systems" href="the-guide/robotics,-dynamics-and-control/dynamics/static-and-dynamic-systems.html" class="internal-link" target="_self" rel="noopener nofollow">system</a> whose values vary over time  based on an <a data-tooltip-position="top" aria-label="ODEs - Ordinary Differential Equations" data-href="ODEs - Ordinary Differential Equations" href="the-guide/mathematics/differential-equations/odes-ordinary-differential-equations.html" class="internal-link" target="_self" rel="noopener nofollow">ordinary differential equation</a> relating <a data-tooltip-position="top" aria-label="Control Overview" data-href="Control Overview" href="the-guide/robotics,-dynamics-and-control/control/control-overview.html" class="internal-link" target="_self" rel="noopener nofollow">control</a> inputs  and the system state via a functional
<br>Static System
Special case, in which the differential equation is of order , the relationship is purely algebraic and therefore memoryless
<br>From ODEs to physical Laws
In basically every application, the system is not dependent on the independent variable , as a basic property of a physical law is time invariance.
<br><br><br>Continuous Time Dynamical System
We consider a <a data-tooltip-position="top" aria-label="Mathematical Relations" data-href="Mathematical Relations" href="the-guide/mathematics/general-stuff/mathematical-relations.html" class="internal-link" target="_self" rel="noopener nofollow">function</a>  with an <a data-tooltip-position="top" aria-label="ODEs - Ordinary Differential Equations" data-href="ODEs - Ordinary Differential Equations" href="the-guide/mathematics/differential-equations/odes-ordinary-differential-equations.html" class="internal-link" target="_self" rel="noopener nofollow">ODE</a> If a solution exists for all  and , this solution is element of the space  for any given . We say that the ODE generates a continuous-time dynamical system.
<br> If the solution exists, there is a one-parameter semi-<a data-tooltip-position="top" aria-label="Group" data-href="Group" href="the-guide/mathematics/group-theory/group.html" class="internal-link" target="_self" rel="noopener nofollow">group</a> of operators  parameterized by time, with the properties that<br>
<br>For a given , the operator yields the solution at any time 
<br>The solution operator can be used recursively 
<br>For , the operator always yields the given initial condition To link this continuous ODE setting to the discrete time setting encountered in e.g. <a data-href="Data Assimilation" href="the-guide/computational-statistics/data-assimilation/data-assimilation.html" class="internal-link" target="_self" rel="noopener nofollow">Data Assimilation</a>, we can use the iterated map defined by  with  being the time-step.
<br>(Gaussian) Stochastic Dynamical System
We consider the stochastic dynamical system by adding a Gaussian noise term via an <a data-tooltip-position="top" aria-label="Statistical Independence" data-href="Statistical Independence" href="the-guide/mathematics/statistics/statistical-independence.html" class="internal-link" target="_self" rel="noopener nofollow">i.i.d.</a> sequence  <a data-tooltip-position="top" aria-label="Probability Distribution" data-href="Probability Distribution" href="the-guide/mathematics/probability-theory/probability-distribution.html" class="internal-link" target="_self" rel="noopener nofollow">distributed</a> according to a <a data-tooltip-position="top" aria-label="Measure" data-href="Measure" href="the-guide/mathematics/measure-theory/measure.html" class="internal-link" target="_self" rel="noopener nofollow">probability measure</a> on  with density . The initial condition  is independent of , but possibly random.
<br>This is the setting used in the <a href=".?query=tag:Data-Assimilation" class="tag" target="_blank" rel="noopener nofollow">#Data-Assimilation</a> lecture. For a more general concept, see <a data-href="Stochastic Differential Equation" href="the-guide/mathematics/probability-theory/stochastic-differential-equation.html" class="internal-link" target="_self" rel="noopener nofollow">Stochastic Differential Equation</a> and <a data-href="Stochastic Process" href="the-guide/mathematics/probability-theory/stochastic-process.html" class="internal-link" target="_self" rel="noopener nofollow">Stochastic Process</a>.<br><br><br>A formulation that often serves as a starting point in <a href=".?query=tag:Control" class="tag" target="_blank" rel="noopener nofollow">#Control</a> is based on the <a data-tooltip-position="top" aria-label="ODEs - Ordinary Differential Equations" data-href="ODEs - Ordinary Differential Equations" href="the-guide/mathematics/differential-equations/odes-ordinary-differential-equations.html" class="internal-link" target="_self" rel="noopener nofollow">reformulation to a first order system</a> in form of a continuous or discrete time-invariant state space model.<br><br><br> As mentioned in <a data-href="- Notation - ODEs, Robotics, Dynamics and Control -" href="notation-world/-notation-odes,-robotics,-dynamics-and-control-.html" class="internal-link" target="_self" rel="noopener nofollow">- Notation - ODEs, Robotics, Dynamics and Control -</a>, certain subfields may use slightly different notation based on the application. Robotics usually analysis a system in <a data-tooltip-position="top" aria-label="Generalized Coordinates, Configuration or Joint Space" data-href="Generalized Coordinates, Configuration or Joint Space" href="the-guide/robotics,-dynamics-and-control/generalized-coordinates,-configuration-or-joint-space.html" class="internal-link" target="_self" rel="noopener nofollow">configuration</a> <a data-tooltip-position="top" aria-label="Space" data-href="Space" href="the-guide/mathematics/general-stuff/space.html" class="internal-link" target="_self" rel="noopener nofollow">space</a>.<br>
<img alt="center" src="lib/media/pasted-image-20230216190121.png" style="width: 450px; max-width: 100%;"><br>Inverse Dynamics Model
The general form of the system dynamics is relating control inputs to system states via the <a data-tooltip-position="top" aria-label="ODEs - Ordinary Differential Equations" data-href="ODEs - Ordinary Differential Equations" href="the-guide/mathematics/differential-equations/odes-ordinary-differential-equations.html" class="internal-link" target="_self" rel="noopener nofollow">nonlinear ODE</a>where the external joint torques are a result of an external <a data-tooltip-position="top" aria-label="Robot Grasping" data-href="Robot Grasping" href="the-guide/robotics,-dynamics-and-control/robot-grasping.html" class="internal-link" target="_self" rel="noopener nofollow">wrench</a>
<br>Intuition

<br>Mass matrix ...
<br>Coriolis term describes redistribution of energy based on movement.

<br>Wrench
A wrench is a -dimensional vector of cartesian forces and torques applied at contact point  with index  
<br>Forward Dynamics Model
Determine a <a data-tooltip-position="top" aria-label="Static and Dynamic Systems" data-href="Static and Dynamic Systems" href="the-guide/robotics,-dynamics-and-control/dynamics/static-and-dynamic-systems.html" class="internal-link" target="_self" rel="noopener nofollow">systems</a> trajectory by solving the ODE with appropriate initial state. Numerical integration yields the full system state.
<br>]]></description><link>the-guide/robotics,-dynamics-and-control/dynamics/static-and-dynamic-systems.html</link><guid isPermaLink="false">The Guide/Robotics, Dynamics and Control/Dynamics/Static and Dynamic Systems.md</guid><pubDate>Sat, 12 Apr 2025 10:31:12 GMT</pubDate><enclosure url="lib/media/pasted-image-20230216190121.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src=&quot;lib/media/pasted-image-20230216190121.png&quot;&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[Dynamixel Motors and SDK]]></title><description><![CDATA[ 
 <br><br><br>In a Nutshell
Set of programming tools (SDK = Software Development Kit) designed to interface with Dynamixel motors, which are intelligent actuators widely used in robotics, mechatronics, and automation projects.
<br>pip install dynamixel-sdk
<br>
<br>Initialization: The SDK initializes the communication with a Dynamixel motor through a compatible interface (such as USB2Dynamixel or USB2TTL). This may involve setting the communication baud rate and protocol type.<br>

<br>Motor Commands: You can send commands to the motor, such as setting desired positions, velocities, or torque limits. Depending on the motor model, there may be advanced functions like controlling the motor in a "torque mode" or "velocity mode."<br>

<br>Reading Feedback: Dynamixel motors provide feedback about their state (e.g., position, load, temperature), and the SDK allows you to read this feedback.<br>

<br>Error Handling: The SDK provides error codes and mechanisms for handling errors, such as communication timeouts, invalid command responses, or mechanical issues with the motors.
<br><a rel="noopener nofollow" class="external-link" href="https://emanual.robotis.com/docs/en/software/dynamixel/dynamixel_sdk/library_setup/python_linux/#python-linux" target="_blank">https://emanual.robotis.com/docs/en/software/dynamixel/dynamixel_sdk/library_setup/python_linux/#python-linux</a><br><br><br><a rel="noopener nofollow" class="external-link" href="https://emanual.robotis.com/docs/en/dxl/x/xm540-w270/#control-table-data-address" target="_blank">https://emanual.robotis.com/docs/en/dxl/x/xm540-w270/#control-table-data-address</a><br><br><br><br><a rel="noopener nofollow" class="external-link" href="https://emanual.robotis.com/docs/en/dxl/x/xm430-w350/" target="_blank">https://emanual.robotis.com/docs/en/dxl/x/xm430-w350/</a><br>From least to most significant bit is right to left.<br>[7 6 5 4 3 2 1 0] # [0 0 0 0 X X X X]
<br>E.g. a drive mode 4 is [0 0 0 0 0 1 0 0].  ]]></description><link>the-guide/robotics,-dynamics-and-control/hardware/dynamixel-motors-and-sdk.html</link><guid isPermaLink="false">The Guide/Robotics, Dynamics and Control/Hardware/Dynamixel Motors and SDK.md</guid><pubDate>Tue, 15 Apr 2025 12:53:25 GMT</pubDate></item><item><title><![CDATA[ViperX-300-6DOF]]></title><description><![CDATA[ 
 <br>
<br>Uses <a data-href="Dynamixel Motors and SDK" href="the-guide/robotics,-dynamics-and-control/hardware/dynamixel-motors-and-sdk.html" class="internal-link" target="_self" rel="noopener nofollow">Dynamixel Motors and SDK</a>
<br><br><br><br>XM540-W270-T <a rel="noopener nofollow" class="external-link" href="https://emanual.robotis.com/docs/en/dxl/x/xm540-w270/#drive-mode" target="_blank">https://emanual.robotis.com/docs/en/dxl/x/xm540-w270/#drive-mode</a><br>
<br>stall torque 10.60 Nm
<br>stall current 4.4 A
<br>T/A 2.4
<br>XM430-W350-T <a rel="noopener nofollow" class="external-link" href="https://emanual.robotis.com/docs/en/dxl/x/xm430-w350/" target="_blank">https://emanual.robotis.com/docs/en/dxl/x/xm430-w350/</a><br>
<br>stall torque 4.1 Nm
<br>stall current 2.3 A
<br>T/A 2.3
<br>Current-Torque relationship<br><br><br>
<br>set up a registry on 10.206.165.77
<br>roslaunch interbotix_xsarm_control xsarm_control.launch robot_model:=vx300s
<br>
<br>run bridge on laptop
]]></description><link>the-guide/robotics,-dynamics-and-control/hardware/viperx-300-6dof.html</link><guid isPermaLink="false">The Guide/Robotics, Dynamics and Control/Hardware/ViperX-300-6DOF.md</guid><pubDate>Wed, 09 Apr 2025 10:06:38 GMT</pubDate></item><item><title><![CDATA[Analytical vs. Geometric Jacobian in Robotics]]></title><description><![CDATA[ 
 <br>
<br>Basically the geometric Jacobian can have different <a data-tooltip-position="top" aria-label="A Hitchhiker's Guide to Rotation Representations" data-href="A Hitchhiker's Guide to Rotation Representations" href="the-guide/robotics,-dynamics-and-control/a-hitchhiker's-guide-to-rotation-representations.html" class="internal-link" target="_self" rel="noopener nofollow">rotation representation</a>
<br>E.g. in robotics, a D parametrization is often used to describe position and orientation of an end-effector The analytical jacobian results from the direct differential of the <a data-tooltip-position="top" aria-label="Kinematics" data-href="Kinematics" href="the-guide/robotics,-dynamics-and-control/kinematics/kinematics.html" class="internal-link" target="_self" rel="noopener nofollow">forward kinematics</a>and thereby describes how changes in the joint space lead to changes in this D state. <br>In contrast, requiring a mapping from generalized velocities to end-effector linear and angular velocities  around , we need to use the <a data-tooltip-position="top" aria-label="Kinematics" data-href="Kinematics" href="the-guide/robotics,-dynamics-and-control/kinematics/kinematics.html" class="internal-link" target="_self" rel="noopener nofollow">robots kinematic</a> to express it in the inertia frame (compare <a data-tooltip-position="top" aria-label="Denavit-Hartenberg Convention" data-href="Denavit-Hartenberg Convention" href="the-guide/robotics,-dynamics-and-control/kinematics/denavit-hartenberg-convention.html" class="internal-link" target="_self" rel="noopener nofollow">DHC</a>). This leads to the geometric Jacobian In order to map between both jacobians, we can use a transformation <a data-tooltip-position="top" aria-label="Matrices" data-href="Matrices" href="the-guide/mathematics/linear-algebra/matrices.html" class="internal-link" target="_self" rel="noopener nofollow">matrix</a> where  refers the end-effector orientation (e.g. given by euler angles) to a frame (unsure).<img alt="center" src="lib/media/pasted-image-20240203111633.png" style="width: 400px; max-width: 100%;"><br>Take-Home Message

<br>Linear parts of both jacobians coincide
<br>Angular part depends on how orientation of end-effector is encoded in 
<br>Angles in analytical jacobian can have different meanings based on chosen angles, while geometric always refers to angles around  in the respective frame.

]]></description><link>the-guide/robotics,-dynamics-and-control/kinematics/analytical-vs.-geometric-jacobian-in-robotics.html</link><guid isPermaLink="false">The Guide/Robotics, Dynamics and Control/Kinematics/Analytical vs. Geometric Jacobian in Robotics.md</guid><pubDate>Sat, 12 Apr 2025 10:34:33 GMT</pubDate><enclosure url="lib/media/pasted-image-20240203111633.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src=&quot;lib/media/pasted-image-20240203111633.png&quot;&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[Denavit-Hartenberg Convention]]></title><description><![CDATA[ 
 <br>Info
Systematic way to determine <a data-tooltip-position="top" aria-label="Kinematics" data-href="Kinematics" href="the-guide/robotics,-dynamics-and-control/kinematics/kinematics.html" class="internal-link" target="_self" rel="noopener nofollow">homogeneous transformation</a> between a robot's frames using 4 parameters.
<br>
<br>Transform position given in frame  to position in frame 

<br>


<br>Parameters

<br>  offset along previous 
<br>  angle about prvious , between old an new 
<br>  radius around previous 
<br>  angle between old an new <img alt="center" src="lib/media/pasted-image-20230225143959.png" style="width: 400px; max-width: 100%;">


<br>Layout of Reference Frames

<br>-axis in direction of joint axis
<br>-axis parallel to common normal 

<br>if parallel  axis choose from  to 


<br>-axis follows by choosing right-handed coordinate system


<br>Example<img alt="center" src="lib/media/pasted-image-20230225150257.png">

<br>


]]></description><link>the-guide/robotics,-dynamics-and-control/kinematics/denavit-hartenberg-convention.html</link><guid isPermaLink="false">The Guide/Robotics, Dynamics and Control/Kinematics/Denavit-Hartenberg Convention.md</guid><pubDate>Wed, 04 Dec 2024 10:36:15 GMT</pubDate><enclosure url="lib/media/pasted-image-20230225143959.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src=&quot;lib/media/pasted-image-20230225143959.png&quot;&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[End-Effector Pose Representations]]></title><description><![CDATA[ 
 <br><br>...<br><br>
<br><a data-href="Unit Quaternions and SU(2)" href="the-guide/mathematics/lie-theory/unit-quaternions-and-su(2).html" class="internal-link" target="_self" rel="noopener nofollow">Unit Quaternions and SU(2)</a>
<br>Euler Angles 
<br>Axis angle / <a data-href="The Lie Group SO(n) and its Lie Algebra" href="the-guide/mathematics/lie-theory/special-orthogonal-group-so(3)/the-lie-group-so(n)-and-its-lie-algebra.html" class="internal-link" target="_self" rel="noopener nofollow">The Lie Group SO(n) and its Lie Algebra</a>
<br><a data-href="A Hitchhiker's Guide to Rotation Representations" href="the-guide/robotics,-dynamics-and-control/a-hitchhiker's-guide-to-rotation-representations.html" class="internal-link" target="_self" rel="noopener nofollow">A Hitchhiker's Guide to Rotation Representations</a>
<br>The pose of a robots end-effector can be represented as elements of <br><br><br>Both are <a data-tooltip-position="top" aria-label="Group" data-href="Group" href="the-guide/mathematics/group-theory/group.html" class="internal-link" target="_self" rel="noopener nofollow">groups</a> and <a data-tooltip-position="top" aria-label="Mannigfaltigkeiten" data-href="Mannigfaltigkeiten" href="the-guide/mathematics/differential-geometry/riemannian-geometry/mannigfaltigkeiten-und-differenzierbare-abbildungen/mannigfaltigkeiten.html" class="internal-link" target="_self" rel="noopener nofollow">manifolds</a> and thereby form <a data-tooltip-position="top" aria-label="Lie Group" data-href="Lie Group" href="the-guide/mathematics/lie-theory/lie-group.html" class="internal-link" target="_self" rel="noopener nofollow">Lie groups</a>, but they differ in group structure and <a data-tooltip-position="top" aria-label="Topology and Topological Space" data-href="Topology and Topological Space" href="the-guide/mathematics/topology/topology-and-topological-space.html" class="internal-link" target="_self" rel="noopener nofollow">topology</a>. For , the composition of two poses  and  is realized via while  is realized via homogeneous coordinates, which yields ]]></description><link>the-guide/robotics,-dynamics-and-control/kinematics/end-effector-pose-representations.html</link><guid isPermaLink="false">The Guide/Robotics, Dynamics and Control/Kinematics/End-Effector Pose Representations.md</guid><pubDate>Wed, 23 Apr 2025 08:34:38 GMT</pubDate></item><item><title><![CDATA[Inverse Kinematics Problem]]></title><description><![CDATA[ 
 <br>In a Nutshell
Given <a data-tooltip-position="top" aria-label="End-Effector Pose Representations" data-href="End-Effector Pose Representations" href="the-guide/robotics,-dynamics-and-control/kinematics/end-effector-pose-representations.html" class="internal-link" target="_self" rel="noopener nofollow">end-effector</a> goals in task space, we need to map these to desired joint positions in order to actually move the robot. However, this mapping is usually not unique and affected by <a data-tooltip-position="top" aria-label="Robot Singularities" data-href="Robot Singularities" href="the-guide/robotics,-dynamics-and-control/kinematics/robot-singularities.html" class="internal-link" target="_self" rel="noopener nofollow">singularities</a>.
<br>We assume <a data-tooltip-position="top" aria-label="Static and Dynamic Systems" data-href="Static and Dynamic Systems" href="the-guide/robotics,-dynamics-and-control/dynamics/static-and-dynamic-systems.html" class="internal-link" target="_self" rel="noopener nofollow">inverse dynamics</a>  and <a data-tooltip-position="top" aria-label="Kinematics" data-href="Kinematics" href="the-guide/robotics,-dynamics-and-control/kinematics/kinematics.html" class="internal-link" target="_self" rel="noopener nofollow">kinematics</a> , .<br><br><br>Given current and desired position in <a data-tooltip-position="top" aria-label="Static and Dynamic Systems" data-href="Static and Dynamic Systems" href="the-guide/robotics,-dynamics-and-control/dynamics/static-and-dynamic-systems.html" class="internal-link" target="_self" rel="noopener nofollow">task space</a>, compute joint velocities that minimize errorThe simplest approach is to solve this problem numerically via <a data-tooltip-position="top" aria-label="Gradient Descent" data-href="Gradient Descent" href="the-guide/mathematics/optimization/convex-optimization-lecture/algorithms/gradient-descent.html" class="internal-link" target="_self" rel="noopener nofollow">gradient descent</a> towards the desired end-effector pose, leading to the ...<br>Jacobian Transpose Method
In order to minimize , use <a data-tooltip-position="top" aria-label="Gradient Descent" data-href="Gradient Descent" href="the-guide/mathematics/optimization/convex-optimization-lecture/algorithms/gradient-descent.html" class="internal-link" target="_self" rel="noopener nofollow">gradient descent</a> by following  the negative gradient with a certain step size  :with
<br><img alt="center" src="lib/media/pasted-image-20240312080638.png" style="width: 200px; max-width: 100%;"><br>
<br>Projects the euclidean error on the <a data-tooltip-position="top" aria-label="Generalized Coordinates, Configuration or Joint Space" data-href="Generalized Coordinates, Configuration or Joint Space" href="the-guide/robotics,-dynamics-and-control/generalized-coordinates,-configuration-or-joint-space.html" class="internal-link" target="_self" rel="noopener nofollow">configuration space</a> dimensions that reduce it the most
<br>Steps in cartesian <a data-href="Space" href="the-guide/mathematics/general-stuff/space.html" class="internal-link" target="_self" rel="noopener nofollow">Space</a>, simple and robust but may require multiple iterations
<br>When we know that we are already close to the desired solution, e.g. if the error is very low, we can use another second-order variant that is based on the shortest path in <a data-tooltip-position="top" aria-label="Generalized Coordinates, Configuration or Joint Space" data-href="Generalized Coordinates, Configuration or Joint Space" href="the-guide/robotics,-dynamics-and-control/generalized-coordinates,-configuration-or-joint-space.html" class="internal-link" target="_self" rel="noopener nofollow">configuration space</a>:<br>Jacobian Pseudo-Inverse
If we are not far from solution, we can formulize the alternative <a data-href="Optimization Problem" href="the-guide/mathematics/optimization/convex-optimization-lecture/optimization-problem.html" class="internal-link" target="_self" rel="noopener nofollow">Optimization Problem</a>for which the solution is given by the (right) <a data-tooltip-position="top" aria-label="Moore-Penrose Pseudoinverse" data-href="Moore-Penrose Pseudoinverse" href="the-guide/mathematics/linear-algebra/moore-penrose-pseudoinverse.html" class="internal-link" target="_self" rel="noopener nofollow">pseudo-inverse</a> 
<br><img alt="center" src="lib/media/pasted-image-20240315091212.png" style="width: 250px; max-width: 100%;"><br>Problems

<br>Cannot be inverted for <a data-tooltip-position="top" aria-label="Robot Singularities" data-href="Robot Singularities" href="the-guide/robotics,-dynamics-and-control/kinematics/robot-singularities.html" class="internal-link" target="_self" rel="noopener nofollow">singularities</a> !
<br><a data-tooltip-position="top" aria-label="Matrices" data-href="Matrices" href="the-guide/mathematics/linear-algebra/matrices.html" class="internal-link" target="_self" rel="noopener nofollow">Inversion</a> can be costly and unpredictable

<br>If the chance for encountering <a data-tooltip-position="top" aria-label="Robot Singularities" data-href="Robot Singularities" href="the-guide/robotics,-dynamics-and-control/kinematics/robot-singularities.html" class="internal-link" target="_self" rel="noopener nofollow">singularities</a> is high, the following version offers better performance and <a data-tooltip-position="top" aria-label="Ljapunov Stability of Dynamical Systems" data-href="Ljapunov Stability of Dynamical Systems" href="the-guide/robotics,-dynamics-and-control/dynamics/ljapunov-stability-of-dynamical-systems.html" class="internal-link" target="_self" rel="noopener nofollow">stability</a>:<br>Damped Pseudo-Inverse
Find a tradeoff between minimizing the error and keeping the joint movement smallwith a regularization constant . The solution is given by
]]></description><link>the-guide/robotics,-dynamics-and-control/kinematics/inverse-kinematics-problem.html</link><guid isPermaLink="false">The Guide/Robotics, Dynamics and Control/Kinematics/Inverse Kinematics Problem.md</guid><pubDate>Wed, 23 Apr 2025 21:55:39 GMT</pubDate><enclosure url="lib/media/pasted-image-20240312080638.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src=&quot;lib/media/pasted-image-20240312080638.png&quot;&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[Kinematics]]></title><description><![CDATA[ 
 <br>Describe restrictions / constraints on motion that are not induced by forces. If forces play a role, the constraints are expressed via the <a data-tooltip-position="top" aria-label="Static and Dynamic Systems" data-href="Static and Dynamic Systems" href="the-guide/robotics,-dynamics-and-control/dynamics/static-and-dynamic-systems.html" class="internal-link" target="_self" rel="noopener nofollow">dynamics</a>.<br><br><br>A mapping from <a data-tooltip-position="top" aria-label="Static and Dynamic Systems" data-href="Static and Dynamic Systems" href="the-guide/robotics,-dynamics-and-control/dynamics/static-and-dynamic-systems.html" class="internal-link" target="_self" rel="noopener nofollow">joint space to task space</a>Combine translation and <a data-tooltip-position="top" aria-label="A Hitchhiker's Guide to Rotation Representations" data-href="A Hitchhiker's Guide to Rotation Representations" href="the-guide/robotics,-dynamics-and-control/a-hitchhiker's-guide-to-rotation-representations.html" class="internal-link" target="_self" rel="noopener nofollow">rotation|rotatios</a> via affine 3D <a data-tooltip-position="top" aria-label="Mathematical Relations" data-href="Mathematical Relations" href="the-guide/mathematics/general-stuff/mathematical-relations.html" class="internal-link" target="_self" rel="noopener nofollow">transformation</a> of the <a data-tooltip-position="top" aria-label="Group" data-href="Group" href="the-guide/mathematics/group-theory/group.html" class="internal-link" target="_self" rel="noopener nofollow">group</a>referred to as Homogeneous Transformations. Transforming a representation from a reference frame  to another frame  is done via <img alt="center" src="lib/media/pasted-image-20240628175106.png" style="width: 250px; max-width: 100%;"><br>
Multiple transformations are combined via <a data-tooltip-position="top" aria-label="Matrices" data-href="Matrices" href="the-guide/mathematics/linear-algebra/matrices.html" class="internal-link" target="_self" rel="noopener nofollow">matrix-matrix</a> multiplication These can be computed in a systematic way e.g.via the <a data-href="Denavit-Hartenberg Convention" href="the-guide/robotics,-dynamics-and-control/kinematics/denavit-hartenberg-convention.html" class="internal-link" target="_self" rel="noopener nofollow">Denavit-Hartenberg Convention</a>.<br>
<img alt="center" src="lib/media/pasted-image-20231103182003.png" style="width: 300px; max-width: 100%;"><br>Quote
"Where is my end-effector and what is its orientation ?"
<br><img alt="center" src="lib/media/pasted-image-20240304204436.png" style="width: 200px; max-width: 100%;">For the robot above, the analytical forward kinematics can be computed using Newtonian mechanics, yielding<br><br>The kinematics above yield highly non-linear relationships that can only besolved analytically for very simple robots. Instead considering the velocities yields linear mappings of joint velocities instead of joint positions into end-effector linear () and angular () velocities described by <a data-tooltip-position="top" aria-label="Derivative, Gradient, Jacobian and Hessian > Analytical vs. Geometric Jacobian in Robotics" data-href="Derivative, Gradient, Jacobian and Hessian#Analytical vs. Geometric Jacobian in Robotics" href="the-guide/mathematics/analysis-and-calculus/derivative,-gradient,-jacobian-and-hessian.html#Analytical_vs._Geometric_Jacobian_in_Robotics" class="internal-link" target="_self" rel="noopener nofollow">Jacobian</a>Intuitively, we are computing the tangent vector to the path of the end-effector for each joint and add them up.<img alt="center" src="lib/media/pasted-image-20240304211636.png" style="width: 300px; max-width: 100%;"> <br><br><br>A <a data-tooltip-position="top" aria-label="Mathematical Relations" data-href="Mathematical Relations" href="the-guide/mathematics/general-stuff/mathematical-relations.html" class="internal-link" target="_self" rel="noopener nofollow">mapping</a> from <a data-tooltip-position="top" aria-label="Static and Dynamic Systems" data-href="Static and Dynamic Systems" href="the-guide/robotics,-dynamics-and-control/dynamics/static-and-dynamic-systems.html" class="internal-link" target="_self" rel="noopener nofollow">task space to joint configuration</a>For a detailed analysis, see <a data-href="Inverse Kinematics Problem" href="the-guide/robotics,-dynamics-and-control/kinematics/inverse-kinematics-problem.html" class="internal-link" target="_self" rel="noopener nofollow">Inverse Kinematics Problem</a>.<br>Warning
Inverse kinematics is in general an ill-posed problem, which can lead to infinitely many solutions without any stability guarantees. Additionally, the existence of a solution is not guaranteed. This is can also hold for <a data-tooltip-position="top" aria-label="Redundancy in Robotics" data-href="Redundancy in Robotics" href="the-guide/robotics,-dynamics-and-control/redundancy-in-robotics.html" class="internal-link" target="_self" rel="noopener nofollow">non-redundant robots</a>.
<br>Quote
"How to move my joints in order to get to a given position in task space?"
<br>In general, solving inverse kinematics requires numerical solvers. Only for low DoF problems, we an use analytical methods.<img alt="center" src="lib/media/pasted-image-20240304204322.png" style="width: 200px; max-width: 100%;">For the simple robot shown above, we compute with ,  and .  For more complex examples, we can use the objective  and solve it e.g. via the <a data-href="Newton-Raphson Method" href="the-guide/mathematics/optimization/convex-optimization-lecture/algorithms/newton-raphson-method.html" class="internal-link" target="_self" rel="noopener nofollow">Newton-Raphson Method</a>, yielding the update  with inverse <a data-tooltip-position="top" aria-label="Derivative, Gradient, Jacobian and Hessian" data-href="Derivative, Gradient, Jacobian and Hessian" href="the-guide/mathematics/analysis-and-calculus/derivative,-gradient,-jacobian-and-hessian.html" class="internal-link" target="_self" rel="noopener nofollow">Jacobian</a> .]]></description><link>the-guide/robotics,-dynamics-and-control/kinematics/kinematics.html</link><guid isPermaLink="false">The Guide/Robotics, Dynamics and Control/Kinematics/Kinematics.md</guid><pubDate>Sat, 12 Apr 2025 10:33:04 GMT</pubDate><enclosure url="lib/media/pasted-image-20240628175106.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src=&quot;lib/media/pasted-image-20240628175106.png&quot;&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[PoE - Product of Exponentials Method]]></title><description><![CDATA[<a class="tag" href="?query=tag:Robotics" style="background-color: rgb(4, 108, 116); color: white; font-weight: 700; border: none; border-radius: 1em; padding: 0.2em 0.5em;">#Robotics</a> 
 <br>In a Nutshell
Convention in <a href=".?query=tag:Robotics" class="tag" target="_blank" rel="noopener nofollow">#Robotics</a> for mapping links of a <a data-tooltip-position="top" aria-label="Kinematics" data-href="Kinematics" href="the-guide/robotics,-dynamics-and-control/kinematics/kinematics.html" class="internal-link" target="_self" rel="noopener nofollow">kinematic chain</a>. Alternative to <a data-href="Denavit-Hartenberg Convention" href="the-guide/robotics,-dynamics-and-control/kinematics/denavit-hartenberg-convention.html" class="internal-link" target="_self" rel="noopener nofollow">Denavit-Hartenberg Convention</a> with easier geometric interpretation based on the <a data-href="Lie Group" href="the-guide/mathematics/lie-theory/lie-group.html" class="internal-link" target="_self" rel="noopener nofollow">Lie Group</a> <a data-tooltip-position="top" aria-label="The Lie Group SE(n) and its Lie Algebra" data-href="The Lie Group SE(n) and its Lie Algebra" href="the-guide/mathematics/lie-theory/special-euclidean-group-se(3)/the-lie-group-se(n)-and-its-lie-algebra.html" class="internal-link" target="_self" rel="noopener nofollow">SE(3)</a>.<br>
Mostly used for more complex robots.
<br><br><br>The key idea behind the PoE method is to express the motion of a rigid body as a product of exponentials of twist (or screw) motions. A twist combines both linear and angular components, mathematically it is an element of the <a data-href="Lie Group" href="the-guide/mathematics/lie-theory/lie-group.html" class="internal-link" target="_self" rel="noopener nofollow">Lie Group</a> of <a data-tooltip-position="top" aria-label="The Lie Group SE(n) and its Lie Algebra" data-href="The Lie Group SE(n) and its Lie Algebra" href="the-guide/mathematics/lie-theory/special-euclidean-group-se(3)/the-lie-group-se(n)-and-its-lie-algebra.html" class="internal-link" target="_self" rel="noopener nofollow">SE(3)</a>.<br><br>The first step is to select a "zero configuration" where all the joint angles are defined as being zero. For each joint of the kinematic chain, an origin point&nbsp; and an axis of action are selected for the zero configuration, using the coordinate frame of the base. <br><br>Based on this, we can define an element of the <a data-href="Lie Algebra" href="the-guide/mathematics/lie-theory/lie-algebra.html" class="internal-link" target="_self" rel="noopener nofollow">Lie Algebra</a> of <a data-tooltip-position="top" aria-label="The Lie Group SE(n) and its Lie Algebra" data-href="The Lie Group SE(n) and its Lie Algebra" href="the-guide/mathematics/lie-theory/special-euclidean-group-se(3)/the-lie-group-se(n)-and-its-lie-algebra.html" class="internal-link" target="_self" rel="noopener nofollow">SE(3)</a>.<br>
<br>For  a&nbsp;<a data-tooltip-position="top" aria-label="Robotic Joints" data-href="Robotic Joints" href="the-guide/robotics,-dynamics-and-control/robotic-joints.html" class="internal-link" target="_self" rel="noopener nofollow">prismatic joint</a> the axis of action&nbsp;&nbsp;is the vector along which the joint extends
<br>For a&nbsp;<a data-tooltip-position="top" aria-label="Robotic Joints" data-href="Robotic Joints" href="the-guide/robotics,-dynamics-and-control/robotic-joints.html" class="internal-link" target="_self" rel="noopener nofollow">prismatic joint</a> the axis of action&nbsp;&nbsp;the vector normal to the rotation
<br><br>Using the <a data-tooltip-position="top" aria-label="Exponentialabbildung" data-href="Exponentialabbildung" href="the-guide/mathematics/differential-geometry/riemannian-geometry/metriken-und-zusammenhänge/exponentialabbildung.html" class="internal-link" target="_self" rel="noopener nofollow">exponential map</a> in form of the Rodriguez formula<br>
we can compute the <a data-tooltip-position="top" aria-label="Matrices" data-href="Matrices" href="the-guide/mathematics/linear-algebra/matrices.html" class="internal-link" target="_self" rel="noopener nofollow">matrix</a> representation of each joints twist. For serial manipulators, e.g. the transformation  from the base to the <a data-tooltip-position="top" aria-label="End-Effector Pose Representations" data-href="End-Effector Pose Representations" href="the-guide/robotics,-dynamics-and-control/kinematics/end-effector-pose-representations.html" class="internal-link" target="_self" rel="noopener nofollow">end-effector</a>, we can now simply combine the matrices viaThis enables direct computation of the <a data-tooltip-position="top" aria-label="Kinematics" data-href="Kinematics" href="the-guide/robotics,-dynamics-and-control/kinematics/kinematics.html" class="internal-link" target="_self" rel="noopener nofollow">forward kinematics</a> (Position in end-effector frame, multiply product from the left to get position in e.g. world frame).<br><br><br><br><br><br>Consider a 2D planar robot with two revolute joints. The twists for each joint are as follows:<br>Joint 1 (rotation about the z-axis):<br>
The twist for Joint 1 isThe corresponding exponential map for Joint 1 is: <br>Joint 2 (rotation about the z-axis relative to Joint 1):<br>
The twist for Joint 2 is:The corresponding exponential map for Joint 2 is:The total transformation from the base frame to the end-effector frame is: This gives the complete position and orientation of the end-effector in the 2D plane.]]></description><link>the-guide/robotics,-dynamics-and-control/kinematics/poe-product-of-exponentials-method.html</link><guid isPermaLink="false">The Guide/Robotics, Dynamics and Control/Kinematics/PoE - Product of Exponentials Method.md</guid><pubDate>Mon, 03 Mar 2025 18:11:14 GMT</pubDate></item><item><title><![CDATA[Robot Singularities]]></title><description><![CDATA[ 
 <br>A robot singularity is a <a data-tooltip-position="top" aria-label="Generalized Coordinates, Configuration or Joint Space" data-href="Generalized Coordinates, Configuration or Joint Space" href="the-guide/robotics,-dynamics-and-control/generalized-coordinates,-configuration-or-joint-space.html" class="internal-link" target="_self" rel="noopener nofollow">configuration</a> in which the robot's end-effector loses degrees of freedom / the <a data-tooltip-position="top" aria-label="Derivative, Gradient, Jacobian and Hessian" data-href="Derivative, Gradient, Jacobian and Hessian" href="the-guide/mathematics/analysis-and-calculus/derivative,-gradient,-jacobian-and-hessian.html" class="internal-link" target="_self" rel="noopener nofollow">Jacobian</a> is <a data-tooltip-position="top" aria-label="Matrices" data-href="Matrices" href="the-guide/mathematics/linear-algebra/matrices.html" class="internal-link" target="_self" rel="noopener nofollow">rank</a>-deficient (<a data-tooltip-position="top" aria-label="Matrices" data-href="Matrices" href="the-guide/mathematics/linear-algebra/matrices.html" class="internal-link" target="_self" rel="noopener nofollow">determinant</a> ). They are of particular interest because in these configurations the mobility of the robot is reduced, the <a data-tooltip-position="top" aria-label="Kinematics" data-href="Kinematics" href="the-guide/robotics,-dynamics-and-control/kinematics/kinematics.html" class="internal-link" target="_self" rel="noopener nofollow">inverse kinematics</a> often yields infinitely many solutions and small changes in the task space may require large changes and sudden movements in the <a data-tooltip-position="top" aria-label="Generalized Coordinates, Configuration or Joint Space" data-href="Generalized Coordinates, Configuration or Joint Space" href="the-guide/robotics,-dynamics-and-control/generalized-coordinates,-configuration-or-joint-space.html" class="internal-link" target="_self" rel="noopener nofollow">configuration space</a>. There are three types of singularities:<br><br><br>In this situation, the end-effector of the robot is in locked position, but the joints  and  are demanded to rotate at infinite speeds in opposite directions, because their axis are coincident (middle).<img alt="center" src="lib/media/pasted-image-20240313111326.png"><br>
This behavior is usually avoided by setting the conditions no-flip (, left) of flip (, right).<br><br><br>In this situation, the intersection of the axes of joints  and  lie on the plane passing through joints  and . It looks like the robot has been overstretched (middle).<img alt="center" src="lib/media/pasted-image-20240313111739.png">This can be avoided using either the elbow-up condition (, left) or the elbow-down condition (, right).<br><br><br>In this situation, the intersection of the axes of joints  and  align with the axis of joint , leading to sudden rotations of  by joints  and  (middle).<img alt="center" src="lib/media/pasted-image-20240313112246.png"><br>]]></description><link>the-guide/robotics,-dynamics-and-control/kinematics/robot-singularities.html</link><guid isPermaLink="false">The Guide/Robotics, Dynamics and Control/Kinematics/Robot Singularities.md</guid><pubDate>Tue, 11 Mar 2025 15:43:32 GMT</pubDate><enclosure url="lib/media/pasted-image-20240313111326.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src=&quot;lib/media/pasted-image-20240313111326.png&quot;&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[Bayesian Filtering for Tracking]]></title><description><![CDATA[ 
 <br>In a Nutshell
Concept in Object Tracking. Given a dynamics model of <a data-tooltip-position="top" aria-label="Expectations" data-href="Expectations" href="the-guide/mathematics/statistics/expectations.html" class="internal-link" target="_self" rel="noopener nofollow">expected</a> motion and some observations, predict where an object will appear in next frame / time step. This can be useful to restrict the search space or to improve measurement accuracy by also including the physics-based estimate. A more detailed approach was taken in the <a data-href="Data Assimilation" href="the-guide/computational-statistics/data-assimilation/data-assimilation.html" class="internal-link" target="_self" rel="noopener nofollow">Data Assimilation</a> lecture.
<br>
<br>Alternative, more formal definition in <a data-href="The Filtering Problem" href="the-guide/computational-statistics/data-assimilation/filtering-algorithms/the-filtering-problem.html" class="internal-link" target="_self" rel="noopener nofollow">The Filtering Problem</a>
<br>Assumptions

<br>Continuous motion pattern, no sudden change in position
<br>Object of interest can be in a state  that can be measured, yielding the observation  at each time-step

<br><img alt="center" src="lib/media/pasted-image-20231125153809.png" style="width: 400px; max-width: 100%;"><br><br><br>Given all observations  so far, recover the most likely next state  and knowledge about the underlying <a data-tooltip-position="top" aria-label="Static and Dynamic Systems" data-href="Static and Dynamic Systems" href="the-guide/robotics,-dynamics-and-control/dynamics/static-and-dynamic-systems.html" class="internal-link" target="_self" rel="noopener nofollow">dynamics</a> of the system.<br>
<br>Prediction

<br>Compute the <a data-tooltip-position="top" aria-label="Probability Distribution" data-href="Probability Distribution" href="the-guide/mathematics/probability-theory/probability-distribution.html" class="internal-link" target="_self" rel="noopener nofollow">probability</a>of the systems next state given the current knowledge / measurements.


<br>Correction<br>
- After receiving the measurement , compute updated <a data-tooltip-position="top" aria-label="Estimator" data-href="Estimator" href="the-guide/mathematics/statistics/estimator.html" class="internal-link" target="_self" rel="noopener nofollow">estimate</a> of the stateTracking can be framed as the process of propagating the <a data-tooltip-position="top" aria-label="Bayes Theorem" data-href="Bayes Theorem" href="the-guide/mathematics/statistics/bayes-theorem.html" class="internal-link" target="_self" rel="noopener nofollow">posterior</a> <a data-tooltip-position="top" aria-label="Probability Distribution" data-href="Probability Distribution" href="the-guide/mathematics/probability-theory/probability-distribution.html" class="internal-link" target="_self" rel="noopener nofollow">distribution</a> of the state given the measurements
<br>Simplifying Assumptions

<br><a data-tooltip-position="top" aria-label="Markov Chain" data-href="Markov Chain" href="the-guide/computational-statistics/data-assimilation/markov-chain.html" class="internal-link" target="_self" rel="noopener nofollow">Markovian property</a>, only immediate past matters
<br>Measurements only depend on current state 

<br>
<br>Starting with an initial <a data-tooltip-position="top" aria-label="Bayes Theorem" data-href="Bayes Theorem" href="the-guide/mathematics/statistics/bayes-theorem.html" class="internal-link" target="_self" rel="noopener nofollow">prior</a>  of the initial state  at , we receive a first measurement  
<br>After receiving first measurement  compute to correct our prior based on the observation. This yields a new <a data-tooltip-position="top" aria-label="Probability Distribution" data-href="Probability Distribution" href="the-guide/mathematics/probability-theory/probability-distribution.html" class="internal-link" target="_self" rel="noopener nofollow">distribution</a>.
<br>...
<br>Given corrected estimate (blue) for frame , compute a prediction for frame  via using the given  (how probable is new state given last state) and the  (how probable was last state given the evidence).
<br>Observe new evidence 
<br>Correct current frame  after receiving measurement using <a data-href="Bayes Theorem" href="the-guide/mathematics/statistics/bayes-theorem.html" class="internal-link" target="_self" rel="noopener nofollow">Bayes Theorem</a> via using the given  (how probable was observation given prediction) and the  we computed before
<br>...
<br><br><br>In general, we can assume a linear transformation  for our state and add <a data-tooltip-position="top" aria-label="Gaussian Distribution" data-href="Gaussian Distribution" href="the-guide/mathematics/probability-theory/gaussian-distribution.html" class="internal-link" target="_self" rel="noopener nofollow">Gaussian</a> noise, e.g. yielding the distribution Similarily, the measurement can be modeled as a linear transformation of the state with added <a data-tooltip-position="top" aria-label="Gaussian Distribution" data-href="Gaussian Distribution" href="the-guide/mathematics/probability-theory/gaussian-distribution.html" class="internal-link" target="_self" rel="noopener nofollow">Gaussian noise</a> By tweaking the <a data-tooltip-position="top" aria-label="Covariance and Variance" data-href="Covariance and Variance" href="the-guide/mathematics/statistics/covariance-and-variance.html" class="internal-link" target="_self" rel="noopener nofollow">variances</a> and transformations, we can express our trust in e.g. the measurement. What is actually encoded in the state depends on the underlying physical system, some examples are ...<br>
<br>Position  and (constant) velocity , which can be propagated by linear dynamics  and , yielding <a data-tooltip-position="top" aria-label="Matrices" data-href="Matrices" href="the-guide/mathematics/linear-algebra/matrices.html" class="internal-link" target="_self" rel="noopener nofollow">matrix</a> notation 

<br>Since the measurement is solely based on the position in most cases, we could define .


<br>Position , velocity  and (constant) acceleration , for which we extend the above to

<br>Since the measurement is solely based on the position in most cases, we could define .


]]></description><link>the-guide/robotics,-dynamics-and-control/perception/bayesian-filtering-for-tracking.html</link><guid isPermaLink="false">The Guide/Robotics, Dynamics and Control/Perception/Bayesian Filtering for Tracking.md</guid><pubDate>Mon, 03 Mar 2025 15:30:27 GMT</pubDate><enclosure url="lib/media/pasted-image-20231125153809.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src=&quot;lib/media/pasted-image-20231125153809.png&quot;&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[Geometric Perception]]></title><description><![CDATA[ 
 <br>
<br>3 Cases

<br>2D perception from image data
<br>3D perception from 3d point clouds
<br>6D perception (position + orientation)


<br>Sensors

<br>Stereeo Cameras -  RGB + Depth

<br>Ca be very noisy, expecially for uniform, flat surfaces
<br>patterns can help




<br>3D Representation

<br>Pointclouds 

<br>Unordered set
<br>COntinous points


<br>Voxel Grids 

<br>Ordered sets, discrete grid
<br>Memory consuming


<br>Signed Distance Function

<br>Function encoding with its sogn whether continuous points are at which side of a decision boundary


<br>Neural Radiance Fields

<br>Function on all 3D points and rotations




<br>
]]></description><link>the-guide/robotics,-dynamics-and-control/perception/geometric-perception.html</link><guid isPermaLink="false">The Guide/Robotics, Dynamics and Control/Perception/Geometric Perception.md</guid><pubDate>Mon, 09 Sep 2024 15:37:42 GMT</pubDate></item><item><title><![CDATA[Object Pose Estimation]]></title><description><![CDATA[ 
 <br>Question
Given e.g. point clouds, how can we estimate an objects pose - its 6D orientation in the <a data-tooltip-position="top" aria-label="Group" data-href="Group" href="the-guide/mathematics/group-theory/group.html" class="internal-link" target="_self" rel="noopener nofollow">group</a> .
<br><br>Assumption 1
We have a point-cloud model of the object we want to detect.
<br>In general, it is assumed that we have a (point-cloud) model of the object whose pose we want to detect available.<img alt="center" src="lib/media/pasted-image-20240314101452.png" style="width: 200px; max-width: 100%;"><br>
If we have point-clouds as an input, we can state the above problem as follows: Find a <a data-tooltip-position="top" aria-label="Mathematical Relations" data-href="Mathematical Relations" href="the-guide/mathematics/general-stuff/mathematical-relations.html" class="internal-link" target="_self" rel="noopener nofollow">transformation</a>  that aligns the two point-clouds. For this, we can use the Iterative Closest Point (ICP) algorithm:<br>Algorithm

<br>Input

<br>Input points 
<br>(known) Model points 
<br>Initial guess for transform 


<br>Alternatively ...

<br>Use closest points for correspondences 
<br>Improve estimate of  by <a data-tooltip-position="top" aria-label="Linear Least Squares and Ridge Regression" data-href="Linear Least Squares and Ridge Regression" href="the-guide/mathematics/optimization/linear-least-squares-and-ridge-regression.html" class="internal-link" target="_self" rel="noopener nofollow">least-squares</a> objective where the <a data-tooltip-position="top" aria-label="A Hitchhiker's Guide to Rotation Representations" data-href="A Hitchhiker's Guide to Rotation Representations" href="the-guide/robotics,-dynamics-and-control/a-hitchhiker's-guide-to-rotation-representations.html" class="internal-link" target="_self" rel="noopener nofollow">rotation</a>  along with the translation  encode 



<br><br>Assumption 2
We know the correspondences of all the points in the point clouds.
<br>In this very unlikely scenario, we could simply solve the <a data-href="Linear Least Squares and Ridge Regression" href="the-guide/mathematics/optimization/linear-least-squares-and-ridge-regression.html" class="internal-link" target="_self" rel="noopener nofollow">Linear Least Squares and Ridge Regression</a> optimization problem from above using the preset correpsondences.<br>Challenges

<br>Points clouds are noisy and can have outliers
<br>Point clouds aren't restricted to single objects / objects of interest
<br>Curse of dimensionality and lack of data for 3D

]]></description><link>the-guide/robotics,-dynamics-and-control/perception/object-pose-estimation.html</link><guid isPermaLink="false">The Guide/Robotics, Dynamics and Control/Perception/Object Pose Estimation.md</guid><pubDate>Wed, 26 Feb 2025 10:33:31 GMT</pubDate><enclosure url="lib/media/pasted-image-20240314101452.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src=&quot;lib/media/pasted-image-20240314101452.png&quot;&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[Motion Planning]]></title><description><![CDATA[ 
 <br>Question
Given an initial state  and a goal state , how can I compute a feasible path between them through a known environment ?
<br>In general, the space in which we search for a path is denoted -space (<a data-href="Generalized Coordinates, Configuration or Joint Space" href="the-guide/robotics,-dynamics-and-control/generalized-coordinates,-configuration-or-joint-space.html" class="internal-link" target="_self" rel="noopener nofollow">Generalized Coordinates, Configuration or Joint Space</a> !) and the part that does not represent obstacles is denoted .<br><br><br>In general, these methods create a discrete representation of , e.g. by constructing a <a data-tooltip-position="top" aria-label="Graph" data-href="Graph" href="the-guide/mathematics/graph-theory/graph.html" class="internal-link" target="_self" rel="noopener nofollow">graph</a> where only neighbors are connected. Following that, graph-search algorithms such as depth-first, breadth-first or djkstra-search are used to find a feasible path.<br>
<img alt="Pasted image 20240203135134.png" src="lib/media/pasted-image-20240203135134.png"><br>
<br>Provides suboptimality bounds based on discretization
<br>Can get computationally expensive for high dimensions
<br><br><br>In contrast, sample-based methods construct a sparse <a data-tooltip-position="top" aria-label="Graph" data-href="Graph" href="the-guide/mathematics/graph-theory/graph.html" class="internal-link" target="_self" rel="noopener nofollow">graph</a> representation where the <a data-tooltip-position="top" aria-label="Node or Vertex Set" data-href="Node or Vertex Set" href="the-guide/mathematics/graph-theory/node-or-vertex-set.html" class="internal-link" target="_self" rel="noopener nofollow">nodes</a> are sampled from . Therefore, the probability of finding a path approaches  when increasing the number of samples. <br>
<br>Provides asymptotic suboptimality bounds
<br>Can be used for high-dimensional planning
<br>Samples only added when necessary<br>
<img alt="center" src="lib/media/pasted-image-20240203135215.png" style="width: 300px; max-width: 100%;">
<br><br>Methods that divide the motion planning problem into a pre-processing phase to build the graph and then use the algorithms from search-based planning on this graph in a second query phase.<br>
<br>Prepocessing Phase<br>
- How to sample a new <a data-tooltip-position="top" aria-label="Node or Vertex Set" data-href="Node or Vertex Set" href="the-guide/mathematics/graph-theory/node-or-vertex-set.html" class="internal-link" target="_self" rel="noopener nofollow">node</a> ?<br>
- Uniformly<br>
- Perturbate the location of existing node, choose this node inversely proportional to its connectivity<br>
- Biased sampling towards or away from obstacles<br>
- How do I connect the new node ? (if no obstacle on direct path)<br>
- Connect to all nodes within a fixed radius<br>
- Connect to the closest  nodes in  or in component direction<br>
<img alt="center" src="lib/media/pasted-image-20240203151249.png">
<br>Query Phase

<br>Use algorithms as in search-based planning.


<br>Advantages

<br>Simple
<br>Effective for high dimensions
<br>Same graph can be used for multiple queries, different start and goal


<br>Problems

<br>Can result in suboptimal paths
<br>Has problems with narrow passages
<br>Hard to deal with differential constraints


<br><br>In contrast do PRMs, who sample new nodes and search for closest nodes, Tree-Based Planners start from a root node and follow an expansion heuristic. Thereby, they build the graph and compute a trajectory simultaneously, stopping when the goal node is connected to the tree. Examples are<br>
<br><a data-href="Rapidly Exploring Random Tree (RRT)" href="the-guide/robotics,-dynamics-and-control/task-and-motion-planning/rapidly-exploring-random-tree-(rrt).html" class="internal-link" target="_self" rel="noopener nofollow">Rapidly Exploring Random Tree (RRT)</a>
<br>Advantages

<br>Require little memory and computation
<br>Easy to add heuristics


<br>Problems

<br>Can be highly suboptimal
<br>Path smoothing required


]]></description><link>the-guide/robotics,-dynamics-and-control/task-and-motion-planning/motion-planning.html</link><guid isPermaLink="false">The Guide/Robotics, Dynamics and Control/Task and Motion Planning/Motion Planning.md</guid><pubDate>Sat, 29 Mar 2025 16:19:06 GMT</pubDate><enclosure url="lib/media/pasted-image-20240203135134.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src=&quot;lib/media/pasted-image-20240203135134.png&quot;&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[Planning Domain Definition Language]]></title><description><![CDATA[ 
 <br>Language for <a data-href="Task Planning" href="the-guide/robotics,-dynamics-and-control/task-and-motion-planning/task-planning.html" class="internal-link" target="_self" rel="noopener nofollow">Task Planning</a> intended to express the physics of the planning domain, namely predicates, actions, structure of compound actions and their effects. The fundamental pieces of PDDL are <br>
<br>The task-agnostic domain consisting of predicates and actions
<br>The task-specific problem consisting of objects, initial state and goal specification<br>
The following is based on a simple example of a robot in a room that has to take an apple from a shelf and place it on a table.<img alt="center" src="lib/media/pasted-image-20240315120820.png" style="width: 150px; max-width: 100%;">
<br><br><br><img alt="center" src="lib/media/pasted-image-20240315120858.png" style="width: 400px; max-width: 100%;">Domains describe how the world works, which is why their components have parameters ?.<br>
<br>(Location ?loc) is a unary predicate, it has one parameter
<br>(At ?r ?loc) is a binary predicate, it has two parameters<br>
As an action language, PDDL is based on actions that interact with predicates. An action contains
<br>Parameters - allow action to be executed for different combinations of objects
<br>Preconditions - predicates which must be true at a given state to allow the action
<br>Effects - change in state that occur as a result of the action<br>
As an example, moving the robot, picking something up and placing something in our example can be encoded in the actions ...<img alt="center" src="lib/media/pasted-image-20240315122445.png" style="width: 400px; max-width: 100%;"><br>
<img alt="center" src="lib/media/pasted-image-20240315122621.png" style="width: 400px; max-width: 100%;">
<br><br><img alt="center" src="lib/media/pasted-image-20240315120942.png" style="width: 400px; max-width: 100%;"><br><br><br>PDDL is standardized and domain-independent and therefore already has a large amount of efficient algorithms available. Additionally, it is itself rather efficient because it encodes differences, leaving most variables unchanged.<br>It is therefore sensible to extend the language to work with the challenges imposed on <a data-tooltip-position="top" aria-label="Task and Motion Planning (TAMP)" data-href="Task and Motion Planning (TAMP)" href="the-guide/robotics,-dynamics-and-control/task-and-motion-planning/task-and-motion-planning-(tamp).html" class="internal-link" target="_self" rel="noopener nofollow">TAMP</a> by incorporating sampling procedures. Additionally, PDDL arguments can now have high-dimensional and even continuous arguments.<br>
<br>What samplers do we need ?<br>
- Have to satisfy (low-dim) constraints<br>
- May need arbitrary many samples<br>
- May need inputs such as pose - conditional samplers<br>
- Directed acyclic graph of conditional samplers<br>
This is realized via streams, function from an input tuple to a potentially infinte sequence of output objects. These streams need specifications on legal inputs and fulfill static facts that all outputs satisfy.
<br><img alt="center" src="lib/media/pasted-image-20240315154139.png" style="width: 500px; max-width: 100%;">]]></description><link>the-guide/robotics,-dynamics-and-control/task-and-motion-planning/planning-domain-definition-language.html</link><guid isPermaLink="false">The Guide/Robotics, Dynamics and Control/Task and Motion Planning/Planning Domain Definition Language.md</guid><pubDate>Mon, 09 Sep 2024 15:37:42 GMT</pubDate><enclosure url="lib/media/pasted-image-20240315120820.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src=&quot;lib/media/pasted-image-20240315120820.png&quot;&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[Rapidly Exploring Random Tree (RRT)]]></title><description><![CDATA[ 
 <br>Info
Class of algorithms to perform <a data-tooltip-position="top" aria-label="Motion Planning" data-href="Motion Planning" href="the-guide/robotics,-dynamics-and-control/task-and-motion-planning/motion-planning.html" class="internal-link" target="_self" rel="noopener nofollow">sample-based motion planning</a> by constructing a tree starting from the initial state  until the goal state  is part of the tree.
<br>In general, the algorithms below are fairly simple to implement and optimizable for different scenarios. <br>
<br>Advantages

<br>Require little memory and computation
<br>Easy to add heuristics
<br>Can include complex dynamics / control information (kinodynamic planning)


<br>Disadvantages

<br>Can yield highly sub-optimal solutions
<br>Path smoothing required


<br><br><br>
<br>Sample a new <a data-tooltip-position="top" aria-label="Node or Vertex Set" data-href="Node or Vertex Set" href="the-guide/mathematics/graph-theory/node-or-vertex-set.html" class="internal-link" target="_self" rel="noopener nofollow">node</a> , find nearest neighbor  in (tree)-<a data-tooltip-position="top" aria-label="Graph" data-href="Graph" href="the-guide/mathematics/graph-theory/graph.html" class="internal-link" target="_self" rel="noopener nofollow">graph</a> <img alt="center" src="lib/media/pasted-image-20240203160715.png" style="width: 450px; max-width: 100%;">
<br>If nearest point is on existing edge, split<img alt="center" src="lib/media/pasted-image-20240203160807.png" style="width: 450px; max-width: 100%;">
<br>If there is an obstacle, create alternative node  as close as possible (generally with minimal distance) to the obstacle<img alt="center" src="lib/media/pasted-image-20240203160927.png" style="width: 450px; max-width: 100%;">
<br>Add goal  instead of  every  iterations to create stopping criterion<br>
<img alt="center" src="lib/media/pasted-image-20240203162326.png" style="width: 300px; max-width: 100%;">
<br><br><br>Set maximum distance  between  and the new node. Thereby, in many cases the tree is connected to an alternative point  on the hypothetical edge between the two nodes.<br>
<img alt="center" src="lib/media/pasted-image-20240203161157.png" style="width: 500px; max-width: 100%;"><br>
This implementation leads to a different behavior<img alt="center" src="lib/media/pasted-image-20240203163143.png" style="width: 500px; max-width: 100%;"><br>
<br>Voronoi Bias

<br>While the RDT provided uniform coverage of space, the RRT exhibits a bias towards the nodes with the largest voronoi regions at each iteration<img alt="center" src="lib/media/pasted-image-20240203163511.png" style="width: 200px; max-width: 100%;">


<br>Goal-Biased Sampling

<br>Can be used to speed up RRT, sample  uniform with probability  and usw goal state  with probability .


<br>Steering

<br>Instead of using direct paths between the samples, generate  by applying optimal control input to bring system from  to  for fixed number of steps. This generate paths that consider dynamics restrictions of the agent.<img alt="center" src="lib/media/pasted-image-20240203165944.png" style="width: 200px; max-width: 100%;">


<br><br><br>In some configurations of obstacles, local pathfinding can get confused, e.g. having to go away from the goal to steer around obstacles or passing very narrow passages.<img alt="center" src="lib/media/pasted-image-20240203161612.png" style="width: 400px; max-width: 100%;"><br>
<br>Solution

<br>Start one tree from  and a second from  and try to connect new points to both trees (the respective nearest node) until they get connected. <img alt="center" src="lib/media/pasted-image-20240203162021.png" style="width: 500px; max-width: 100%;">


<br><br><br>
<br>Grow one tree to random target<img alt="center" src="lib/media/pasted-image-20240203204015.png" style="width: 300px; max-width: 100%;">
<br>New <a data-tooltip-position="top" aria-label="Node or Vertex Set" data-href="Node or Vertex Set" href="the-guide/mathematics/graph-theory/node-or-vertex-set.html" class="internal-link" target="_self" rel="noopener nofollow">node</a> is target for other tree<img alt="center" src="lib/media/pasted-image-20240203204101.png" style="width: 300px; max-width: 100%;">
<br>Add new collision-free branch with fixed stepsize as long as possible<img alt="center" src="lib/media/pasted-image-20240203204204.png" style="width: 300px; max-width: 100%;">
<br>Once this procedure connects both graphs, the algorithm finishes
<br><br><br>All versions above not optimal, since infinitely many samples needed to guarantee optimality. The following stores a weight depending on the distance to the initial state. <br>
<br>Generate new potential node as in vanilla RRT, find all nodes within neighborhood  around <img alt="center" src="lib/media/pasted-image-20240203210910.png" style="width: 300px; max-width: 100%;">
<br>Search through the neighborhood and identify the node with the lowest weight meaning the shortest path to initial state .<br>
<img alt="center" src="lib/media/pasted-image-20240203211021.png" style="width: 300px; max-width: 100%;">
<br>Connect the nodes and set the weight of  to 
<br>Rewiring to correct the labels by checking all the nodes  and remove edge between  and its parent if In this case, a new edge between  and  is added.<br>
<img alt="center" src="lib/media/pasted-image-20240203211143.png" style="width: 300px; max-width: 100%;">
]]></description><link>the-guide/robotics,-dynamics-and-control/task-and-motion-planning/rapidly-exploring-random-tree-(rrt).html</link><guid isPermaLink="false">The Guide/Robotics, Dynamics and Control/Task and Motion Planning/Rapidly Exploring Random Tree (RRT).md</guid><pubDate>Sat, 29 Mar 2025 16:19:06 GMT</pubDate><enclosure url="lib/media/pasted-image-20240203160715.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src=&quot;lib/media/pasted-image-20240203160715.png&quot;&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[Task and Motion Planning (TAMP)]]></title><description><![CDATA[ 
 <br>Algorithm that combines discrete high-level <a data-tooltip-position="top" aria-label="Task Planning" data-href="Task Planning" href="the-guide/robotics,-dynamics-and-control/task-and-motion-planning/task-planning.html" class="internal-link" target="_self" rel="noopener nofollow">task planning</a> with continuous low-level <a data-tooltip-position="top" aria-label="Motion Planning" data-href="Motion Planning" href="the-guide/robotics,-dynamics-and-control/task-and-motion-planning/motion-planning.html" class="internal-link" target="_self" rel="noopener nofollow">motion planning</a>. The result is a planning approach in a factored, hybrid <a data-tooltip-position="top" aria-label="Space" data-href="Space" href="the-guide/mathematics/general-stuff/space.html" class="internal-link" target="_self" rel="noopener nofollow">space</a> with continuous variables such as joint positions or object poses and discrete ones like states. <br>No Free Lunch
In symbolic planning, actions and their consequences are discrete, which enables exhaustive search. If we add detailed motion plans with continuous parameter spaces, the search becomes intractable. Several approaches to TAMP therefore rely on sampling.
<br>In general there are two opposite approaches to work against that with various methods in between the two:<br><br>Sample action parameters from continuous space before search to convert to discrete problem.<br>
<img alt="center" src="lib/media/pasted-image-20240315143621.png" style="width: 400px; max-width: 100%;"><br><br>Purely symbolic skeleton plan. This e.g. ignores continuous <a data-tooltip-position="top" aria-label="Static and Dynamic Systems" data-href="Static and Dynamic Systems" href="the-guide/robotics,-dynamics-and-control/dynamics/static-and-dynamic-systems.html" class="internal-link" target="_self" rel="noopener nofollow">dynamics</a>.<br>
If it is deemed feasible, fill the placeholders by sampling.<br><img alt="center" src="lib/media/pasted-image-20240315143545.png" style="width: 400px; max-width: 100%;"><br>These approaches can then be executed based in two ways:<br><br>Discrete <a data-tooltip-position="top" aria-label="Task Planning" data-href="Task Planning" href="the-guide/robotics,-dynamics-and-control/task-and-motion-planning/task-planning.html" class="internal-link" target="_self" rel="noopener nofollow">task planning</a> followed by continuous <a data-tooltip-position="top" aria-label="Motion Planning" data-href="Motion Planning" href="the-guide/robotics,-dynamics-and-control/task-and-motion-planning/motion-planning.html" class="internal-link" target="_self" rel="noopener nofollow">motion planning</a>. This naturally requires the whole task fulfills a strong downward refinement assumptions.<img alt="center" src="lib/media/pasted-image-20240315144537.png" style="width: 200px; max-width: 100%;"><br><br>Simultaneously try to do both planning stages by alternating between them.<img alt="center" src="lib/media/pasted-image-20240315144921.png" style="width: 300px; max-width: 100%;"><br>TAMP inherits the problems of both <a data-tooltip-position="top" aria-label="Motion Planning" data-href="Motion Planning" href="the-guide/robotics,-dynamics-and-control/task-and-motion-planning/motion-planning.html" class="internal-link" target="_self" rel="noopener nofollow">motion planning</a> and <a data-tooltip-position="top" aria-label="Task Planning" data-href="Task Planning" href="the-guide/robotics,-dynamics-and-control/task-and-motion-planning/task-planning.html" class="internal-link" target="_self" rel="noopener nofollow">task planning</a>, i.e. high-dimensional continuous state spaces, exponential complexities and long horizons.<br><br>Extending the example of the robot that has to get and place an apple used in <a data-tooltip-position="top" aria-label="Task Planning" data-href="Task Planning" href="the-guide/robotics,-dynamics-and-control/task-and-motion-planning/task-planning.html" class="internal-link" target="_self" rel="noopener nofollow">task planning</a> and <a data-tooltip-position="top" aria-label="Planning Domain Definition Language" data-href="Planning Domain Definition Language" href="the-guide/robotics,-dynamics-and-control/task-and-motion-planning/planning-domain-definition-language.html" class="internal-link" target="_self" rel="noopener nofollow">PDDL</a>, we could encounter the symbolic plan<br>
<br>Pick the apple
<br>Move to table
<br>Place the apple on table<br>
However, for TAMP we now need to fill this skeleton with parameters such as 
<br>
<br>?pt = pose of robot when navigating
<br>?path = output of motion planner (e.g. <a data-tooltip-position="top" aria-label="Rapidly Exploring Random Tree (RRT)" data-href="Rapidly Exploring Random Tree (RRT)" href="the-guide/robotics,-dynamics-and-control/task-and-motion-planning/rapidly-exploring-random-tree-(rrt).html" class="internal-link" target="_self" rel="noopener nofollow">RRT</a>)
<br>?pa-1 = pose of apple after being placed, consequence of (given) inital pose ?pa-0<br>
Additionally, the navigation part requires us to plan a path in a continuous setting while incorporating a continuous goal pose. Another pose is needed when placing the apple, while also avoiding collisions along the way. For this, we need to use an extension of PDDL, e.g. <a data-tooltip-position="top" aria-label="Planning Domain Definition Language" data-href="Planning Domain Definition Language" href="the-guide/robotics,-dynamics-and-control/task-and-motion-planning/planning-domain-definition-language.html" class="internal-link" target="_self" rel="noopener nofollow">PDDLStream</a>.
<br><br><br>Required, if the underlying model is a hybrid <a data-tooltip-position="top" aria-label="Markov Decision Process" data-href="Markov Decision Process" href="the-guide/machine-learning/reinforcement-learning/markov-decision-process.html" class="internal-link" target="_self" rel="noopener nofollow">MDP</a>, where actions have stochastic effects, meaning the agent might arrive at other states than intended. In this case, we need a <a data-tooltip-position="top" aria-label="Policy" data-href="Policy" href="the-guide/machine-learning/reinforcement-learning/policy.html" class="internal-link" target="_self" rel="noopener nofollow">policy</a> instead of a plan.<br>
<br>Offline computation intractable, instead use online and re-planning
<br>Action determinization - deterministically select action outcome, but penalize unlikely outcomes with high cost<br>
Examples are
<br>Partially-Observable TAMP

<br>Belief state distribution 


<br>POMDP PDDLStream

<br>Information gathering actions, e.g. scan room and detect


<br>Belief-Space TAMP

<br>CNN object detector + point cloud for plane and pose estimation + occupancy grid


]]></description><link>the-guide/robotics,-dynamics-and-control/task-and-motion-planning/task-and-motion-planning-(tamp).html</link><guid isPermaLink="false">The Guide/Robotics, Dynamics and Control/Task and Motion Planning/Task and Motion Planning (TAMP).md</guid><pubDate>Mon, 03 Mar 2025 15:30:27 GMT</pubDate><enclosure url="lib/media/pasted-image-20240315143621.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src=&quot;lib/media/pasted-image-20240315143621.png&quot;&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[Task Planning]]></title><description><![CDATA[ 
 <br>Process of generating sequence of actions to achieve a goal / a set of goals in an environment. Assumes internal model of the world that enables autonomous reasoning. High level tasks are decomposed into smaller, manageable subtasks that are then split into a sequence of specific actions. Additionally, feasibility is ensured based on capabilities and the environment.<br>The world model is usually described using states and actions to enable transitions between them. This is encoded via planning languages, e.g. <a data-tooltip-position="top" aria-label="Planning Domain Definition Language" data-href="Planning Domain Definition Language" href="the-guide/robotics,-dynamics-and-control/task-and-motion-planning/planning-domain-definition-language.html" class="internal-link" target="_self" rel="noopener nofollow">PDDL</a>.<br>
Task planning is concerned with action feasibility, the resulting symbolic plans need to be made concrete via motion planning, e.g. through <a data-tooltip-position="top" aria-label="Task and Motion Planning (TAMP)" data-href="Task and Motion Planning (TAMP)" href="the-guide/robotics,-dynamics-and-control/task-and-motion-planning/task-and-motion-planning-(tamp).html" class="internal-link" target="_self" rel="noopener nofollow">TAMP</a>.<br><br><br>Based on <a data-tooltip-position="top" aria-label="Planning Domain Definition Language" data-href="Planning Domain Definition Language" href="the-guide/robotics,-dynamics-and-control/task-and-motion-planning/planning-domain-definition-language.html" class="internal-link" target="_self" rel="noopener nofollow">PDDL</a>, we can consider the problem of a robot in a room that should take an apple from a shelf and place it on a table.<br>
<img alt="center" src="lib/media/pasted-image-20240315131741.png" style="width: 500px; max-width: 100%;"><br>In general, the computational demand of the task planning problem grows exponentially with the number of variables. <br><br>
<br>Breadth-First-Search (BFS)

<br>Only limited actions required, but many possible diverging paths


<br>Depth-First-Search (DFS)

<br>Good for few paths, many actions to reach goal


<br>Heuristic Search Planning (HSP)

<br>...


<br>Fast-Forward (FF)

<br>Use relaxed plan graph that ignores delete effects
<br>Hill Climbing - steepest ascent via greedy search + breadth-first for plateaus
<br>Heuristic is computed for parent state at expansion and then inherited, lowers number of evaluations
<br>Helpful Actions - focus on actions that directly contribute by achieving previously unsatisfied goal conditions.


<br>Fast Downward

<br>Decompose problem hierarchically


<br>Pitfalls
Level of abstraction can potentially be suboptimal, e.g. 

<br>Unknown environment details, e.g. robot too big to go through door or object to be placed do not fit on table

]]></description><link>the-guide/robotics,-dynamics-and-control/task-and-motion-planning/task-planning.html</link><guid isPermaLink="false">The Guide/Robotics, Dynamics and Control/Task and Motion Planning/Task Planning.md</guid><pubDate>Mon, 09 Sep 2024 15:37:42 GMT</pubDate><enclosure url="lib/media/pasted-image-20240315131741.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src=&quot;lib/media/pasted-image-20240315131741.png&quot;&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[Trajectory Optimization]]></title><description><![CDATA[ 
 <br><br>Problem Statement
Assuming smooth and consistent <a data-tooltip-position="top" aria-label="Static and Dynamic Systems" data-href="Static and Dynamic Systems" href="the-guide/robotics,-dynamics-and-control/dynamics/static-and-dynamic-systems.html" class="internal-link" target="_self" rel="noopener nofollow">dynamics</a> , objectives and constraints, a general trajectory optimization problem takes the form with various possible constraints.
<br>--- start-multi-column: ExampleRegion1  <br>number of columns: 2  
<br><br>
<br>General boundary constraints 
<br>Bound on time boundaries 
<br>Bound in initial state 
<br>Bound on final state 
<br>--- end-column ---<br>Objective Constraints<br>
<br>Dynamics constraint
<br>Path constraints 
<br>Continuous bound on state 
<br>Continuous bound on control 
<br>--- end-multi-column<br><br><br>The general problem stated above is an infinite dimensional problem, because it consists of vector-valued <a data-tooltip-position="top" aria-label="Mathematical Relations" data-href="Mathematical Relations" href="the-guide/mathematics/general-stuff/mathematical-relations.html" class="internal-link" target="_self" rel="noopener nofollow">functions</a> related via differential operators. Our aim is to turn this into a finite-dimensional problem involving real numbers and algebraic equations of the form The process of this translation is called transcription and includes various methods depending on the setting.<br>
--- start-multi-column: Methods1  <br>number of columns: 2  
<br><br>
<br>First Optimize, then discretize
<br>More accurate, harder to pose and solve
<br>--- end-column ---<br><br>
<br>First discretize, then optimize
<br>Less accurate, easier to pose and solve
<br>--- end-multi-column<br>
<br><br>
--- start-multi-column: Methods2 <br>number of columns: 2  
<br><br>
<br>Based on simulations
<br>Good for problems with simple control and no path constraints
<br>--- end-column ---<br><br>
<br>Based on function approximation
<br>Better for complicated control and path constraints
<br>--- end-multi-column<br>
<br><br>
--- start-multi-column: Methods3 <br>number of columns: 2  
<br><br>
<br>Low-order method
<br>Increase number for convergence
<br>--- end-column ---<br><br>
<br>High-order method
<br>Increase order for convergence
<br>--- end-multi-column<br><br><br>Once the transcription is done, we have a non-linear optimization problem to solve. In order to solve this, we create an initial guess by instead solving  a simplified version of the problem by e.g. removing the dynamics, work with very rough approximations or remove certain constraints.<br>(...)<br><br>Given a solution , we want to estimate the error w.r.t. the unknown true solution .<br>
For this, we combine the collocation constraint with the known dynamics model. We first compute  at every point (not just the collocation points). This can be used to construct the error term , which can be integrated for each segment between two collocation points as   <br><br><br><br>To give intuition on the complicated problem stated above, consider the well-known cart-pole system with the parameters shown below.<br>
<img alt="center" src="lib/media/pasted-image-20240204191820.png" style="width: 200px; max-width: 100%;"><br>
This <a data-tooltip-position="top" aria-label="Static and Dynamic Systems" data-href="Static and Dynamic Systems" href="the-guide/robotics,-dynamics-and-control/dynamics/static-and-dynamic-systems.html" class="internal-link" target="_self" rel="noopener nofollow">system's</a> <a data-tooltip-position="top" aria-label="Static and Dynamic Systems" data-href="Static and Dynamic Systems" href="the-guide/robotics,-dynamics-and-control/dynamics/static-and-dynamic-systems.html" class="internal-link" target="_self" rel="noopener nofollow">dynamics</a> can be expressed via <br><br>We want to find inputs  that lift the pole from the stable rest position to an upright position, yielding the initial and final state bounds<img alt="center" src="lib/media/pasted-image-20240204192753.png" style="width: 250px; max-width: 100%;">Additionally, we limit the track length  and maximum motor commands  via In general, we have to respect the dynamics of the system, leading to an additional constraint of the form . We want to find an optimal solution in a given time-frame by minimizing over the expended energy of the motor commands via <br><br>
<br>Direct Method - Instead of having continuous time and trajectories , we switch to the discrete intervalls 
<br>Collocation - We discretize the integral in the objective by a discrete sum using the trapezoid method with  and . Based on this, we replace the dynamics constraints between points by a linear approximation based on the same trapezoid method. We first rewrite the dynamics for the discrete time-steps asand then use the approximation to reach 
<br>Non-Linear Programm The above steps result in the optimization problem (...)
<br><br>The above problem is relatively simple and we can simply use a linear interpolation without control input from start to finish as an initialization<img alt="center" src="lib/media/pasted-image-20240315112305.png" style="width: 600px; max-width: 100%;">We then interpolate the solution trajectories using spline interpolation. This has to be consistent with the used collocation method. We use linear splines for  and quadratic ones for position and velocity<br>
<br>Improving Accuracy via h-Method - The above method with  segments yields the error plot<img alt="center" src="lib/media/pasted-image-20240315113000.png" style="width: 400px; max-width: 100%;">Using the principles of the h-method we can increase the accuracy by instead using  segments. While this can drastically increase computational demand, the solution will be far more accurate.<img alt="center" src="lib/media/pasted-image-20240315113119.png" style="width: 400px; max-width: 100%;">
]]></description><link>the-guide/robotics,-dynamics-and-control/task-and-motion-planning/trajectory-optimization.html</link><guid isPermaLink="false">The Guide/Robotics, Dynamics and Control/Task and Motion Planning/Trajectory Optimization.md</guid><pubDate>Sun, 06 Apr 2025 17:51:14 GMT</pubDate><enclosure url="lib/media/pasted-image-20240204191820.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src=&quot;lib/media/pasted-image-20240204191820.png&quot;&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[A Hitchhiker's Guide to Rotation Representations]]></title><description><![CDATA[ 
 <br>In a Nutshell
The <a data-href="Lie Group" href="the-guide/mathematics/lie-theory/lie-group.html" class="internal-link" target="_self" rel="noopener nofollow">Lie Group</a> <a data-tooltip-position="top" aria-label="The Lie Group SO(n) and its Lie Algebra" data-href="The Lie Group SO(n) and its Lie Algebra" href="the-guide/mathematics/lie-theory/special-orthogonal-group-so(3)/the-lie-group-so(n)-and-its-lie-algebra.html" class="internal-link" target="_self" rel="noopener nofollow">SO(3)</a> describes all rotations about the origin in three-dimensional space and is therefore heavily used in engineering and physics. To work with the abstract object in practice, there exist multiple representations, each with unique advantages and disadvantages.
The following is mainly based on the similarly-named paper, but I worked in other sources over time to create my own guide.
<br><img alt="center" src="lib/media/pasted-image-20250319234911.png" style="width: 200px; max-width: 100%;"><br>
Learning mappings on the <a data-tooltip-position="top" aria-label="Mannigfaltigkeiten" data-href="Mannigfaltigkeiten" href="the-guide/mathematics/differential-geometry/riemannian-geometry/mannigfaltigkeiten-und-differenzierbare-abbildungen/mannigfaltigkeiten.html" class="internal-link" target="_self" rel="noopener nofollow">manifold</a> of rotations using <a data-tooltip-position="top" aria-label="Gradient Descent" data-href="Gradient Descent" href="the-guide/mathematics/optimization/convex-optimization-lecture/algorithms/gradient-descent.html" class="internal-link" target="_self" rel="noopener nofollow">gradient</a>-based optimization algorithms requires special care, since non-trivial <a data-tooltip-position="top" aria-label="Topology and Topological Space" data-href="Topology and Topological Space" href="the-guide/mathematics/topology/topology-and-topological-space.html" class="internal-link" target="_self" rel="noopener nofollow">topologies</a> introduce additional challenges during the learning process.<br>In the following, we consider different representations  with functions<br><br><br>Problem
No representation is non-redundant, continuous and free of singularities !
<br><br><br><br>Rotation Matrices
The subset of linear transformations  that is interpretable as a rotation is denotedIn contrast to , this excludes reflections. The above <a data-tooltip-position="top" aria-label="Set" data-href="Set" href="the-guide/mathematics/general-stuff/set.html" class="internal-link" target="_self" rel="noopener nofollow">set</a> forms a <a data-tooltip-position="top" aria-label="Group" data-href="Group" href="the-guide/mathematics/group-theory/group.html" class="internal-link" target="_self" rel="noopener nofollow">group</a> under matrix multiplication that is also a <a data-href="Lie Group" href="the-guide/mathematics/lie-theory/lie-group.html" class="internal-link" target="_self" rel="noopener nofollow">Lie Group</a>.  For D, the rotation <a data-tooltip-position="top" aria-label="Matrices" data-href="Matrices" href="the-guide/mathematics/linear-algebra/matrices.html" class="internal-link" target="_self" rel="noopener nofollow">matrices</a> around each axis are 
<br>Unit Quaternions
Covers the rotational group  twice, but the mapping between them is continuous and thereby singularity-free. Additionally, the set of unit quaternions  is also a <a data-tooltip-position="top" aria-label="Mannigfaltigkeiten" data-href="Mannigfaltigkeiten" href="the-guide/mathematics/differential-geometry/riemannian-geometry/mannigfaltigkeiten-und-differenzierbare-abbildungen/mannigfaltigkeiten.html" class="internal-link" target="_self" rel="noopener nofollow">Riemannian manifold</a> and a <a data-href="Lie Group" href="the-guide/mathematics/lie-theory/lie-group.html" class="internal-link" target="_self" rel="noopener nofollow">Lie Group</a>. 
See <a data-href="Unit Quaternions and SU(2)" href="the-guide/mathematics/lie-theory/unit-quaternions-and-su(2).html" class="internal-link" target="_self" rel="noopener nofollow">Unit Quaternions and SU(2)</a> and <a data-href="Less Weird Quaternions - A Geometric Algebra Perspective" href="the-guide/robotics,-dynamics-and-control/less-weird-quaternions-a-geometric-algebra-perspective.html" class="internal-link" target="_self" rel="noopener nofollow">Less Weird Quaternions - A Geometric Algebra Perspective</a> for more information.
<br>Advantages

<br>Intuitive - composition of rotations that are applied one after the other can be visualized well.

<br>Problems

<br>Double Cover - we have thatWe can safely do computations with quaternions, but have to be careful when we convert is back to a usable representation like rotation matrices.

<br>Euler Angles
Describes a rotation in  using three scalars written in a three tuple (because they are no coordinate vectors), each representing a rotation about a local coordinate system.
<br><img alt="center" src="lib/media/pasted-image-20250320103214.png" style="width: 350px; max-width: 100%;"><br>Advantages

<br>Intuitive - composition of rotations that are applied one after the other can be visualized well.

<br>Problems

<br>Gimbal Lock - when rotating around certain axes by , they align with others. Thereby, we loose one degree of freedom.  
<br>Operations - since euler angles are simply a tuple without any additional structure, operations like addition, computing distances or <a data-tooltip-position="top" aria-label="Covariance and Variance" data-href="Covariance and Variance" href="the-guide/mathematics/statistics/covariance-and-variance.html" class="internal-link" target="_self" rel="noopener nofollow">covariances</a> are not well defined.
<br>Order - the convention, i.e. in which order the operations are performed around which axis is 
<br>Many-to-one - the same rotation is encoded by various euler angles.

<br>Axis-Angle
Describes a rotation by a unit vector  for the axis of rotation and a scalar angle  for the magnitude.
In contrast to euler-angles, this enables computations in a vector space, the <a data-href="Lie Algebra" href="the-guide/mathematics/lie-theory/lie-algebra.html" class="internal-link" target="_self" rel="noopener nofollow">Lie Algebra</a> of the <a data-href="Lie Group" href="the-guide/mathematics/lie-theory/lie-group.html" class="internal-link" target="_self" rel="noopener nofollow">Lie Group</a> <a data-tooltip-position="top" aria-label="The Lie Group SO(n) and its Lie Algebra" data-href="The Lie Group SO(n) and its Lie Algebra" href="the-guide/mathematics/lie-theory/special-orthogonal-group-so(3)/the-lie-group-so(n)-and-its-lie-algebra.html" class="internal-link" target="_self" rel="noopener nofollow">SO(3)</a>.
<br><img alt="center" src="lib/media/pasted-image-20240313093157.png" style="width: 100px; max-width: 100%;"><br>Problems

<br>Discontinuous at 

<br> + GSO
Representation of the form . From this, a rotation matrix is constructed using <a data-tooltip-position="top" aria-label="Gram-Schmidt-Process" data-href="Gram-Schmidt-Process" href="the-guide/mathematics/linear-algebra/gram-schmidt-process.html" class="internal-link" target="_self" rel="noopener nofollow">Gram-Schmidt orthonormalization</a>
<br> + SVD
Given any  matrix , we can project is to  using <a data-tooltip-position="top" aria-label="Singular Value Decomposition" data-href="Singular Value Decomposition" href="the-guide/mathematics/linear-algebra/singular-value-decomposition.html" class="internal-link" target="_self" rel="noopener nofollow">SVD</a> via the <a data-tooltip-position="top" aria-label="Mathematical Relations" data-href="Mathematical Relations" href="the-guide/mathematics/general-stuff/mathematical-relations.html" class="internal-link" target="_self" rel="noopener nofollow">mapping</a> This finds the rotation matrix with the least squares distance to .
<br><br><br>Defining distances via <a data-tooltip-position="top" aria-label="Metric Space and Completeness" data-href="Metric Space and Completeness" href="the-guide/mathematics/general-stuff/metric-space-and-completeness.html" class="internal-link" target="_self" rel="noopener nofollow">metrics</a> is essential to define loss functions for machine learning objectives. However, this is not straight forward for rotations and a lot of authors use other concepts such as pseudo metrics. For a more mathematical discussion, see <a data-href="Metrics on SO(3)" href="the-guide/mathematics/lie-theory/special-orthogonal-group-so(3)/metrics-on-so(3).html" class="internal-link" target="_self" rel="noopener nofollow">Metrics on SO(3)</a>.<br>Unit Quaternions<br>
An essential problem for unit quaternions is the double cover, which leads to violations for the basic properties of a metric. To mitigate, we can pick the shortest metric between negative counterparts<br>Euler Angles<br>
For euler angles, the problem above is even worse, which is why we need to pick  for every angle and then compute <br>Rotation Matrices<br>
This is usually computed using the <a data-tooltip-position="top" aria-label="Norms" data-href="Norms" href="the-guide/mathematics/linear-algebra/norms.html" class="internal-link" target="_self" rel="noopener nofollow">Frobenius norm</a> of the difference between two rotations. This defines the Chordal distancewhich is considered numerically stable and computationally efficient. To achieve <a data-tooltip-position="top" aria-label="Convexity" data-href="Convexity" href="the-guide/mathematics/optimization/convex-optimization-lecture/convexity.html" class="internal-link" target="_self" rel="noopener nofollow">convexity</a>, we can use the square.<br>A more rigorous metric is given by the <a data-tooltip-position="top" aria-label="Geodesic Distance" data-href="Geodesic Distance" href="the-guide/mathematics/graph-theory/geodesic-distance.html" class="internal-link" target="_self" rel="noopener nofollow">geodesic distance</a> of the relative rotation <br>
<br>Hyberbolic metric ?
<br><br><br>We need Double Cover
As explained in <a data-href="Less Weird Quaternions - A Geometric Algebra Perspective" href="the-guide/robotics,-dynamics-and-control/less-weird-quaternions-a-geometric-algebra-perspective.html" class="internal-link" target="_self" rel="noopener nofollow">Less Weird Quaternions - A Geometric Algebra Perspective</a>, the double cover is important for interpolation, since the topology of  leads to information loss when e.g. interpolating between rotations by  (need to store which way we took there)
<br><br><br>The main problem in a classical <a data-tooltip-position="top" aria-label="The Filtering Problem" data-href="The Filtering Problem" href="the-guide/computational-statistics/data-assimilation/filtering-algorithms/the-filtering-problem.html" class="internal-link" target="_self" rel="noopener nofollow">filtering</a> setting is that the <a data-tooltip-position="top" aria-label="Expectations" data-href="Expectations" href="the-guide/mathematics/statistics/expectations.html" class="internal-link" target="_self" rel="noopener nofollow">mean</a> cannot be easily transfered from euclidean to the space of rotations.<br>
We have to use the Frechet/Karcker meanwhich does not have a closed-form solution and requires us to solve an optimization problem at each step.<br><br><br>Problems

<br> is not <a data-tooltip-position="top" aria-label="Homeomorphism" data-href="Homeomorphism" href="the-guide/mathematics/topology/homeomorphism.html" class="internal-link" target="_self" rel="noopener nofollow">homeomorphic</a> to , which is why there is no continuous representation that has less than  dimensions
<br><a data-tooltip-position="top" aria-label="Gradient Descent" data-href="Gradient Descent" href="the-guide/mathematics/optimization/convex-optimization-lecture/algorithms/gradient-descent.html" class="internal-link" target="_self" rel="noopener nofollow">Gradient-descent-based</a> algorithms require some notion of continuity to work reliably, as discontinuities can blow up gradients

<br>
<br>If we deal with total rotations, we have to resort to high dimensional representations or quaternions with additional tricks like picking or half-space maps
<br>For relative and therefore small rotations, like in <a data-tooltip-position="top" aria-label="Diffusion Models" data-href="Diffusion Models" href="the-guide/machine-learning/generative-models/diffusion-models.html" class="internal-link" target="_self" rel="noopener nofollow">diffusion models</a>, we can use e.g. Lie algebra elements
]]></description><link>the-guide/robotics,-dynamics-and-control/a-hitchhiker&apos;s-guide-to-rotation-representations.html</link><guid isPermaLink="false">The Guide/Robotics, Dynamics and Control/A Hitchhiker&apos;s Guide to Rotation Representations.md</guid><pubDate>Wed, 23 Apr 2025 08:34:38 GMT</pubDate><enclosure url="lib/media/pasted-image-20250319234911.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src=&quot;lib/media/pasted-image-20250319234911.png&quot;&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[Generalized Coordinates, Configuration or Joint Space]]></title><description><![CDATA[ 
 <br>Definition
<a data-tooltip-position="top" aria-label="Set" data-href="Set" href="the-guide/mathematics/general-stuff/set.html" class="internal-link" target="_self" rel="noopener nofollow">Set</a>  of all possible configurations (implies full specification of position of every point in the system) a robot can obtain. Exists within a fixed <a data-tooltip-position="top" aria-label="Topology and Topological Space" data-href="Topology and Topological Space" href="the-guide/mathematics/topology/topology-and-topological-space.html" class="internal-link" target="_self" rel="noopener nofollow">topology</a> in a given <a data-tooltip-position="top" aria-label="Space" data-href="Space" href="the-guide/mathematics/general-stuff/space.html" class="internal-link" target="_self" rel="noopener nofollow">space</a>.
<br>
<br><a data-tooltip-position="top" aria-label="Topology and Topological Space" data-href="Topology and Topological Space" href="the-guide/mathematics/topology/topology-and-topological-space.html" class="internal-link" target="_self" rel="noopener nofollow">Topology and Topological Space</a> more fundamental than representation, as it is independent of how it is represented, e.g. two-link robot
<br>In robotics, the term is sometimes also used to describe the subset of possible end-effector positions
<br>Degrees of Freedom
Number of parameters to define configuration space.
<br>
<br>A robot with 6 DoFs is considered general, more than that lead to redundant robots
<br>To compute the number of DOFs based on other information, we can use Grübler's Formula: <br>Grübler's Formula 
Formula to compute the number of DOFs of a (rigid) robot

<br> - number of bodies, including ground
<br> - number of joints
<br> - constant,  for spatial,  for planar bodies
<br> - number of DOF for joint 

<br><br><br><img alt="center" src="lib/media/pasted-image-20240229163450.png">]]></description><link>the-guide/robotics,-dynamics-and-control/generalized-coordinates,-configuration-or-joint-space.html</link><guid isPermaLink="false">The Guide/Robotics, Dynamics and Control/Generalized Coordinates, Configuration or Joint Space.md</guid><pubDate>Wed, 23 Apr 2025 08:34:38 GMT</pubDate><enclosure url="lib/media/pasted-image-20240229163450.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src=&quot;lib/media/pasted-image-20240229163450.png&quot;&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[Less Weird Quaternions - A Geometric Algebra Perspective]]></title><description><![CDATA[ 
 <br>In a Nutshell
<a data-tooltip-position="top" aria-label="Unit Quaternions and SU(2)" data-href="Unit Quaternions and SU(2)" href="the-guide/mathematics/lie-theory/unit-quaternions-and-su(2).html" class="internal-link" target="_self" rel="noopener nofollow">Unit quaternions</a>, a special case of general <a data-tooltip-position="top" aria-label="Quaternions" data-href="Quaternions" href="the-guide/mathematics/general-stuff/quaternions.html" class="internal-link" target="_self" rel="noopener nofollow">quaternions</a> can be used to <a data-tooltip-position="top" aria-label="Group Representation" data-href="Group Representation" href="the-guide/mathematics/group-theory/group-representation.html" class="internal-link" target="_self" rel="noopener nofollow">represent</a> <a data-tooltip-position="top" aria-label="The Lie Group SO(n) and its Lie Algebra" data-href="The Lie Group SO(n) and its Lie Algebra" href="the-guide/mathematics/lie-theory/special-orthogonal-group-so(3)/the-lie-group-so(n)-and-its-lie-algebra.html" class="internal-link" target="_self" rel="noopener nofollow">rotations</a> and offers advantages such as the ability to safely interpolate between them. However, they are very hard to interpret. <a data-href="Geometric Algebra" href="the-guide/mathematics/geometric-algebra/geometric-algebra.html" class="internal-link" target="_self" rel="noopener nofollow">Geometric Algebra</a> offers a fascinating perspective based on reflections in -dimensional space. For an overview, see <a data-href="A Hitchhiker's Guide to Rotation Representations" href="the-guide/robotics,-dynamics-and-control/a-hitchhiker's-guide-to-rotation-representations.html" class="internal-link" target="_self" rel="noopener nofollow">A Hitchhiker's Guide to Rotation Representations</a>.
<br><br>We know from basic <a data-href="Geometric Algebra" href="the-guide/mathematics/geometric-algebra/geometric-algebra.html" class="internal-link" target="_self" rel="noopener nofollow">Geometric Algebra</a> that we can describe the -dimensional euclidean space  via the Clifford <a data-href="Algebra" href="the-guide/mathematics/general-stuff/algebra.html" class="internal-link" target="_self" rel="noopener nofollow">Algebra</a> with signature , leading to multivectors with<br>
<br>
1 scalar

<br>
3 vector

<br>
3 bivector

<br>
1 <a data-tooltip-position="top" aria-label="Pseudoscalar" data-href="Pseudoscalar" href="the-guide/mathematics/geometric-algebra/pseudoscalar.html" class="internal-link" target="_self" rel="noopener nofollow">pseudoscalar</a><br>
components. We also know using the <a data-tooltip-position="top" aria-label="Geometric Product" data-href="Geometric Product" href="the-guide/mathematics/geometric-algebra/operations/geometric-product.html" class="internal-link" target="_self" rel="noopener nofollow">geometric product</a> to multiply a bivector to a vector, we obtain a rotation by  in the plane described by that bivector, e.g.Because a single transformation is an orthogonal transformation in an at most -dimensional <a data-tooltip-position="top" aria-label="Space" data-href="Space" href="the-guide/mathematics/general-stuff/space.html" class="internal-link" target="_self" rel="noopener nofollow">space</a>, <a data-href="Cartan-Dieudonne Theorem" href="the-guide/mathematics/geometric-algebra/cartan-dieudonne-theorem.html" class="internal-link" target="_self" rel="noopener nofollow">Cartan-Dieudonne Theorem</a> tells us that we can describe it with at most  reflections (which is precisely where the double cover is introduced, because signs cancel out). In geometric algebra, this leads to the sandwich product.

<br>
How do we get from the  interpretation to the sandwich product ?

<br>Multiplying with a quaternion i smultiplying with a scalar (stretch) and a bivector (rotate)
<br>rotating a vector in the plane it lies on only needs 1 quaternion
<br>We need the sandwich product to cancel out the effect on the part that i not in the plane. To have the correct angles, we instead rotate twice by half the angle


<br>
Double Cover

<br>When interpolating along a rotation by , we need additional information on how to reach the point since there are multiple ways 
<br>


<br>
Keeping a rotation smaller than  to avoid the sign flip

<br>Add in more information and instead do 2 rotations
<br>We preserve information we don't need for e.g. rotation by , but when we interpolate we need it


]]></description><link>the-guide/robotics,-dynamics-and-control/less-weird-quaternions-a-geometric-algebra-perspective.html</link><guid isPermaLink="false">The Guide/Robotics, Dynamics and Control/Less Weird Quaternions - A Geometric Algebra Perspective.md</guid><pubDate>Sun, 30 Mar 2025 15:12:02 GMT</pubDate></item><item><title><![CDATA[Manipulability Measure]]></title><description><![CDATA[ 
 <br>We want to represent the attitude of a manipulator to arbitrary changes of position and orientation of the end-effector. Here, we consider that the joint velocities have unit normThis can also be seen as trying to get the feasible operational space velocities given available joint velocities in a given posture. <br><br><br><img alt="center" src="lib/media/pasted-image-20231107195429.png" style="width: 250px; max-width: 100%;"><br>The direction of the principal axes of the ellipsoid are determined by the <a data-tooltip-position="top" aria-label="Eigenvalue Problem" data-href="Eigenvalue Problem" href="the-guide/mathematics/linear-algebra/eigenvalue-problem.html" class="internal-link" target="_self" rel="noopener nofollow">eigenvectors</a>  of the <a data-tooltip-position="top" aria-label="Matrices" data-href="Matrices" href="the-guide/mathematics/linear-algebra/matrices.html" class="internal-link" target="_self" rel="noopener nofollow">matrix</a>  while the dimensions of the axes are given by the <a data-tooltip-position="top" aria-label="Singular Value Decomposition" data-href="Singular Value Decomposition" href="the-guide/mathematics/linear-algebra/singular-value-decomposition.html" class="internal-link" target="_self" rel="noopener nofollow">singular values</a> of .<br>Definition
The manipulability measure is given by which is proportional to the volume of the ellipsoids above. For non-<a data-tooltip-position="top" aria-label="Redundancy in Robotics" data-href="Redundancy in Robotics" href="the-guide/robotics,-dynamics-and-control/redundancy-in-robotics.html" class="internal-link" target="_self" rel="noopener nofollow">redudant</a> robots, this reduces to .
<br>
<br>Non-negative, vanishes at <a data-tooltip-position="top" aria-label="Robot Singularities" data-href="Robot Singularities" href="the-guide/robotics,-dynamics-and-control/kinematics/robot-singularities.html" class="internal-link" target="_self" rel="noopener nofollow">singularities</a>
]]></description><link>the-guide/robotics,-dynamics-and-control/manipulability-measure.html</link><guid isPermaLink="false">The Guide/Robotics, Dynamics and Control/Manipulability Measure.md</guid><pubDate>Tue, 25 Feb 2025 18:53:42 GMT</pubDate><enclosure url="lib/media/pasted-image-20231107195429.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src=&quot;lib/media/pasted-image-20231107195429.png&quot;&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[Pfaffian Constraints]]></title><description><![CDATA[ 
 <br>Definition
Special form of <a data-tooltip-position="top" aria-label="Kinematics" data-href="Kinematics" href="the-guide/robotics,-dynamics-and-control/kinematics/kinematics.html" class="internal-link" target="_self" rel="noopener nofollow">velocity constraints</a> that can be linearized w.r.t.  of the configurationwith <a data-tooltip-position="top" aria-label="Matrices" data-href="Matrices" href="the-guide/mathematics/linear-algebra/matrices.html" class="internal-link" target="_self" rel="noopener nofollow">matrix</a>  representing an -dimensional <a data-tooltip-position="top" aria-label="Generalized Coordinates, Configuration or Joint Space" data-href="Generalized Coordinates, Configuration or Joint Space" href="the-guide/robotics,-dynamics-and-control/generalized-coordinates,-configuration-or-joint-space.html" class="internal-link" target="_self" rel="noopener nofollow">C-space</a> and  constraints.
<br><br>Holonomic Constraints
Constraints that only restrict the configuration (not the velocities) and that are integrable, e.g. assuming that , one can integrate and reacheliminating some of the dynamical variables.
<br>
<br>Allows to convert velocity constraints to equivalent configurational constraints
<br>Reduce dimensions, as 
<br> holonomic constraints imply the existence of  <a data-tooltip-position="top" aria-label="Kinematics" data-href="Kinematics" href="the-guide/robotics,-dynamics-and-control/kinematics/kinematics.html" class="internal-link" target="_self" rel="noopener nofollow">kinematic constraints</a>.
<br>All holonomic constraints are Pfaffian, but not all Pfaffian constraints are holonomic
<br>Examples 

<br>Velocity constraints can be integrated out when ...

<br>... rolling a cylinder . The orientation is uniquely defined by its position (no slip). We can therefore eliminate either the location or the rotation angle, because we can always recover the other.
<br>For a pendulum, we can also describe the position solely based on the angle.



<br>Non-Holonomic Constraints
<a data-tooltip-position="top" aria-label="Kinematics" data-href="Kinematics" href="the-guide/robotics,-dynamics-and-control/kinematics/kinematics.html" class="internal-link" target="_self" rel="noopener nofollow">Velocity constraints</a> that are not integrable to comparable configurational constraints
<br>
<br>Examples

<br>Rolling a sphere in the -plane, encoding the state as two rolling angles. There are many different possibilities to reach a position by manipulating the two angels describing the spheres rotation (no slip). Given this non-unique relation, we would need to know the rolling history in addition. (Matrix multiplication (rotations) do not commute)


]]></description><link>the-guide/robotics,-dynamics-and-control/pfaffian-constraints.html</link><guid isPermaLink="false">The Guide/Robotics, Dynamics and Control/Pfaffian Constraints.md</guid><pubDate>Thu, 10 Apr 2025 21:17:20 GMT</pubDate></item><item><title><![CDATA[Redundancy in Robotics]]></title><description><![CDATA[ 
 <br>Intrinsic Redundancy
A manipulator is intrinisically redundant, if the dimension of the operational space is smaller than the dimension of the joint space
<br>Kinematic Redundancy
A manipulator is <a data-tooltip-position="top" aria-label="Kinematics" data-href="Kinematics" href="the-guide/robotics,-dynamics-and-control/kinematics/kinematics.html" class="internal-link" target="_self" rel="noopener nofollow">kinematically</a> redundant, when the number of <a data-tooltip-position="top" aria-label="Generalized Coordinates, Configuration or Joint Space" data-href="Generalized Coordinates, Configuration or Joint Space" href="the-guide/robotics,-dynamics-and-control/generalized-coordinates,-configuration-or-joint-space.html" class="internal-link" target="_self" rel="noopener nofollow">DoFs</a>  is greater than the variables  necessary to describe the task. It is therefore relative to a specific task. 
<br>
<br>Note that even if the task space is of dimension , we may only be concerned with  components to describe the task, e.g. moving in a plane in . This is sometimes denoted functional redundancy.
<br> redundant DoFs
<br>e.g. more than 3-dimensional to follow unconstrained path in 
<br><br><br>Based on the <a data-tooltip-position="top" aria-label="Kinematics" data-href="Kinematics" href="the-guide/robotics,-dynamics-and-control/kinematics/kinematics.html" class="internal-link" target="_self" rel="noopener nofollow">differential kinematics</a> equation , we can analyse<br>
<br>Range Space, subspace of end-effector velocities that can be generated by joint velocities
<br>Null Space - subspace of joint velocities that do not produce any end-effector velocity
<br>The <a data-tooltip-position="top" aria-label="Derivative, Gradient, Jacobian and Hessian" data-href="Derivative, Gradient, Jacobian and Hessian" href="the-guide/mathematics/analysis-and-calculus/derivative,-gradient,-jacobian-and-hessian.html" class="internal-link" target="_self" rel="noopener nofollow">Jacobian</a> has full rank, if  (all end-effector velocities in -dimensional operation space are reachable) and . At all times, the relation holds.]]></description><link>the-guide/robotics,-dynamics-and-control/redundancy-in-robotics.html</link><guid isPermaLink="false">The Guide/Robotics, Dynamics and Control/Redundancy in Robotics.md</guid><pubDate>Tue, 11 Mar 2025 15:43:32 GMT</pubDate></item><item><title><![CDATA[Robot Grasping]]></title><description><![CDATA[ 
 <br>Grasping - Restraining an object’s motion through application of forces and torques at a set of contact points.<br>Pipeline <br>
<br>Grasp Generation, parameterized via 

<br>Approach vector
<br>Wrist orientation of hand
<br>Initial finger configuration
<br>Points of contact
<br>High dimensional search problem 


<br>Grasp Aquisition, execution of grasp
<br>Grasp Maintenance, maintaining the grasp under external forces
<br>Grasp Evaluation, objective <a data-tooltip-position="top" aria-label="Mathematical Relations" data-href="Mathematical Relations" href="the-guide/mathematics/general-stuff/mathematical-relations.html" class="internal-link" target="_self" rel="noopener nofollow">function</a> for reacting online

<br>Maintainability - prevent contact separation, sliding
<br>Closure - grasp can be maintained under disturbances


<br><br><br><img alt="center" src="lib/media/pasted-image-20231221111517.png" style="width: 300px; max-width: 100%;"><br>
<br>Contact Models 

<br>Frictionless Point Contact 
<br>Point Contact with Friction

<br>Yields so-called Friction Cone (dark red) of admissable forces without slipping. 


<br>Soft-Finger Contact Model

<br>Torsional friction coefficient 




<br>Wrench
A <a data-tooltip-position="top" aria-label="Static and Dynamic Systems" data-href="Static and Dynamic Systems" href="the-guide/robotics,-dynamics-and-control/dynamics/static-and-dynamic-systems.html" class="internal-link" target="_self" rel="noopener nofollow">wrench</a> is a -dimensional vector of forces and torques applied at contact point  with index  Here,  is generally understood as a linear combination of edge forces of an approximated (by -vertex polygon) friction cone .
<br>Grasp
A grasp or total wrench is the combination of all wrenches that act on an object through the  given contact points

<br> is <a data-tooltip-position="top" aria-label="Matrices" data-href="Matrices" href="the-guide/mathematics/linear-algebra/matrices.html" class="internal-link" target="_self" rel="noopener nofollow">matrix</a> transforms local coordinates with  representing tangent forces and  normal forces to wrench, depends on geometry because of 
<br>G is grasp <a data-tooltip-position="top" aria-label="Mathematical Relations" data-href="Mathematical Relations" href="the-guide/mathematics/general-stuff/mathematical-relations.html" class="internal-link" target="_self" rel="noopener nofollow">map</a>, mapping all applied forces to a grasp

<br>Grasp Wrench Space
The Grasp Wrench Space is the <a data-tooltip-position="top" aria-label="Set" data-href="Set" href="the-guide/mathematics/general-stuff/set.html" class="internal-link" target="_self" rel="noopener nofollow">set</a> of all possible wrenches that can be applied to an object via  given contact points results in  <a data-tooltip-position="top" aria-label="Space" data-href="Space" href="the-guide/mathematics/general-stuff/space.html" class="internal-link" target="_self" rel="noopener nofollow">space</a> for  objects and  <a data-tooltip-position="top" aria-label="Space" data-href="Space" href="the-guide/mathematics/general-stuff/space.html" class="internal-link" target="_self" rel="noopener nofollow">space</a> for  objects. In other words, the Grasp Wrench Space is the <a data-tooltip-position="top" aria-label="Set" data-href="Set" href="the-guide/mathematics/general-stuff/set.html" class="internal-link" target="_self" rel="noopener nofollow">set</a> of all possible outputs of the Grasp equation using only possible force constellations as inputs.

<br>Can be computed from the friction cones

<br><img alt="center" src="lib/media/pasted-image-20231221120158.png" style="width: 250px; max-width: 100%;"><br>Grasp Wrench Hull
Grasp Wrench Hull  is the convex hull of a given <a data-tooltip-position="top" aria-label="Set" data-href="Set" href="the-guide/mathematics/general-stuff/set.html" class="internal-link" target="_self" rel="noopener nofollow">subset</a> of wrenches or forces , yields a subset of the Grasp Wrench Space
<br><img alt="center" src="lib/media/pasted-image-20231221120621.png" style="width: 250px; max-width: 100%;"><br><br><br>
<br>Form Closure -  locked joints, object not movable under wrenches
<br>Force Closure - friction helps balancing, grasp can be maintained under external forces, fewer contacts needed than for form closure
<br>Grasp Closure
A grasp is a force closure, if for any external wrench , there exist contact forces , such that meaning that every external force can be compensated by changing the wrenches at the contact points.
<br>
<br>Metrics for Grasp Quality

<br>Stability can be derived from the origin being contained in the Wrench Hull
<br>Volume of Grasp Wrench Hull 
<br>Radius  of largest ball around origin that is contained by Grasp Wrench Hull<img alt="center" src="lib/media/pasted-image-20231221162438.png" style="width: 250px; max-width: 100%;">

<br>Worst case scenario, as this is restricted by most sensitive direction

<br>In this case the sum magnitude of contact wrenches would need to be  times the disturbance wrench


<br>Varies between 0 and 1 due to normalization




<br><br><br>In order to have obtain the forces that are needed for the desired Grasp Wrench Hull you have to formalize an optimization problem. There are several constraints that need to be incorporated<br>
<br>Constraints given by Friction Cones

<br>The friction cones introduced above result in a constraint for each of the  contact points  that are denoted 


<br>Equilibrium Constraints for the Object

<br>The forces have to be applied in a way to counteract external forces. The force vector at each contact point  is given in the local frame with  representing tangent forces and  normal forces. The transformation  maps this into the objects global frame. The resulting constraints are  for the forces and for torques. Using , a contact force vector  and the grasp <a data-tooltip-position="top" aria-label="Matrices" data-href="Matrices" href="the-guide/mathematics/linear-algebra/matrices.html" class="internal-link" target="_self" rel="noopener nofollow">matrix</a>  that can be assembled from contact matrices these constraints can be compactly written as 


<br>Hardware Constraints 
Convex Optimization Problem for Grasp Force
The resulting <a data-tooltip-position="top" aria-label="Convexity" data-href="Convexity" href="the-guide/mathematics/optimization/convex-optimization-lecture/convexity.html" class="internal-link" target="_self" rel="noopener nofollow">convex</a> optimization problem then reads 


]]></description><link>the-guide/robotics,-dynamics-and-control/robot-grasping.html</link><guid isPermaLink="false">The Guide/Robotics, Dynamics and Control/Robot Grasping.md</guid><pubDate>Thu, 17 Apr 2025 10:57:03 GMT</pubDate><enclosure url="lib/media/pasted-image-20231221111517.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src=&quot;lib/media/pasted-image-20231221111517.png&quot;&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[Robotic Joints]]></title><description><![CDATA[ 
 <br>TODO<br><br>A prismatic joint allows linear motion (translation) along a fixed axis. The motion is characterized by a displacement , which is the distance the joint has moved along its axis.<br><br><br>A revolute joint allows rotational motion between two links around a fixed axis. The motion is characterized by an angular displacement .]]></description><link>the-guide/robotics,-dynamics-and-control/robotic-joints.html</link><guid isPermaLink="false">The Guide/Robotics, Dynamics and Control/Robotic Joints.md</guid><pubDate>Wed, 18 Dec 2024 11:32:00 GMT</pubDate></item><item><title><![CDATA[URDF - Unified Robot Description Format]]></title><description><![CDATA[ 
 <br>In a Nutshell
XML format used for representing a robot model, e.g. in <a data-tooltip-position="top" aria-label="ROS - Fundamentals" data-href="ROS - Fundamentals" href="life,-universe-and-everything/ros/ros-fundamentals.html" class="internal-link" target="_self" rel="noopener nofollow">ROS</a>. It describes the robot’s physical configuration, including links, joints, and properties.
<br><br><br>
<br>name: Specifies the name of the link, joint, or robot.
<br>type: Defines the joint type (e.g., revolute, prismatic, fixed).
<br>xyz, rpy: Defines the position (translation) and orientation (rotation) of elements in 3D space.
<br>effort, velocity, lower, upper: Define physical limits of joints such as effort, velocity range, and motion bounds.
<br><br><br>
<br>
&lt;robot&gt;<br>
The robot element that contains the entire robot model.
&lt;robot name="example_robot"&gt;
    &lt;!-- robot description here --&gt;
&lt;/robot&gt;


<br>
&lt;link&gt;<br>
Represents a rigid body or a part of the robot.
&lt;link name="base_link"&gt;
    &lt;visual&gt;
        &lt;geometry&gt;
            &lt;box size="1 1 1"/&gt;
        &lt;/geometry&gt;
    &lt;/visual&gt;
&lt;/link&gt;


<br>
&lt;joint&gt;<br>
Defines how two links are connected, specifying their relative motion (e.g., revolute, prismatic).
&lt;joint name="base_to_arm" type="revolute"&gt;
    &lt;parent link="base_link"/&gt;
    &lt;child link="arm_link"/&gt;
    &lt;origin rpy="0 0 0" xyz="0 0 0"/&gt;
    &lt;axis xyz="0 1 0"/&gt;
    &lt;limit effort="100" velocity="1.0" lower="0" upper="3.14"/&gt;
&lt;/joint&gt;


<br>
&lt;material&gt;<br>
Defines the color or texture of a link’s visual representation.
&lt;material name="red"&gt;
    &lt;color rgba="1 0 0 1"/&gt;
&lt;/material&gt;


<br>
&lt;sensor&gt;<br>
Specifies sensors attached to a link, such as cameras or lasers.
&lt;sensor name="camera" type="camera"&gt;
    &lt;origin xyz="0 0 1" rpy="0 0 0"/&gt;
    &lt;camera&gt;
        &lt;horizontal_fov&gt;1.5707&lt;/horizontal_fov&gt;
        &lt;image width="640" height="480"/&gt;
    &lt;/camera&gt;
&lt;/sensor&gt;


<br>
&lt;transmission&gt;<br>
Describes how a joint is actuated, often for motors or gearboxes.
&lt;transmission type="hardware_interface/PositionJointInterface"&gt;     
	&lt;joint name="arm_joint"/&gt;     
	&lt;actuator name="arm_motor"&gt;
		&lt;mechanicalReduction&gt;1.0&lt;/mechanicalReduction&gt;     
	&lt;/actuator&gt; 
&lt;/transmission&gt;


<br><br><br>&lt;robot name="simple_robot"&gt;
    &lt;!-- Base Link --&gt;
    &lt;link name="base_link"&gt;
        &lt;visual&gt;
            &lt;geometry&gt;
                &lt;box size="1 1 0.1"/&gt;
            &lt;/geometry&gt;
            &lt;material name="grey"/&gt;
        &lt;/visual&gt;
    &lt;/link&gt;

    &lt;!-- Arm Link --&gt;
    &lt;link name="arm_link"&gt;
        &lt;visual&gt;
            &lt;geometry&gt;
                &lt;cylinder radius="0.05" length="1.0"/&gt;
            &lt;/geometry&gt;
            &lt;material name="blue"/&gt;
        &lt;/visual&gt;
    &lt;/link&gt;

    &lt;!-- Revolute Joint --&gt;
    &lt;joint name="base_to_arm" type="revolute"&gt;
        &lt;parent link="base_link"/&gt;
        &lt;child link="arm_link"/&gt;
        &lt;origin rpy="0 0 0" xyz="0 0 0"/&gt;
        &lt;axis xyz="0 0 1"/&gt;
        &lt;limit effort="100" velocity="1.0" lower="-1.57" upper="1.57"/&gt;
    &lt;/joint&gt;
&lt;/robot&gt;

]]></description><link>the-guide/robotics,-dynamics-and-control/urdf-unified-robot-description-format.html</link><guid isPermaLink="false">The Guide/Robotics, Dynamics and Control/URDF - Unified Robot Description Format.md</guid><pubDate>Wed, 04 Dec 2024 10:43:46 GMT</pubDate></item><item><title><![CDATA[- My Obsidian Conventions -]]></title><description><![CDATA[<a class="tag" href="?query=tag:Notation" style="background-color: rgb(4, 108, 116); color: white; font-weight: 700; border: none; border-radius: 1em; padding: 0.2em 0.5em;">#Notation</a> <a class="tag" href="?query=tag:Paper" style="background-color: rgb(4, 108, 116); color: white; font-weight: 700; border: none; border-radius: 1em; padding: 0.2em 0.5em;">#Paper</a> 
 <br><br>Don't overdo it with Papers
Research Papers for context of a single paper, if something was introduced among several works, just use a regular note for that concept.
<br>Avoid Listing Examples
Apart from e.g. commands avoid lists of things you simply have to look up, e.g. integrals or derivatives. Instead add good resources and concentrate on the concepts with 2 or 3 good examples.
<br>Conventions and Overviews
Notes functioning as an entry-point to a vast subject or as a convention overview are named "- &lt;...&gt; -".
<br>Notation
Stick to unified notation as much as possible. Group together related subjects and make a <a href=".?query=tag:Notation" class="tag" target="_blank" rel="noopener nofollow">#Notation</a> note. 

<br><a data-href="- Notation - General -" href="notation-world/-notation-general-.html" class="internal-link" target="_self" rel="noopener nofollow">- Notation - General -</a>
<br><a data-href="- Notation - Geometry, Group and Lie Theory -" href="notation-world/-notation-geometry,-group-and-lie-theory-.html" class="internal-link" target="_self" rel="noopener nofollow">- Notation - Geometry, Group and Lie Theory -</a>
<br><a data-href="- Notation - Machine Learning -" href="notation-world/-notation-machine-learning-.html" class="internal-link" target="_self" rel="noopener nofollow">- Notation - Machine Learning -</a>
<br><a data-href="- Notation - ODEs, Robotics, Dynamics and Control -" href="notation-world/-notation-odes,-robotics,-dynamics-and-control-.html" class="internal-link" target="_self" rel="noopener nofollow">- Notation - ODEs, Robotics, Dynamics and Control -</a>
<br><a data-href="- Notation - Probability, Statistics and related Fields -" href="notation-world/-notation-probability,-statistics-and-related-fields-.html" class="internal-link" target="_self" rel="noopener nofollow">- Notation - Probability, Statistics and related Fields -</a>
<br><a data-href="- Notation - Integral Transforms, Signals and Communication -" href="notation-world/-notation-integral-transforms,-signals-and-communication-.html" class="internal-link" target="_self" rel="noopener nofollow">- Notation - Integral Transforms, Signals and Communication -</a>
<br><a data-href="- Notation - Data Assimilation - A Mathematical Perspective -" href="notation-world/-notation-data-assimilation-a-mathematical-perspective-.html" class="internal-link" target="_self" rel="noopener nofollow">- Notation - Data Assimilation - A Mathematical Perspective -</a>

<br>Units
In math mode, use  for units.
<br><br>
<br>Context for Daily Notes and Research Papers to know why I looked into that particular paper
<br>For Big Projects, e.g. Thesis add Importance (Central, good to know, ...)
<br><br><br>
<br>Function Plot  supports <a rel="noopener nofollow" class="external-link" href="https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Global_Objects/Math" target="_blank">https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Global_Objects/Math</a>
<br>
<br><br><br>
<br>
!quote for in a nutshell
In a Nutshell
Quick overview in own words what a note / its core concept is about, relation to more general topics


<br>
!info for definitions
Definitions
Definitions or methods etc. , core idea / concepts


<br>
!note for heuristics, general notes or tips and tricks
Notes
General remark, experience, tips and tricks


<br>
!success for theorems etc.
Theorems / Lemmas / Corollarys
Important  (rigorous) mathematical results 


<br>
!proof for proofs
Proof
For proofs in math lectures that are important, but not central


<br>
!brainwaves for additional information, intuition and important things for understanding
Additional Information / Intuition
For informaiton not central to the topic at hand,but important for inutition or deriving concepts etc.


<br>
!warning for Assumptions or Warnings
Assumptions
Assumptions in settings that have to be considered for applications etc.


<br>
!abstract for algorithms
Algorithm
Pseudocode where no snapshot is available


<br>
!danger for disadvantages or problems
Disadvantage / Problems
Quickly summarize problematic behaviors, disadvantages to other methods etc.


<br>
!adv for advantages
Advantages
Main advantages of method / theory etc


<br>
!idea for ideas to extend <a href=".?query=tag:Paper" class="tag" target="_blank" rel="noopener nofollow">#Paper</a> 
Idea / Extensions
....


]]></description><link>-my-obsidian-conventions-.html</link><guid isPermaLink="false">- My Obsidian Conventions -.md</guid><pubDate>Wed, 23 Apr 2025 07:56:01 GMT</pubDate></item></channel></rss>